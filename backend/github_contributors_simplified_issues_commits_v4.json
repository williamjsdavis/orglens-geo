{
  "contributors": [
    {
      "id": 19390,
      "username": "ashwinb",
      "url": "https://github.com/ashwinb",
      "avatar_url": "https://avatars.githubusercontent.com/u/19390?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/llama-models/issues/311",
              "number": 311,
              "title": "L2Norm in Llama4",
              "body": "Hey there,\n\nYour L2Norm implementation in Llama4 (https://github.com/meta-llama/llama-models/blob/main/models/llama4/model.py#L44) is actually RMSNorm without scaling parameters. Is this a bug?",
              "labels": [],
              "comments": 1,
              "state_reason": "completed"
            }
          ],
          "commits": [
            {
              "sha": "823cd8622e44d90b8e989e9f41ca364c06f5701d",
              "url": "https://github.com/meta-llama/llama-models/commit/823cd8622e44d90b8e989e9f41ca364c06f5701d",
              "message": "fix: re-add python_start, python_end tokens (#327)",
              "files_changed": [
                {
                  "filename": "models/llama4/tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/tokenizer.py ---\n@@ -57,8 +57,8 @@ def get_reserved_special_tokens(name, count, start_index=0):\n     \"<|text_post_train_reserved_special_token_3|>\",\n     \"<|text_post_train_reserved_special_token_4|>\",\n     \"<|text_post_train_reserved_special_token_5|>\",\n-    \"<|text_post_train_reserved_special_token_6|>\",\n-    \"<|text_post_train_reserved_special_token_7|>\",\n+    \"<|python_start|>\",\n+    \"<|python_end|>\",\n     \"<|finetune_right_pad|>\",\n ] + get_reserved_special_tokens(\n     \"text_post_train\", 61, 8"
            },
            {
              "sha": "408c577168a187941f9359c916004fa3b21fbc3b",
              "url": "https://github.com/meta-llama/llama-models/commit/408c577168a187941f9359c916004fa3b21fbc3b",
              "message": "fix: update rope scaling for Llama-4-Scout (#322)",
              "files_changed": [
                {
                  "filename": "models/llama4/args.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/model.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/args.py ---\n@@ -71,6 +71,9 @@ class ModelArgs(BaseModel):\n     attention_chunk_size: Optional[int] = None\n     rope_theta: float = 500000\n     use_scaled_rope: bool = False\n+    rope_scaling_factor: Optional[float] = None\n+    rope_high_freq_factor: Optional[float] = None\n+\n     nope_layer_interval: Optional[int] = None  # No position encoding in every n layers\n     use_qk_norm: bool = False\n     # Set to True to enable inference-time temperature tuning (useful for very long context)\n@@ -93,4 +96,14 @@ def validate(self) -> \"ModelArgs\":\n             f\"n_heads ({self.n_heads}) must be divisible by n_kv_heads ({self.n_kv_heads})\"\n         )\n         assert self.dim % self.n_heads == 0, f\"dim ({self.dim}) must be divisible by n_heads ({self.n_heads})\"\n+\n+        if self.use_scaled_rope:\n+            # NOTE: ideally these values should have come from params.json. However, we have\n+            # shipped the models everywhere. Only Llama-4-Scout uses scaled rope and needs these\n+            # specific values.\n+            if self.rope_scaling_factor is None:\n+                self.rope_scaling_factor = 16\n+            if self.rope_high_freq_factor is None:\n+                self.rope_high_freq_factor = 1\n+\n         return self\n\n--- File: models/llama4/model.py ---\n@@ -41,11 +41,8 @@ def forward(self, x):\n         return rmsnorm(x, self.eps) * self.weight\n \n \n-def apply_scaling(freqs: torch.Tensor):\n-    # Values obtained from grid search\n-    scale_factor = 8\n+def apply_scaling(freqs: torch.Tensor, scale_factor: float, high_freq_factor: float):\n     low_freq_factor = 1\n-    high_freq_factor = 4\n     old_context_len = 8192  # original llama3 length\n \n     low_freq_wavelen = old_context_len / low_freq_factor\n@@ -64,11 +61,18 @@ def apply_scaling(freqs: torch.Tensor):\n     return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n \n \n-def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False):\n+def precompute_freqs_cis(\n+    dim: int,\n+    end: int,\n+    theta: float,\n+    use_scaled: bool,\n+    scale_factor: float,\n+    high_freq_factor: float,\n+):\n     freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n     t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n     if use_scaled:\n-        freqs = apply_scaling(freqs)\n+        freqs = apply_scaling(freqs, scale_factor, high_freq_factor)\n     freqs = torch.outer(t, freqs)\n     freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n     return freqs_cis\n@@ -352,6 +356,8 @@ def __init__(self, args: ModelArgs, **kwargs) -> None:\n             args.max_seq_len * 2,\n             args.rope_theta,\n             args.use_scaled_rope,\n+            args.rope_scaling_factor,\n+            args.rope_high_freq_factor,\n         )\n         vision_args = self.args.vision_args\n         if vision_args:"
            },
            {
              "sha": "78eb422f717d4de4707cebf520f0f8224537e4d3",
              "url": "https://github.com/meta-llama/llama-models/commit/78eb422f717d4de4707cebf520f0f8224537e4d3",
              "message": "fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320)",
              "files_changed": [
                {
                  "filename": "models/llama4/model.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/model.py ---\n@@ -24,30 +24,21 @@\n from .moe import MoE\n \n \n-class RMSNorm(torch.nn.Module):\n-    def __init__(self, dim: int, eps: float = 1e-6):\n-        super().__init__()\n-        self.eps = eps\n-        self.weight = nn.Parameter(torch.ones(dim))\n+def rmsnorm(x, eps):\n+    def _norm(y):\n+        return y * torch.rsqrt(y.pow(2).mean(-1, keepdim=True) + eps)\n \n-    def _norm(self, x):\n-        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+    return _norm(x.float()).type_as(x)\n \n-    def forward(self, x):\n-        output = self._norm(x.float()).type_as(x)\n-        return output * self.weight\n \n-\n-class L2Norm(torch.nn.Module):\n+class RMSNorm(torch.nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n         self.eps = eps\n-\n-    def _norm(self, x):\n-        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+        self.weight = nn.Parameter(torch.ones(dim))\n \n     def forward(self, x):\n-        return self._norm(x.float()).type_as(x)\n+        return rmsnorm(x, self.eps) * self.weight\n \n \n def apply_scaling(freqs: torch.Tensor):\n@@ -175,9 +166,7 @@ def __init__(\n                 self.head_dim,\n             )\n         ).cuda()\n-        self.qk_norm = None\n-        if self.use_qk_norm:\n-            self.qk_norm = L2Norm(args.norm_eps)\n+        self.norm_eps = args.norm_eps\n         self._register_load_state_dict_pre_hook(self.load_hook)\n \n     def load_hook(\n@@ -221,8 +210,8 @@ def forward(\n             xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n \n         if self.use_qk_norm:\n-            xq = self.qk_norm(xq)\n-            xk = self.qk_norm(xk)\n+            xq = rmsnorm(xq, self.norm_eps)\n+            xk = rmsnorm(xk, self.norm_eps)\n \n         # We are applying temperature tuning (https://arxiv.org/abs/2501.19399) to NoPE layers, where\n         # the inference-time temperature tuning function is customized to not affect short context"
            },
            {
              "sha": "f62cac6d2d145033df6016901aca53548193ead3",
              "url": "https://github.com/meta-llama/llama-models/commit/f62cac6d2d145033df6016901aca53548193ead3",
              "message": "fix: ensure finetune_right_pad stays at the same position before the hiding of python_start, python_end",
              "files_changed": [
                {
                  "filename": "models/llama4/tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/tokenizer.py ---\n@@ -57,10 +57,12 @@ def get_reserved_special_tokens(name, count, start_index=0):\n     \"<|text_post_train_reserved_special_token_3|>\",\n     \"<|text_post_train_reserved_special_token_4|>\",\n     \"<|text_post_train_reserved_special_token_5|>\",\n+    \"<|text_post_train_reserved_special_token_6|>\",\n+    \"<|text_post_train_reserved_special_token_7|>\",\n     \"<|finetune_right_pad|>\",\n ] + get_reserved_special_tokens(\n-    \"text_post_train\", 61, 6\n-)  # <|text_post_train_reserved_special_token_6|>, ..., <|text_post_train_reserved_special_token_66|>\n+    \"text_post_train\", 61, 8\n+)  # <|text_post_train_reserved_special_token_8|>, ..., <|text_post_train_reserved_special_token_68|>\n \n # 200080, ..., 201133\n LLAMA4_VISION_SPECIAL_TOKENS = ["
            },
            {
              "sha": "63172b319ee6412d165095cc5a8f0fde12288ec0",
              "url": "https://github.com/meta-llama/llama-models/commit/63172b319ee6412d165095cc5a8f0fde12288ec0",
              "message": "fix: a couple minor bugs",
              "files_changed": [
                {
                  "filename": "models/llama4/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/quantization/loader.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/generation.py ---\n@@ -136,7 +136,7 @@ def generate(\n         if print_model_input:\n             cprint(\"Input to model:\\n\", \"yellow\")\n             for inp in llm_inputs:\n-                cprint(self.tokenizer.decode(inp.tokens.tolist()), \"grey\")\n+                cprint(self.tokenizer.decode(inp.tokens), \"grey\")\n         prompt_tokens = [inp.tokens for inp in llm_inputs]\n \n         bsz = len(llm_inputs)\n\n--- File: models/llama4/quantization/loader.py ---\n@@ -92,7 +92,7 @@ def apply_quantization(key, weight):\n             log_status(f\"Rank {rank}: Quantizing int4 weights from bf16\")\n \n             def apply_quantization(_, weight):\n-                return quantize_int4(weight, output_device=torch.device(\"cuda\"))\n+                return quantize_int4(weight, fp8_activation_scale_ub, output_device=torch.device(\"cuda\"))\n \n     else:\n         fp8_scales_path = os.path.join(checkpoint_dir, f\"fp8_scales_{rank}.pt\")"
            },
            {
              "sha": "88e9f6aadc0067cb7a4724cc5e46eca384c11a91",
              "url": "https://github.com/meta-llama/llama-models/commit/88e9f6aadc0067cb7a4724cc5e46eca384c11a91",
              "message": "fix: iron out differences between llama-models and llama-stack",
              "files_changed": [
                {
                  "filename": "models/checkpoint.py",
                  "status": "modified"
                },
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/multimodal/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/checkpoint.py ---\n@@ -49,12 +49,12 @@ def maybe_reshard_state_dict(\n     old_mp_size = len(ckpt_paths)\n     old_mp_ranks = map_mp_rank(old_mp_size, new_mp_size, new_mp_rank)\n \n-    print(f\"Loading checkpoint shards:\\n{str(ckpt_paths[old_mp_ranks])}\")\n-    paths = ckpt_paths[old_mp_ranks]\n+    print(f\"Loading checkpoint shards:\\n{str(ckpt_paths[old_mp_ranks])}\")  # type: ignore\n+    paths = ckpt_paths[old_mp_ranks]  # type: ignore\n     state_dicts = [torch.load(str(p), map_location=map_location, mmap=mmap) for p in paths]\n \n     if new_mp_size == old_mp_size:\n-        return state_dicts[0]\n+        return state_dicts[0]  # type: ignore\n \n     if moe_num_experts is not None:\n         state_dicts = [convert_moe_weights(d, moe_num_experts) for d in state_dicts]\n\n--- File: models/datatypes.py ---\n@@ -8,7 +8,7 @@\n import base64\n from enum import Enum\n from io import BytesIO\n-from typing import Dict, List, Literal, Optional, Union\n+from typing import Any, Dict, List, Literal, Optional, Union\n \n from pydantic import BaseModel, ConfigDict, Field, field_serializer, field_validator\n from typing_extensions import Annotated\n@@ -39,7 +39,14 @@ class BuiltinTool(Enum):\n class ToolCall(BaseModel):\n     call_id: str\n     tool_name: Union[BuiltinTool, str]\n-    arguments: Dict[str, RecursiveType]\n+    # Plan is to deprecate the Dict in favor of a JSON string\n+    # that is parsed on the client side instead of trying to manage\n+    # the recursive type here.\n+    # Making this a union so that client side can start prepping for this change.\n+    # Eventually, we will remove both the Dict and arguments_json field,\n+    # and arguments will just be a str\n+    arguments: Union[str, Dict[str, RecursiveType]]\n+    arguments_json: Optional[str] = None\n \n     @field_validator(\"tool_name\", mode=\"before\")\n     @classmethod\n@@ -83,6 +90,29 @@ class StopReason(Enum):\n     out_of_tokens = \"out_of_tokens\"\n \n \n+class ToolParamDefinition(BaseModel):\n+    param_type: str\n+    description: Optional[str] = None\n+    required: Optional[bool] = True\n+    default: Optional[Any] = None\n+\n+\n+class ToolDefinition(BaseModel):\n+    tool_name: Union[BuiltinTool, str]\n+    description: Optional[str] = None\n+    parameters: Optional[Dict[str, ToolParamDefinition]] = None\n+\n+    @field_validator(\"tool_name\", mode=\"before\")\n+    @classmethod\n+    def validate_field(cls, v):\n+        if isinstance(v, str):\n+            try:\n+                return BuiltinTool(v)\n+            except ValueError:\n+                return v\n+        return v\n+\n+\n class RawMediaItem(BaseModel):\n     type: Literal[\"image\"] = \"image\"\n     data: bytes | BytesIO\n\n--- File: models/llama3/multimodal/model.py ---\n@@ -431,7 +431,6 @@ def load_hook(\n             return state_dict\n \n     def apply_positional_embedding(self, x, ar):\n-        out = []\n         # apply regular position embedding\n         bsz, num_chunks, num_tokens, dim = x.shape\n         x = x.view(bsz * num_chunks, num_tokens, dim)\n@@ -1033,7 +1032,7 @@ def __init__(self, args: ModelArgs) -> None:\n         self.image_res = args.vision_chunk_size\n         self.max_num_chunks = args.vision_max_num_chunks\n         if return_intermediate is not None:\n-            return_intermediate = [int(l) for l in return_intermediate.split(\",\")]\n+            return_intermediate = [int(layer) for layer in return_intermediate.split(\",\")]\n             self.vision_input_dim = (len(return_intermediate) + 1) * self.vision_input_dim\n         self.patch_size = 14\n         self.vision_encoder = VisionEncoder(\n\n--- File: models/llama4/chat_format.py ---\n@@ -49,7 +49,7 @@ class TransformedImage:\n     aspect_ratio: Tuple[int, int]\n \n \n-def pil_RGBA2RGB(image: PIL_Image.Image, bg: Tuple[int, int, int] = (255, 255, 255)) -> PIL_Image.Image:\n+def convert_image_to_rgb(image: PIL_Image.Image, bg: Tuple[int, int, int] = (255, 255, 255)) -> PIL_Image.Image:\n     if image.mode == \"RGBA\":\n         image.load()  # for png.split()\n         new_img = PIL_Image.new(\"RGB\", image.size, bg)\n@@ -126,7 +126,7 @@ def _encode_image(\n             tokens += [self.tokenizer.special_tokens[\"<|image_end|>\"]]\n         else:\n             ratio_h, ratio_w = image_ar\n-            for yy in range(ratio_h):\n+            for _ in range(ratio_h):\n                 for xx in range(ratio_w):\n                     tokens += [self.tokenizer.special_tokens[\"<|patch|>\"]] * n_patches_per_chunk\n                     if xx < ratio_w - 1:\n@@ -166,7 +166,7 @@ def _process(c):\n \n                 bytes_io = io.BytesIO(c.data) if isinstance(c.data, bytes) else c.data\n                 image = PIL_Image.open(bytes_io)\n-                image = pil_RGBA2RGB(image)\n+                image = convert_image_to_rgb(image)\n                 image_tiles, ar = self.dynamic_image_transform(image, max_num_chunks=self.max_num_chunks)\n \n                 if image_tiles.shape[0] > 1:\n@@ -213,7 +213,9 @@ def _process_content(c):\n \n         eom = False\n         if message.role == \"assistant\":\n-            eom = message.stop_reason == StopReason.end_of_message\n+            eom = message.stop_reason == StopReason.end_of_message or message.tool_calls\n+        elif message.role == \"tool\":\n+            eom = True\n \n         tokens.append(self.tokenizer.special_tokens[\"<|eom|>\" if eom else \"<|eot|>\"])\n         return tokens, images\n@@ -248,6 +250,11 @@ def decode_assistant_message_from_content(self, content: str, stop_reason: StopR\n         if content.startswith(header_str):\n             content = content[len(header_str) :]\n \n+        ipython = content.startswith(\"<|python_start|>\")\n+        if ipython:\n+            content = content[len(\"<|python_start|>\") :]\n+            content = content.replace(\"<|python_end|>\", \"\")\n+\n         if content.endswith(\"<|eot|>\"):\n             content = content[: -len(\"<|eot|>\")]\n             stop_reason = StopReason.end_of_turn\n@@ -278,7 +285,11 @@ def decode_assistant_message_from_content(self, content: str, stop_reason: StopR\n                 }\n                 if tool_name in BuiltinTool.__members__:\n                     tool_name = BuiltinTool[tool_name]\n-\n+            elif ipython:\n+                tool_name = BuiltinTool.code_interpreter\n+                tool_arguments = {\n+                    \"code\": content,\n+                }\n         tool_calls = []\n         if tool_name is not None and tool_arguments is not None:\n             call_id = str(uuid.uuid4())\n\n--- File: models/llama4/generation.py ---\n@@ -5,15 +5,6 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# the root directory of this source tree.\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n-\n import codecs\n import io\n import json\n@@ -145,8 +136,7 @@ def generate(\n         if print_model_input:\n             cprint(\"Input to model:\\n\", \"yellow\")\n             for inp in llm_inputs:\n-                tokens_to_print = [t for t in inp.tokens]\n-                cprint(self.tokenizer.decode(tokens_to_print), \"grey\")\n+                cprint(self.tokenizer.decode(inp.tokens.tolist()), \"grey\")\n         prompt_tokens = [inp.tokens for inp in llm_inputs]\n \n         bsz = len(llm_inputs)\n\n--- File: models/llama4/model.py ---\n@@ -5,9 +5,6 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n-\n import math\n from typing import Any, Dict, List, Optional, Tuple\n \n\n--- File: models/llama4/tokenizer.py ---\n@@ -5,9 +5,6 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n-\n import os\n from logging import getLogger\n from pathlib import Path"
            },
            {
              "sha": "b12e46273bf002ab1318064bc0e14c49ecbe63a0",
              "url": "https://github.com/meta-llama/llama-models/commit/b12e46273bf002ab1318064bc0e14c49ecbe63a0",
              "message": "refactor: make llama3 generation closer to llama4 (#309)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/checkpoint.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/multimodal/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/quantization/loader.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/scripts/chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/scripts/completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/scripts/multimodal_chat_completion.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/scripts/multimodal_completion.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama4/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/quantization/loader.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/scripts/chat_completion.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -54,7 +54,7 @@ In order to run the models, you will need to install dependencies after checking\n \n ```bash\n # Run this within a suitable Python environment (uv, conda, or virtualenv)\n-pip install -e .[torch]\n+pip install .[torch]\n ```\n \n Example scripts are available in `models/{ llama3, llama4 }/scripts/` sub-directory. Note that the Llama4 series of models require at least 4 GPUs to run inference at full (bf16) precision.\n@@ -115,7 +115,7 @@ huggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --lo\n - To use with transformers, the following snippet will download and cache the weights:\n \n   ```python\n-  #inference.py\n+  # inference.py\n   from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n   import torch\n \n@@ -126,16 +126,16 @@ huggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --lo\n   messages = [\n       {\"role\": \"user\", \"content\": \"Who are you?\"},\n   ]\n-  inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+  inputs = tokenizer.apply_chat_template(\n+      messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n+  )\n \n   model = Llama4ForConditionalGeneration.from_pretrained(\n-      model_id,\n-      device_map=\"auto\",\n-      torch_dtype=torch.bfloat16\n+      model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n   )\n \n   outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n-  outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+  outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1] :])\n   print(outputs[0])\n   ```\n   ```bash\n\n--- File: models/checkpoint.py ---\n@@ -8,14 +8,12 @@\n import concurrent.futures\n import re\n from pathlib import Path\n-from typing import Any, Dict, List, Union\n+from typing import Any, Dict, List, Optional, Union\n \n import numpy as np\n import torch\n from fairscale.nn.model_parallel.initialize import get_model_parallel_rank, get_model_parallel_world_size\n \n-from .args import ModelArgs\n-\n \n def map_mp_rank(old_mp_size: int, new_mp_size: int, new_mp_rank: int) -> List[int]:\n     \"\"\"Map a new MP rank to a list of old MP ranks given a change in MP size.\"\"\"\n@@ -33,9 +31,10 @@ def map_mp_rank(old_mp_size: int, new_mp_size: int, new_mp_rank: int) -> List[in\n         )\n \n \n-def load_state_dict(\n+def maybe_reshard_state_dict(\n     ckpt_paths: List[Path],\n-    model_args: ModelArgs,\n+    n_kv_heads: int,\n+    moe_num_experts: Optional[int] = None,\n     map_location: Union[str, torch.device] = \"cpu\",\n     mmap: bool = True,\n ) -> Dict[str, torch.Tensor]:\n@@ -57,10 +56,9 @@ def load_state_dict(\n     if new_mp_size == old_mp_size:\n         return state_dicts[0]\n \n-    num_experts = model_args.moe_args.num_experts\n-    state_dicts = [convert_moe_weights(d, num_experts=num_experts) for d in state_dicts]\n+    if moe_num_experts is not None:\n+        state_dicts = [convert_moe_weights(d, moe_num_experts) for d in state_dicts]\n \n-    n_kv_heads: int = model_args.n_kv_heads if model_args.n_kv_heads is not None else model_args.n_heads\n     print(f\"Resharding {len(state_dicts)} state dicts from MP size {old_mp_size} to MP size {new_mp_size}\")\n     return reshard_mp(\n         state_dicts,\n\n--- File: models/datatypes.py ---\n@@ -7,15 +7,12 @@\n \n import base64\n from enum import Enum\n-\n from io import BytesIO\n from typing import Dict, List, Literal, Optional, Union\n \n from pydantic import BaseModel, ConfigDict, Field, field_serializer, field_validator\n-\n from typing_extensions import Annotated\n \n-\n # The goal is that these set of types are relevant for all Llama models.\n # That isn't the current state yet -- e.g., BuiltinTool is somewhat specific to\n # the llama3 series of models.\n@@ -126,3 +123,27 @@ class RawMessage(BaseModel):\n     # These are for the output message coming from the assistant\n     stop_reason: Optional[StopReason] = None\n     tool_calls: List[ToolCall] = Field(default_factory=list)\n+\n+\n+class GenerationResult(BaseModel):\n+    token: int\n+    text: str\n+    logprobs: Optional[List[float]] = None\n+\n+    source: Literal[\"input\"] | Literal[\"output\"]\n+\n+    # index within the batch\n+    batch_idx: int\n+    # whether generation for this item is already finished. note that tokens can\n+    # get returned even afterwards since other items in the batch can still be generating tokens\n+    finished: bool\n+    # because a batch is parallel processed, useful decoding for one item can correspond to processing\n+    # pad tokens or tokens beyond EOS for other items. we could have decided to return None for this case\n+    # but it's more convenient to return a list of GenerationResult and filter out the ignored tokens\n+    ignore_token: bool\n+\n+\n+class QuantizationMode(str, Enum):\n+    none = \"none\"\n+    fp8_mixed = \"fp8_mixed\"\n+    int4_mixed = \"int4_mixed\"\n\n--- File: models/llama3/generation.py ---\n@@ -5,97 +5,41 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# the root directory of this source tree.\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n-\n import json\n import os\n import sys\n import time\n-from dataclasses import dataclass\n from pathlib import Path\n from typing import Callable, Generator, List, Optional\n \n import torch\n import torch.nn.functional as F\n from fairscale.nn.model_parallel.initialize import (\n-    get_model_parallel_rank,\n     initialize_model_parallel,\n     model_parallel_is_initialized,\n )\n from termcolor import cprint\n \n-from ..datatypes import RawContent, RawMessage, StopReason, ToolPromptFormat\n+from ..checkpoint import maybe_reshard_state_dict\n+from ..datatypes import GenerationResult, QuantizationMode, RawContent, RawMessage, ToolPromptFormat\n from .args import ModelArgs\n from .chat_format import ChatFormat, LLMInput\n from .model import Transformer\n+from .multimodal.model import CrossAttentionTransformer\n from .tokenizer import Tokenizer\n \n \n-@dataclass\n-class CompletionPrediction:\n-    generation: str\n-    decoded_tokens: Optional[List[str]] = None\n-    logprobs: Optional[List[List[float]]] = None\n-\n-\n-@dataclass\n-class ChatPrediction:\n-    generation: RawMessage\n-    decoded_tokens: Optional[List[str]] = None\n-    logprobs: Optional[List[List[float]]] = None\n-\n-\n-@dataclass\n-class TokenResult:\n-    token: int\n-    text: str\n-    logprobs: Optional[List[float]] = None\n-\n-\n-class Llama:\n+class Llama3:\n     @staticmethod\n     def build(\n         ckpt_dir: str,\n         max_seq_len: int,\n         max_batch_size: int,\n         world_size: Optional[int] = None,\n-        tokenizer_path: Optional[str] = None,\n+        quantization_mode: Optional[QuantizationMode] = None,\n         seed: int = 1,\n         device: str = \"cuda\",\n     ):\n-        \"\"\"\n-        Build a Llama instance by initializing and loading a model checkpoint.\n-\n-        Args:\n-            ckpt_dir (str): Path to the directory containing checkpoint files.\n-            tokenizer_path (str): Path to the tokenizer file.\n-            max_seq_len (int): Maximum sequence length for input text.\n-            max_batch_size (int): Maximum batch size for inference.\n-            world_size (Optional[int], optional): Number of model parallel processes.\n-                If not provided, it's determined from the environment. Defaults to None.\n-            device (str, optional): Device to use, e.g. cuda (default), xpu, cpu, etc.\n-\n-        Returns:\n-            Llama: An instance of the Llama class with the loaded model and tokenizer.\n-\n-        Raises:\n-            AssertionError: If there are no checkpoint files in the specified directory,\n-                or if the model parallel size does not match the number of checkpoint files.\n-            RuntimeError: If PyTorch backend for the specified device is not available.\n-\n-\n-        Note:\n-            This method initializes the distributed process group, sets the device to CUDA,\n-            and loads the pre-trained model and tokenizer.\n-        \"\"\"\n-\n         device = torch.device(device)\n         if (\n             device.type == \"cuda\"\n@@ -129,13 +73,9 @@ def build(\n \n         start_time = time.time()\n \n-        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n-        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n-        assert world_size == len(checkpoints), (\n-            f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n-        )\n-        ckpt_path = checkpoints[get_model_parallel_rank()]\n-        checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n+        ckpt_paths = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n+        assert len(ckpt_paths) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n+        print(f\"Loading a checkpoint (shards={len(ckpt_paths)}, current-mp-size={world_size})\")\n         with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n             params = json.loads(f.read())\n \n@@ -144,40 +84,58 @@ def build(\n             max_batch_size=max_batch_size,\n             **params,\n         )\n-        if tokenizer_path:\n-            tokenizer = Tokenizer(model_path=tokenizer_path)\n-        else:\n-            tokenizer = Tokenizer.get_instance()\n+        tokenizer = Tokenizer.get_instance()\n+\n+        state_dict = maybe_reshard_state_dict(\n+            ckpt_paths,\n+            n_kv_heads=model_args.n_kv_heads if model_args.n_kv_heads else model_args.n_heads,\n+        )\n \n         assert model_args.vocab_size == tokenizer.n_words\n-        torch.set_default_device(device)\n-        if device.type == \"cuda\":\n-            if torch.cuda.is_bf16_supported():\n-                torch.set_default_dtype(torch.bfloat16)\n-            else:\n-                torch.set_default_dtype(torch.half)\n-        elif device.type == \"xpu\":\n-            if torch.xpu.is_bf16_supported():\n-                torch.set_default_dtype(torch.bfloat16)\n+\n+        def build_model():\n+            if model_args.vision_chunk_size > 0:\n+                model = CrossAttentionTransformer(model_args)\n+                model.setup_cache(model_args.max_batch_size, device=device, dtype=torch.get_default_dtype())\n             else:\n-                torch.set_default_dtype(torch.half)\n+                model = Transformer(model_args)\n+            return model\n+\n+        if quantization_mode == QuantizationMode.fp8_mixed or quantization_mode == QuantizationMode.int4_mixed:\n+            from .quantization.loader import convert_to_quantized_model\n+\n+            torch.set_default_tensor_type(torch.BFloat16Tensor)\n+            model = build_model()\n+            print(\"Loading state dict...\")\n+            model.load_state_dict(state_dict, strict=False)\n+            print(\"Done...\")\n+            model = convert_to_quantized_model(model, ckpt_dir, quantization_mode, device=device)\n+            torch.set_default_device(device)\n         else:\n-            torch.set_default_dtype(torch.half)\n+            print(f\"Setting default device to {device}\")\n+            torch.set_default_device(device)\n+            if device.type == \"cuda\":\n+                if torch.cuda.is_bf16_supported():\n+                    torch.set_default_dtype(torch.bfloat16)\n+                else:\n+                    torch.set_default_dtype(torch.half)\n+            elif device.type == \"xpu\":\n+                if torch.xpu.is_bf16_supported():\n+                    torch.set_default_dtype(torch.bfloat16)\n+                else:\n+                    torch.set_default_dtype(torch.half)\n \n-        if model_args.vision_chunk_size > 0:\n-            from .multimodal.model import CrossAttentionTransformer\n+            model = build_model()\n+            print(\"Loading state dict...\")\n+            model.load_state_dict(state_dict, strict=True)\n+            model.to(device)\n+            print(\"Done...\")\n \n-            model = CrossAttentionTransformer(model_args)\n-            model.setup_cache(model_args.max_batch_size, torch.get_default_dtype())\n-        else:\n-            model = Transformer(model_args)\n-        model.load_state_dict(checkpoint, strict=True)\n-        model.to(device)\n         print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n \n-        return Llama(model, tokenizer, model_args)\n+        return Llama3(model, tokenizer, model_args)\n \n-    def __init__(self, model: Transformer, tokenizer: Tokenizer, args: ModelArgs):\n+    def __init__(self, model: Transformer | CrossAttentionTransformer, tokenizer: Tokenizer, args: ModelArgs):\n         self.args = args\n         self.model = model\n         self.tokenizer = tokenizer\n@@ -186,26 +144,30 @@ def __init__(self, model: Transformer, tokenizer: Tokenizer, args: ModelArgs):\n     @torch.inference_mode()\n     def generate(\n         self,\n-        model_input: LLMInput,\n-        max_gen_len: int,\n+        model_inputs: List[LLMInput],\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n         echo: bool = False,\n         print_model_input: bool = False,\n         logits_processor: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n-    ) -> Generator:\n+    ) -> Generator[List[GenerationResult], None, None]:\n+        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.args.max_seq_len:\n+            max_gen_len = self.args.max_seq_len - 1\n         params = self.model.params\n \n+        print_model_input = print_model_input or os.environ.get(\"LLAMA_MODELS_DEBUG\", \"0\") == \"1\"\n         if print_model_input:\n-            tokens_to_print = [self.formatter.vision_token if t == 128256 else t for t in model_input.tokens]\n-            cprint(\n-                \"Input to model:\\n\" + self.tokenizer.decode(tokens_to_print) + \"\\n\",\n-                \"red\",\n-            )\n-        prompt_tokens = [model_input.tokens]\n+            for inp in model_inputs:\n+                tokens_to_print = [self.formatter.vision_token if t == 128256 else t for t in inp.tokens]\n+                cprint(\n+                    \"Input to model:\\n\" + self.tokenizer.decode(tokens_to_print) + \"\\n\",\n+                    \"red\",\n+                )\n+        prompt_tokens = [inp.tokens for inp in model_inputs]\n \n-        bsz = 1\n+        bsz = len(model_inputs)\n         assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n \n         min_prompt_len = min(len(t) for t in prompt_tokens)\n@@ -217,42 +179,52 @@ def generate(\n \n         total_len = min(max_gen_len + max_prompt_len, params.max_seq_len)\n \n-        is_vision = not isinstance(self.model, Transformer)\n-        if is_vision:\n-            images = model_input.vision.images if model_input.vision is not None else []\n-            mask = model_input.vision.mask if model_input.vision is not None else []\n-\n-            # the method works for bsz > 1 so add a batch dimension\n-            xattn_caches, cross_attention_masks, full_text_row_masked_out_mask = self.model.compute_vision_tokens_masks(\n-                batch_images=[images],\n-                batch_masks=[mask],\n-                total_len=total_len,\n-            )\n-\n         pad_id = self.tokenizer.pad_id\n         tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long)\n         for k, t in enumerate(prompt_tokens):\n             tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long)\n         if logprobs:\n             token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n \n-        prev_pos = 0\n+        is_vision = not isinstance(self.model, Transformer)\n+        if is_vision:\n+            images = [inp.vision.images if inp.vision is not None else [] for inp in model_inputs]\n+            mask = [inp.vision.mask if inp.vision is not None else [] for inp in model_inputs]\n+\n+            xattn_caches, cross_attention_masks, full_text_row_masked_out_mask = self.model.compute_vision_tokens_masks(\n+                batch_images=images,\n+                batch_masks=mask,\n+                total_len=total_len,\n+                device=tokens.device,\n+            )\n+\n         eos_reached = torch.tensor([False] * bsz)\n         input_text_mask = tokens != pad_id\n \n         if echo:\n-            for i, t in enumerate(model_input.tokens):\n-                yield TokenResult(\n-                    token=t,\n-                    text=self.tokenizer.decode([t]),\n-                    logprobs=(token_logprobs[0, i : i + 1].tolist() if logprobs else None),\n-                )\n+            for i in range(max_prompt_len):\n+                results = []\n+                for j, t in enumerate(tokens[:, i]):\n+                    results.append(\n+                        GenerationResult(\n+                            token=t.item(),\n+                            text=self.tokenizer.decode([t.item()]),\n+                            source=\"input\",\n+                            logprobs=(token_logprobs[j, i : i + 1].tolist() if logprobs else None),\n+                            batch_idx=j,\n+                            finished=False,\n+                            ignore_token=t.item() == pad_id,\n+                        )\n+                    )\n+                yield results\n \n         stop_tokens = torch.tensor(self.tokenizer.stop_tokens)\n+\n+        prev_pos = 0\n         for cur_pos in range(min_prompt_len, total_len):\n             if is_vision:\n                 position_ids = torch.arange(prev_pos, cur_pos, dtype=torch.long)\n-                text_only_inference = model_input.vision is None\n+                text_only_inference = all(inp.vision is None for inp in model_inputs)\n                 logits = self.model.forward(\n                     position_ids,\n                     tokens,\n@@ -299,155 +271,69 @@ def generate(\n                     ignore_index=pad_id,\n                 )\n             eos_reached |= (~input_text_mask[:, cur_pos]) & (torch.isin(next_token, stop_tokens))\n-            yield TokenResult(\n-                token=next_token[0].item(),\n-                text=self.tokenizer.decode(next_token.tolist()),\n-                logprobs=(token_logprobs[:, cur_pos : cur_pos + 1][0].tolist() if logprobs else None),\n-            )\n+            results = []\n+            for idx, t in enumerate(next_token):\n+                results.append(\n+                    GenerationResult(\n+                        token=t.item(),\n+                        text=self.tokenizer.decode([t.item()]),\n+                        source=\"output\",\n+                        logprobs=(token_logprobs[idx, cur_pos : cur_pos + 1].tolist() if logprobs else None),\n+                        batch_idx=idx,\n+                        finished=eos_reached[idx],\n+                        ignore_token=cur_pos < len(prompt_tokens[idx]),\n+                    )\n+                )\n+            yield results\n \n             prev_pos = cur_pos\n             if all(eos_reached):\n                 break\n \n-    def text_completion(\n+    def completion(\n         self,\n-        content: RawContent,\n+        contents: List[RawContent],\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n         echo: bool = False,\n-    ) -> CompletionPrediction:\n-        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n-            max_gen_len = self.model.params.max_seq_len - 1\n-\n-        model_input = self.formatter.encode_content(content)\n-\n-        tokens = []\n-        token_logprobs = []\n-        decoded_tokens = []\n+    ) -> Generator[List[GenerationResult], None, None]:\n+        model_inputs = [self.formatter.encode_content(c) for c in contents]\n         for result in self.generate(\n-            model_input=model_input,\n-            max_gen_len=max_gen_len,\n+            model_inputs=model_inputs,\n             temperature=temperature,\n             top_p=top_p,\n+            max_gen_len=max_gen_len,\n             logprobs=logprobs,\n             echo=echo,\n         ):\n-            tokens.append(result.token)\n-            if logprobs:\n-                decoded_tokens.append(result.text)\n-                token_logprobs.append(result.logprobs)\n-\n-        generation = self.tokenizer.decode(tokens)\n-        if logprobs:\n-            return CompletionPrediction(\n-                generation=generation,\n-                logprobs=token_logprobs,\n-                decoded_tokens=decoded_tokens,\n-            )\n-\n-        return CompletionPrediction(generation=generation)\n+            yield result\n+            if all(r.finished for r in result):\n+                break\n \n     def chat_completion(\n         self,\n-        messages: List[RawMessage],\n+        messages_batch: List[List[RawMessage]],\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n         echo: bool = False,\n-    ) -> ChatPrediction:\n-        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n-            max_gen_len = self.model.params.max_seq_len - 1\n-\n-        tokens = []\n-        token_logprobs = []\n-        decoded_tokens = []\n-\n-        stop_reason = None\n+    ) -> Generator[List[GenerationResult], None, None]:\n+        model_inputs = [self.formatter.encode_dialog_prompt(messages) for messages in messages_batch]\n         for result in self.generate(\n-            model_input=self.formatter.encode_dialog_prompt(messages, tool_prompt_format),\n-            max_gen_len=max_gen_len,\n+            model_inputs=model_inputs,\n             temperature=temperature,\n             top_p=top_p,\n+            max_gen_len=max_gen_len,\n             logprobs=logprobs,\n             echo=echo,\n         ):\n-            tokens.append(result.token)\n-            if result.text == \"<|eot_id|>\":\n-                stop_reason = StopReason.end_of_turn\n-            elif result.text == \"<|eom_id|>\":\n-                stop_reason = StopReason.end_of_message\n-\n-            if logprobs:\n-                decoded_tokens.append(result.text)\n-                token_logprobs.append(result.logprobs)\n-\n-        if stop_reason is None:\n-            stop_reason = StopReason.out_of_tokens\n-\n-        message = self.formatter.decode_assistant_message(tokens, stop_reason)\n-\n-        if logprobs:\n-            return ChatPrediction(\n-                generation=message,\n-                logprobs=token_logprobs,\n-                decoded_tokens=decoded_tokens,\n-            )\n-\n-        return ChatPrediction(generation=message)\n-\n-    def chat_completion_raw(\n-        self,\n-        messages: List[RawMessage],\n-        temperature: float = 0.6,\n-        top_p: float = 0.9,\n-        max_gen_len: Optional[int] = None,\n-        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n-    ) -> List[int]:\n-        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n-            max_gen_len = self.model.params.max_seq_len - 1\n-\n-        output_tokens = []\n-        model_input = self.formatter.encode_dialog_prompt(messages, tool_prompt_format)\n-        input_tokens = model_input.tokens\n-        for result in self.generate(\n-            model_input=model_input,\n-            max_gen_len=max_gen_len,\n-            temperature=temperature,\n-            top_p=top_p,\n-            logprobs=False,\n-        ):\n-            output_tokens.append(result.token)\n-\n-        return input_tokens, output_tokens\n-\n-    def text_completion_raw(\n-        self,\n-        content: RawContent,\n-        temperature: float = 0.6,\n-        top_p: float = 0.9,\n-        max_gen_len: Optional[int] = None,\n-    ):\n-        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n-            max_gen_len = self.model.params.max_seq_len - 1\n-\n-        model_input = self.formatter.encode_content(content)\n-        input_tokens = model_input.tokens\n-\n-        output_tokens = []\n-        for result in self.generate(\n-            model_input=model_input,\n-            max_gen_len=max_gen_len,\n-            temperature=temperature,\n-            top_p=top_p,\n-            logprobs=False,\n-        ):\n-            output_tokens.append(result.token)\n-\n-        return input_tokens, output_tokens\n+            yield result\n+            if all(r.finished for r in result):\n+                break\n \n \n def sample_top_p(probs, p):\n\n--- File: models/llama3/multimodal/model.py ---\n@@ -5,31 +5,24 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n-\n import logging\n import math\n from functools import partial\n from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import fairscale.nn.model_parallel.initialize as fs_init\n-\n import torch\n import torch.nn.functional as F\n from fairscale.nn.model_parallel.layers import (\n     ColumnParallelLinear,\n     RowParallelLinear,\n     VocabParallelEmbedding,\n )\n-\n from PIL import Image as PIL_Image\n-\n-from torch import nn, Tensor\n+from torch import Tensor, nn\n from torch.distributed import _functional_collectives as funcol\n \n-from ..model import apply_rotary_emb, ModelArgs, precompute_freqs_cis, RMSNorm\n-\n+from ..model import ModelArgs, RMSNorm, apply_rotary_emb, precompute_freqs_cis\n from .encoder_utils import (\n     build_encoder_attention_mask,\n     contract_num_tokens_from_mult8,\n@@ -41,7 +34,6 @@\n from .image_transform import VariableSizeImageTransform\n from .utils import get_negative_inf_value, to_2tuple\n \n-\n logger = logging.getLogger(__name__)\n MP_SCALE = 8\n \n@@ -587,13 +579,11 @@ def setup_cache(self, max_batch_size: int, dtype: torch.dtype):\n             self.n_local_kv_heads,\n             self.head_dim,\n         )\n-        device = next(self.parameters()).device\n         self.register_buffer(\n             \"key_cache\",\n             torch.zeros(\n                 cache_shape,\n                 dtype=dtype,\n-                device=device,\n             ),\n             persistent=False,\n         )\n@@ -602,7 +592,6 @@ def setup_cache(self, max_batch_size: int, dtype: torch.dtype):\n             torch.zeros(\n                 cache_shape,\n                 dtype=dtype,\n-                device=device,\n             ),\n             persistent=False,\n         )\n@@ -614,6 +603,9 @@ def forward(\n         freqs_cis: torch.Tensor,\n         position_ids: torch.LongTensor,\n     ):\n+        self.key_cache = self.key_cache.to(x.device)\n+        self.value_cache = self.value_cache.to(x.device)\n+\n         xq, xk, xv = [F.linear(x, w) for w in [self.wq.weight, self.wk.weight, self.wv.weight]]\n \n         bs, slen, _ = xq.shape\n@@ -1184,14 +1176,16 @@ def forward(\n         text_only_inference: bool = False,\n     ):\n         assert self.cache_is_setup, \"Please set up cache before calling forward\"\n+        self.mask_cache = self.mask_cache.to(h.device)\n+        self.freqs_cis = self.freqs_cis.to(h.device)\n         mask = self.mask_cache.index_select(2, position_ids)\n         freqs_cis = self.freqs_cis.index_select(0, position_ids)\n \n-        for idx, (\n+        for (\n             layer,\n             xattn_layer,\n             xattn_layer_idx,\n-        ) in enumerate(self.text_and_xattn_layers):\n+        ) in self.text_and_xattn_layers:\n             if not text_only_inference:\n                 h = xattn_layer(\n                     x=h,\n@@ -1212,9 +1206,8 @@ def forward(\n         output = gather_from_tensor_model_parallel_region(output)\n         return output.float()\n \n-    def setup_cache(self, max_batch_size: int, dtype=torch.bfloat16):\n+    def setup_cache(self, max_batch_size: int, device: torch.device, dtype=torch.bfloat16):\n         # Set up the text kv caches\n-        device = next(self.parameters()).device\n         ones = torch.ones(\n             (self.max_seq_len, self.max_seq_len),\n             dtype=torch.bool,\n@@ -1265,7 +1258,7 @@ def _get_xattn_mask(\n \n         return (\n             cross_attention_masks.to(device=text_device, dtype=text_dtype),\n-            full_text_row_masked_out_mask,\n+            full_text_row_masked_out_mask.to(device=text_device),\n         )\n \n \n@@ -1284,14 +1277,15 @@ def __init__(self, args: ModelArgs) -> None:\n             max_num_chunks=args.vision_max_num_chunks,\n         )\n \n-    def setup_cache(self, max_batch_size: int, dtype: torch.dtype):\n-        self.text_model.setup_cache(max_batch_size, dtype)\n+    def setup_cache(self, max_batch_size: int, device: torch.device, dtype: torch.dtype):\n+        self.text_model.setup_cache(max_batch_size, device, dtype)\n \n     def compute_vision_tokens_masks(\n         self,\n         batch_images: List[List[PIL_Image.Image]],\n         batch_masks: List[List[List[int]]],\n         total_len: int,\n+        device: torch.device,\n     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         skip_vision_encoder = False\n \n@@ -1318,6 +1312,7 @@ def compute_vision_tokens_masks(\n                 image_res=self.params.vision_chunk_size,\n                 max_num_images=max_num_images,\n             )\n+            stacked_images = stacked_images.to(device=device)\n \n         if skip_vision_encoder:\n             vision_tokens = torch.zeros(\n@@ -1330,7 +1325,7 @@ def compute_vision_tokens_masks(\n                 ),\n             )\n         else:\n-            vision_tokens = self.vision_model(stacked_images, aspect_ratios)\n+            vision_tokens = self.vision_model(stacked_images, aspect_ratios).to(device=device)\n \n         bsz, nimg, nchunk, ntok, image_token_dim = tuple(vision_tokens.shape)\n         xattn_caches = torch.stack(\n\n--- File: models/llama3/quantization/loader.py ---\n@@ -0,0 +1,317 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# type: ignore\n+import os\n+from typing import Any, Dict, List, Optional, cast\n+\n+import torch\n+from fairscale.nn.model_parallel.initialize import get_model_parallel_rank\n+from fairscale.nn.model_parallel.layers import ColumnParallelLinear, RowParallelLinear\n+from fairscale.nn.model_parallel.mappings import reduce_from_model_parallel_region\n+from torch import Tensor, nn\n+from torchao.quantization.GPTQ import Int8DynActInt4WeightLinear\n+\n+from ...datatypes import QuantizationMode\n+from ...quantize_impls import (\n+    Fp8ScaledWeights,\n+    ffn_swiglu,\n+    load_fp8,\n+    quantize_fp8,\n+)\n+from ..model import Transformer, TransformerBlock\n+from ..multimodal.model import CrossAttentionTransformer\n+\n+\n+def swiglu_wrapper(\n+    self,\n+    x: Tensor,\n+):\n+    out = ffn_swiglu(x, self.w1.weight, self.w3.weight, self.w2.weight)\n+    return reduce_from_model_parallel_region(out)\n+\n+\n+def convert_to_quantized_model(\n+    model: Transformer | CrossAttentionTransformer,\n+    checkpoint_dir: str,\n+    quantization_mode: Optional[str] = None,\n+    fp8_activation_scale_ub: Optional[float] = 1200.0,\n+    device: Optional[torch.device] = None,\n+) -> Transformer | CrossAttentionTransformer:\n+    if quantization_mode == QuantizationMode.fp8_mixed:\n+        return convert_to_fp8_quantized_model(model, checkpoint_dir, fp8_activation_scale_ub, device)\n+    elif quantization_mode == QuantizationMode.int4_mixed:\n+        return convert_to_int4_quantized_model(model, checkpoint_dir, device)\n+    else:\n+        raise ValueError(f\"Unsupported quantization mode: {quantization_mode}\")\n+\n+\n+def convert_to_fp8_quantized_model(\n+    model: Transformer,\n+    checkpoint_dir: str,\n+    fp8_activation_scale_ub: Optional[float] = 1200.0,\n+    device: Optional[torch.device] = None,\n+) -> Transformer:\n+    # Move weights to GPU with quantization\n+    fp8_scales_path = os.path.join(checkpoint_dir, f\"fp8_scales_{get_model_parallel_rank()}.pt\")\n+    if os.path.isfile(fp8_scales_path):\n+        print(\"Loading fp8 scales...\")\n+        fp8_scales = torch.load(fp8_scales_path, weights_only=True)\n+\n+        for _, block in model.named_modules():\n+            if isinstance(block, TransformerBlock):\n+                if block.layer_id == 0 or block.layer_id == (model.n_layers - 1):\n+                    continue\n+\n+                block.feed_forward.forward = swiglu_wrapper.__get__(block.feed_forward)\n+                for key in (\"w1\", \"w3\", \"w2\"):\n+                    param = getattr(block.feed_forward, key)\n+                    param.weight = load_fp8(\n+                        param.weight,\n+                        fp8_scales[f\"{block.layer_id}_feed_forward.{key}_{get_model_parallel_rank()}\"],\n+                        fp8_activation_scale_ub,\n+                    )\n+    else:\n+        print(\"Quantizing fp8 weights from bf16...\")\n+        for _, block in model.named_modules():\n+            if isinstance(block, TransformerBlock):\n+                if block.layer_id == 0 or block.layer_id == (model.n_layers - 1):\n+                    continue\n+                block.feed_forward.forward = swiglu_wrapper.__get__(block.feed_forward)  # type: ignore\n+                for key in (\"w1\", \"w3\", \"w2\"):\n+                    param = getattr(block.feed_forward, key)\n+                    param.weight = quantize_fp8(\n+                        param.weight,\n+                        fp8_activation_scale_ub,\n+                        output_device=device,\n+                    )\n+\n+    for _, parameter in model.named_parameters():\n+        if not isinstance(parameter, Fp8ScaledWeights):\n+            parameter.data = parameter.to(device=device)\n+    return model\n+\n+\n+class Int8DynActInt4WeightLinearLoRA(Int8DynActInt4WeightLinear):\n+    \"\"\"\n+    Int8DynActInt4WeightLinear with LoRA adaptor.\n+\n+    Args:\n+        in_features: Number of input features.\n+        out_features: Number of output features.\n+        bias: Whether to use bias.\n+        device: Device to use.\n+        group_size: Group size for quantization.\n+        precision: Precision of quantization.\n+        scales_precision: Precision of scales.\n+        lora_rank: Rank of LoRA adaptor.\n+        lora_scale: Scale of LoRA adaptor.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        in_features: int,\n+        out_features: int,\n+        bias=False,\n+        device=None,\n+        # quantization parameters\n+        group_size: int = 256,\n+        precision: torch.dtype = torch.float32,\n+        scales_precision: torch.dtype = torch.float32,\n+        # LoRA parameters\n+        lora_rank: Optional[int] = None,\n+        lora_scale: Optional[float] = None,\n+    ) -> None:\n+        super().__init__(\n+            in_features,\n+            out_features,\n+            bias=bias,\n+            device=device,\n+            groupsize=group_size,\n+            precision=precision,\n+            scales_precision=scales_precision,\n+        )\n+        self.lora_scale: Optional[float] = None\n+        self.adaptor: Optional[nn.Sequential] = None\n+        if lora_rank is not None:\n+            assert lora_scale is not None, \"Please specify lora scale for LoRA.\"\n+            # Low-rank adaptation. See paper for more details: https://arxiv.org/abs/2106.09685\n+            self.adaptor = nn.Sequential()\n+            self.adaptor.add_module(\"A\", nn.Linear(in_features, lora_rank, bias=False))\n+            self.adaptor.add_module(\"B\", nn.Linear(lora_rank, out_features, bias=False))\n+            self.lora_scale = lora_scale\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        \"\"\"A hook to load the quantized weights from the state dict.\"\"\"\n+        if prefix + \"zeros\" not in state_dict:\n+            # Zero-point may not be saved in the state dict. In this case, we assume it's zero.\n+            assert prefix + \"scales\" in state_dict\n+            state_dict[prefix + \"zeros\"] = torch.zeros_like(state_dict[prefix + \"scales\"])\n+\n+    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n+        module_out = super().forward(input_)\n+        if self.adaptor is not None:\n+            adaptor_out = self.adaptor(input_) * self.lora_scale\n+            return module_out + adaptor_out\n+        return module_out\n+\n+\n+class Int8WeightEmbedding(torch.nn.Embedding):\n+    \"\"\"An embedding layer to load int8 weights.\n+\n+    Args:\n+        num_embeddings: Number of embeddings.\n+        embedding_dim: Embedding dimension.\n+        padding_idx: Padding index.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_embeddings: int,\n+        embedding_dim: int,\n+        padding_idx: int,\n+        device=None,\n+    ) -> None:\n+        super().__init__(num_embeddings, embedding_dim, padding_idx, device=device)\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        \"\"\"A hook to load the quantized embedding weight and scales from the state dict.\"\"\"\n+        weights = state_dict.pop(prefix + \"weight\")\n+        scales = state_dict.pop(prefix + \"scales\")\n+        state_dict[prefix + \"weight\"] = weights * scales\n+\n+\n+class Int8WeightLinear(torch.nn.Linear):\n+    \"\"\"A linear layer to load int8 weights.\n+\n+    Args:\n+        in_features: Number of input features.\n+        out_features: Number of output features.\n+        bias: Whether to use bias.\n+    \"\"\"\n+\n+    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None) -> None:\n+        super().__init__(in_features, out_features, bias, device=device)\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        \"\"\"A hook to load the quantized linear weight and scales from the state dict.\"\"\"\n+        weights = state_dict.pop(prefix + \"weight\")\n+        scales = state_dict.pop(prefix + \"scales\")\n+        state_dict[prefix + \"weight\"] = weights * scales\n+\n+\n+def _prepare_model_int4_weight_int8_dynamic_activation(\n+    model: torch.nn.Module,\n+    group_size: int,\n+    lora_rank: Optional[int],\n+    lora_scale: Optional[float],\n+):\n+    \"\"\"Prepare the model for int4 weight and int8 dynamic activation quantization.\n+\n+    Note that the weights of embedding and output layers are quantized to int8.\n+    \"\"\"\n+    device = None\n+    for module_name, module in model.named_children():\n+        if module_name == \"output\":\n+            quantized_module = Int8WeightLinear(\n+                in_features=module.in_features,\n+                out_features=module.out_features,\n+                bias=module.bias,\n+                device=device,\n+            )\n+            del module\n+            setattr(model, module_name, quantized_module)\n+        elif module_name == \"tok_embeddings\":\n+            quantized_module = Int8WeightEmbedding(\n+                num_embeddings=module.num_embeddings,\n+                embedding_dim=module.embedding_dim,\n+                padding_idx=module.padding_idx,\n+                device=device,\n+            )\n+            del module\n+            setattr(model, module_name, quantized_module)\n+        elif isinstance(module, (ColumnParallelLinear, RowParallelLinear, nn.Linear)):\n+            quantized_module = Int8DynActInt4WeightLinearLoRA(\n+                in_features=module.in_features,\n+                out_features=module.out_features,\n+                bias=False,\n+                group_size=group_size,\n+                lora_rank=lora_rank,\n+                lora_scale=lora_scale,\n+                device=device,\n+            )\n+            del module\n+            setattr(model, module_name, quantized_module)\n+        else:\n+            _prepare_model_int4_weight_int8_dynamic_activation(module, group_size, lora_rank, lora_scale)\n+\n+    return model\n+\n+\n+def convert_to_int4_quantized_model(\n+    model: Transformer | CrossAttentionTransformer,\n+    checkpoint_dir: str,\n+    device: Optional[torch.device] = None,\n+) -> Transformer | CrossAttentionTransformer:\n+    \"\"\"Convert the model to int4 quantized model.\"\"\"\n+    model_args = model.params\n+    assert model_args.quantization_args is not None, \"Quantization args must be specified.\"\n+    quantization_args = model_args.quantization_args\n+    if quantization_args.scheme is None:\n+        raise ValueError(\"Quantization scheme must be specified in 'quantization_args'.\")\n+\n+    if quantization_args.scheme.value != \"int4_weight_int8_dynamic_activation\":\n+        raise NotImplementedError(\n+            \"Only int4 quantization with 'int4_weight_int8_dynamic_activation' scheme is supported.\"\n+        )\n+\n+    group_size = model_args.quantization_args.group_size\n+    if group_size is None:\n+        raise ValueError(\"'group_size' cannot be None in 'quantization_args'. Please specify it.\")\n+\n+    if model_args.lora_args is None:\n+        # Certain quantized models (e.g., SpinQuant) may not have LoRA.\n+        lora_rank = None\n+        lora_scale = None\n+    else:\n+        lora_rank = model_args.lora_args.rank\n+        lora_scale = model_args.lora_args.scale\n+\n+    _prepare_model_int4_weight_int8_dynamic_activation(model, group_size, lora_rank, lora_scale)\n+    return cast(Transformer | CrossAttentionTransformer, model.to(device=device))\n\n--- File: models/llama3/scripts/chat_completion.py ---\n@@ -8,12 +8,17 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n+from io import BytesIO\n+from pathlib import Path\n from typing import Optional\n \n import fire\n+from termcolor import cprint\n \n-from models.datatypes import RawMessage, StopReason\n-from models.llama3.generation import Llama\n+from models.datatypes import RawMediaItem, RawMessage, RawTextItem, StopReason\n+from models.llama3.generation import Llama3\n+\n+THIS_DIR = Path(__file__).parent\n \n \n def run_main(\n@@ -22,23 +27,15 @@ def run_main(\n     top_p: float = 0.9,\n     max_seq_len: int = 512,\n     max_batch_size: int = 4,\n-    max_gen_len: Optional[int] = None,\n     world_size: Optional[int] = None,\n+    quantization_mode: Optional[str] = None,\n ):\n-    \"\"\"\n-    Examples to run with the models finetuned for chat. Prompts correspond of chat\n-    turns between the user and assistant with the final one always being the user.\n-\n-    An optional system prompt at the beginning to control how the model should respond\n-    is also supported.\n-\n-    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n-    \"\"\"\n-    generator = Llama.build(\n+    generator = Llama3.build(\n         ckpt_dir=ckpt_dir,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n         world_size=world_size,\n+        quantization_mode=quantization_mode,\n     )\n \n     dialogs = [\n@@ -71,20 +68,39 @@ def run_main(\n             RawMessage(role=\"user\", content=\"How to go from Beijing to NY?\"),\n         ],\n     ]\n-    for dialog in dialogs:\n-        result = generator.chat_completion(\n-            dialog,\n-            max_gen_len=max_gen_len,\n-            temperature=temperature,\n-            top_p=top_p,\n+    if generator.args.vision_chunk_size > 0:\n+        with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n+            img = f.read()\n+\n+        dialogs.append(\n+            [\n+                RawMessage(\n+                    role=\"user\",\n+                    content=[\n+                        RawMediaItem(data=BytesIO(img)),\n+                        RawTextItem(text=\"Describe this image in two sentences\"),\n+                    ],\n+                ),\n+            ]\n         )\n \n+    for dialog in dialogs:\n         for msg in dialog:\n             print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n \n-        out_message = result.generation\n-        print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n-        print(\"\\n==================================\\n\")\n+        batch = [dialog]\n+        for token_results in generator.chat_completion(\n+            batch,\n+            temperature=temperature,\n+            top_p=top_p,\n+            max_gen_len=max_seq_len,\n+        ):\n+            result = token_results[0]\n+            if result.finished:\n+                break\n+\n+            cprint(result.text, color=\"yellow\", end=\"\")\n+        print(\"\\n\")\n \n \n def main():\n\n--- File: models/llama3/scripts/completion.py ---\n@@ -8,12 +8,17 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n+from io import BytesIO\n+from pathlib import Path\n from typing import Optional\n \n import fire\n from termcolor import cprint\n \n-from models.llama3.generation import Llama\n+from models.datatypes import RawMediaItem\n+from models.llama3.generation import Llama3\n+\n+THIS_DIR = Path(__file__).parent\n \n \n def run_main(\n@@ -22,17 +27,18 @@ def run_main(\n     top_p: float = 0.9,\n     max_seq_len: int = 512,\n     max_batch_size: int = 4,\n-    max_gen_len: int = 64,\n     world_size: Optional[int] = None,\n+    quantization_mode: Optional[str] = None,\n ):\n-    generator = Llama.build(\n+    generator = Llama3.build(\n         ckpt_dir=ckpt_dir,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n         world_size=world_size,\n+        quantization_mode=quantization_mode,\n     )\n \n-    prompts = [\n+    interleaved_contents = [\n         \"The color of the sky is blue but sometimes it can also be\",\n         \"\"\"\\\n apple is pomme,\n@@ -41,17 +47,30 @@ def run_main(\n         \"1, 2, 3, 5, 8, 13\",\n         \"ba ba black sheep, have you any wool?\",\n     ]\n-    for prompt in prompts:\n-        result = generator.text_completion(\n-            prompt,\n-            temperature=0.6,\n-            top_p=0.9,\n-            max_gen_len=max_gen_len,\n-            logprobs=False,\n+    if generator.args.vision_chunk_size > 0:\n+        with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n+            img = f.read()\n+\n+        interleaved_contents.append(\n+            [\n+                RawMediaItem(type=\"image\", data=BytesIO(img)),\n+                \"If I had to write a haiku for this one\",\n+            ]\n         )\n \n-        cprint(f\"{prompt}\", end=\"\")\n-        cprint(f\"{result.generation}\", color=\"yellow\")\n+    for content in interleaved_contents:\n+        cprint(f\"{content}\", end=\"\")\n+        batch = [content]\n+        for token_results in generator.completion(\n+            batch,\n+            temperature=temperature,\n+            top_p=top_p,\n+        ):\n+            result = token_results[0]\n+            if result.finished:\n+                break\n+\n+            cprint(result.text, color=\"yellow\", end=\"\")\n         print(\"\\n==================================\\n\")\n \n \n\n--- File: models/llama3/scripts/multimodal_chat_completion.py ---\n@@ -1,88 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n-\n-from io import BytesIO\n-from pathlib import Path\n-from typing import Optional\n-\n-import fire\n-\n-from models.datatypes import RawMediaItem, RawMessage, RawTextItem\n-from models.llama3.generation import Llama\n-\n-THIS_DIR = Path(__file__).parent\n-\n-\n-def run_main(\n-    ckpt_dir: str,\n-    temperature: float = 0.6,\n-    top_p: float = 0.9,\n-    max_seq_len: int = 512,\n-    max_batch_size: int = 4,\n-    max_gen_len: Optional[int] = None,\n-    world_size: Optional[int] = None,\n-):\n-    generator = Llama.build(\n-        ckpt_dir=ckpt_dir,\n-        max_seq_len=max_seq_len,\n-        max_batch_size=max_batch_size,\n-        world_size=world_size,\n-    )\n-\n-    # image understanding\n-    dialogs = []\n-    with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n-        img = f.read()\n-\n-    dialogs = [\n-        [\n-            RawMessage(\n-                role=\"user\",\n-                content=[\n-                    RawMediaItem(data=BytesIO(img)),\n-                    RawTextItem(text=\"Describe this image in two sentences\"),\n-                ],\n-            )\n-        ],\n-    ]\n-    # text only\n-    dialogs += [\n-        [\n-            RawMessage(\n-                role=\"user\",\n-                content=\"what is the recipe of mayonnaise in two sentences?\",\n-            )\n-        ],\n-    ]\n-\n-    for dialog in dialogs:\n-        result = generator.chat_completion(\n-            dialog,\n-            max_gen_len=max_gen_len,\n-            temperature=temperature,\n-            top_p=top_p,\n-        )\n-\n-        for msg in dialog:\n-            print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n-\n-        out_message = result.generation\n-        print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n-        for t in out_message.tool_calls:\n-            print(f\"  Tool call: {t.tool_name} ({t.arguments})\")\n-        print(\"\\n==================================\\n\")\n-\n-\n-def main():\n-    fire.Fire(run_main)\n-\n-\n-if __name__ == \"__main__\":\n-    main()\n\n--- File: models/llama3/scripts/multimodal_completion.py ---\n@@ -1,71 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n-\n-from io import BytesIO\n-from pathlib import Path\n-from typing import Optional\n-\n-import fire\n-from termcolor import cprint\n-\n-from models.datatypes import RawMediaItem\n-from models.llama3.generation import Llama\n-\n-THIS_DIR = Path(__file__).parent\n-\n-\n-def run_main(\n-    ckpt_dir: str,\n-    temperature: float = 0.6,\n-    top_p: float = 0.9,\n-    max_seq_len: int = 512,\n-    max_batch_size: int = 4,\n-    max_gen_len: Optional[int] = None,\n-    world_size: Optional[int] = None,\n-):\n-    generator = Llama.build(\n-        ckpt_dir=ckpt_dir,\n-        max_seq_len=max_seq_len,\n-        max_batch_size=max_batch_size,\n-        world_size=world_size,\n-    )\n-\n-    with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n-        img = f.read()\n-\n-    interleaved_contents = [\n-        # text only\n-        \"The color of the sky is blue but sometimes it can also be\",\n-        # image understanding\n-        [\n-            RawMediaItem(type=\"image\", data=BytesIO(img)),\n-            \"If I had to write a haiku for this one\",\n-        ],\n-    ]\n-\n-    for content in interleaved_contents:\n-        result = generator.text_completion(\n-            content,\n-            max_gen_len=max_gen_len,\n-            temperature=temperature,\n-            top_p=top_p,\n-        )\n-\n-        cprint(f\"{content}\", end=\"\")\n-        cprint(f\"{result.generation}\", color=\"yellow\")\n-        print(\"\\n==================================\\n\")\n-\n-\n-def main():\n-    fire.Fire(run_main)\n-\n-\n-if __name__ == \"__main__\":\n-    main()\n\n--- File: models/llama4/generation.py ---\n@@ -20,65 +20,36 @@\n import os\n import sys\n import time\n-from dataclasses import dataclass\n-from enum import Enum\n from pathlib import Path\n-from typing import Callable, Generator, List, Literal, Optional\n+from typing import Callable, Generator, List, Optional\n \n import torch\n import torch.nn.functional as F\n from fairscale.nn.model_parallel.initialize import (\n-    get_model_parallel_rank,\n     initialize_model_parallel,\n     model_parallel_is_initialized,\n )\n from termcolor import cprint\n \n+from ..checkpoint import maybe_reshard_state_dict\n+from ..datatypes import GenerationResult, QuantizationMode\n from .args import ModelArgs\n from .chat_format import ChatFormat, RawContent, RawMessage\n-from .checkpoint import load_state_dict\n from .datatypes import LLMInput, MaskedEmbedding, TransformerInput\n from .model import Transformer\n from .tokenizer import Tokenizer\n \n-\n-@dataclass\n-class GenerationResult:\n-    token: int\n-    text: str\n-\n-    source: Literal[\"input\", \"output\"]\n-\n-    # index within the batch\n-    batch_idx: int\n-    # whether generation for this item is already finished. note that tokens can\n-    # get returned even afterwards since other items in the batch can still be generating tokens\n-    finished: bool\n-    # because a batch is parallel processed, useful decoding for one item can correspond to processing\n-    # pad tokens or tokens beyond EOS for other items. we could have decided to return None for this case\n-    # but it's more convenient to return a list of GenerationResult and filter out the ignored tokens\n-    ignore_token: bool\n-\n-    logprobs: Optional[List[float]] = None\n-\n-\n torch.serialization.add_safe_globals([io.BytesIO, codecs.encode])\n \n \n-class QuantizationMode(str, Enum):\n-    none = \"none\"\n-    fp8_mixed = \"fp8_mixed\"\n-    int4_mixed = \"int4_mixed\"\n-\n-\n class Llama4:\n     @staticmethod\n     def build(\n         ckpt_dir: str,\n         max_seq_len: int,\n         max_batch_size: int,\n         world_size: Optional[int] = None,\n-        quantization_mode: Optional[str] = None,\n+        quantization_mode: Optional[QuantizationMode] = None,\n         seed: int = 1,\n     ):\n         if not torch.distributed.is_initialized():\n@@ -118,7 +89,11 @@ def build(\n         assert model_args.vocab_size == tokenizer.n_words, f\"{model_args.vocab_size=} vs. {tokenizer.n_words=} mismatch\"\n         print(\"Model args:\\n\", model_args.model_dump_json(indent=2))\n \n-        state_dict = load_state_dict(ckpt_paths, model_args)\n+        state_dict = maybe_reshard_state_dict(\n+            ckpt_paths,\n+            n_kv_heads=model_args.n_kv_heads if model_args.n_kv_heads else model_args.n_heads,\n+            moe_num_experts=model_args.moe_args.num_experts,\n+        )\n         print(\"Loaded checkpoint\")\n         if quantization_mode == QuantizationMode.fp8_mixed or quantization_mode == QuantizationMode.int4_mixed:\n             from .quantization.loader import convert_to_quantized_model\n@@ -167,7 +142,7 @@ def generate(\n         params = self.model.args\n \n         print_model_input = print_model_input or os.environ.get(\"LLAMA_MODELS_DEBUG\", \"0\") == \"1\"\n-        if print_model_input and get_model_parallel_rank() == 0:\n+        if print_model_input:\n             cprint(\"Input to model:\\n\", \"yellow\")\n             for inp in llm_inputs:\n                 tokens_to_print = [t for t in inp.tokens]\n@@ -288,7 +263,7 @@ def completion(\n         logprobs: bool = False,\n         echo: bool = False,\n     ) -> Generator[List[GenerationResult], None, None]:\n-        llm_inputs = [self.formatter.encode_contents(c) for c in contents]\n+        llm_inputs = [self.formatter.encode_content(c) for c in contents]\n         for result in self.generate(\n             llm_inputs=llm_inputs,\n             temperature=temperature,\n@@ -297,9 +272,9 @@ def completion(\n             logprobs=logprobs,\n             echo=echo,\n         ):\n+            yield result\n             if all(r.finished for r in result):\n                 break\n-            yield result\n \n     def chat_completion(\n         self,\n@@ -319,9 +294,9 @@ def chat_completion(\n             logprobs=logprobs,\n             echo=echo,\n         ):\n+            yield result\n             if all(r.finished for r in result):\n                 break\n-            yield result\n \n \n def sample_top_p(probs, p):\n\n--- File: models/llama4/quantization/loader.py ---\n@@ -14,7 +14,7 @@\n from torch import Tensor, nn\n from torch.nn import functional as F\n \n-from ..generation import QuantizationMode\n+from ...datatypes import QuantizationMode\n from ..model import Transformer, TransformerBlock\n from ..moe import MoE\n \n\n--- File: models/llama4/scripts/chat_completion.py ---\n@@ -68,25 +68,24 @@ def run_main(\n         #     RawMessage(role=\"user\", content=\"How to go from Beijing to NY?\"),\n         # ],\n     ]\n-    if generator.args.vision_args:\n-        with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n-            img1 = f.read()\n-\n-        with open(THIS_DIR / \"../../resources/pasta.jpeg\", \"rb\") as f:\n-            img2 = f.read()\n-\n-        dialogs.append(\n-            [\n-                RawMessage(\n-                    role=\"user\",\n-                    content=[\n-                        RawMediaItem(data=BytesIO(img1)),\n-                        RawMediaItem(data=BytesIO(img2)),\n-                        RawTextItem(text=\"Write a haiku that brings both images together\"),\n-                    ],\n-                ),\n-            ]\n-        )\n+    with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n+        img1 = f.read()\n+\n+    with open(THIS_DIR / \"../../resources/pasta.jpeg\", \"rb\") as f:\n+        img2 = f.read()\n+\n+    dialogs.append(\n+        [\n+            RawMessage(\n+                role=\"user\",\n+                content=[\n+                    RawMediaItem(data=BytesIO(img1)),\n+                    RawMediaItem(data=BytesIO(img2)),\n+                    RawTextItem(text=\"Write a haiku that brings both images together\"),\n+                ],\n+            ),\n+        ]\n+    )\n \n     for dialog in dialogs:\n         for msg in dialog:"
            },
            {
              "sha": "699a02993512fb36936b1b0741e13c06790bcf98",
              "url": "https://github.com/meta-llama/llama-models/commit/699a02993512fb36936b1b0741e13c06790bcf98",
              "message": "fix: quantization script had regressed (#308)",
              "files_changed": [
                {
                  "filename": "models/llama4/scripts/quantize.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama4/tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/scripts/quantize.py ---\n@@ -13,11 +13,11 @@\n \n import fire\n import torch\n-from models.llama4.args import ModelArgs\n \n+from models.llama4.args import ModelArgs\n from models.llama4.generation import QuantizationMode\n from models.llama4.model import MoE, Transformer, TransformerBlock\n-from models.llama4.quantization.quantize_impls import int4_row_quantize, pack_int4\n+from models.quantize_impls import int4_row_quantize, pack_int4\n \n try:\n     import fbgemm_gpu.experimental.gen_ai  # noqa: F401\n@@ -170,12 +170,6 @@ def ffn_quantize(\n                     int4_scales[f\"{prefix}.experts.{key}\"] = w_scale\n                     print(f\"Quantized {prefix}.experts.{state_dict_key_map[key]} {wq.shape=} {w_scale.shape=}\")\n \n-                new_keys = set(new_state_dict.keys())\n-                assert old_keys == new_keys, f\"old_keys != new_keys: {old_keys - new_keys}\"\n-\n-                print(\"Saving int4 scales\")\n-                int4_scales_path = os.path.join(output_dir, f\"int4_scales_{ckpt_idx}.pt\")\n-                torch.save(int4_scales, int4_scales_path)\n             else:\n                 for key in (\"w1\", \"w3\", \"w2\"):\n                     param = getattr(moe.experts, key)\n@@ -195,12 +189,17 @@ def ffn_quantize(\n                     fp8_scales[f\"{prefix}.experts.{key}\"] = w_scale\n                     print(f\"Quantized {prefix}.experts.{state_dict_key_map[key]} {wq.shape=} {w_scale.shape=}\")\n \n-                new_keys = set(new_state_dict.keys())\n-                assert old_keys == new_keys, f\"old_keys != new_keys: {old_keys - new_keys}\"\n-\n-                print(\"Saving fp8 scales\")\n-                fp8_scales_path = os.path.join(output_dir, f\"fp8_scales_{ckpt_idx}.pt\")\n-                torch.save(fp8_scales, fp8_scales_path)\n+    new_keys = set(new_state_dict.keys())\n+    assert old_keys == new_keys, f\"old_keys != new_keys: {old_keys - new_keys}\"\n+\n+    if quantization_mode == QuantizationMode.int4_mixed:\n+        print(\"Saving int4 scales\")\n+        int4_scales_path = os.path.join(output_dir, f\"int4_scales_{ckpt_idx}.pt\")\n+        torch.save(int4_scales, int4_scales_path)\n+    else:\n+        print(\"Saving fp8 scales\")\n+        fp8_scales_path = os.path.join(output_dir, f\"fp8_scales_{ckpt_idx}.pt\")\n+        torch.save(fp8_scales, fp8_scales_path)\n \n     ckpt_path = os.path.join(\n         output_dir,\n\n--- File: models/llama4/tokenizer.py ---\n@@ -84,14 +84,16 @@ def get_reserved_special_tokens(name, count, start_index=0):\n     \"vision\", 1041, 7\n )  # <|vision_reserved_special_token_7|>, ..., <|vision_reserved_special_token_1047|>\n \n-# 201134, ..., 201141\n+# 201134, ..., 201143\n LLAMA4_REASONING_SPECIAL_TOKENS = [\n     \"<|reasoning_reserved_special_token_0|>\",\n     \"<|reasoning_reserved_special_token_1|>\",\n     \"<|reasoning_reserved_special_token_2|>\",\n     \"<|reasoning_reserved_special_token_3|>\",\n     \"<|reasoning_reserved_special_token_4|>\",\n     \"<|reasoning_reserved_special_token_5|>\",\n+    \"<|reasoning_reserved_special_token_6|>\",\n+    \"<|reasoning_reserved_special_token_7|>\",\n     \"<|reasoning_thinking_start|>\",\n     \"<|reasoning_thinking_end|>\",\n ]"
            },
            {
              "sha": "4f45ca9d0522581aedcb7b2e2231e87b084bbadf",
              "url": "https://github.com/meta-llama/llama-models/commit/4f45ca9d0522581aedcb7b2e2231e87b084bbadf",
              "message": "fix: make sure fp8 quantization only quantizes MoE layers",
              "files_changed": [
                {
                  "filename": "models/llama4/quantization/loader.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/quantization/loader.py ---\n@@ -65,12 +65,12 @@ def should_quantize_block(block: nn.Module) -> bool:\n         if not isinstance(block, TransformerBlock):\n             return False\n \n+        is_moe = isinstance(block.feed_forward, MoE)\n         if quantization_mode == QuantizationMode.fp8_mixed:\n             # skip quantization on first and last layers\n-            return not (block.layer_id == 0 or block.layer_id == (model.n_layers - 1))\n+            return is_moe and not (block.layer_id == 0 or block.layer_id == (model.n_layers - 1))\n \n-        # always skip quantization on dense layers\n-        return isinstance(block.feed_forward, MoE)\n+        return is_moe\n \n     use_rich_progress = use_rich_progress and rank == 0\n     progress, log_status, update_status = logging_callbacks(use_rich_progress, rank, model, should_quantize_block)"
            },
            {
              "sha": "2b2e5b2645c962f92dc004aa868696ec0e53b05c",
              "url": "https://github.com/meta-llama/llama-models/commit/2b2e5b2645c962f92dc004aa868696ec0e53b05c",
              "message": "fix: cleanup MoE docs",
              "files_changed": [
                {
                  "filename": "models/llama4/moe.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/moe.py ---\n@@ -101,31 +101,21 @@ def batched_swiglu(self, x: Tensor, w1: Tensor, w3: Tensor, w2: Tensor) -> Tenso\n \n class MoE(torch.nn.Module):\n     \"\"\"\n-    This EC implementation is modified from the original EC module.\n-    We refactored the token permutation and unpermutation logic and added support to tp and dp2ep sharding.\n-    This module supports 3 sharding methods of the experts:\n-    - tp: each TP rank has n_experts experts. Experts are sharded following the conventional row/column-parallel TP sharding.\n-    - tp2ep: each TP rank has n_experts/tp experts. Experts are not sharded.\n-    - dp2ep: each EP rank has n_experts/ep experts. Experts are sharded following the row/column-parallel TP sharding.\n     Tensors used in this module are annotated with the suffixes that indicate the shape of the tensor.\n     Several commonly used annotations include:\n     - a: bsz*slen\n     - E: number of experts\n     - e: number of local experts per ep (n_experts/ep)\n-    - et: number of local experts per tp (n_experts/tp)\n     - D: hidden dimension\n     - d: D/tp\n     - F: model dimension\n-    - f: F/tp (used in column/row-parallel linear)\n     - G: number of tokens per expert (a * capacity_factor / E)\n     - g: number of tokens per expert per TP rank (i.e., G/TP)\n-    - GG: G*EP (number of tokens per expert received via inter-EP a2a when ag_along_first_dim=False)\n-    - gg: g*EP (number of tokens per expert received via inter-EP a2a when ag_along_first_dim=True)\n \n     Examples:\n     x_aD [a, D]\n     routed_in_etG_D [et*G, D]\n-    x_eGGD: [e, GG, D]\n+    x_eGD: [e, G, D]\n     \"\"\"\n \n     def __init__(\n@@ -208,13 +198,13 @@ def forward(self, x_bsD: Tensor) -> Tensor:  # noqa: N803\n         routed_in_EG_D = routed_in_EG_D * router_scores.reshape(-1, 1)\n \n         out_aD = self.shared_expert(x_aD)\n-        routed_out_egg_D = self.experts(routed_in_EG_D.detach())\n+        routed_out_eg_D = self.experts(routed_in_EG_D.detach())\n \n         router_indices_EG_D = router_indices.reshape(-1, 1).expand(-1, D)\n         out_aD.scatter_add_(\n             dim=0,\n             index=router_indices_EG_D,\n-            src=routed_out_egg_D.view(-1, D),\n+            src=routed_out_eg_D.view(-1, D),\n         )\n         out_aD = reduce_from_model_parallel_region(out_aD)\n         return out_aD.view(-1, slen, D)"
            },
            {
              "sha": "5fdf83110cc9daa7435dfba6eb304892cc0041b8",
              "url": "https://github.com/meta-llama/llama-models/commit/5fdf83110cc9daa7435dfba6eb304892cc0041b8",
              "message": "feat: introduce llama4 support (#299)",
              "files_changed": [
                {
                  "filename": ".github/workflows/gha_workflow_llama_models_tests.yml",
                  "status": "removed"
                },
                {
                  "filename": "MANIFEST.in",
                  "status": "modified"
                },
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/__init__.pyc",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/__init__.pyc",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/args.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/chat_format.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/generation.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/model.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/multimodal/__init__.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/multimodal/encoder_utils.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/multimodal/image_transform.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/multimodal/model.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/multimodal/utils.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/scripts/__init__.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/scripts/chat_completion.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/scripts/completion.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/scripts/multimodal_chat_completion.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/scripts/multimodal_completion.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_tokenizer.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tokenizer.model",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/tokenizer.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/tool_utils.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama4/LICENSE",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/MODEL_CARD.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/USE_POLICY.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/__init__.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama4/args.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/chat_format.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/checkpoint.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/datatypes.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/ffn.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/generation.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/model.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/moe.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/preprocess.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/quantization/__init__.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama4/quantization/loader.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/scripts/chat_completion.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/scripts/completion.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/scripts/quantize.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/tokenizer.model",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/tokenizer.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/vision/embedding.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama4/vision/encoder.py",
                  "status": "added"
                },
                {
                  "filename": "models/quantize_impls.py",
                  "status": "added"
                },
                {
                  "filename": "models/resources/dog.jpg",
                  "status": "renamed"
                },
                {
                  "filename": "models/resources/pasta.jpeg",
                  "status": "renamed"
                },
                {
                  "filename": "pyproject.toml",
                  "status": "modified"
                },
                {
                  "filename": "requirements.txt",
                  "status": "modified"
                },
                {
                  "filename": "uv.lock",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/gha_workflow_llama_models_tests.yml ---\n@@ -1,243 +0,0 @@\n-name: \"Run Llama-models Tests\"\n-\n-on:\n-  # Schedule cron job at 2:30 am EST every day of the week.\n-  # Will run a manual workflow off of 'main' branch.\n-  schedule:\n-    - cron: '30 7 * * *'\n-  pull_request_target:\n-    types: [\"opened\"]\n-    branches:\n-      - 'main'\n-    paths:\n-      - 'models/**/*.py'\n-\n-  workflow_dispatch:\n-    inputs:\n-      runner:\n-        description: 'GHA Runner Scale Set label to run workflow on.'\n-        required: true\n-        default: \"llama-models-gha-runner-gpu\"\n-\n-      checkout_reference:\n-        description: \"The branch, tag, or SHA to checkout\"\n-        required: true\n-        default: \"main\"\n-\n-      debug:\n-        description: 'Run debugging steps?'\n-        required: false\n-        default: \"true\"\n-\n-      sleep_time:\n-        description: '[DEBUG] sleep time for debugging'\n-        required: true\n-        default: \"0\"\n-\n-      require_model:\n-        description: 'Is a model required?'\n-        required: true\n-        default: \"true\"\n-\n-      model_vision:\n-        description: 'Llama vision model ID'\n-        required: false\n-        default: \"Llama3.2-11B-Vision-Instruct\"\n-\n-      model_text:\n-        description: 'Llama text model ID'\n-        required: false\n-        default: \"Llama3.2-3B-Instruct\"\n-\n-      api_key:\n-        description: 'Provider API key'\n-        required: false\n-        default: \"---\"\n-\n-env:\n-  TOKENIZER_PATH: \"models/llama3/api/tokenizer.model\"\n-  MODELS_PATH: \"/data/llama\"\n-  VISION_MODEL_CHECKPOINT_DIR: \"/data/llama/${{ inputs.model_vision }}\"\n-  TEXT_MODEL_CHECKPOINT_DIR: \"/data/llama/${{ inputs.model_text }}\"\n-  API_KEY: \"${{ inputs.api_key || '' }}\"\n-\n-jobs:\n-  execute_workflow:\n-    name: Execute workload on Self-Hosted CPU k8s runner\n-    permissions:\n-      pull-requests: write\n-    defaults:\n-      run:\n-        shell: bash # default shell to run all steps for a given job.\n-    runs-on: ${{ inputs.runner != '' && inputs.runner || 'llama-models-gha-runner-cpu' }}\n-    if: always()\n-    steps:\n-\n-      ##############################\n-      #### INITIAL DEBUG CHECKS ####\n-      ##############################\n-      - name: \"[DEBUG] Check content of the EFS mount\"\n-        id: debug_efs_volume\n-        continue-on-error: true\n-        if: inputs.debug == 'true'\n-        run: |\n-            echo \"========= Content of the EFS mount =============\"\n-            ls -la ${{ env.MODELS_PATH }}\n-\n-      - name: \"Check if models exist in EFS volume\"\n-        id: check_if_models_exist\n-        if: ${{ inputs.require_model == 'true' }}\n-        run: |\n-          # Check if vision model is provided and exists\n-          if [ -n \"${{ inputs.model_vision }}\" ]; then\n-            if [ ! -d \"${{ env.VISION_MODEL_CHECKPOINT_DIR }}\" ]; then\n-              echo \"Model '${{ inputs.model_vision }}' does not exist in mounted EFS volume, Terminating workflow.\"\n-              exit 1\n-            else\n-              echo \"Content of '${{ inputs.model_vision }}' model\"\n-              ls -la \"${{ env.VISION_MODEL_CHECKPOINT_DIR }}\"\n-            fi\n-          fi\n-\n-          # Check if text model is provided and exists\n-          if [ -n \"${{ inputs.model_text }}\" ]; then\n-            if [ ! -d \"${{ env.TEXT_MODEL_CHECKPOINT_DIR }}\" ]; then\n-              echo \"Model '${{ inputs.model_text }}' does not exist in mounted EFS volume, Terminating workflow.\"\n-              exit 1\n-            else\n-              echo \"Content of '${{ inputs.model_text }}' model\"\n-              ls -la \"${{ env.TEXT_MODEL_CHECKPOINT_DIR }}\"\n-            fi\n-          fi\n-\n-      - name: \"[DEBUG] Get runner container OS information\"\n-        id: debug_os_info\n-        if: ${{ inputs.debug == 'true' }}\n-        run: |\n-            cat /etc/os-release\n-\n-      #######################\n-      #### CODE CHECKOUT ####\n-      #######################\n-      - name: \"Checkout 'meta-llama/llama-models' repository\"\n-        id: checkout_repo\n-        uses: actions/checkout@v4\n-        with:\n-          ref: ${{ inputs.checkout_reference }}\n-\n-      - name: \"[DEBUG] Content of the repository after checkout\"\n-        id: debug_content_after_checkout\n-        if: ${{ inputs.debug == 'true' }}\n-        run: |\n-            ls -la ${GITHUB_WORKSPACE}\n-\n-      ##########################################################\n-      ####              OPTIONAL SLEEP DEBUG                ####\n-      #                                                        #\n-      # Use to \"exec\" into the test k8s POD and run tests      #\n-      # manually to identify what dependencies are being used. #\n-      #                                                        #\n-      ##########################################################\n-      - name: \"[DEBUG] sleep\"\n-        id: debug_sleep\n-        if: ${{ inputs.debug == 'true' && inputs.sleep_time != '' }}\n-        run: |\n-            sleep ${{ inputs.sleep_time }}\n-\n-      ##################################\n-      #### DEPENDENCY INSTALLATIONS ####\n-      ##################################\n-      - name: \"Installing 'apt' required packages\"\n-        id: install_apt\n-        run: |\n-          echo \"[STEP] Installing 'apt' required packages\"\n-          sudo apt update -y\n-          sudo apt upgrade -y\n-          sudo apt install python3-pip -y\n-\n-      - name: \"Installing 'llama-models' dependencies\"\n-        id: install_pip_generic\n-        run: |\n-          echo \"[STEP] Installing 'llama-models' models\"\n-          pip install -U pip setuptools\n-          pip install -r requirements.txt\n-          pip install blobfile\n-          pip install llama-models\n-          pip install xmlrunner\n-          pip install pytest\n-          pip install numpy\n-\n-      - name: \"Installing specific manual_dispatch dependencies\"\n-        id: manual_install_pip\n-        if: github.event_name == 'workflow_dispatch'\n-        run: |\n-          echo \"[STEP] Installing specific dependencies for manual dispatch workflows\"\n-          pip install numpy\n-          pip install torch\n-          pip install fairscale\n-          pip install termcolor\n-          pip install torchvision\n-\n-      ############################################\n-      #### AUTOMATIC TESTING ON PULL REQUESTS ####\n-      ############################################\n-\n-      #### Run tests ####\n-      - name: \"PR - Run Tests\"\n-        id: pr_run_tests\n-        working-directory: \"${{ github.workspace }}\"\n-        if: github.event_name == 'pull_request_target'\n-        run: |\n-          echo \"[STEP] Running PyTest tests at 'GITHUB_WORKSPACE' path: ${GITHUB_WORKSPACE} | path: ${{ github.workspace }}\"\n-          python3 -m pytest --ignore=models/llama3/tests/api/test_generation.py --ignore=llama_models/llama3/tests/api/test_generation.py --junitxml=\"${{ github.workspace }}/result.xml\"\n-\n-      #### Create test summary ####\n-\n-      - name: \"PR - Test Summary\"\n-        id: pr_test_summary_create\n-        if: github.event_name == 'pull_request_target'\n-        uses: test-summary/action@v2\n-        with:\n-          paths: \"${{ github.workspace }}/result.xml\"\n-          output: test-summary.md\n-\n-      - name: \"PR - Upload Test Summary\"\n-        id: pr_test_summary_upload\n-        if: github.event_name == 'pull_request_target'\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test-summary\n-          path: test-summary.md\n-\n-      #### Update PR request ####\n-\n-      - name: \"PR - Update comment\"\n-        id: pr_update_comment\n-        if: github.event_name == 'pull_request_target'\n-        uses: thollander/actions-comment-pull-request@v2\n-        with:\n-          filePath: test-summary.md\n-\n-      ########################\n-      #### MANUAL TESTING ####\n-      ########################\n-\n-      #### Run tests ####\n-\n-      - name: \"Manual - Run Tests\"\n-        id: manual_run_tests\n-        working-directory: \"${{ github.workspace }}\"\n-        if: github.event_name == 'workflow_dispatch'\n-        run: |\n-          echo \"[STEP] Running PyTest tests at 'GITHUB_WORKSPACE' path: ${GITHUB_WORKSPACE} | path: ${{ github.workspace }}\"\n-          free -m\n-          python3 -m pytest --junitxml=\"${{ github.workspace }}/result.xml\"\n-\n-      #### Create test summary ####\n-\n-      - name: \"Manual - Test Summary\"\n-        id: manual_test_summary\n-        if: always() && github.event_name == 'workflow_dispatch'\n-        uses: test-summary/action@v2\n-        with:\n-          paths: \"${{ github.workspace }}/result.xml\"\n\n--- File: MANIFEST.in ---\n@@ -1,8 +1,16 @@\n include pyproject.toml\n include README.md\n-include llama_models/llama3/api/tokenizer.model\n-include llama_models/scripts/resources/dog.jpg\n-include llama_models/scripts/resources/pasta.jpeg\n+include models/llama3/tokenizer.model\n+include models/llama4/tokenizer.model\n+include models/resources/dog.jpg\n+include models/resources/pasta.jpeg\n+include models/llama3_1/prompt_format.md\n+include models/llama3_2/text_prompt_format.md\n+include models/llama3_2/vision_prompt_format.md\n+include llama_models/llama3/tokenizer.model\n+include llama_models/llama4/tokenizer.model\n+include llama_models/resources/dog.jpg\n+include llama_models/resources/pasta.jpeg\n include llama_models/llama3_1/prompt_format.md\n include llama_models/llama3_2/text_prompt_format.md\n include llama_models/llama3_2/vision_prompt_format.md\n\n--- File: README.md ---\n@@ -3,7 +3,7 @@\n </p>\n \n <p align=\"center\">\n-         <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp\n+         <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp | <a href=\"https://github.com/meta-llama/llama-cookbook\">Llama Cookbook</a>&nbsp\n <br>\n \n ---\n@@ -29,6 +29,8 @@ Our mission is to empower individuals and industry through this opportunity whil\n | Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n | Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n | Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n+| Llama 3.3 | 12/04/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |\n+| Llama 4 | 4/5/2025 | Scout-17B-16E, Maverick-17B-128E | 10M, 1M | TikToken-based | [Use Policy](models/llama4/USE_POLICY.md) | [License](models/llama4/LICENSE) | [Model Card](models/llama4/MODEL_CARD.md) |\n \n ## Download\n \n@@ -48,60 +50,97 @@ Remember that the links expire after 24 hours and a certain amount of downloads.\n \n ## Running the models\n \n-You need to `pip install llama_models[torch]` to run the models. After installing the dependencies, you can run the example scripts (within `llama_models/scripts/` sub-directory) as follows:\n-```bash\n-#!/bin/bash\n+In order to run the models, you will need to install dependencies after checking out the repository.\n \n-CHECKPOINT_DIR=~/.llama/checkpoints/Meta-Llama3.1-8B-Instruct\n-PYTHONPATH=$(git rev-parse --show-toplevel) torchrun llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n+```bash\n+# Run this within a suitable Python environment (uv, conda, or virtualenv)\n+pip install -e .[torch]\n ```\n \n-The above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `llama_models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n+Example scripts are available in `models/{ llama3, llama4 }/scripts/` sub-directory. Note that the Llama4 series of models require at least 4 GPUs to run inference at full (bf16) precision.\n \n-For running larger models with tensor parallelism, you should modify as:\n ```bash\n #!/bin/bash\n \n-NGPUS=8\n-PYTHONPATH=$(git rev-parse --show-toplevel) torchrun \\\n-  --nproc_per_node=$NGPUS \\\n-  llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR \\\n-  --model_parallel_size $NGPUS\n+NGPUS=4\n+CHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\n+PYTHONPATH=$(git rev-parse --show-toplevel) \\\n+  torchrun --nproc_per_node=$NGPUS \\\n+  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n+  --world_size $NGPUS\n ```\n \n-For more flexibility in running inference (including running FP8 inference), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) repository.\n+The above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `models.llama4.scripts.completion`.\n+\n+\n+## Running inference with FP8 and Int4 Quantization\n+\n+You can reduce the memory footprint of the models at the cost of minimal loss in accuracy by running inference with FP8 or Int4 quantization. Use the `--quantization-mode` flag to specify the quantization mode. There are two modes:\n+- `fp8_mixed`: Mixed precision inference with FP8 for some weights and bfloat16 for activations.\n+- `int4_mixed`: Mixed precision inference with Int4 for some weights and bfloat16 for activations.\n+\n+Using FP8, running Llama-4-Scout-17B-16E-Instruct requires 2 GPUs with 80GB of memory. Using Int4, you need a single GPU with 80GB of memory.\n+\n+```bash\n+MODE=fp8_mixed  # or int4_mixed\n+if [ $MODE == \"fp8_mixed\" ]; then\n+  NGPUS=2\n+else\n+  NGPUS=1\n+fi\n+CHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\n+PYTHONPATH=$(git rev-parse --show-toplevel) \\\n+  torchrun --nproc_per_node=$NGPUS \\\n+  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n+  --world_size $NGPUS \\\n+  --quantization-mode $MODE\n+```\n+\n+\n+For more flexibility in running inference (including using other providers), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) toolset.\n \n \n ## Access to Hugging Face\n \n We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n \n-- Visit one of the repos, for example [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n+- Visit one of the repos, for example [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).\n - Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n - To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n \n ```bash\n-huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n+huggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\n ```\n \n-**NOTE** The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.\n-\n-\n - To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n \n   ```python\n-  import transformers\n+  #inference.py\n+  from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n   import torch\n \n-  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n+  model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+  tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n-  pipeline = transformers.pipeline(\n-      \"text-generation\",\n-      model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-      model_kwargs={\"torch_dtype\": torch.bfloat16},\n-      device=\"cuda\",\n+  messages = [\n+      {\"role\": \"user\", \"content\": \"Who are you?\"},\n+  ]\n+  inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+\n+  model = Llama4ForConditionalGeneration.from_pretrained(\n+      model_id,\n+      device_map=\"auto\",\n+      torch_dtype=torch.bfloat16\n   )\n+\n+  outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n+  outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+  print(outputs[0])\n   ```\n+  ```bash\n+   torchrun --nnodes=1 --nproc_per_node=8 inference.py\n+   ```\n \n ## Installations\n \n\n--- File: models/llama3/args.py ---\n@@ -65,6 +65,8 @@ def __init__(self, **kwargs):\n                 setattr(self, k, LoRAArgs(**v))\n             elif k == \"quantization_args\":\n                 setattr(self, k, QuantizationArgs(**v))\n+            elif k == \"vision_model\" and \"cross_attention_adapter\" in v:\n+                self.vision_num_cross_attention_layers = v[\"cross_attention_adapter\"][\"num_layers\"]\n             else:\n                 if hasattr(self, k):\n                     setattr(self, k, v)\n\n--- File: models/llama3/chat_format.py ---\n@@ -7,13 +7,12 @@\n \n import io\n import uuid\n-\n from dataclasses import dataclass\n from typing import Dict, List, Optional, Tuple\n \n from PIL import Image as PIL_Image\n \n-from ...datatypes import (\n+from ..datatypes import (\n     BuiltinTool,\n     RawContent,\n     RawMediaItem,\n@@ -24,9 +23,7 @@\n     ToolCall,\n     ToolPromptFormat,\n )\n-\n from .tokenizer import Tokenizer\n-\n from .tool_utils import ToolUtils\n \n \n\n--- File: models/llama3/generation.py ---\n@@ -20,7 +20,7 @@\n import time\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Generator, List, Optional\n+from typing import Callable, Generator, List, Optional\n \n import torch\n import torch.nn.functional as F\n@@ -31,12 +31,11 @@\n )\n from termcolor import cprint\n \n-from ...datatypes import RawContent, RawMessage, StopReason, ToolPromptFormat\n-\n-from ..api.args import ModelArgs\n-from ..api.chat_format import ChatFormat, LLMInput\n-from ..api.tokenizer import Tokenizer\n+from ..datatypes import RawContent, RawMessage, StopReason, ToolPromptFormat\n+from .args import ModelArgs\n+from .chat_format import ChatFormat, LLMInput\n from .model import Transformer\n+from .tokenizer import Tokenizer\n \n \n @dataclass\n@@ -66,7 +65,7 @@ def build(\n         ckpt_dir: str,\n         max_seq_len: int,\n         max_batch_size: int,\n-        model_parallel_size: Optional[int] = None,\n+        world_size: Optional[int] = None,\n         tokenizer_path: Optional[str] = None,\n         seed: int = 1,\n         device: str = \"cuda\",\n@@ -79,7 +78,7 @@ def build(\n             tokenizer_path (str): Path to the tokenizer file.\n             max_seq_len (int): Maximum sequence length for input text.\n             max_batch_size (int): Maximum batch size for inference.\n-            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n+            world_size (Optional[int], optional): Number of model parallel processes.\n                 If not provided, it's determined from the environment. Defaults to None.\n             device (str, optional): Device to use, e.g. cuda (default), xpu, cpu, etc.\n \n@@ -113,9 +112,9 @@ def build(\n                 torch.distributed.init_process_group(\"gloo\")\n \n         if not model_parallel_is_initialized():\n-            if model_parallel_size is None:\n-                model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n-            initialize_model_parallel(model_parallel_size)\n+            if world_size is None:\n+                world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n+            initialize_model_parallel(world_size)\n \n         local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n         if device.type == \"cuda\":\n@@ -132,8 +131,8 @@ def build(\n \n         checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n         assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n-        assert model_parallel_size == len(checkpoints), (\n-            f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n+        assert world_size == len(checkpoints), (\n+            f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n         )\n         ckpt_path = checkpoints[get_model_parallel_rank()]\n         checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n@@ -194,6 +193,7 @@ def generate(\n         logprobs: bool = False,\n         echo: bool = False,\n         print_model_input: bool = False,\n+        logits_processor: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n     ) -> Generator:\n         params = self.model.params\n \n@@ -264,6 +264,9 @@ def generate(\n             else:\n                 logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n \n+            if logits_processor is not None:\n+                logits = logits_processor(tokens[:, :cur_pos], logits)\n+\n             if temperature > 0:\n                 probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n                 next_token = sample_top_p(probs, top_p)\n\n--- File: models/llama3/model.py ---\n@@ -21,7 +21,7 @@\n )\n from torch import nn\n \n-from ..api import ModelArgs\n+from .args import ModelArgs\n \n # **NOTE**: This code is not runnable without installing `torch` and `fairscale`\n # dependencies. These dependencies are not part of the default dependencies\n@@ -109,9 +109,9 @@ class Attention(nn.Module):\n     def __init__(self, args: ModelArgs):\n         super().__init__()\n         self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n-        model_parallel_size = fs_init.get_model_parallel_world_size()\n-        self.n_local_heads = args.n_heads // model_parallel_size\n-        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n+        world_size = fs_init.get_model_parallel_world_size()\n+        self.n_local_heads = args.n_heads // world_size\n+        self.n_local_kv_heads = self.n_kv_heads // world_size\n         self.n_rep = self.n_local_heads // self.n_local_kv_heads\n         self.head_dim = args.dim // args.n_heads\n \n\n--- File: models/llama3/multimodal/model.py ---\n@@ -179,14 +179,14 @@ def __init__(\n         n_heads,\n     ):\n         super().__init__()\n-        model_parallel_size = fs_init.get_model_parallel_world_size()\n+        world_size = fs_init.get_model_parallel_world_size()\n         qkvo_replication = 1\n-        if model_parallel_size > 16:\n-            qkvo_replication = model_parallel_size // 8\n+        if world_size > 16:\n+            qkvo_replication = world_size // 8\n \n         self.n_kv_heads = n_heads\n-        self.n_local_heads = n_heads * qkvo_replication // model_parallel_size\n-        self.n_local_kv_heads = self.n_kv_heads * qkvo_replication // model_parallel_size\n+        self.n_local_heads = n_heads * qkvo_replication // world_size\n+        self.n_local_kv_heads = self.n_kv_heads * qkvo_replication // world_size\n         self.n_rep = self.n_local_heads // self.n_local_kv_heads\n         self.head_dim = dim // n_heads\n \n@@ -536,16 +536,16 @@ def __init__(self, args: ModelArgs):\n             cache_v (torch.Tensor): Cached values for attention.\n         \"\"\"\n         super().__init__()\n-        model_parallel_size = fs_init.get_model_parallel_world_size()\n+        world_size = fs_init.get_model_parallel_world_size()\n         replication_factor = 1\n-        if model_parallel_size > 8:\n-            replication_factor = model_parallel_size // MP_SCALE\n+        if world_size > 8:\n+            replication_factor = world_size // MP_SCALE\n \n         self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n         self.n_kv_heads *= replication_factor\n \n-        self.n_local_heads = args.n_heads // model_parallel_size\n-        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n+        self.n_local_heads = args.n_heads // world_size\n+        self.n_local_kv_heads = self.n_kv_heads // world_size\n         self.n_rep = self.n_local_heads // self.n_local_kv_heads\n         self.head_dim = args.dim // args.n_heads\n         self.max_seq_len = args.max_seq_len\n@@ -832,10 +832,10 @@ def __init__(\n         norm_eps: float,\n     ):\n         super().__init__()\n-        self.model_parallel_size = fs_init.get_model_parallel_world_size()\n+        self.world_size = fs_init.get_model_parallel_world_size()\n         replication_factor = 1\n-        if self.model_parallel_size > 8:\n-            replication_factor = self.model_parallel_size // MP_SCALE\n+        if self.world_size > 8:\n+            replication_factor = self.world_size // MP_SCALE\n         n_kv_heads *= replication_factor\n \n         assert n_heads % n_kv_heads == 0\n@@ -889,10 +889,10 @@ def __init__(\n         # trunk LLM (i.e., group query attention) -- @dubeya\n         # local heads\n         assert self.n_heads % self.n_kv_heads == 0\n-        assert self.n_heads % self.model_parallel_size == 0\n-        assert self.n_kv_heads % self.model_parallel_size == 0\n-        self.n_local_heads = self.n_heads // self.model_parallel_size\n-        self.n_local_kv_heads = self.n_kv_heads // self.model_parallel_size\n+        assert self.n_heads % self.world_size == 0\n+        assert self.n_kv_heads % self.world_size == 0\n+        self.n_local_heads = self.n_heads // self.world_size\n+        self.n_local_kv_heads = self.n_kv_heads // self.world_size\n         self.n_rep = self.n_local_heads // self.n_local_kv_heads\n \n     def _compute_xattn_kv_cache(self, xattn_tokens: torch.Tensor) -> torch.Tensor:\n@@ -1076,15 +1076,15 @@ class CrossAttentionTransformerText(torch.nn.Module):\n \n     def __init__(self, args: ModelArgs) -> None:\n         super().__init__()\n-        self.model_parallel_size = fs_init.get_model_parallel_world_size()\n+        self.world_size = fs_init.get_model_parallel_world_size()\n         assert args.vocab_size > 0\n         self.vocab_size = args.vocab_size\n         self.n_layers = args.n_layers\n         self.dim = args.dim\n         self.head_dim = args.dim // args.n_heads\n         self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n-        self.n_local_kv_heads = self.n_kv_heads // self.model_parallel_size\n-        assert self.vocab_size % self.model_parallel_size == 0\n+        self.n_local_kv_heads = self.n_kv_heads // self.world_size\n+        assert self.vocab_size % self.world_size == 0\n         self.tok_embeddings = VocabParallelEmbedding(args.vocab_size, args.dim, init_method=lambda x: x)\n         self.pos_embeddings = None\n         # final norm layer (not necessary for post-norm)\n\n--- File: models/llama3/scripts/chat_completion.py ---\n@@ -12,9 +12,8 @@\n \n import fire\n \n-from llama_models.datatypes import RawMessage, StopReason\n-\n-from llama_models.llama3.reference_impl.generation import Llama\n+from models.datatypes import RawMessage, StopReason\n+from models.llama3.generation import Llama\n \n \n def run_main(\n@@ -24,7 +23,7 @@ def run_main(\n     max_seq_len: int = 512,\n     max_batch_size: int = 4,\n     max_gen_len: Optional[int] = None,\n-    model_parallel_size: Optional[int] = None,\n+    world_size: Optional[int] = None,\n ):\n     \"\"\"\n     Examples to run with the models finetuned for chat. Prompts correspond of chat\n@@ -39,7 +38,7 @@ def run_main(\n         ckpt_dir=ckpt_dir,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n-        model_parallel_size=model_parallel_size,\n+        world_size=world_size,\n     )\n \n     dialogs = [\n\n--- File: models/llama3/scripts/completion.py ---\n@@ -11,10 +11,10 @@\n from typing import Optional\n \n import fire\n-\n-from llama_models.llama3.reference_impl.generation import Llama\n from termcolor import cprint\n \n+from models.llama3.generation import Llama\n+\n \n def run_main(\n     ckpt_dir: str,\n@@ -23,13 +23,13 @@ def run_main(\n     max_seq_len: int = 512,\n     max_batch_size: int = 4,\n     max_gen_len: int = 64,\n-    model_parallel_size: Optional[int] = None,\n+    world_size: Optional[int] = None,\n ):\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n-        model_parallel_size=model_parallel_size,\n+        world_size=world_size,\n     )\n \n     prompts = [\n\n--- File: models/llama3/scripts/multimodal_chat_completion.py ---\n@@ -13,9 +13,9 @@\n from typing import Optional\n \n import fire\n-from llama_models.datatypes import RawMediaItem, RawMessage, RawTextItem\n \n-from llama_models.llama3.reference_impl.generation import Llama\n+from models.datatypes import RawMediaItem, RawMessage, RawTextItem\n+from models.llama3.generation import Llama\n \n THIS_DIR = Path(__file__).parent\n \n@@ -27,18 +27,18 @@ def run_main(\n     max_seq_len: int = 512,\n     max_batch_size: int = 4,\n     max_gen_len: Optional[int] = None,\n-    model_parallel_size: Optional[int] = None,\n+    world_size: Optional[int] = None,\n ):\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n-        model_parallel_size=model_parallel_size,\n+        world_size=world_size,\n     )\n \n     # image understanding\n     dialogs = []\n-    with open(THIS_DIR / \"resources/dog.jpg\", \"rb\") as f:\n+    with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n         img = f.read()\n \n     dialogs = [\n\n--- File: models/llama3/scripts/multimodal_completion.py ---\n@@ -13,12 +13,10 @@\n from typing import Optional\n \n import fire\n-from llama_models.datatypes import RawMediaItem\n-\n-from llama_models.llama3.reference_impl.generation import Llama\n-\n from termcolor import cprint\n \n+from models.datatypes import RawMediaItem\n+from models.llama3.generation import Llama\n \n THIS_DIR = Path(__file__).parent\n \n@@ -30,16 +28,16 @@ def run_main(\n     max_seq_len: int = 512,\n     max_batch_size: int = 4,\n     max_gen_len: Optional[int] = None,\n-    model_parallel_size: Optional[int] = None,\n+    world_size: Optional[int] = None,\n ):\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n-        model_parallel_size=model_parallel_size,\n+        world_size=world_size,\n     )\n \n-    with open(THIS_DIR / \"resources/dog.jpg\", \"rb\") as f:\n+    with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n         img = f.read()\n \n     interleaved_contents = [\n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -7,15 +7,14 @@\n \n import os\n import unittest\n-\n from pathlib import Path\n \n import numpy as np\n import pytest\n import torch\n-from llama_models.datatypes import RawMediaItem, RawMessage, RawTextItem\n \n-from llama_models.llama3.reference_impl.generation import Llama\n+from llama_models.datatypes import RawMediaItem, RawMessage, RawTextItem\n+from llama_models.llama3.generation import Llama\n \n THIS_DIR = Path(__file__).parent\n \n@@ -39,9 +38,7 @@ def build_generator(env_var: str, device: str):\n     os.environ[\"WORLD_SIZE\"] = \"1\"\n     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n     os.environ[\"MASTER_PORT\"] = \"29501\"\n-    return Llama.build(\n-        ckpt_dir=os.environ[env_var], max_seq_len=128, max_batch_size=1, model_parallel_size=1, device=device\n-    )\n+    return Llama.build(ckpt_dir=os.environ[env_var], max_seq_len=128, max_batch_size=1, world_size=1, device=device)\n \n \n class TestTextModelInference(unittest.TestCase):\n\n--- File: models/llama3/tests/api/test_tokenizer.py ---\n@@ -10,10 +10,9 @@\n \n from unittest import TestCase\n \n-from llama_models.datatypes import BuiltinTool, RawMessage, ToolCall, ToolPromptFormat\n-\n-from llama_models.llama3.api.chat_format import ChatFormat\n-from llama_models.llama3.api.tokenizer import Tokenizer\n+from llama_models.datatypes import RawMessage, ToolPromptFormat\n+from llama_models.llama3.chat_format import ChatFormat\n+from llama_models.llama3.tokenizer import Tokenizer\n \n \n class TokenizerTests(TestCase):\n@@ -63,51 +62,6 @@ def test_encode_message(self):\n             ],\n         )\n \n-    def test_encode_message_with_tool_call(self):\n-        message = RawMessage(\n-            role=\"assistant\",\n-            content=\"\",\n-            tool_calls=[\n-                ToolCall(\n-                    tool_name=BuiltinTool.code_interpreter,\n-                    arguments={\"code\": \"print('Hello, world!')\"},\n-                    call_id=\"1\",\n-                )\n-            ],\n-        )\n-        self.assertEqual(\n-            self.format.encode_message(message, tool_prompt_format=ToolPromptFormat.python_list)[0][:5],\n-            [\n-                self.format.tokenizer.special_tokens[\"<|start_header_id|>\"],\n-                self.format.tokenizer.encode(message.role, bos=False, eos=False)[0],\n-                self.format.tokenizer.special_tokens[\"<|end_header_id|>\"],\n-                self.format.tokenizer.encode(\"\\n\\n\", bos=False, eos=False)[0],\n-                self.format.tokenizer.special_tokens[\"<|python_tag|>\"],\n-            ],\n-        )\n-        message = RawMessage(\n-            role=\"assistant\",\n-            content=\"\",\n-            tool_calls=[\n-                ToolCall(\n-                    tool_name=\"custom_tool\",\n-                    arguments={\"some_param\": \"value\"},\n-                    call_id=\"1\",\n-                )\n-            ],\n-        )\n-        self.assertEqual(\n-            self.format.encode_message(message, tool_prompt_format=ToolPromptFormat.python_list)[0][:5],\n-            [\n-                self.format.tokenizer.special_tokens[\"<|start_header_id|>\"],\n-                self.format.tokenizer.encode(message.role, bos=False, eos=False)[0],\n-                self.format.tokenizer.special_tokens[\"<|end_header_id|>\"],\n-                self.format.tokenizer.encode(\"\\n\\n\", bos=False, eos=False)[0],\n-                # beginning of `[custom_tool(...)]`\n-                self.format.tokenizer.encode(\"[\", bos=False, eos=False)[0],\n-            ],\n-        )\n-\n     def test_encode_dialog(self):\n         messages = [\n             RawMessage(\n\n--- File: models/llama3/tests/api/test_tool_utils.py ---\n@@ -6,10 +6,10 @@\n # the top-level of this source tree.\n import unittest\n \n-from llama_models.llama3.api.tool_utils import (\n+from llama_models.llama3.tool_utils import (\n+    ToolUtils,\n     is_valid_python_list,\n     parse_python_list_for_function_calls,\n-    ToolUtils,\n )\n \n \n\n--- File: models/llama3/tokenizer.py ---\n@@ -13,7 +13,6 @@\n from pathlib import Path\n from typing import (\n     AbstractSet,\n-    cast,\n     Collection,\n     Dict,\n     Iterator,\n@@ -22,10 +21,10 @@\n     Optional,\n     Sequence,\n     Union,\n+    cast,\n )\n \n import tiktoken\n-\n from tiktoken.load import load_tiktoken_bpe\n \n logger = getLogger(__name__)\n\n--- File: models/llama3/tool_utils.py ---\n@@ -9,8 +9,7 @@\n import re\n from typing import Optional, Tuple\n \n-from ...datatypes import BuiltinTool, RecursiveType, ToolCall, ToolPromptFormat\n-\n+from ..datatypes import BuiltinTool, RecursiveType, ToolCall, ToolPromptFormat\n \n BUILTIN_TOOL_PATTERN = r'\\b(?P<tool_name>\\w+)\\.call\\(query=\"(?P<query>[^\"]*)\"\\)'\n CUSTOM_TOOL_CALL_PATTERN = re.compile(r\"<function=(?P<function_name>[^}]+)>(?P<args>{.*?})\")\n\n--- File: models/llama4/LICENSE ---\n@@ -0,0 +1,70 @@\n+LLAMA 4 COMMUNITY LICENSE AGREEMENT\n+\n+Llama 4 Version Effective Date: April 5, 2025\n+\n+Agreement means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n+\n+\n+Documentation means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at https://www.llama.com/docs/overview.\n+\n+\n+Licensee or you means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entitys behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n+\n+\n+Llama 4 means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://www.llama.com/llama-downloads.\n+\n+\n+Llama Materials means, collectively, Metas proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\n+\n+\n+Meta or we means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \n+\n+\n+By clicking I Accept below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n+\n+\n+1. License Rights and Redistribution.\n+\n+\n+a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metas intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.  \n+\n+\n+b. Redistribution and Use.  \n+\n+\n+i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display Built with Llama on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include Llama at the beginning of any such AI model name.\n+\n+\n+ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you. \n+\n+\n+iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a Notice text file distributed as a part of such copies: Llama 4 is licensed under the Llama 4 Community License, Copyright  Meta Platforms, Inc. All Rights Reserved.\n+\n+\n+iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama4/use-policy), which is hereby incorporated by reference into this Agreement.\n+  \n+2. Additional Commercial Terms. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensees affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n+\n+\n+3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN AS IS BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n+\n+\n+4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n+\n+\n+5. Intellectual Property.\n+\n+\n+a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use Llama (the Mark) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Metas brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n+\n+\n+b. Subject to Metas ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n+\n+\n+c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n+\n+\n+6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement. \n+\n+\n+7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n\n--- File: models/llama4/MODEL_CARD.md ---\n@@ -0,0 +1,407 @@\n+## Model Information\n+\n+The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. \n+\n+These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n+\n+**Model developer**: Meta\n+\n+**Model Architecture:**  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. \n+\n+<table>\n+  <tr>\n+    <th>Model Name</th>\n+    <th>Training Data </th>\n+    <th>Params</th>\n+    <th>Input modalities</th>\n+    <th>Output modalities</th>\n+    <th>Context length</th>\n+    <th>Token count</th>\n+    <th>Knowledge cutoff</th>\n+  </tr>\n+  <tr>\n+    <td>Llama 4 Scout (17Bx16E) </td>\n+    <td rowspan=\"2\">A mix of publicly available, licensed data and information from Metas products and services. This includes publicly shared posts from Instagram and Facebook and peoples interactions with Meta AI. Learn more in our <a href=\"https://www.facebook.com/privacy/guide/genai/\">Privacy Center</a>.\n+    </td>\n+    <td>17B (Activated)\n+        109B (Total)\n+    </td>\n+    <td>Multilingual text and image</td>\n+    <td>Multilingual text and code</td>\n+    <td>10M</td>\n+    <td>~40T</td>\n+    <td>August 2024</td>\n+  </tr>\n+  <tr>\n+    <td>Llama 4 Maverick (17Bx128E)</td>\n+    <td>17B (Activated)\n+        400B (Total)\n+    </td>\n+    <td>Multilingual text and image</td>\n+    <td>Multilingual text and code</td>\n+    <td>1M</td>\n+    <td>~22T</td>\n+    <td>August 2024</td>\n+  </tr>\n+</table>\n+\n+**Supported languages:** Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. \n+\n+**Model Release Date:** April 7, 2025\n+\n+**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\n+\n+**License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](/models/llama4/LICENSE)\n+\n+**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go [here](https://github.com/meta-llama/llama-cookbook).\n+\n+## Intended Use\n+\n+**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. \n+\n+**Out-of-scope**: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card\\*\\*.\n+\n+\\*\\*Note: \n+\n+1\\. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes [200 total languages](https://ai.meta.com/research/no-language-left-behind/)). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n+\n+2\\. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\n+\n+## Hardware and Software\n+\n+**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n+\n+**Training Energy Use:**  Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n+\n+## \n+\n+**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **1,999 tons** CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n+\n+| Model Name | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n+| :---- | :---: | :---: | :---: | :---: |\n+| Llama 4 Scout | 5.0M | 700 | 1,354 | 0 |\n+| Llama 4 Maverick | 2.38M | 700 | 645 | 0 |\n+| Total | 7.38M | \\- | 1,999 | 0 |\n+\n+The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n+\n+## Training Data\n+\n+**Overview:** Llama 4 Scout was pretrained on \\~40 trillion tokens and Llama 4 Maverick was pretrained on \\~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Metas products and services. This includes publicly shared posts from Instagram and Facebook and peoples interactions with Meta AI.\n+\n+**Data Freshness:** The pretraining data has a cutoff of August 2024\\.\n+\n+## Benchmarks\n+\n+In this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\n+\n+### Pre-trained models\n+\n+##\n+\n+\n+<table>\n+  <tr>\n+    <th>Category</th>\n+    <th>Benchmark</th>\n+    <th># Shots</th>\n+    <th>Metric</th>\n+    <th>Llama 3.1 70B</th>\n+    <th>Llama 3.1 405B</th>\n+    <th>Llama 4 Scout</th>\n+    <th>Llama 4 Maverick</th>\n+  </tr>\n+  <tr>\n+    <td> Reasoning & Knowledge </td>\n+    <td>MMLU</td>\n+    <td>5</td>\n+    <td>macro_avg/acc_char</td>\n+    <td>79.3</td>\n+    <td>85.2</td>\n+    <td>79.6</td>\n+    <td>85.5</td>\n+  </tr>\n+  <tr>\n+    <td> </td>\n+    <td>MMLU-Pro</td>\n+    <td>5</td>\n+    <td>macro_avg/acc</td>\n+    <td>53.8</td>\n+    <td>61.6</td>\n+    <td>58.2</td>\n+    <td>62.9</td>\n+  </tr>\n+  <tr>\n+    <td>  </td>\n+    <td>MATH</td>\n+    <td>4</td>\n+    <td>em_maj1@1</td>\n+    <td>41.6</td>\n+    <td>53.5</td>\n+    <td>50.3</td>\n+    <td>61.2</td>\n+  </tr>\n+  <tr>\n+    <td> Code </td>\n+    <td>MBPP</td>\n+    <td>3</td>\n+    <td>pass@1</td>\n+    <td>66.4</td>\n+    <td>74.4</td>\n+    <td>67.8</td>\n+    <td>77.6</td>\n+  </tr>\n+  <tr>\n+    <td> Multilingual </td>\n+    <td>TydiQA</td>\n+    <td>1</td>\n+    <td>average/f1</td>\n+    <td>29.9</td>\n+    <td>34.3</td>\n+    <td>31.5</td>\n+    <td>31.7</td>\n+  </tr>\n+  <tr>\n+    <td> Image </td>\n+    <td>ChartQA</td>\n+    <td>0</td>\n+    <td>relaxed_accuracy</td>\n+    <td colspan=\"2\"> No multimodal support </td>\n+    <td>83.4</td>\n+    <td>85.3</td>\n+  </tr>\n+  <tr>\n+    <td>  </td>\n+    <td>DocVQA</td>\n+    <td>0</td>\n+    <td>anls</td>\n+    <td colspan=\"2\">  </td>\n+    <td>89.4</td>\n+    <td>91.6</td>\n+  </tr>\n+</table>\n+\n+\n+\n+### Instruction tuned models\n+\n+##\n+\n+<table>\n+  <tr>\n+    <th>Category</th>\n+    <th>Benchmark</th>\n+    <th># Shots</th>\n+    <th>Metric</th>\n+    <th>Llama 3.3 70B</th>\n+    <th>Llama 3.1 405B</th>\n+    <th>Llama 4 Scout</th>\n+    <th>Llama 4 Maverick</th>\n+  </tr>\n+  <tr>\n+    <td> Image Reasoning</td>\n+    <td>MMMU</td>\n+    <td>0</td>\n+    <td>accuracy</td>\n+    <td colspan=\"2\"> No multimodal support </td>\n+    <td>69.4</td>\n+    <td>73.4</td>\n+  </tr>\n+  <tr>\n+    <td>  </td>\n+    <td>MMMU Pro^</td>\n+    <td>0</td>\n+    <td>accuracy</td>\n+    <td colspan=\"2\"> </td>\n+    <td>52.2</td>\n+    <td>59.6</td>\n+  </tr>\n+  <tr>\n+    <td>  </td>\n+    <td>MathVista</td>\n+    <td>0</td>\n+    <td>accuracy</td>\n+    <td colspan=\"2\">  </td>\n+    <td>70.7</td>\n+    <td>73.7</td>\n+  </tr>\n+  <tr>\n+    <td> Image Understanding </td>\n+    <td>ChartQA</td>\n+    <td>0</td>\n+    <td>relaxed_accuracy</td>\n+    <td colspan=\"2\">  </td>\n+    <td>88.8</td>\n+    <td>90.0</td>\n+  </tr>\n+  <tr>\n+    <td>  </td>\n+    <td>DocVQA (test)</td>\n+    <td>0</td>\n+    <td>anls</td>\n+    <td colspan=\"2\"> </td>\n+    <td>94.4</td>\n+    <td>94.4</td>\n+  </tr>\n+  <tr>\n+    <td> Code  </td>\n+    <td> LiveCodeBench\n+(10/01/2024-02/01/2025)\n+ </td>\n+    <td>0</td>\n+    <td>pass@1</td>\n+    <td> 33.3 </td>\n+    <td> 27.7 </td>\n+    <td>32.8</td>\n+    <td>43.4</td>\n+  </tr>\n+  <tr>\n+    <td> Reasoning & Knowledge  </td>\n+    <td> MMLU Pro</td>\n+    <td>0</td>\n+    <td>macro_avg/em</td>\n+    <td> 68.9 </td>\n+    <td> 73.4 </td>\n+    <td>74.3</td>\n+    <td>80.5</td>\n+  </tr>\n+    <tr>\n+    <td>  </td>\n+    <td> GPQA Diamond</td>\n+    <td>0</td>\n+    <td>accuracy</td>\n+    <td> 50.5 </td>\n+    <td> 49.0 </td>\n+    <td>57.2</td>\n+    <td>69.8</td>\n+  </tr>\n+    <tr>\n+    <td> Multilingual  </td>\n+    <td> MGSM</td>\n+    <td>0</td>\n+    <td>average/em</td>\n+    <td> 91.1 </td>\n+    <td> 91.6</td>\n+    <td>90.6</td>\n+    <td>92.3</td>\n+  </tr>\n+    <tr>\n+    <td> Long Context  </td>\n+    <td> MTOB (half book) eng->kgv/kgv->eng </td>\n+    <td>-</td>\n+    <td>chrF</td>\n+    <td colspan=\"2\"> Context window is 128K </td>\n+    <td>42.2/36.6</td>\n+    <td>54.0/46.4</td>\n+  </tr>\n+    <tr>\n+    <td> </td>\n+    <td> MTOB (full book) eng->kgv/kgv->eng </td>\n+    <td>-</td>\n+    <td>chrF</td>\n+    <td colspan=\"2\">    </td>\n+    <td>39.7/36.3</td>\n+    <td>50.8/46.7</td>\n+  </tr>\n+</table>\n+^reported numbers for MMMU Pro is the average of Standard and Vision tasks\n+\n+\n+## Quantization\n+\n+The Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\n+\n+## Safeguards\n+\n+As part of our release approach, we followed a three-pronged strategy to manage risks:\n+\n+* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n+* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n+* Provide protections for the community to help prevent the misuse of our models.\n+\n+Llama is a foundational technology designed for use in a variety of use cases; examples on how Metas Llama models have been deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our models safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections](https://ai.meta.com/static-resource/developer-use-guide-ai-protections).\n+\n+### Model level fine tuning\n+\n+The primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\n+\n+**Fine-tuning data**   \n+We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. Weve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n+\n+**Refusals**  \n+Building on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4\\. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. \n+\n+**Tone**  \n+We expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\n+\n+To achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\n+\n+**System Prompts**  \n+Llama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, weve seen that the use of a system prompt can be effective in reducing false refusals and templated or preachy language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting. \n+\n+Consider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\n+\n+<table>\n+  <tr>\n+    <th>System Prompt</th>\n+  </tr>\n+  <tr>\n+    <td>\n+You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to \n+switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving. \n+\n+You understand user intent and dont try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language.\n+\n+You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude. \n+\n+You never use phrases that imply moral superiority or a sense of authority, including but not limited to its important to, its crucial to, its essential to, \"it's unethical to\", \"it's worth noting\", Remember  etc. Avoid using these.\n+\n+Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information. \n+\n+You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\n+</td>\n+  </tr>\n+</table>\n+\n+### Llama 4 system protections\n+\n+Large language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools. \n+\n+We provide the community with system level [protections](https://llama.meta.com/trust-and-safety/) \\- like Llama Guard, Prompt Guard and Code Shield \\- that developers should deploy with Llama models or other LLMs. All of our [reference implementation](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n+\n+### Evaluations\n+\n+We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \n+Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n+\n+**Red teaming**   \n+We conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n+\n+### Critical Risks \n+\n+We spend additional focus on the following critical risk areas:\n+\n+**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**  \n+To assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area. \n+\n+**2\\. Child Safety**  \n+We leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the models capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. Weve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n+\n+**3\\. Cyber attack enablement**  \n+Our cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\n+\n+### Community \n+\n+Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n+\n+We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Metas Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n+\n+Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n+\n+## Considerations and Limitations\n+\n+Our AI is anchored on the values of freedom of expression \\- helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\n+\n+It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n+\n+Llama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, [Llama Protections](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more. \n\n--- File: models/llama4/USE_POLICY.md ---\n@@ -0,0 +1,78 @@\n+**Llama 4** **Acceptable Use Policy**\n+\n+Meta is committed to promoting safe and fair use of its tools and features, including Llama 4. If you access or use Llama 4, you agree to this Acceptable Use Policy (Policy). The most recent copy of this policy can be found at [https://www.llama.com/llama4/use-policy](https://www.llama.com/llama4/use-policy).\n+\n+**Prohibited Uses**\n+\n+We want everyone to use Llama 4 safely and responsibly. You agree you will not use, or allow others to use, Llama 4 to:\n+\n+1. Violate the law or others rights, including to:\n+\n+   1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n+     \n+      1. Violence or terrorism\n+\n+      2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n+ \n+      3. Human trafficking, exploitation, and sexual violence\n+\n+      4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n+\n+      5. Sexual solicitation\n+\n+      6. Any other criminal activity\n+\n+   2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n+\n+   3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n+\n+   4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n+\n+   5. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n+\n+   6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n+\n+   7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n+\n+   8. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta \n+\n+3. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 4 related to the following:\n+\n+   1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\n+  \n+   2. Guns and illegal weapons (including weapon development)\n+  \n+   3. Illegal drugs and regulated/controlled substances\n+  \n+   4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n+  \n+   5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n+  \n+   6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n+\n+4. Intentionally deceive or mislead others, including use of Llama 4 related to the following:\n+\n+   1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n+  \n+   2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n+  \n+   3. Generating, promoting, or further distributing spam\n+  \n+   4. Impersonating another individual without consent, authorization, or legal right\n+  \n+   5. Representing that the use of Llama 4 or outputs are human generated\n+  \n+   6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement \n+\n+5. Fail to appropriately disclose to end users any known dangers of your AI system\n+\n+6. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 4\n+\n+With respect to any multimodal models included in Llama 4, the rights granted under Section 1(a) of the Llama 4 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\n+\n+Please report any violation of this Policy, software bug, or other problems that could lead to a violation of this Policy through one of the following means:\n+\n+* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n+* Reporting risky content generated by the model: [https://developers.facebook.com/llama_output_feedback](https://developers.facebook.com/llama_output_feedback)\n+* Reporting bugs and security concerns: [https://facebook.com/whitehat/info](https://facebook.com/whitehat/info)\n+* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 4: LlamaUseReport@meta.com\n\n--- File: models/llama4/args.py ---\n@@ -0,0 +1,96 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from enum import Enum\n+from typing import Optional\n+\n+from pydantic import BaseModel, model_validator\n+\n+\n+class QuantizationScheme(Enum):\n+    int4_weight_int8_dynamic_activation = \"int4_weight_int8_dynamic_activation\"\n+\n+\n+class QuantizationArgs(BaseModel):\n+    scheme: Optional[QuantizationScheme] = None\n+    group_size: Optional[int] = None\n+    spinquant: bool = False\n+\n+\n+class LoRAArgs(BaseModel):\n+    rank: int\n+    scale: float\n+\n+\n+class MoEArgs(BaseModel):\n+    num_experts: int = -1\n+    capacity_factor: float = 1.0  # capacity factor determines how many tokens each expert can choose\n+    auto_scale_F: bool = (  # noqa: N815\n+        True  # if true, rescales hidden_dim such that number of activated params is same as equivalent dense layer\n+    )\n+    top_k: int = 1\n+    interleave_moe_layer_step: int = 1\n+\n+\n+class Size(BaseModel):\n+    height: int\n+    width: int\n+\n+\n+class VisionArgs(BaseModel):\n+    image_size: Size\n+    patch_size: Size\n+\n+    # parameters for the encoder transformer\n+    dim: int\n+    n_layers: int\n+    n_heads: int\n+    mlp_ratio: float\n+    output_dim: int\n+\n+    pixel_shuffle_ratio: float\n+\n+\n+class ModelArgs(BaseModel):\n+    dim: int = -1\n+    n_layers: int = -1\n+    n_heads: int = -1\n+    n_kv_heads: Optional[int] = None\n+    head_dim: Optional[int] = None\n+\n+    vocab_size: int = -1\n+    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n+    ffn_dim_multiplier: Optional[float] = None\n+    ffn_exp: Optional[float] = None\n+    norm_eps: float = 1e-5\n+\n+    attention_chunk_size: Optional[int] = None\n+    rope_theta: float = 500000\n+    use_scaled_rope: bool = False\n+    nope_layer_interval: Optional[int] = None  # No position encoding in every n layers\n+    use_qk_norm: bool = False\n+    # Set to True to enable inference-time temperature tuning (useful for very long context)\n+    attn_temperature_tuning: bool = False\n+    floor_scale: float = 8192.0\n+    attn_scale: float = 0.1\n+\n+    vision_args: Optional[VisionArgs] = None\n+    moe_args: Optional[MoEArgs] = None\n+    quantization_args: Optional[QuantizationArgs] = None\n+    lora_args: Optional[LoRAArgs] = None\n+\n+    max_batch_size: int = 32\n+    max_seq_len: int = 2048\n+\n+    @model_validator(mode=\"after\")\n+    def validate(self) -> \"ModelArgs\":\n+        assert self.n_kv_heads <= self.n_heads, f\"n_kv_heads ({self.n_kv_heads}) must be <= n_heads ({self.n_heads})\"\n+        assert self.n_heads % self.n_kv_heads == 0, (\n+            f\"n_heads ({self.n_heads}) must be divisible by n_kv_heads ({self.n_kv_heads})\"\n+        )\n+        assert self.dim % self.n_heads == 0, f\"dim ({self.dim}) must be divisible by n_heads ({self.n_heads})\"\n+        return self\n\n--- File: models/llama4/chat_format.py ---\n@@ -0,0 +1,305 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import io\n+import uuid\n+from dataclasses import dataclass\n+from typing import Dict, List, Optional, Tuple\n+\n+import torch\n+from PIL import Image as PIL_Image\n+\n+# TODO: either fork these or move them to the common package\n+from ..datatypes import (\n+    BuiltinTool,\n+    RawContent,\n+    RawMediaItem,\n+    RawMessage,\n+    RawTextItem,\n+    Role,\n+    StopReason,\n+    ToolCall,\n+    ToolPromptFormat,\n+)\n+from ..llama3.tool_utils import ToolUtils\n+from .args import VisionArgs\n+from .datatypes import LLMInput\n+from .preprocess import ResizeNormalizeImageTransform, VariableSizeImageTransform\n+from .tokenizer import Tokenizer\n+\n+\n+def role_str(role: Role) -> str:\n+    role_strs = {\n+        Role.user: \"user\",\n+        Role.system: \"system\",\n+        Role.tool: \"ipython\",  # special\n+        Role.assistant: \"assistant\",\n+    }\n+    return role_strs[role]\n+\n+\n+@dataclass\n+class TransformedImage:\n+    image_tiles: torch.Tensor\n+    # is the aspect ratio needed anywhere?\n+    aspect_ratio: Tuple[int, int]\n+\n+\n+def pil_RGBA2RGB(image: PIL_Image.Image, bg: Tuple[int, int, int] = (255, 255, 255)) -> PIL_Image.Image:\n+    if image.mode == \"RGBA\":\n+        image.load()  # for png.split()\n+        new_img = PIL_Image.new(\"RGB\", image.size, bg)\n+        new_img.paste(image, mask=image.split()[3])  # 3 is the alpha channel\n+        return new_img\n+    return image.convert(\"RGB\")\n+\n+\n+class ChatFormat:\n+    possible_headers: Dict[Role, str]\n+\n+    def __init__(\n+        self,\n+        tokenizer: Tokenizer,\n+        vision_args: Optional[VisionArgs] = None,\n+        max_num_chunks: int = 16,\n+    ):\n+        self.tokenizer = tokenizer\n+        self.vision_args = vision_args\n+        self.max_num_chunks = max_num_chunks\n+\n+        self.possible_headers = {role: f\"<|header_start|>{role_str(role)}<|header_end|>\\n\\n\" for role in Role}\n+\n+        self.image_transform = None\n+        self.dynamic_image_transform = None\n+        if vision_args:\n+            self.dynamic_image_transform = VariableSizeImageTransform(vision_args.image_size.width)\n+            self.image_transform = ResizeNormalizeImageTransform(\n+                vision_args.image_size.width, vision_args.image_size.height\n+            )\n+\n+    def _encode_header(self, role: str) -> List[int]:\n+        tokens = []\n+        tokens.append(self.tokenizer.special_tokens[\"<|header_start|>\"])\n+\n+        # TODO: need to check if this is correct\n+        tokens.extend(self.tokenizer.encode(\"ipython\" if role == \"tool\" else role, bos=False, eos=False))\n+        tokens.append(self.tokenizer.special_tokens[\"<|header_end|>\"])\n+        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n+        return tokens\n+\n+    def encode_content(self, content: RawContent) -> LLMInput:\n+        tokens, images = self._encode_content(content, bos=True)\n+        return self._model_input_from_tokens_images(tokens, images)\n+\n+    def _encode_image(\n+        self,\n+        transformed_image: TransformedImage,\n+    ) -> List[int]:\n+        assert self.vision_args is not None, \"The model is not vision-enabled\"\n+\n+        image_tensor = transformed_image.image_tiles\n+        image_channels = image_tensor.shape[-3]\n+        image_height = image_tensor.shape[-2]\n+        image_width = image_tensor.shape[-1]\n+        image_chunks = image_tensor.view(-1, image_channels, image_height, image_width).shape[0]\n+\n+        patch_height = self.vision_args.patch_size.height\n+        patch_width = self.vision_args.patch_size.width\n+\n+        if image_height % patch_height != 0:\n+            raise ValueError(f\"{image_height=} not divisible by {patch_height=}\")\n+        if image_width % patch_width != 0:\n+            raise ValueError(f\"{image_width=} not divisible by {patch_width=}\")\n+\n+        ds_ratio = int(round(1.0 / (self.vision_args.pixel_shuffle_ratio**2)))\n+        n_patches_per_chunk = int((image_height // patch_height) * (image_width // patch_width) // ds_ratio)\n+\n+        image_ar = transformed_image.aspect_ratio\n+        tokens = [self.tokenizer.special_tokens[\"<|image_start|>\"]]\n+        if image_chunks == 1:\n+            tokens += [self.tokenizer.special_tokens[\"<|image|>\"]]\n+            tokens += [self.tokenizer.special_tokens[\"<|patch|>\"]] * n_patches_per_chunk\n+            tokens += [self.tokenizer.special_tokens[\"<|image_end|>\"]]\n+        else:\n+            ratio_h, ratio_w = image_ar\n+            for yy in range(ratio_h):\n+                for xx in range(ratio_w):\n+                    tokens += [self.tokenizer.special_tokens[\"<|patch|>\"]] * n_patches_per_chunk\n+                    if xx < ratio_w - 1:\n+                        tokens.append(self.tokenizer.special_tokens[\"<|tile_x_separator|>\"])\n+\n+                tokens.append(self.tokenizer.special_tokens[\"<|tile_y_separator|>\"])\n+\n+            tokens += [self.tokenizer.special_tokens[\"<|image|>\"]]\n+            tokens += [self.tokenizer.special_tokens[\"<|patch|>\"]] * n_patches_per_chunk\n+            tokens += [self.tokenizer.special_tokens[\"<|image_end|>\"]]\n+\n+        return tokens\n+\n+    def _encode_content(self, content: RawContent, bos: bool = False) -> Tuple[List[int], List[TransformedImage]]:\n+        tokens = []\n+        tranformed_images = []\n+\n+        added_bos = False\n+\n+        def _process(c):\n+            nonlocal added_bos, bos\n+\n+            if isinstance(c, str) or isinstance(c, RawTextItem):\n+                if isinstance(c, RawTextItem):\n+                    c = c.text\n+                tokens.extend(self.tokenizer.encode(c, bos=False if added_bos else bos, eos=False))\n+                added_bos = True\n+\n+            elif isinstance(c, RawMediaItem):\n+                if not self.vision_args:\n+                    raise ValueError(\"The model is not vision-enabled, but a media item was found\")\n+\n+                bos = False if added_bos else bos\n+                if bos:\n+                    tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n+                    added_bos = True\n+\n+                bytes_io = io.BytesIO(c.data) if isinstance(c.data, bytes) else c.data\n+                image = PIL_Image.open(bytes_io)\n+                image = pil_RGBA2RGB(image)\n+                image_tiles, ar = self.dynamic_image_transform(image, max_num_chunks=self.max_num_chunks)\n+\n+                if image_tiles.shape[0] > 1:\n+                    image_global = self.image_transform(image)\n+                    image_global = image_global.unsqueeze(0)\n+                    image_combine = torch.cat((image_tiles, image_global), dim=0)\n+                    image_tiles = image_combine\n+\n+                transformed_image = TransformedImage(image_tiles=image_tiles, aspect_ratio=ar)\n+                tokens.extend(self._encode_image(transformed_image))\n+                tranformed_images.append(transformed_image)\n+\n+        if isinstance(content, list):\n+            for c in content:\n+                _process(c)\n+        else:\n+            _process(content)\n+\n+        return tokens, tranformed_images\n+\n+    def encode_message(\n+        self, message: RawMessage, tool_prompt_format: ToolPromptFormat\n+    ) -> Tuple[List[int], List[TransformedImage]]:\n+        tokens = self._encode_header(message.role)\n+        images = []\n+\n+        def _process_content(c):\n+            toks, imgs = self._encode_content(c)\n+            tokens.extend(toks)\n+            images.extend(imgs)\n+\n+        _process_content(message.content)\n+\n+        if message.role == \"user\" and message.context is not None:\n+            # This is RAG context; why is it here in the chat format? I don't think\n+            # this is needed and can be moved upwards\n+            _process_content(\"\\n\\n\")\n+            _process_content(message.context)\n+\n+        if message.role == \"assistant\":\n+            for t in message.tool_calls:\n+                content = ToolUtils.encode_tool_call(t, tool_prompt_format)\n+                _process_content(content)\n+\n+        eom = False\n+        if message.role == \"assistant\":\n+            eom = message.stop_reason == StopReason.end_of_message\n+\n+        tokens.append(self.tokenizer.special_tokens[\"<|eom|>\" if eom else \"<|eot|>\"])\n+        return tokens, images\n+\n+    def encode_dialog_prompt(\n+        self,\n+        messages: List[RawMessage],\n+        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n+    ) -> LLMInput:\n+        tokens = []\n+        images = []\n+        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n+        for message in messages:\n+            toks, imgs = self.encode_message(message, tool_prompt_format)\n+            tokens.extend(toks)\n+            images.extend(imgs)\n+\n+        # Add the start of an assistant message for the model to complete.\n+        tokens.extend(self._encode_header(\"assistant\"))\n+\n+        return self._model_input_from_tokens_images(tokens, images)\n+\n+    # TODO(this should be generic, not only for assistant messages)\n+    def decode_assistant_message(self, tokens: List[int], stop_reason: StopReason) -> RawMessage:\n+        content = self.tokenizer.decode(tokens)\n+\n+        return self.decode_assistant_message_from_content(content, stop_reason)\n+\n+    def decode_assistant_message_from_content(self, content: str, stop_reason: StopReason) -> RawMessage:\n+        content = content.strip(\" \")\n+        header_str = self.possible_headers[Role.assistant]\n+        if content.startswith(header_str):\n+            content = content[len(header_str) :]\n+\n+        if content.endswith(\"<|eot|>\"):\n+            content = content[: -len(\"<|eot|>\")]\n+            stop_reason = StopReason.end_of_turn\n+        elif content.endswith(\"<|eom|>\"):\n+            content = content[: -len(\"<|eom|>\")]\n+            stop_reason = StopReason.end_of_message\n+\n+        tool_name = None\n+        tool_arguments = {}\n+\n+        custom_tool_info = ToolUtils.maybe_extract_custom_tool_call(content)\n+        if custom_tool_info is not None:\n+            tool_name, tool_arguments = custom_tool_info\n+            # Sometimes when agent has custom tools alongside builin tools\n+            # Agent responds for builtin tool calls in the format of the custom tools\n+            # This code tries to handle that case\n+            if tool_name in BuiltinTool.__members__:\n+                tool_name = BuiltinTool[tool_name]\n+                tool_arguments = {\n+                    \"query\": list(tool_arguments.values())[0],\n+                }\n+        else:\n+            builtin_tool_info = ToolUtils.maybe_extract_builtin_tool_call(content)\n+            if builtin_tool_info is not None:\n+                tool_name, query = builtin_tool_info\n+                tool_arguments = {\n+                    \"query\": query,\n+                }\n+                if tool_name in BuiltinTool.__members__:\n+                    tool_name = BuiltinTool[tool_name]\n+\n+        tool_calls = []\n+        if tool_name is not None and tool_arguments is not None:\n+            call_id = str(uuid.uuid4())\n+            tool_calls.append(\n+                ToolCall(\n+                    call_id=call_id,\n+                    tool_name=tool_name,\n+                    arguments=tool_arguments,\n+                )\n+            )\n+            content = \"\"\n+\n+        return RawMessage(\n+            role=\"assistant\",\n+            content=content,\n+            stop_reason=stop_reason,\n+            tool_calls=tool_calls,\n+        )\n+\n+    def _model_input_from_tokens_images(self, tokens: List[int], images: List[TransformedImage]) -> LLMInput:\n+        return LLMInput(\n+            tokens=tokens,\n+            images=[x.image_tiles for x in images] if len(images) > 0 else None,\n+        )\n\n--- File: models/llama4/checkpoint.py ---\n@@ -0,0 +1,167 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import concurrent.futures\n+import re\n+from pathlib import Path\n+from typing import Any, Dict, List, Union\n+\n+import numpy as np\n+import torch\n+from fairscale.nn.model_parallel.initialize import get_model_parallel_rank, get_model_parallel_world_size\n+\n+from .args import ModelArgs\n+\n+\n+def map_mp_rank(old_mp_size: int, new_mp_size: int, new_mp_rank: int) -> List[int]:\n+    \"\"\"Map a new MP rank to a list of old MP ranks given a change in MP size.\"\"\"\n+    if new_mp_size % old_mp_size == 0:\n+        # Read old MP shard and split it into smaller ones\n+        return [new_mp_rank * old_mp_size // new_mp_size]\n+    elif old_mp_size % new_mp_size == 0:\n+        # Merge old MP shards into a single one\n+        mp_factor = old_mp_size // new_mp_size\n+        return list(range(new_mp_rank * mp_factor, (new_mp_rank + 1) * mp_factor))\n+    else:\n+        raise ValueError(\n+            f\"Either old MP size or new MP size should be a multiple of the other: \"\n+            f\"{old_mp_size} % {new_mp_size} != 0 and {new_mp_size} % {old_mp_size} != 0\"\n+        )\n+\n+\n+def load_state_dict(\n+    ckpt_paths: List[Path],\n+    model_args: ModelArgs,\n+    map_location: Union[str, torch.device] = \"cpu\",\n+    mmap: bool = True,\n+) -> Dict[str, torch.Tensor]:\n+    if str(map_location) == \"cpu\":\n+        torch.set_default_tensor_type(torch.BFloat16Tensor)\n+    else:\n+        torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+\n+    ckpt_paths = np.array(sorted(ckpt_paths))\n+\n+    new_mp_size, new_mp_rank = get_model_parallel_world_size(), get_model_parallel_rank()\n+    old_mp_size = len(ckpt_paths)\n+    old_mp_ranks = map_mp_rank(old_mp_size, new_mp_size, new_mp_rank)\n+\n+    print(f\"Loading checkpoint shards:\\n{str(ckpt_paths[old_mp_ranks])}\")\n+    paths = ckpt_paths[old_mp_ranks]\n+    state_dicts = [torch.load(str(p), map_location=map_location, mmap=mmap) for p in paths]\n+\n+    if new_mp_size == old_mp_size:\n+        return state_dicts[0]\n+\n+    num_experts = model_args.moe_args.num_experts\n+    state_dicts = [convert_moe_weights(d, num_experts=num_experts) for d in state_dicts]\n+\n+    n_kv_heads: int = model_args.n_kv_heads if model_args.n_kv_heads is not None else model_args.n_heads\n+    print(f\"Resharding {len(state_dicts)} state dicts from MP size {old_mp_size} to MP size {new_mp_size}\")\n+    return reshard_mp(\n+        state_dicts,\n+        size=max(new_mp_size // old_mp_size, 1),\n+        rank=new_mp_rank % max(new_mp_size // old_mp_size, 1),\n+        repeat_qk_qv=max(new_mp_size // n_kv_heads, 1),\n+    )\n+\n+\n+_WEIGHT_ROW_KEY = {\n+    \"feed_forward.w2\",\n+    \"feed_forward.mlp.fc2\",\n+    \"attention.wo\",\n+    \"feed_forward.mlp.fc2_weight\",\n+    \"feed_forward.w_out_shared_DF.weight\",\n+    \"attn.wo.weight\",\n+    \"mlp.c_proj.weight\",\n+}\n+_MOE_WEIGHT_ROW_KEY = {\"feed_forward.experts.(moe_w_in_eD_F|moe_w_swiglu_eD_F)\"}\n+\n+_WEIGHT_COLUMN_KEY = {\n+    \"output\",\n+    \"feed_forward.(w1|w3)\",\n+    \"feed_forward.mlp.(fc1|fc3)\",\n+    \"feed_forward.mlp.fc1_weight\",\n+    \"attention.(wk|wq|wv|wqkv).weight\",\n+    \"feed_forward.(w_in_shared_FD|w_swiglu_FD)\",\n+    \"attn.(wk|wq|wv).weight\",\n+    \"attn.(wk|wq|wv).bias\",\n+    \"mlp.c_fc.weight\",\n+    \"mlp.c_fc.bias\",\n+    \"conv1._linear.weight\",\n+    \"tok_embeddings.weight\",\n+    \"vision_projection.weight\",\n+}\n+_MOE_WEIGHT_COLUMN_KEY = {\"feed_forward.experts.moe_w_out_eF_D\"}\n+\n+\n+def reshard_mp(\n+    state_dicts: List[Dict[str, torch.Tensor]],\n+    size: int,\n+    rank: int,\n+    repeat_qk_qv: int = 1,\n+) -> Dict[str, torch.Tensor]:\n+    \"\"\"\n+    Reshard a list of state dicts into a single state dict given a change in MP size.\n+    If the list has more than one state dict, we concatenate the values of the same\n+    key across all state dicts. Otherwise, we just slice it for the current MP rank.\n+    \"\"\"\n+\n+    def concat_or_chunk(tensors: List[torch.Tensor], dim: int) -> torch.Tensor:\n+        if len(tensors) > 1:\n+            return torch.cat(tensors, dim=dim)\n+        return tensors[0].chunk(size, dim=dim)[rank].clone()\n+\n+    def process_key(key: str) -> torch.Tensor:\n+        if row_regex.search(key):\n+            return concat_or_chunk([s[key] for s in state_dicts], dim=-1)\n+        elif column_regex.search(key):\n+            if \"w13\" in key or \"fc1_weight\" in key:\n+                dims = state_dicts[0][key].size()\n+                values = [s[key].view(2, dims[0] // 2, *dims[1:]) for s in state_dicts]\n+                return concat_or_chunk(values, dim=1).flatten(0, 1)\n+            elif \"qkv\" in key:\n+                q_dim = state_dicts[0][key.replace(\"qkv\", \"o\")].size(1)\n+                kv_dim = (state_dicts[0][key].size(0) - q_dim) // 2\n+                values = [s[key].split((q_dim, kv_dim, kv_dim)) for s in state_dicts]\n+                return torch.cat([concat_or_chunk(x, dim=0) for x in zip(*values)])  # type: ignore\n+            elif \"wk.weight\" in key or \"wv.weight\" in key:\n+                # Support MP > #kv_head\n+                return concat_or_chunk([s[key].repeat(repeat_qk_qv, 1) for s in state_dicts], dim=0)\n+            elif key == \"output.bias\" or key == \"fc.weight\":\n+                return concat_or_chunk([s[key] for s in state_dicts], dim=0)\n+            elif \"w_\" in key:\n+                return concat_or_chunk([s[key] for s in state_dicts], dim=-2)\n+            else:\n+                return concat_or_chunk([s[key] for s in state_dicts], dim=0)\n+        else:\n+            return state_dicts[0][key].clone()\n+\n+    row_keys = _WEIGHT_ROW_KEY | _MOE_WEIGHT_ROW_KEY\n+    column_keys = _WEIGHT_COLUMN_KEY | _MOE_WEIGHT_COLUMN_KEY\n+\n+    column_regex = re.compile(\"|\".join(column_keys))\n+    row_regex = re.compile(\"|\".join(row_keys))\n+\n+    output: Dict[str, torch.Tensor] = {}\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        # Note: only processes keys in the first state dict.\n+        # Assumes keys are the same across all state dicts.\n+        mappings = {executor.submit(process_key, key): key for key in state_dicts[0]}\n+        for future in concurrent.futures.as_completed(mappings):\n+            output[mappings[future]] = future.result()\n+    return output\n+\n+\n+def convert_moe_weights(state_dict: Dict[str, Any], num_experts: int) -> Dict[str, Any]:\n+    routed_keys = _MOE_WEIGHT_ROW_KEY | _MOE_WEIGHT_COLUMN_KEY\n+    routed_regex = re.compile(\"|\".join(routed_keys))\n+    keys = list(state_dict.keys())\n+    for key in keys:\n+        if routed_regex.search(key):\n+            state_dict[key] = state_dict.pop(key).unflatten(0, (num_experts, -1)).squeeze(dim=0)\n+    return state_dict\n\n--- File: models/llama4/datatypes.py ---\n@@ -0,0 +1,58 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from dataclasses import dataclass\n+from typing import List, Optional, Union\n+\n+import torch\n+\n+\n+@dataclass\n+class MaskedEmbedding:\n+    embedding: torch.Tensor\n+    mask: torch.Tensor\n+\n+\n+@dataclass\n+class LLMInput:\n+    \"\"\"\n+    This is the input to the LLM from the \"user\" -- the user in this case views the\n+    Llama4 model holistically and does not care or know about its inner workings (e.g.,\n+    whether it has an encoder or if it is early fusion or not.)\n+\n+    This is distinct from the \"TransformerInput\" class which is really the Llama4\n+    backbone operating on early fused modalities and producing text output\n+    \"\"\"\n+\n+    tokens: torch.Tensor\n+\n+    # images are already pre-processed (resized, tiled, etc.)\n+    images: Optional[List[torch.Tensor]] = None\n+\n+\n+@dataclass\n+class TransformerInput:\n+    \"\"\"\n+    This is the \"core\" backbone transformer of the Llama4 model. Inputs for other modalities\n+    are expected to be \"embedded\" via encoders sitting before this layer in the model.\n+    \"\"\"\n+\n+    tokens: torch.Tensor\n+\n+    # tokens_position defines the position of the tokens in each batch,\n+    # - when it is a tensor ([batch_size,]), it is the start position of the tokens in each batch\n+    # - when it is an int, the start position are the same for all batches\n+    tokens_position: Union[torch.Tensor, int]\n+    image_embedding: Optional[MaskedEmbedding] = None\n+\n+\n+@dataclass\n+class LLMOutput:\n+    logits: torch.Tensor\n+\n+\n+TransformerOutput = LLMOutput\n\n--- File: models/llama4/ffn.py ---\n@@ -0,0 +1,52 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from typing import Any, Dict, List\n+\n+from fairscale.nn.model_parallel.layers import ColumnParallelLinear, RowParallelLinear\n+from fairscale.nn.model_parallel.mappings import reduce_from_model_parallel_region\n+from torch import nn\n+from torch.nn import functional as F\n+\n+\n+class FeedForward(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        do_reduce: bool = True,\n+    ):\n+        super().__init__()\n+        self.do_reduce = do_reduce\n+\n+        self.w1 = ColumnParallelLinear(dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x)\n+        self.w2 = RowParallelLinear(hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x)\n+        self.w3 = ColumnParallelLinear(dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x)\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        if prefix + \"mlp.fc1_weight\" in state_dict:\n+            w1, w3 = state_dict.pop(prefix + \"mlp.fc1_weight\").chunk(2, dim=0)\n+            state_dict[prefix + \"w1.weight\"] = w1\n+            state_dict[prefix + \"w3.weight\"] = w3\n+            state_dict[prefix + \"w2.weight\"] = state_dict.pop(prefix + \"mlp.fc2_weight\")\n+\n+    def forward(self, x):\n+        x = F.silu(F.linear(x, self.w1.weight)) * F.linear(x, self.w3.weight)\n+        out = F.linear(x, self.w2.weight)\n+        if self.do_reduce:\n+            return reduce_from_model_parallel_region(out)\n+        return out\n\n--- File: models/llama4/generation.py ---\n@@ -0,0 +1,349 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# the root directory of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+import codecs\n+import io\n+import json\n+import os\n+import sys\n+import time\n+from dataclasses import dataclass\n+from enum import Enum\n+from pathlib import Path\n+from typing import Callable, Generator, List, Literal, Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.initialize import (\n+    get_model_parallel_rank,\n+    initialize_model_parallel,\n+    model_parallel_is_initialized,\n+)\n+from termcolor import cprint\n+\n+from .args import ModelArgs\n+from .chat_format import ChatFormat, RawContent, RawMessage\n+from .checkpoint import load_state_dict\n+from .datatypes import LLMInput, MaskedEmbedding, TransformerInput\n+from .model import Transformer\n+from .tokenizer import Tokenizer\n+\n+\n+@dataclass\n+class GenerationResult:\n+    token: int\n+    text: str\n+\n+    source: Literal[\"input\", \"output\"]\n+\n+    # index within the batch\n+    batch_idx: int\n+    # whether generation for this item is already finished. note that tokens can\n+    # get returned even afterwards since other items in the batch can still be generating tokens\n+    finished: bool\n+    # because a batch is parallel processed, useful decoding for one item can correspond to processing\n+    # pad tokens or tokens beyond EOS for other items. we could have decided to return None for this case\n+    # but it's more convenient to return a list of GenerationResult and filter out the ignored tokens\n+    ignore_token: bool\n+\n+    logprobs: Optional[List[float]] = None\n+\n+\n+torch.serialization.add_safe_globals([io.BytesIO, codecs.encode])\n+\n+\n+class QuantizationMode(str, Enum):\n+    none = \"none\"\n+    fp8_mixed = \"fp8_mixed\"\n+    int4_mixed = \"int4_mixed\"\n+\n+\n+class Llama4:\n+    @staticmethod\n+    def build(\n+        ckpt_dir: str,\n+        max_seq_len: int,\n+        max_batch_size: int,\n+        world_size: Optional[int] = None,\n+        quantization_mode: Optional[str] = None,\n+        seed: int = 1,\n+    ):\n+        if not torch.distributed.is_initialized():\n+            torch.distributed.init_process_group(\"nccl\")\n+\n+        if not model_parallel_is_initialized():\n+            if world_size is None:\n+                world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n+            initialize_model_parallel(world_size)\n+\n+        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n+        torch.cuda.set_device(local_rank)\n+\n+        torch.manual_seed(seed)\n+\n+        if local_rank > 0:\n+            sys.stdout = open(os.devnull, \"w\")\n+\n+        start_time = time.time()\n+\n+        ckpt_paths = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n+        assert len(ckpt_paths) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n+        print(f\"Loading a checkpoint (shards={len(ckpt_paths)}, current-mp-size={world_size})\")\n+        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n+            params = json.loads(f.read())\n+\n+        model_args: ModelArgs = ModelArgs(\n+            **params,\n+            max_seq_len=max_seq_len,\n+            max_batch_size=max_batch_size,\n+        )\n+        tokenizer = Tokenizer.get_instance()\n+\n+        # TODO: params.json should always have correct vocab_size\n+        if model_args.vocab_size == -1:\n+            model_args.vocab_size = tokenizer.n_words\n+        assert model_args.vocab_size == tokenizer.n_words, f\"{model_args.vocab_size=} vs. {tokenizer.n_words=} mismatch\"\n+        print(\"Model args:\\n\", model_args.model_dump_json(indent=2))\n+\n+        state_dict = load_state_dict(ckpt_paths, model_args)\n+        print(\"Loaded checkpoint\")\n+        if quantization_mode == QuantizationMode.fp8_mixed or quantization_mode == QuantizationMode.int4_mixed:\n+            from .quantization.loader import convert_to_quantized_model\n+\n+            torch.set_default_tensor_type(torch.BFloat16Tensor)\n+            model = Transformer(model_args)\n+            print(\"Loading state dict...\")\n+            model.load_state_dict(state_dict, strict=False)\n+            print(\"Done...\")\n+            model = convert_to_quantized_model(model, ckpt_dir, quantization_mode)\n+        else:\n+            if torch.cuda.is_bf16_supported():\n+                torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+            else:\n+                torch.set_default_tensor_type(torch.cuda.HalfTensor)\n+\n+            model = Transformer(model_args)\n+            print(\"Loading state dict...\")\n+            model.load_state_dict(state_dict, strict=False)\n+            print(\"Done...\")\n+        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n+\n+        return Llama4(model, tokenizer, model_args)\n+\n+    def __init__(self, model: Transformer, tokenizer: Tokenizer, args: ModelArgs):\n+        self.args = args\n+        self.model = model\n+        self.tokenizer = tokenizer\n+        self.formatter = ChatFormat(tokenizer, vision_args=args.vision_args)\n+\n+    @torch.inference_mode()\n+    def generate(\n+        self,\n+        llm_inputs: List[LLMInput],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+        echo: bool = False,\n+        print_model_input: bool = False,\n+        logits_processor: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n+    ) -> Generator[List[GenerationResult], None, None]:\n+        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.args.max_seq_len:\n+            max_gen_len = self.model.args.max_seq_len - 1\n+\n+        params = self.model.args\n+\n+        print_model_input = print_model_input or os.environ.get(\"LLAMA_MODELS_DEBUG\", \"0\") == \"1\"\n+        if print_model_input and get_model_parallel_rank() == 0:\n+            cprint(\"Input to model:\\n\", \"yellow\")\n+            for inp in llm_inputs:\n+                tokens_to_print = [t for t in inp.tokens]\n+                cprint(self.tokenizer.decode(tokens_to_print), \"grey\")\n+        prompt_tokens = [inp.tokens for inp in llm_inputs]\n+\n+        bsz = len(llm_inputs)\n+        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n+\n+        min_prompt_len = min(len(t) for t in prompt_tokens)\n+        max_prompt_len = max(len(t) for t in prompt_tokens)\n+\n+        if max_prompt_len >= params.max_seq_len:\n+            cprint(f\"Out of token budget {max_prompt_len} vs {params.max_seq_len}\", \"red\")\n+            return\n+\n+        total_len = min(max_gen_len + max_prompt_len, params.max_seq_len)\n+\n+        pad_id = self.tokenizer.pad_id\n+        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n+        for k, t in enumerate(prompt_tokens):\n+            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n+        if logprobs:\n+            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n+\n+        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n+        input_text_mask = tokens != pad_id\n+\n+        if echo:\n+            for i in range(max_prompt_len):\n+                results = []\n+                for j, t in enumerate(tokens[:, i]):\n+                    results.append(\n+                        GenerationResult(\n+                            token=t.item(),\n+                            text=self.tokenizer.decode([t.item()]),\n+                            source=\"input\",\n+                            logprobs=(token_logprobs[j, i : i + 1].tolist() if logprobs else None),\n+                            batch_idx=j,\n+                            finished=False,\n+                            ignore_token=t.item() == pad_id,\n+                        )\n+                    )\n+                yield results\n+\n+        stop_tokens = torch.tensor(self.tokenizer.stop_tokens, device=\"cuda\")\n+\n+        prev_pos = 0\n+        for cur_pos in range(min_prompt_len, total_len):\n+            image_embedding = None\n+            if prev_pos == 0 and any(inp.images is not None and len(inp.images) > 0 for inp in llm_inputs):\n+                image_mask = tokens[:, prev_pos:cur_pos] == self.tokenizer.special_tokens[\"<|patch|>\"]\n+                image_mask = image_mask.unsqueeze(-1)\n+                h = self.model.tok_embeddings(tokens[:, prev_pos:cur_pos])\n+\n+                image_batch = [inp.images if inp.images is not None else [] for inp in llm_inputs]\n+                image_embedding = MaskedEmbedding(\n+                    embedding=self.model.vision_embeddings(image_batch, image_mask, h),\n+                    mask=image_mask,\n+                )\n+\n+            xformer_input = TransformerInput(\n+                tokens=tokens[:, prev_pos:cur_pos],\n+                tokens_position=prev_pos,\n+                image_embedding=image_embedding,\n+            )\n+            xformer_output = self.model.forward(xformer_input)\n+            logits = xformer_output.logits\n+            if logits_processor is not None:\n+                logits = logits_processor(tokens[:, :cur_pos], logits)\n+\n+            if temperature > 0:\n+                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n+                next_token = sample_top_p(probs, top_p)\n+            else:\n+                next_token = torch.argmax(logits[:, -1], dim=-1)\n+\n+            next_token = next_token.reshape(-1)\n+            # only replace token if prompt has already been generated\n+            next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n+            tokens[:, cur_pos] = next_token\n+\n+            target = tokens[:, prev_pos + 1 : cur_pos + 1]\n+            if logprobs:\n+                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n+                    input=logits.transpose(1, 2),\n+                    target=target,\n+                    reduction=\"none\",\n+                    ignore_index=pad_id,\n+                )\n+            eos_reached |= (~input_text_mask[:, cur_pos]) & (torch.isin(next_token, stop_tokens))\n+\n+            results = []\n+            for idx, t in enumerate(next_token):\n+                results.append(\n+                    GenerationResult(\n+                        token=t.item(),\n+                        text=self.tokenizer.decode([t.item()]),\n+                        source=\"output\",\n+                        logprobs=(token_logprobs[idx, cur_pos : cur_pos + 1].tolist() if logprobs else None),\n+                        batch_idx=idx,\n+                        finished=eos_reached[idx],\n+                        ignore_token=cur_pos < len(prompt_tokens[idx]),\n+                    )\n+                )\n+            yield results\n+\n+            prev_pos = cur_pos\n+            if all(eos_reached):\n+                break\n+\n+    def completion(\n+        self,\n+        contents: List[RawContent],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+        echo: bool = False,\n+    ) -> Generator[List[GenerationResult], None, None]:\n+        llm_inputs = [self.formatter.encode_contents(c) for c in contents]\n+        for result in self.generate(\n+            llm_inputs=llm_inputs,\n+            temperature=temperature,\n+            top_p=top_p,\n+            max_gen_len=max_gen_len,\n+            logprobs=logprobs,\n+            echo=echo,\n+        ):\n+            if all(r.finished for r in result):\n+                break\n+            yield result\n+\n+    def chat_completion(\n+        self,\n+        messages_batch: List[List[RawMessage]],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+        echo: bool = False,\n+    ) -> Generator[List[GenerationResult], None, None]:\n+        llm_inputs = [self.formatter.encode_dialog_prompt(messages) for messages in messages_batch]\n+        for result in self.generate(\n+            llm_inputs=llm_inputs,\n+            temperature=temperature,\n+            top_p=top_p,\n+            max_gen_len=max_gen_len,\n+            logprobs=logprobs,\n+            echo=echo,\n+        ):\n+            if all(r.finished for r in result):\n+                break\n+            yield result\n+\n+\n+def sample_top_p(probs, p):\n+    \"\"\"\n+    Perform top-p (nucleus) sampling on a probability distribution.\n+\n+    Args:\n+        probs (torch.Tensor): Probability distribution tensor.\n+        p (float): Probability threshold for top-p sampling.\n+\n+    Returns:\n+        torch.Tensor: Sampled token indices.\n+\n+    Note:\n+        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n+        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n+    \"\"\"\n+    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n+    probs_sum = torch.cumsum(probs_sort, dim=-1)\n+    mask = probs_sum - probs_sort > p\n+    probs_sort[mask] = 0.0\n+    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n+    next_token = torch.multinomial(probs_sort, num_samples=1)\n+    next_token = torch.gather(probs_idx, -1, next_token)\n+    return next_token\n\n--- File: models/llama4/model.py ---\n@@ -0,0 +1,446 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+import math\n+from typing import Any, Dict, List, Optional, Tuple\n+\n+import fairscale.nn.model_parallel.initialize as fs_init\n+import torch\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.layers import (\n+    ColumnParallelLinear,\n+    RowParallelLinear,\n+    VocabParallelEmbedding,\n+)\n+from torch import nn\n+\n+from .args import ModelArgs\n+from .datatypes import TransformerInput, TransformerOutput\n+from .ffn import FeedForward\n+from .moe import MoE\n+\n+\n+class RMSNorm(torch.nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.ones(dim))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float()).type_as(x)\n+        return output * self.weight\n+\n+\n+class L2Norm(torch.nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        return self._norm(x.float()).type_as(x)\n+\n+\n+def apply_scaling(freqs: torch.Tensor):\n+    # Values obtained from grid search\n+    scale_factor = 8\n+    low_freq_factor = 1\n+    high_freq_factor = 4\n+    old_context_len = 8192  # original llama3 length\n+\n+    low_freq_wavelen = old_context_len / low_freq_factor\n+    high_freq_wavelen = old_context_len / high_freq_factor\n+    new_freqs = []\n+    for freq in freqs:\n+        wavelen = 2 * math.pi / freq\n+        if wavelen < high_freq_wavelen:\n+            new_freqs.append(freq)\n+        elif wavelen > low_freq_wavelen:\n+            new_freqs.append(freq / scale_factor)\n+        else:\n+            assert low_freq_wavelen != high_freq_wavelen\n+            smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n+            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n+    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n+\n+\n+def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False):\n+    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n+    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n+    if use_scaled:\n+        freqs = apply_scaling(freqs)\n+    freqs = torch.outer(t, freqs)\n+    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n+    return freqs_cis\n+\n+\n+def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n+    ndim = x.ndim\n+    assert 0 <= 1 < ndim\n+    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n+    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n+    return freqs_cis.view(*shape)\n+\n+\n+def apply_rotary_emb(\n+    xq: torch.Tensor,\n+    xk: torch.Tensor,\n+    freqs_cis: torch.Tensor,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n+    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n+    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n+    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n+    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n+    return xq_out.type_as(xq), xk_out.type_as(xk)\n+\n+\n+class Attention(nn.Module):\n+    # TODO: this module needs to be moved into a separate file since it can be used by\n+    # the vision encoder as well.\n+    def __init__(\n+        self,\n+        args: ModelArgs,\n+        use_qk_norm: bool,\n+        use_rope: bool,\n+        add_bias: bool = False,\n+    ):\n+        super().__init__()\n+        self.use_rope = use_rope\n+        self.use_qk_norm = use_qk_norm\n+        # For attention temperature tuning\n+        self.attn_temperature_tuning = args.attn_temperature_tuning\n+        self.floor_scale = args.floor_scale\n+        self.attn_scale = args.attn_scale\n+\n+        self.n_heads = args.n_heads\n+        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n+        world_size = fs_init.get_model_parallel_world_size()\n+        self.n_local_heads = args.n_heads // world_size\n+        self.n_local_kv_heads = self.n_kv_heads // world_size\n+        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n+        self.head_dim = args.dim // args.n_heads\n+\n+        self.wq = ColumnParallelLinear(\n+            args.dim,\n+            args.n_heads * self.head_dim,\n+            bias=add_bias,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wk = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=add_bias,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wv = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=add_bias,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wo = RowParallelLinear(\n+            args.n_heads * self.head_dim,\n+            args.dim,\n+            bias=add_bias,\n+            input_is_parallel=True,\n+            init_method=lambda x: x,\n+        )\n+\n+        self.cache_k = torch.zeros(\n+            (\n+                args.max_batch_size,\n+                args.max_seq_len,\n+                self.n_local_kv_heads,\n+                self.head_dim,\n+            )\n+        ).cuda()\n+        self.cache_v = torch.zeros(\n+            (\n+                args.max_batch_size,\n+                args.max_seq_len,\n+                self.n_local_kv_heads,\n+                self.head_dim,\n+            )\n+        ).cuda()\n+        self.qk_norm = None\n+        if self.use_qk_norm:\n+            self.qk_norm = L2Norm(args.norm_eps)\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        if prefix + \"wqkv.weight\" in state_dict:\n+            wqkv = state_dict.pop(prefix + \"wqkv.weight\")\n+            d, r = divmod(wqkv.shape[0], self.n_heads + 2 * self.n_kv_heads)\n+            if r != 0:\n+                raise ValueError(\n+                    f\"shape={tuple(wqkv.shape)} is not divisible by \"\n+                    f\"n_heads ({self.n_heads}) + 2 * n_kv_heads ({self.n_kv_heads})\"\n+                )\n+            wq, wk, wv = wqkv.split([d * self.n_heads, d * self.n_kv_heads, d * self.n_kv_heads], dim=0)\n+            state_dict[prefix + \"wq.weight\"] = wq\n+            state_dict[prefix + \"wk.weight\"] = wk\n+            state_dict[prefix + \"wv.weight\"] = wv\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        start_pos: int,\n+        freqs_cis: torch.Tensor,\n+        mask: Optional[torch.Tensor] = None,\n+    ):\n+        bsz, seqlen, _ = x.shape\n+        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n+\n+        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n+        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n+        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n+\n+        if self.use_rope:\n+            xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n+\n+        if self.use_qk_norm:\n+            xq = self.qk_norm(xq)\n+            xk = self.qk_norm(xk)\n+\n+        # We are applying temperature tuning (https://arxiv.org/abs/2501.19399) to NoPE layers, where\n+        # the inference-time temperature tuning function is customized to not affect short context\n+        # while working at very long context\n+        if self.attn_temperature_tuning and not self.use_rope:\n+            seq_positions = torch.arange(start_pos, start_pos + seqlen, device=xq.device, dtype=torch.float32)\n+            attn_scales = torch.log(torch.floor((seq_positions + 1.0) / self.floor_scale) + 1.0) * self.attn_scale + 1.0\n+\n+            # reshape for broadcasting [seqlen] -> [1, seqlen, 1, 1]\n+            attn_scales = attn_scales.view(1, seqlen, 1, 1)\n+            xq = xq * attn_scales\n+\n+        self.cache_k = self.cache_k.to(xq)\n+        self.cache_v = self.cache_v.to(xq)\n+\n+        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n+        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n+\n+        xk = self.cache_k[:bsz, : start_pos + seqlen]\n+        xv = self.cache_v[:bsz, : start_pos + seqlen]\n+\n+        xq, xk, xv = [t.transpose(1, 2) for t in (xq, xk, xv)]\n+\n+        xk = xk.repeat_interleave(self.n_rep, dim=1)\n+        xv = xv.repeat_interleave(self.n_rep, dim=1)\n+\n+        attn_output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=mask, dropout_p=0.0)\n+        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n+        output = self.wo(attn_output)\n+        return output\n+\n+\n+class TransformerBlock(nn.Module):\n+    def __init__(self, layer_id: int, args: ModelArgs):\n+        super().__init__()\n+        self.n_heads = args.n_heads\n+        self.dim = args.dim\n+        self.head_dim = args.dim // args.n_heads if args.head_dim is None else args.head_dim\n+\n+        self.is_nope_layer = args.nope_layer_interval is not None and (layer_id + 1) % args.nope_layer_interval == 0\n+\n+        use_rope = not self.is_nope_layer\n+        use_qk_norm = args.use_qk_norm and not self.is_nope_layer\n+\n+        self.attention = Attention(args, use_rope=use_rope, use_qk_norm=use_qk_norm)\n+\n+        if args.moe_args and (layer_id + 1) % args.moe_args.interleave_moe_layer_step == 0:\n+            self.feed_forward = MoE(\n+                dim=args.dim,\n+                hidden_dim=int(args.ffn_exp * args.dim),\n+                ffn_dim_multiplier=args.ffn_dim_multiplier,\n+                multiple_of=args.multiple_of,\n+                moe_args=args.moe_args,\n+            )\n+        else:\n+            hidden_dim = int(4 * args.dim)\n+            hidden_dim = int(2 * hidden_dim / 3)\n+            if args.ffn_dim_multiplier is not None:\n+                hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n+            hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n+\n+            self.feed_forward = FeedForward(\n+                dim=args.dim,\n+                hidden_dim=hidden_dim,\n+            )\n+        self.layer_id = layer_id\n+        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        if prefix + \"attention.wqkv.layer_norm_weight\" in state_dict:\n+            state_dict[prefix + \"attention_norm.weight\"] = state_dict.pop(prefix + \"attention.wqkv.layer_norm_weight\")\n+\n+        if prefix + \"feed_forward.mlp.layer_norm_weight\" in state_dict:\n+            state_dict[prefix + \"ffn_norm.weight\"] = state_dict.pop(prefix + \"feed_forward.mlp.layer_norm_weight\")\n+        elif prefix + \"feed_forward.norm.weight\" in state_dict:\n+            state_dict[prefix + \"ffn_norm.weight\"] = state_dict.pop(prefix + \"feed_forward.norm.weight\")\n+\n+        for k in (\n+            \"feed_forward.experts.mlp\",\n+            \"feed_forward.mlp_shared\",\n+            \"attention.wo\",\n+            \"attention.wqkv\",\n+        ):\n+            if prefix + k + \"._extra_state\" in state_dict:\n+                state_dict.pop(prefix + k + \"._extra_state\")\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        start_pos: int,\n+        freqs_cis: torch.Tensor,\n+        global_attn_mask: Optional[torch.Tensor],\n+        local_attn_mask: Optional[torch.Tensor],\n+    ):\n+        # The iRoPE architecture uses global attention mask for NoPE layers or\n+        # if chunked local attention is not used\n+        if self.is_nope_layer or local_attn_mask is None:\n+            mask = global_attn_mask\n+        else:\n+            mask = local_attn_mask\n+\n+        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n+        out = h + self.feed_forward(self.ffn_norm(h))\n+        return out\n+\n+\n+class Transformer(nn.Module):\n+    def __init__(self, args: ModelArgs, **kwargs) -> None:\n+        super().__init__()\n+        self.args = args\n+\n+        self.vocab_size = args.vocab_size\n+        self.n_layers = args.n_layers\n+\n+        self.tok_embeddings = VocabParallelEmbedding(args.vocab_size, args.dim, init_method=lambda x: x)\n+\n+        self.layers = torch.nn.ModuleList()\n+        for layer_id in range(args.n_layers):\n+            self.layers.append(TransformerBlock(layer_id, args))\n+\n+        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n+        self.output = ColumnParallelLinear(args.dim, args.vocab_size, bias=False, init_method=lambda x: x)\n+\n+        self.freqs_cis = precompute_freqs_cis(\n+            args.dim // args.n_heads,\n+            args.max_seq_len * 2,\n+            args.rope_theta,\n+            args.use_scaled_rope,\n+        )\n+        vision_args = self.args.vision_args\n+        if vision_args:\n+            # circular import otherwise until we refactor out Attention\n+            from .vision.embedding import VisionEmbeddings\n+\n+            self.vision_embeddings = VisionEmbeddings(vision_args)\n+            self.vision_projection = ColumnParallelLinear(\n+                vision_args.output_dim,\n+                args.dim,\n+                bias=False,\n+                init_method=lambda x: x,\n+            )\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        if prefix + \"rope.freqs\" in state_dict:\n+            state_dict.pop(prefix + \"rope.freqs\")\n+\n+    @torch.inference_mode()\n+    def forward(self, model_input: TransformerInput) -> TransformerOutput:\n+        tokens = model_input.tokens\n+        start_pos = model_input.tokens_position\n+        assert isinstance(start_pos, int), (\n+            \"This implementation does not support different start positions per batch item\"\n+        )\n+\n+        _bsz, seqlen = tokens.shape\n+        h = self.tok_embeddings(tokens)\n+\n+        if image_embedding := model_input.image_embedding:\n+            h_image = self.vision_projection(image_embedding.embedding)\n+            h = h * ~image_embedding.mask + h_image * image_embedding.mask\n+\n+        self.freqs_cis = self.freqs_cis.to(h.device)\n+        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n+\n+        global_attn_mask, local_attn_mask = None, None\n+        if seqlen > 1:\n+            global_attn_mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n+            global_attn_mask = torch.triu(global_attn_mask, diagonal=1).type_as(h)\n+\n+            # https://github.com/pytorch/pytorch/issues/100005\n+            # torch.triu is buggy when the device is mps: filled values are\n+            # nan instead of 0.\n+            if global_attn_mask.device.type == torch.device(\"mps\").type:\n+                global_attn_mask = torch.nan_to_num(global_attn_mask, nan=0.0)\n+\n+            if chunk_size := self.args.attention_chunk_size:\n+                local_attn_mask = create_chunked_attention_mask(seqlen, chunk_size, tokens.device)\n+\n+        for layer in self.layers:\n+            h = layer(h, start_pos, freqs_cis, global_attn_mask, local_attn_mask)\n+        h = self.norm(h)\n+        output = self.output(h).float()\n+\n+        return TransformerOutput(logits=output)\n+\n+\n+# tokens (0, K), (K, 2K), (2K, 3K) attend to each other when doing local chunked attention\n+# in the iRoPE architecture\n+def create_chunked_attention_mask(seq_len: int, attention_chunk_size: int, device: torch.device) -> torch.Tensor:\n+    block_pos = torch.abs(\n+        (torch.arange(seq_len).unsqueeze(0) // attention_chunk_size)\n+        - (torch.arange(seq_len).unsqueeze(1) // attention_chunk_size)\n+    )\n+    token_pos = torch.arange(seq_len).unsqueeze(0) - torch.arange(seq_len).unsqueeze(1)\n+    mask = (block_pos == 0) & (token_pos <= 0)\n+    return mask.to(device)\n\n--- File: models/llama4/moe.py ---\n@@ -0,0 +1,225 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# ruff: noqa: N806\n+# pyre-strict\n+from typing import Any, Dict, List\n+\n+import fairscale.nn.model_parallel.initialize as fs_init\n+import torch\n+from fairscale.nn.model_parallel.mappings import reduce_from_model_parallel_region\n+from torch import Tensor, nn\n+from torch.nn import functional as F\n+\n+from .args import MoEArgs\n+from .ffn import FeedForward\n+\n+\n+class Experts(nn.Module):\n+    def __init__(\n+        self,\n+        num_local_experts: int,\n+        dim: int,\n+        hidden_dim: int,\n+    ) -> None:\n+        super().__init__()\n+\n+        dtype = torch.get_default_dtype()\n+        self.num_local_experts = num_local_experts\n+        self.dim = dim\n+        divide_factor = fs_init.get_model_parallel_world_size()\n+\n+        self.w1: nn.Parameter = nn.Parameter(\n+            torch.empty(\n+                num_local_experts,\n+                dim,\n+                divide_exact(hidden_dim, divide_factor),\n+                dtype=dtype,\n+            )\n+        )\n+\n+        self.w2: nn.Parameter = nn.Parameter(\n+            torch.empty(\n+                num_local_experts,\n+                divide_exact(hidden_dim, divide_factor),\n+                dim,\n+                dtype=dtype,\n+            )\n+        )\n+\n+        self.w3: nn.Parameter = nn.Parameter(\n+            torch.empty(\n+                num_local_experts,\n+                dim,\n+                divide_exact(hidden_dim, divide_factor),\n+                dtype=dtype,\n+            )\n+        )\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        self.prefix = prefix\n+        if prefix + \"moe_w_in_eD_F\" in state_dict:\n+            e = self.num_local_experts\n+            D = self.dim\n+            state_dict[prefix + \"w1\"] = state_dict.pop(prefix + \"moe_w_in_eD_F\").view(e, D, -1)\n+            state_dict[prefix + \"w2\"] = state_dict.pop(prefix + \"moe_w_out_eF_D\").view(e, -1, D)\n+            state_dict[prefix + \"w3\"] = state_dict.pop(prefix + \"moe_w_swiglu_eD_F\").view(e, D, -1)\n+\n+    def forward(\n+        self,\n+        routed_in_egD: torch.Tensor,  # noqa: N803\n+    ) -> torch.Tensor:\n+        e = self.num_local_experts\n+        D = self.dim\n+\n+        x_egD = routed_in_egD.view(e, -1, D)\n+\n+        out_egD = self.batched_swiglu(x_egD, self.w1, self.w3, self.w2)\n+        out_egD = out_egD.view(-1, D)\n+\n+        return out_egD\n+\n+    def batched_swiglu(self, x: Tensor, w1: Tensor, w3: Tensor, w2: Tensor) -> Tensor:\n+        middle_out_egF = F.silu(torch.bmm(x, w1)) * torch.bmm(x, w3)\n+        return torch.bmm(middle_out_egF, w2)\n+\n+\n+class MoE(torch.nn.Module):\n+    \"\"\"\n+    This EC implementation is modified from the original EC module.\n+    We refactored the token permutation and unpermutation logic and added support to tp and dp2ep sharding.\n+    This module supports 3 sharding methods of the experts:\n+    - tp: each TP rank has n_experts experts. Experts are sharded following the conventional row/column-parallel TP sharding.\n+    - tp2ep: each TP rank has n_experts/tp experts. Experts are not sharded.\n+    - dp2ep: each EP rank has n_experts/ep experts. Experts are sharded following the row/column-parallel TP sharding.\n+    Tensors used in this module are annotated with the suffixes that indicate the shape of the tensor.\n+    Several commonly used annotations include:\n+    - a: bsz*slen\n+    - E: number of experts\n+    - e: number of local experts per ep (n_experts/ep)\n+    - et: number of local experts per tp (n_experts/tp)\n+    - D: hidden dimension\n+    - d: D/tp\n+    - F: model dimension\n+    - f: F/tp (used in column/row-parallel linear)\n+    - G: number of tokens per expert (a * capacity_factor / E)\n+    - g: number of tokens per expert per TP rank (i.e., G/TP)\n+    - GG: G*EP (number of tokens per expert received via inter-EP a2a when ag_along_first_dim=False)\n+    - gg: g*EP (number of tokens per expert received via inter-EP a2a when ag_along_first_dim=True)\n+\n+    Examples:\n+    x_aD [a, D]\n+    routed_in_etG_D [et*G, D]\n+    x_eGGD: [e, GG, D]\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        ffn_dim_multiplier: float,\n+        multiple_of: int,\n+        moe_args: MoEArgs,\n+    ) -> None:\n+        super().__init__()\n+\n+        self.moe_args = moe_args\n+\n+        hidden_dim_denom: float = 1\n+        if moe_args.auto_scale_F:\n+            hidden_dim_denom = moe_args.capacity_factor + 1\n+\n+        hidden_dim = int(2 * hidden_dim / 3)\n+\n+        # custom dim factor multiplier\n+        hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n+\n+        if moe_args.auto_scale_F:\n+            hidden_dim = int(hidden_dim / hidden_dim_denom)\n+\n+        hidden_dim += -hidden_dim % multiple_of\n+\n+        num_local_experts: int = moe_args.num_experts\n+        dtype: torch.dtype = torch.get_default_dtype()\n+        self.experts = Experts(\n+            num_local_experts,\n+            dim,\n+            hidden_dim,\n+        )\n+\n+        self.router_DE: nn.Parameter = nn.Parameter(torch.empty(dim, moe_args.num_experts, dtype=dtype))\n+        self.shared_expert = FeedForward(dim, hidden_dim, do_reduce=False)\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool,\n+        missing_keys: List[str],\n+        unexpected_keys: List[str],\n+        error_msgs: List[str],\n+    ) -> None:\n+        if prefix + \"w_in_shared_FD.weight\" in state_dict:\n+            state_dict[prefix + \"shared_expert.w1.weight\"] = state_dict.pop(prefix + \"w_in_shared_FD.weight\")\n+            state_dict[prefix + \"shared_expert.w3.weight\"] = state_dict.pop(prefix + \"w_swiglu_FD.weight\")\n+            state_dict[prefix + \"shared_expert.w2.weight\"] = state_dict.pop(prefix + \"w_out_shared_DF.weight\")\n+\n+    def forward(self, x_bsD: Tensor) -> Tensor:  # noqa: N803\n+        _, slen, D = x_bsD.shape\n+        x_aD = x_bsD.view(-1, D)\n+\n+        a = x_aD.shape[0]\n+\n+        router_scores: Tensor = torch.matmul(x_aD, self.router_DE).transpose(0, 1)\n+\n+        router_scores_aK, router_indices_aK = torch.topk(router_scores.transpose(0, 1), self.moe_args.top_k, dim=1)\n+        router_scores = (\n+            torch.full_like(router_scores.transpose(0, 1), float(\"-inf\"))\n+            .scatter_(1, router_indices_aK, router_scores_aK)\n+            .transpose(0, 1)\n+        )\n+        router_indices = torch.arange(a, device=x_aD.device).view(1, -1).expand(router_scores.size(0), -1)\n+\n+        router_scores = torch.sigmoid(router_scores)\n+\n+        routed_in_EG_D: Tensor = torch.gather(\n+            x_aD,\n+            dim=0,\n+            index=router_indices.reshape(-1, 1).expand(-1, D),\n+        )\n+        routed_in_EG_D = routed_in_EG_D * router_scores.reshape(-1, 1)\n+\n+        out_aD = self.shared_expert(x_aD)\n+        routed_out_egg_D = self.experts(routed_in_EG_D.detach())\n+\n+        router_indices_EG_D = router_indices.reshape(-1, 1).expand(-1, D)\n+        out_aD.scatter_add_(\n+            dim=0,\n+            index=router_indices_EG_D,\n+            src=routed_out_egg_D.view(-1, D),\n+        )\n+        out_aD = reduce_from_model_parallel_region(out_aD)\n+        return out_aD.view(-1, slen, D)\n+\n+\n+def divide_exact(numerator: int, denominator: int) -> int:\n+    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(numerator, denominator)\n+    return numerator // denominator\n\n--- File: models/llama4/preprocess.py ---\n@@ -0,0 +1,430 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import math\n+from collections import defaultdict\n+from typing import Optional, Set, Tuple\n+\n+import torch\n+import torchvision.transforms as tv\n+from PIL import Image, ImageFile\n+from torchvision.transforms import functional as F\n+\n+ImageFile.LOAD_TRUNCATED_IMAGES = True\n+\n+IMAGE_RES = 448\n+\n+\n+class ResizeNormalizeImageTransform:\n+    def __init__(\n+        self,\n+        size_width=None,\n+        size_height=None,\n+    ) -> None:\n+        self._size_width = size_width or IMAGE_RES\n+        self._size_height = size_height or IMAGE_RES\n+        self._mean = (0.5, 0.5, 0.5)\n+        self._std = (0.5, 0.5, 0.5)\n+\n+        self.tv_transform = tv.Compose(\n+            [\n+                tv.Resize((self._size_height, self._size_width)),\n+                tv.ToTensor(),\n+                tv.Normalize(\n+                    mean=self._mean,\n+                    std=self._std,\n+                    inplace=True,\n+                ),\n+            ]\n+        )\n+\n+    def __call__(self, image: Image.Image) -> torch.Tensor:\n+        return self.tv_transform(image)\n+\n+\n+class VariableSizeImageTransform(object):\n+    \"\"\"\n+    This class accepts images of any size and dynamically resize, pads and chunks it\n+    based on the image aspect ratio and the number of image chunks we allow.\n+\n+    The algorithm will NOT distort the image fit a certain aspect ratio, because\n+    that leads to a significant degradation in image quality.\n+\n+    It can be summarized in 6 steps:\n+    1. Find all possible canvas combinations of max_num_chunks;\n+    2. Find the best canvas to fit the image;\n+    3. Resize without distortion\n+    4. Pad\n+    5. Normalize\n+    6. Chunk\n+\n+    For example, if an input image is of size 300x800, patch_size of 224,\n+    and max_num_chunks = 8, it will find the closest aspect ratio that\n+    is allowed within 8 image chunks, with some restrictions.\n+    In this case, 2:4 = 2 horizontal patches and 4 vertical patches,\n+    giving a total of 8 chunks.\n+\n+    If resize_to_max_canvas, the image will be resized (without distortion),\n+    to the largest possible resolution. In this case, 388:896, and padded to 448:896,\n+    where we maintain the original aspect ratio and pad with zeros value for the rest.\n+    This approach minimizes the amount of padding required for any arbitrary resolution.\n+\n+    However, if limit_upscaling_to_patch_size is set to True,\n+    the upscaling will be limited to the patch size. In the example above,\n+    the image would remain 300x800 (no upscaling), and then padded to 448:896.\n+\n+    The final output will therefore be of shape (8, 3, 224, 224), where 2x4\n+    patches are coming from the resizing and chunking.\n+    \"\"\"\n+\n+    def __init__(self, size: int = IMAGE_RES) -> None:\n+        self.size = size\n+        self.to_tensor = tv.ToTensor()\n+        self._mean = (0.5, 0.5, 0.5)\n+        self._std = (0.5, 0.5, 0.5)\n+        self.normalize = tv.Normalize(\n+            mean=self._mean,\n+            std=self._std,\n+            inplace=True,\n+        )\n+        self.resample = tv.InterpolationMode.BILINEAR\n+\n+    @staticmethod\n+    def get_factors(n: int) -> Set[int]:\n+        \"\"\"\n+        Calculate all factors of a given number, i.e. a dividor that leaves\n+        no remainder. For example, if n=12, it will return {1, 2, 3, 4, 6, 12}.\n+\n+        Args:\n+            n (int): The number to find factors for.\n+\n+        Returns:\n+            set: A set containing all factors of the number.\n+        \"\"\"\n+        factors_set = set()\n+\n+        for i in range(1, int(n**0.5) + 1):\n+            if n % i == 0:\n+                factors_set.add(i)\n+                factors_set.add(n // i)\n+        return factors_set\n+\n+    def find_supported_resolutions(self, max_num_chunks: int, patch_size: int) -> torch.Tensor:\n+        \"\"\"\n+        Computes all of the allowed resoltuions for a fixed number of chunks\n+        and patch_size. Useful for when dividing an image into chunks.\n+\n+        Args:\n+            max_num_chunks (int): Maximum number of chunks for processing.\n+            patch_size (int): Size of the side of the patch.\n+\n+        Returns:\n+            torch.Tensor: List of possible resolutions as tuples (height, width).\n+\n+        Example:\n+            >>> max_num_chunks = 5\n+            >>> patch_size = 224\n+            >>> find_supported_resolutions(max_num_chunks, patch_size)\n+            tensor([(224, 896), (448, 448), (224, 224), (896, 224), (224, 672),\n+            (672, 224), (224, 448), (448, 224)])\n+\n+            Given max_num_chunks=4, patch_size=224, it will create a dictionary:\n+            {\n+            0.25: [(1, 4)],\n+            1.0: [(2, 2), (1, 1)],\n+            4.0: [(4, 1)],\n+            0.33: [(1, 3)],\n+            3.0: [(3, 1)],\n+            0.5: [(1, 2)],\n+            2.0: [(2, 1)]\n+            }\n+\n+            and return the resolutions multiplied by the patch_size:\n+            [(1*224, 4*224), (2*224, 2*224), ..., (2*224, 1*224)]\n+        \"\"\"\n+        asp_dict = defaultdict(list)\n+        for chunk_size in range(max_num_chunks, 0, -1):\n+            _factors = sorted(self.get_factors(chunk_size))\n+            _asp_ratios = [(factor, chunk_size // factor) for factor in _factors]\n+            for height, width in _asp_ratios:\n+                ratio_float = height / width\n+                asp_dict[ratio_float].append((height, width))\n+\n+        # get the resolutions multiplied by the patch_size\n+        possible_resolutions = []\n+        for key, value in asp_dict.items():\n+            for height, depth in value:\n+                possible_resolutions.append((height * patch_size, depth * patch_size))\n+\n+        return possible_resolutions\n+\n+    @staticmethod\n+    def get_max_res_without_distortion(\n+        image_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+    ) -> Tuple[int, int]:\n+        \"\"\"\n+        Determines the maximum resolution to which an image can be resized to without distorting its\n+        aspect ratio, based on the target resolution.\n+\n+        Args:\n+            image_size (Tuple[int, int]): The original resolution of the image (height, width).\n+            target_resolution (Tuple[int, int]): The desired resolution to fit the image into (height, width).\n+        Returns:\n+            Tuple[int, int]: The optimal dimensions (height, width) to which the image should be resized.\n+        Example:\n+            >>> _get_max_res_without_distortion([200, 300], target_size = [450, 200])\n+            (134, 200)\n+            >>> _get_max_res_without_distortion([800, 600], target_size = [450, 1300])\n+            (450, 338)\n+        \"\"\"\n+\n+        original_width, original_height = image_size\n+        target_width, target_height = target_size\n+\n+        scale_w = target_width / original_width\n+        scale_h = target_height / original_height\n+\n+        if scale_w < scale_h:\n+            new_width = target_width\n+            new_height = min(math.floor(original_height * scale_w), target_height)\n+        else:\n+            new_height = target_height\n+            new_width = min(math.floor(original_width * scale_h), target_width)\n+\n+        return new_width, new_height\n+\n+    def _pad(self, image: Image.Image, target_size) -> Image.Image:\n+        new_width, new_height = target_size\n+        new_im = Image.new(mode=\"RGB\", size=(new_width, new_height), color=(0, 0, 0))  # type: ignore\n+        new_im.paste(image)\n+        return new_im\n+\n+    def _split(self, image: torch.Tensor, ncw: int, nch: int) -> torch.Tensor:\n+        # Split image into number of required tiles (width x height)\n+        num_channels, height, width = image.size()\n+        image = image.view(num_channels, nch, height // nch, ncw, width // ncw)\n+        # Permute dimensions to reorder the axes\n+        image = image.permute(1, 3, 0, 2, 4).contiguous()\n+        # Reshape into the desired output shape (batch_size * 4, num_channels, width/2, height/2)\n+        image = image.view(ncw * nch, num_channels, height // nch, width // ncw)\n+        return image\n+\n+    def resize_without_distortion(\n+        self,\n+        image: torch.Tensor,\n+        target_size: Tuple[int, int],\n+        max_upscaling_size: Optional[int],\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Used to resize an image to target_resolution, without distortion.\n+\n+        If target_size requires upscaling the image, the user can set max_upscaling_size to\n+        limit the upscaling to a maximum size. In this case, since we rescale without distortion,\n+        modifying target_size works as a boundary for the image's largest side.\n+\n+        Args:\n+            resample (str): Resampling method used when resizing images.\n+                Supports \"nearest\", \"nearest_exact\", \"bilinear\", \"bicubic\".\n+            max_upscaling_size (int): The maximum size to upscale the image to.\n+                If None, there is no limit.\n+        Examples:\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = 600\n+        >>> image_size = (400, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (600, 300)  # new_size_without_distortion\n+\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = 600\n+        >>> image_size = (2000, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (1000, 100)  # new_size_without_distortion\n+\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = 2000\n+        >>> image_size = (400, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (1000, 500)  # new_size_without_distortion\n+\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = None\n+        >>> image_size = (400, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (1000, 500)  # new_size_without_distortion\n+        \"\"\"\n+\n+        image_width, image_height = image.size\n+        image_size = (image_width, image_height)\n+\n+        # If target_size requires upscaling, we might want to limit the upscaling to max_upscaling_size\n+        if max_upscaling_size is not None:\n+            new_target_width = min(max(image_width, max_upscaling_size), target_size[0])\n+            new_target_height = min(max(image_height, max_upscaling_size), target_size[1])\n+            target_size = (new_target_width, new_target_height)\n+\n+        # resize to target_size while preserving aspect ratio\n+        new_size_without_distortion = self.get_max_res_without_distortion(image_size, target_size)\n+\n+        image = F.resize(\n+            image,\n+            (\n+                max(new_size_without_distortion[1], 1),\n+                max(new_size_without_distortion[0], 1),\n+            ),\n+            interpolation=self.resample,\n+        )\n+\n+        return image\n+\n+    def get_best_fit(\n+        self,\n+        image_size: Tuple[int, int],\n+        possible_resolutions: torch.Tensor,\n+        resize_to_max_canvas: bool = False,\n+    ) -> Tuple[int, int]:\n+        \"\"\"\n+        Determines the best canvas possible from a list of possible resolutions to, without distortion,\n+        resize an image to.\n+\n+        For each possible resolution, calculates the scaling factors for\n+        width and height, and selects the smallest one, which is the limiting side.\n+        E.g. to match the canvas you can upscale height by 2x, and width by 1.5x,\n+        therefore, the maximum upscaling you can do is min(2, 1.5) = 1.5.\n+\n+        If upscaling is possible (any of the scaling factors is greater than 1),\n+        then picks the smallest upscaling factor > 1, unless resize_to_max_canvas is True.\n+\n+        If upscaling is not possible, then picks the largest scaling factor <= 1, i.e.\n+        reduce downscaling as much as possible.\n+\n+        If there are multiple resolutions with the same max scale, we pick the one with the lowest area,\n+        to minimize padding. E.g., the same image can be upscaled to 224x224 and 224x448, but the latter\n+        has more padding.\n+\n+        Args:\n+            image_size (Tuple[int, int]): A tuple containing the height and width of the image.\n+            possible_resolutions (torch.Tensor): A tensor of shape (N, 2) where each\n+                row represents a possible resolution (height, width).\n+            use_max_upscaling (bool): If True, will return the largest upscaling resolution.\n+\n+        Returns:\n+            List[int]: The best resolution [height, width] for the given image.\n+\n+        Example:\n+            >>> image_size = (200, 300)\n+            >>> possible_resolutions = torch.tensor([[224, 672],\n+            ...                                     [672, 224],\n+            ...                                     [224, 448],\n+            ...                                     [448, 224],\n+            ...                                     [224, 224]])\n+            >>> _get_smallest_upscaling_possibility(image_size, possible_resolutions)\n+            [224, 448]\n+\n+            We have:\n+                scale_w = tensor([2.2400, 0.7467, 1.4933, 0.7467, 0.7467])\n+                scale_h = tensor([1.1200, 3.3600, 1.1200, 2.2400, 1.1200])\n+                scales = tensor([1.1200, 0.7467, 1.1200, 0.7467, 0.7467])\n+            Only one of the scales > 1:\n+                upscaling_possible = tensor([1.1200, 1.1200])\n+                smallest_rescale = tensor(1.1200)\n+            So we pick the resolution with the smallest smallest area:\n+                areas = tensor([150528, 100352]) # [672, 224], [224, 448]\n+                optimal_canvas = tensor([224, 448])\n+        \"\"\"\n+\n+        original_width, original_height = image_size\n+\n+        # get all possible resolutions heights/widths\n+        target_widths, target_heights = (\n+            possible_resolutions[:, 0],\n+            possible_resolutions[:, 1],\n+        )\n+\n+        # get scaling factors to resize the image without distortion\n+        scale_w = target_widths / original_width\n+        scale_h = target_heights / original_height\n+\n+        # get the min scale between width and height (limiting side -> no distortion)\n+        scales = torch.where(scale_w > scale_h, scale_h, scale_w)\n+\n+        # filter only scales that allow upscaling\n+        upscaling_options = scales[scales >= 1]\n+        if len(upscaling_options) > 0:\n+            if resize_to_max_canvas:\n+                selected_scale = torch.max(upscaling_options)\n+            else:\n+                selected_scale = torch.min(upscaling_options)\n+        else:\n+            # no upscaling possible,\n+            # get the minimum downscaling (max scale for scales<1)\n+            downscaling_options = scales[scales < 1]\n+            selected_scale = torch.max(downscaling_options)\n+\n+        # get all resolutions that support this scaling factor,\n+        # e.g. you can upscale to 224x224, 224x448, 224x672 without distortion\n+        chosen_canvas = possible_resolutions[scales == selected_scale]\n+\n+        # if there are multiple resolutions,\n+        # get the one with minimum area to reduce padding\n+        if len(chosen_canvas) > 1:\n+            areas = chosen_canvas[:, 0] * chosen_canvas[:, 1]\n+            optimal_idx = torch.argmin(areas)\n+            optimal_canvas = chosen_canvas[optimal_idx]\n+        else:\n+            optimal_canvas = chosen_canvas[0]\n+\n+        return tuple(optimal_canvas.tolist())\n+\n+    def __call__(\n+        self,\n+        image: Image.Image,\n+        max_num_chunks: int,\n+        normalize_img: bool = True,\n+        resize_to_max_canvas: bool = False,\n+    ) -> Tuple[torch.Tensor, Tuple[int, int]]:\n+        \"\"\"\n+        Args:\n+            image (PIL.Image): Image to be resized.\n+            max_num_chunks (int): Maximum number of chunks to split the image into.\n+            normalize_img (bool): Whether to normalize the image.\n+            resize_to_max_canvas (bool): Whether to resize the image to the maximum canvas size.\n+            If True, picks the canvas the allows the largest resizing without distortion.\n+            If False, downsample as little as possible, including no resizing at all,\n+            but never upsample, unless the image is smaller than the patch size.\n+        \"\"\"\n+        assert max_num_chunks > 0\n+        assert isinstance(image, Image.Image), type(image)\n+        w, h = image.size\n+\n+        possible_resolutions = self.find_supported_resolutions(max_num_chunks=max_num_chunks, patch_size=self.size)\n+        possible_resolutions = torch.tensor(possible_resolutions)\n+\n+        best_resolution = self.get_best_fit(\n+            image_size=(w, h),\n+            possible_resolutions=possible_resolutions,\n+            resize_to_max_canvas=resize_to_max_canvas,\n+        )\n+\n+        max_upscaling_size = None if resize_to_max_canvas else self.size\n+        image = self.resize_without_distortion(image, best_resolution, max_upscaling_size)\n+        image = self._pad(image, best_resolution)\n+\n+        image = self.to_tensor(image)\n+\n+        if normalize_img:\n+            image = self.normalize(image)\n+\n+        ratio_w, ratio_h = (\n+            best_resolution[0] // self.size,\n+            best_resolution[1] // self.size,\n+        )\n+\n+        image = self._split(image, ratio_w, ratio_h)  # type: ignore\n+\n+        ar = (ratio_h, ratio_w)\n+        return image, ar\n\n--- File: models/llama4/quantization/__init__.py ---\n@@ -5,6 +5,8 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-from .args import *  # noqa\n-from .chat_format import *  # noqa\n-from .tokenizer import *  # noqa\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# the root directory of this source tree.\n\n--- File: models/llama4/quantization/loader.py ---\n@@ -0,0 +1,226 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import logging\n+import os\n+from typing import Callable, Optional\n+\n+import torch\n+from fairscale.nn.model_parallel.initialize import get_model_parallel_rank\n+from torch import Tensor, nn\n+from torch.nn import functional as F\n+\n+from ..generation import QuantizationMode\n+from ..model import Transformer, TransformerBlock\n+from ..moe import MoE\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def swiglu_wrapper_no_reduce(\n+    self,\n+    x: Tensor,\n+):\n+    from ...quantize_impls import ffn_swiglu\n+\n+    return ffn_swiglu(x, self.w1.weight, self.w3.weight, self.w2.weight)\n+\n+\n+def experts_batched_swiglu_wrapper(\n+    self,\n+    x: Tensor,  # (e, g, D)\n+    w1: Tensor,  # (e, D, F)\n+    w3: Tensor,  # (e, D, F)\n+    w2: Tensor,  # (e, F, D)\n+) -> torch.Tensor:\n+    from ...quantize_impls import bmm_nt\n+\n+    middle_out_egF = F.silu(bmm_nt(x, w1)) * bmm_nt(x, w3)  # noqa: N806\n+    return bmm_nt(middle_out_egF, w2)\n+\n+\n+def convert_to_quantized_model(\n+    model: Transformer,\n+    checkpoint_dir: str,\n+    quantization_mode: Optional[str] = None,\n+    fp8_activation_scale_ub: Optional[float] = 1200.0,\n+    use_rich_progress: bool = True,\n+) -> Transformer:\n+    from ...quantize_impls import (\n+        Fp8ScaledWeights,\n+        Int4ScaledWeights,\n+        load_fp8,\n+        load_int4,\n+        quantize_fp8,\n+        quantize_int4,\n+    )\n+\n+    rank = get_model_parallel_rank()\n+\n+    def should_quantize_block(block: nn.Module) -> bool:\n+        if not isinstance(block, TransformerBlock):\n+            return False\n+\n+        if quantization_mode == QuantizationMode.fp8_mixed:\n+            # skip quantization on first and last layers\n+            return not (block.layer_id == 0 or block.layer_id == (model.n_layers - 1))\n+\n+        # always skip quantization on dense layers\n+        return isinstance(block.feed_forward, MoE)\n+\n+    use_rich_progress = use_rich_progress and rank == 0\n+    progress, log_status, update_status = logging_callbacks(use_rich_progress, rank, model, should_quantize_block)\n+    if quantization_mode == QuantizationMode.int4_mixed:\n+        int4_scales_path = os.path.join(checkpoint_dir, f\"int4_scales_{rank}.pt\")\n+        if os.path.isfile(int4_scales_path):\n+            log_status(f\"Rank {rank}: Loading int4 scales\")\n+            int4_scales = torch.load(int4_scales_path, weights_only=True)\n+\n+            def apply_quantization(key, weight):\n+                scale = int4_scales[key]\n+                return load_int4(\n+                    weight,\n+                    scale,\n+                    output_device=torch.device(\"cuda\"),\n+                )\n+\n+        else:\n+            log_status(f\"Rank {rank}: Quantizing int4 weights from bf16\")\n+\n+            def apply_quantization(_, weight):\n+                return quantize_int4(weight, output_device=torch.device(\"cuda\"))\n+\n+    else:\n+        fp8_scales_path = os.path.join(checkpoint_dir, f\"fp8_scales_{rank}.pt\")\n+        if os.path.isfile(fp8_scales_path):\n+            log_status(f\"Rank {rank}: Loading fp8 scales\")\n+            fp8_scales = torch.load(fp8_scales_path, weights_only=True)\n+\n+            def apply_quantization(key, weight):\n+                scale = fp8_scales[key]\n+                return load_fp8(\n+                    weight,\n+                    scale,\n+                    fp8_activation_scale_ub,\n+                    output_device=torch.device(\"cuda\"),\n+                )\n+\n+        else:\n+            log_status(f\"Rank {rank}: Quantizing fp8 weights from bf16\")\n+\n+            def apply_quantization(_, weight):\n+                return quantize_fp8(weight, fp8_activation_scale_ub, output_device=torch.device(\"cuda\"))\n+\n+    processed_blocks = 0\n+    try:\n+        if use_rich_progress:\n+            progress.start()\n+\n+        for _, block in model.named_modules():\n+            if not should_quantize_block(block):\n+                continue\n+\n+            update_status(f\"Rank {rank} - Layer {block.layer_id}\")\n+\n+            # Quantize only routed experts, not shared\n+            prefix = f\"layers.{block.layer_id}.feed_forward\"\n+            moe = block.feed_forward\n+            moe.experts.batched_swiglu = experts_batched_swiglu_wrapper.__get__(moe.experts)\n+\n+            for key in (\"w1\", \"w3\", \"w2\"):\n+                param = getattr(moe.experts, key)\n+                update_status(f\"Rank {rank} - Layer {block.layer_id} - MoE {key}\")\n+                setattr(\n+                    moe.experts,\n+                    key,\n+                    apply_quantization(\n+                        f\"{prefix}.experts.{key}\",\n+                        param.transpose(1, 2).contiguous(),\n+                    ),\n+                )\n+\n+            if quantization_mode == QuantizationMode.int4_mixed:\n+                # Quantize shared experts\n+                moe.shared_expert.forward = swiglu_wrapper_no_reduce.__get__(moe.shared_expert)\n+                for key in (\"w1\", \"w3\", \"w2\"):\n+                    param = getattr(moe.shared_expert, key)\n+                    update_status(f\"Rank {rank} - Layer {block.layer_id} - MoE shared expert {key}\")\n+                    param.weight = apply_quantization(f\"{prefix}.shared_expert.{key}\", param.weight)\n+\n+            processed_blocks += 1\n+            update_status(message=None, completed=processed_blocks)\n+\n+        update_status(f\"Rank {rank} - Moving parameters to CUDA\")\n+\n+        param_count = 0\n+        for _, parameter in model.named_parameters():\n+            if not isinstance(parameter, Fp8ScaledWeights) and not isinstance(parameter, Int4ScaledWeights):\n+                parameter.data = parameter.to(device=\"cuda\")\n+                param_count += 1\n+\n+        update_status(f\"Rank {rank} - Completed - moved {param_count} parameters to CUDA\")\n+    finally:\n+        if use_rich_progress:\n+            progress.stop()\n+\n+    return model\n+\n+\n+# fp8/int4 loading can be very slow so we add progress bars to make life slightly better\n+def logging_callbacks(\n+    use_rich_progress: bool,\n+    rank: int,\n+    model: Transformer,\n+    should_quantize_block: Callable[[nn.Module], bool],\n+):\n+    console = None\n+    if use_rich_progress:\n+        from rich.console import Console\n+\n+        console = Console(highlight=False)\n+\n+    def log_status(message: str) -> None:\n+        if use_rich_progress:\n+            console.print(message)\n+        elif rank == 0:  # Only log from rank 0 for non-rich logging\n+            log.info(message)\n+\n+    total_blocks = sum(1 for _, block in model.named_modules() if should_quantize_block(block))\n+    progress = None\n+    if use_rich_progress:\n+        from rich.progress import (\n+            BarColumn,\n+            Progress,\n+            SpinnerColumn,\n+            TextColumn,\n+            TimeElapsedColumn,\n+            TimeRemainingColumn,\n+        )\n+\n+        progress = Progress(\n+            SpinnerColumn(),\n+            BarColumn(complete_style=\"green\", finished_style=\"bright_green\"),\n+            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n+            TimeElapsedColumn(),\n+            TextColumn(\"ETA:\"),\n+            TimeRemainingColumn(),\n+            TextColumn(\"[bold]{task.fields[status]}\"),\n+            console=console,\n+            expand=True,\n+        )\n+        task_id = progress.add_task(\"[blue]Converting layers...\", total=total_blocks, status=\"Starting\")\n+\n+    def update_status(message: Optional[str], completed: Optional[int] = None) -> None:\n+        if use_rich_progress:\n+            if message is not None:\n+                progress.update(task_id, status=message)\n+            if completed is not None:\n+                progress.update(task_id, completed=completed)\n+        elif rank == 0 and completed and completed % 10 == 0:\n+            log.info(f\"Rank {rank}: {completed}/{total_blocks} blocks completed\")\n+\n+    return progress, log_status, update_status\n\n--- File: models/llama4/scripts/chat_completion.py ---\n@@ -0,0 +1,115 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+from io import BytesIO\n+from pathlib import Path\n+from typing import Optional\n+\n+import fire\n+from termcolor import cprint\n+\n+from models.datatypes import RawMediaItem, RawMessage, RawTextItem, StopReason\n+from models.llama4.generation import Llama4\n+\n+THIS_DIR = Path(__file__).parent\n+\n+\n+def run_main(\n+    checkpoint_dir: str,\n+    world_size: int = 1,\n+    max_seq_len: Optional[int] = 4096,\n+    max_batch_size: Optional[int] = 1,\n+    temperature: float = 0.6,\n+    top_p: float = 0.9,\n+    quantization_mode: Optional[str] = None,\n+):\n+    generator = Llama4.build(\n+        checkpoint_dir,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+        world_size=world_size,\n+        quantization_mode=quantization_mode,\n+    )\n+\n+    dialogs = [\n+        [RawMessage(role=\"user\", content=\"what is the recipe of mayonnaise?\")],\n+        [\n+            RawMessage(\n+                role=\"user\",\n+                content=\"I am going to Paris, what should I see?\",\n+            ),\n+            RawMessage(\n+                role=\"assistant\",\n+                content=\"\"\"\\\n+Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n+\n+1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n+2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n+3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n+\n+These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n+                stop_reason=StopReason.end_of_turn,\n+            ),\n+            RawMessage(role=\"user\", content=\"What is so great about #1?\"),\n+        ],\n+        [\n+            RawMessage(role=\"system\", content=\"Always answer with Haiku\"),\n+            RawMessage(role=\"user\", content=\"I am going to Paris, what should I see?\"),\n+        ],\n+        # [\n+        #     RawMessage(role=\"system\", content=\"Always answer with emojis\"),\n+        #     RawMessage(role=\"user\", content=\"How to go from Beijing to NY?\"),\n+        # ],\n+    ]\n+    if generator.args.vision_args:\n+        with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n+            img1 = f.read()\n+\n+        with open(THIS_DIR / \"../../resources/pasta.jpeg\", \"rb\") as f:\n+            img2 = f.read()\n+\n+        dialogs.append(\n+            [\n+                RawMessage(\n+                    role=\"user\",\n+                    content=[\n+                        RawMediaItem(data=BytesIO(img1)),\n+                        RawMediaItem(data=BytesIO(img2)),\n+                        RawTextItem(text=\"Write a haiku that brings both images together\"),\n+                    ],\n+                ),\n+            ]\n+        )\n+\n+    for dialog in dialogs:\n+        for msg in dialog:\n+            print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n+\n+        batch = [dialog]\n+        for token_results in generator.chat_completion(\n+            batch,\n+            temperature=temperature,\n+            top_p=top_p,\n+            max_gen_len=max_seq_len,\n+        ):\n+            result = token_results[0]\n+            if result.finished:\n+                break\n+\n+            cprint(result.text, color=\"yellow\", end=\"\")\n+        print(\"\\n\")\n+\n+\n+def main():\n+    fire.Fire(run_main)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n\n--- File: models/llama4/scripts/completion.py ---\n@@ -0,0 +1,76 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+from io import BytesIO\n+from pathlib import Path\n+from typing import Optional\n+\n+import fire\n+from termcolor import cprint\n+\n+from models.datatypes import RawMediaItem\n+from models.llama4.generation import Llama4\n+\n+THIS_DIR = Path(__file__).parent\n+\n+\n+def run_main(\n+    checkpoint_dir: str,\n+    world_size: int = 1,\n+    max_seq_len: Optional[int] = 1024,\n+    max_batch_size: Optional[int] = 1,\n+    temperature: float = 0.6,\n+    top_p: float = 0.9,\n+    quantization_mode: Optional[str] = None,\n+):\n+    generator = Llama4.build(\n+        checkpoint_dir,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+        world_size=world_size,\n+        quantization_mode=quantization_mode,\n+    )\n+\n+    with open(THIS_DIR / \"../../resources/dog.jpg\", \"rb\") as f:\n+        img = f.read()\n+\n+    interleaved_contents = [\n+        # text only\n+        \"The color of the sky is blue but sometimes it can also be\",\n+        \"The capital of France is\",\n+        # image understanding\n+        [\n+            RawMediaItem(type=\"image\", data=BytesIO(img)),\n+            \"If I had to write a haiku for this one\",\n+        ],\n+    ]\n+\n+    for content in interleaved_contents:\n+        cprint(f\"{content}\", end=\"\")\n+        batch = [content]\n+        for token_results in generator.completion(\n+            batch,\n+            temperature=temperature,\n+            top_p=top_p,\n+        ):\n+            result = token_results[0]\n+            if result.finished:\n+                break\n+\n+            cprint(result.text, color=\"yellow\", end=\"\")\n+        print(\"\\n==================================\\n\")\n+\n+\n+def main():\n+    fire.Fire(run_main)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n\n--- File: models/llama4/scripts/quantize.py ---\n@@ -0,0 +1,220 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import json\n+import os\n+import sys\n+from pathlib import Path\n+from typing import Optional\n+\n+import fire\n+import torch\n+from models.llama4.args import ModelArgs\n+\n+from models.llama4.generation import QuantizationMode\n+from models.llama4.model import MoE, Transformer, TransformerBlock\n+from models.llama4.quantization.quantize_impls import int4_row_quantize, pack_int4\n+\n+try:\n+    import fbgemm_gpu.experimental.gen_ai  # noqa: F401\n+\n+    print(\"Using efficient FP8/INT4 operators in FBGEMM.\")\n+except ImportError:\n+    print(\"No efficient FP8/INT4 operators. Please install FBGEMM.\")\n+    raise\n+\n+from fairscale.nn.model_parallel.initialize import (\n+    get_model_parallel_rank,\n+    initialize_model_parallel,\n+    model_parallel_is_initialized,\n+)\n+\n+# You can run this script with:\n+#\n+# torchrun --nproc-per-node=8 -m models.llama4.scripts.quantize \\\n+#     --ckpt_dir /path/to/llama4/checkpoints \\\n+#     --output_dir /path/to/output/dir \\\n+#     --quantization_mode fp8_mixed \\\n+#     --world_size 8\n+\n+\n+def ffn_quantize(\n+    ckpt_dir: str,\n+    output_dir: str,\n+    quantization_mode: str,\n+    world_size: Optional[int] = None,\n+) -> None:\n+    \"\"\"\n+    Quantizes BF16 weights using \"rowwise fp8\" quantization or \"int4-weight-bf16-activation (int4_mixed)\" quantization. This method works well for FFNs.\n+\n+    It produces two outputs:\n+     - quantized weights (in consolidated.0X.pth)\n+     - fp8 scales (in fp8_scales_X.pt)\n+\n+    It produces three outputs for int4_mixed:\n+     - quantized weights (in consolidated.0X.pth)\n+     - int4 scales (in int4_scales_X.pt)\n+\n+    The keys in the fp8/int4 scales are named so that they can be loaded using quantization/loader.py\n+\n+    Args:\n+        ckpt_dir (str): The directory containing the checkpoint files.\n+        output_dir (str): The directory to save the quantized checkpoint and fp8/int4 scales.\n+        world_size (Optional[int]): The number of GPUs to use for model parallelization.\n+    \"\"\"\n+    print(f\"checkpoint_dir: {ckpt_dir} output_dir: {output_dir} world_size: {world_size}\")\n+    if not torch.distributed.is_initialized():\n+        torch.distributed.init_process_group(\"nccl\")\n+\n+    if not model_parallel_is_initialized():\n+        if world_size is None:\n+            world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n+        initialize_model_parallel(world_size)\n+\n+    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n+    torch.cuda.set_device(local_rank)\n+\n+    if local_rank > 0:\n+        sys.stdout = open(os.devnull, \"w\")\n+    if get_model_parallel_rank() == 0:\n+        os.makedirs(output_dir, exist_ok=True)\n+\n+    seed = 1\n+    torch.manual_seed(seed)\n+\n+    checkpoints = set(Path(ckpt_dir).glob(\"*.pth\"))\n+    assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n+    assert world_size == len(checkpoints), (\n+        f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n+    )\n+    checkpoints = sorted(checkpoints)\n+    ckpt_idx = get_model_parallel_rank()\n+    ckpt_path = checkpoints[ckpt_idx]\n+\n+    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n+        params = json.loads(f.read())\n+    model_args: ModelArgs = ModelArgs(**params, max_seq_len=1024, max_batch_size=1)\n+\n+    torch.set_default_tensor_type(torch.BFloat16Tensor)\n+    model = Transformer(model_args)\n+\n+    print(f\"Quantizing {ckpt_path}\")\n+    with open(ckpt_path, \"rb\") as f:\n+        print(\"Reading checkpoint...\")\n+        checkpoint = torch.load(f, map_location=\"cpu\", weights_only=False)\n+        print(\"Done...\")\n+\n+    print(\"Loading state dict...\")\n+    model.load_state_dict(checkpoint, strict=False)\n+    print(\"Done...\")\n+\n+    fp8_scales = {}\n+    int4_scales = {}\n+    old_keys = set(checkpoint.keys())\n+    new_state_dict = checkpoint\n+    for _, block in model.named_modules():\n+        if isinstance(block, TransformerBlock):\n+            if block.layer_id == 0 or block.layer_id == (model.n_layers - 1):\n+                continue\n+\n+            if not isinstance(block.feed_forward, MoE):\n+                continue\n+\n+            # IMPORTANT NOTE:\n+            #  (1) Keys for weights are exactly as in the original state dict\n+            #  (2) Keys for fp8/int4 scales are according to re-mapped keys based on the load_hooks\n+            #\n+            # Why is it? In any case, things are confusing. However, this limits the confusion in the loading\n+            # code. The load_hooks remain simple, and in the fp8/int4 loading code at runtime, we don't need to\n+            # do any key re-mapping.\n+            #\n+            # Secondly:\n+            #   (1) MoE experts weights are transposed back so once again the loading code remains same for\n+            #       on-the-fly vs. from-pre-quantized weights\n+            #   (2) Scales however are _not_ transposed back\n+\n+            prefix = f\"layers.{block.layer_id}.feed_forward\"\n+            moe = block.feed_forward\n+\n+            if quantization_mode == QuantizationMode.int4_mixed:\n+                for key in (\"w1\", \"w3\", \"w2\"):\n+                    param = getattr(moe.experts, key)\n+                    shape = param.shape\n+                    weight = param.transpose(1, 2).contiguous()\n+\n+                    if weight.ndim >= 3:\n+                        wq, scale = zip(*[int4_row_quantize(i.cuda()) for i in weight])\n+                        wq = torch.stack([pack_int4(i.cuda()) for i in wq], dim=0)\n+                        w_scale = torch.stack(scale, dim=0)\n+                    else:\n+                        wq, w_scale = int4_row_quantize(weight.cuda())\n+                        wq = pack_int4(wq.cuda())\n+\n+                    state_dict_key_map = {\n+                        \"w1\": \"moe_w_in_eD_F\",\n+                        \"w2\": \"moe_w_out_eF_D\",\n+                        \"w3\": \"moe_w_swiglu_eD_F\",\n+                    }\n+                    # we must save this as 2D in the state dict, since loading code expects 2D weights\n+                    new_shape = (-1, shape[-1])\n+                    wq = wq.transpose(1, 2).reshape(*new_shape).contiguous()\n+\n+                    # torch.nn.Parameter requires weights be floating point, so we cast packed int8 (2 * int4) to float8_e4m3fn, and will cast back to int8 in loading code\n+                    new_state_dict[f\"{prefix}.experts.{state_dict_key_map[key]}\"] = torch.nn.Parameter(\n+                        wq.view(torch.float8_e4m3fn)\n+                    )\n+                    int4_scales[f\"{prefix}.experts.{key}\"] = w_scale\n+                    print(f\"Quantized {prefix}.experts.{state_dict_key_map[key]} {wq.shape=} {w_scale.shape=}\")\n+\n+                new_keys = set(new_state_dict.keys())\n+                assert old_keys == new_keys, f\"old_keys != new_keys: {old_keys - new_keys}\"\n+\n+                print(\"Saving int4 scales\")\n+                int4_scales_path = os.path.join(output_dir, f\"int4_scales_{ckpt_idx}.pt\")\n+                torch.save(int4_scales, int4_scales_path)\n+            else:\n+                for key in (\"w1\", \"w3\", \"w2\"):\n+                    param = getattr(moe.experts, key)\n+                    shape = param.shape\n+                    wq, w_scale = torch.ops.fbgemm.quantize_fp8_per_row(param.transpose(1, 2).contiguous())\n+\n+                    state_dict_key_map = {\n+                        \"w1\": \"moe_w_in_eD_F\",\n+                        \"w2\": \"moe_w_out_eF_D\",\n+                        \"w3\": \"moe_w_swiglu_eD_F\",\n+                    }\n+                    # we must save this as 2D in the state dict, since loading code expects 2D weights\n+                    new_shape = (-1, shape[-1])\n+                    wq = wq.transpose(1, 2).reshape(*new_shape).contiguous()\n+\n+                    new_state_dict[f\"{prefix}.experts.{state_dict_key_map[key]}\"] = torch.nn.Parameter(wq)\n+                    fp8_scales[f\"{prefix}.experts.{key}\"] = w_scale\n+                    print(f\"Quantized {prefix}.experts.{state_dict_key_map[key]} {wq.shape=} {w_scale.shape=}\")\n+\n+                new_keys = set(new_state_dict.keys())\n+                assert old_keys == new_keys, f\"old_keys != new_keys: {old_keys - new_keys}\"\n+\n+                print(\"Saving fp8 scales\")\n+                fp8_scales_path = os.path.join(output_dir, f\"fp8_scales_{ckpt_idx}.pt\")\n+                torch.save(fp8_scales, fp8_scales_path)\n+\n+    ckpt_path = os.path.join(\n+        output_dir,\n+        \"consolidated.{:02d}.pth\".format(ckpt_idx),\n+    )\n+    print(f\"Saving checkpoint to {ckpt_path}\")\n+    torch.save(new_state_dict, ckpt_path)\n+\n+    torch.distributed.barrier()\n+\n+\n+def main():\n+    fire.Fire(ffn_quantize)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n\n--- File: models/llama4/tokenizer.py ---\n@@ -0,0 +1,270 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+import os\n+from logging import getLogger\n+from pathlib import Path\n+from typing import (\n+    AbstractSet,\n+    Collection,\n+    Dict,\n+    Iterator,\n+    List,\n+    Literal,\n+    Optional,\n+    Sequence,\n+    Union,\n+    cast,\n+)\n+\n+import tiktoken\n+from tiktoken.load import load_tiktoken_bpe\n+\n+logger = getLogger(__name__)\n+\n+\n+# The tiktoken tokenizer can handle <=400k chars without\n+# pyo3_runtime.PanicException.\n+TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n+\n+# https://github.com/openai/tiktoken/issues/195\n+# Here we iterate over subsequences and split if we exceed the limit\n+# of max consecutive non-whitespace or whitespace characters.\n+MAX_NO_WHITESPACES_CHARS = 25_000\n+\n+\n+_INSTANCE = None\n+\n+\n+def get_reserved_special_tokens(name, count, start_index=0):\n+    return [f\"<|{name}_reserved_special_token_{i}|>\" for i in range(start_index, start_index + count)]\n+\n+\n+# 200005, ..., 200079\n+LLAMA4_TEXT_POST_TRAIN_SPECIAL_TOKENS = [\n+    \"<|header_start|>\",\n+    \"<|header_end|>\",\n+    \"<|eom|>\",\n+    \"<|eot|>\",\n+    \"<|step|>\",\n+    \"<|text_post_train_reserved_special_token_0|>\",\n+    \"<|text_post_train_reserved_special_token_1|>\",\n+    \"<|text_post_train_reserved_special_token_2|>\",\n+    \"<|text_post_train_reserved_special_token_3|>\",\n+    \"<|text_post_train_reserved_special_token_4|>\",\n+    \"<|text_post_train_reserved_special_token_5|>\",\n+    \"<|finetune_right_pad|>\",\n+] + get_reserved_special_tokens(\n+    \"text_post_train\", 61, 6\n+)  # <|text_post_train_reserved_special_token_6|>, ..., <|text_post_train_reserved_special_token_66|>\n+\n+# 200080, ..., 201133\n+LLAMA4_VISION_SPECIAL_TOKENS = [\n+    \"<|image_start|>\",\n+    \"<|image_end|>\",\n+    \"<|vision_reserved_special_token_0|>\",\n+    \"<|vision_reserved_special_token_1|>\",\n+    \"<|tile_x_separator|>\",\n+    \"<|tile_y_separator|>\",\n+    \"<|vision_reserved_special_token_2|>\",\n+    \"<|vision_reserved_special_token_3|>\",\n+    \"<|vision_reserved_special_token_4|>\",\n+    \"<|vision_reserved_special_token_5|>\",\n+    \"<|image|>\",\n+    \"<|vision_reserved_special_token_6|>\",\n+    \"<|patch|>\",\n+] + get_reserved_special_tokens(\n+    \"vision\", 1041, 7\n+)  # <|vision_reserved_special_token_7|>, ..., <|vision_reserved_special_token_1047|>\n+\n+# 201134, ..., 201141\n+LLAMA4_REASONING_SPECIAL_TOKENS = [\n+    \"<|reasoning_reserved_special_token_0|>\",\n+    \"<|reasoning_reserved_special_token_1|>\",\n+    \"<|reasoning_reserved_special_token_2|>\",\n+    \"<|reasoning_reserved_special_token_3|>\",\n+    \"<|reasoning_reserved_special_token_4|>\",\n+    \"<|reasoning_reserved_special_token_5|>\",\n+    \"<|reasoning_thinking_start|>\",\n+    \"<|reasoning_thinking_end|>\",\n+]\n+\n+LLAMA4_SPECIAL_TOKENS = (\n+    LLAMA4_TEXT_POST_TRAIN_SPECIAL_TOKENS + LLAMA4_VISION_SPECIAL_TOKENS + LLAMA4_REASONING_SPECIAL_TOKENS\n+)\n+\n+BASIC_SPECIAL_TOKENS = [\n+    \"<|begin_of_text|>\",\n+    \"<|end_of_text|>\",\n+    \"<|fim_prefix|>\",\n+    \"<|fim_middle|>\",\n+    \"<|fim_suffix|>\",\n+]\n+\n+\n+class Tokenizer:\n+    \"\"\"\n+    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n+    \"\"\"\n+\n+    special_tokens: Dict[str, int]\n+\n+    num_reserved_special_tokens = 2048\n+\n+    O200K_PATTERN = r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"  # noqa: E501\n+\n+    @classmethod\n+    def get_instance(cls):\n+        global _INSTANCE\n+\n+        if _INSTANCE is None:\n+            _INSTANCE = Tokenizer(os.path.join(os.path.dirname(__file__), \"tokenizer.model\"))\n+        return _INSTANCE\n+\n+    def __init__(self, model_path: str):\n+        \"\"\"\n+        Initializes the Tokenizer with a Tiktoken model.\n+\n+        Args:\n+            model_path (str): The path to the Tiktoken model file.\n+        \"\"\"\n+        assert os.path.isfile(model_path), model_path\n+\n+        mergeable_ranks = load_tiktoken_bpe(model_path)\n+        num_base_tokens = len(mergeable_ranks)\n+\n+        special_tokens = BASIC_SPECIAL_TOKENS + LLAMA4_SPECIAL_TOKENS\n+        assert len(set(special_tokens)) == len(special_tokens)\n+        assert len(special_tokens) <= self.num_reserved_special_tokens\n+\n+        reserved_tokens = [\n+            f\"<|reserved_special_token_{i}|>\" for i in range(self.num_reserved_special_tokens - len(special_tokens))\n+        ]\n+        special_tokens = special_tokens + reserved_tokens\n+\n+        self.special_tokens = {token: num_base_tokens + i for i, token in enumerate(special_tokens)}\n+        self.model = tiktoken.Encoding(\n+            name=Path(model_path).name,\n+            pat_str=self.O200K_PATTERN,\n+            mergeable_ranks=mergeable_ranks,\n+            special_tokens=self.special_tokens,\n+        )\n+\n+        self.n_words: int = num_base_tokens + len(special_tokens)\n+\n+        # BOS / EOS token IDs\n+        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n+        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n+\n+        self.pad_id: int = self.special_tokens[\"<|finetune_right_pad|>\"]\n+        self.eot_id: int = self.special_tokens[\"<|eot|>\"]\n+        self.eom_id: int = self.special_tokens[\"<|eom|>\"]\n+\n+        self.thinking_start_id: int = self.special_tokens[\"<|reasoning_thinking_start|>\"]\n+        self.thinking_end_id: int = self.special_tokens[\"<|reasoning_thinking_end|>\"]\n+\n+        self.stop_tokens = [\n+            self.eos_id,\n+            self.special_tokens[\"<|eom|>\"],\n+            self.special_tokens[\"<|eot|>\"],\n+        ]\n+\n+    def encode(\n+        self,\n+        s: str,\n+        *,\n+        bos: bool,\n+        eos: bool,\n+        allowed_special: Optional[Union[Literal[\"all\"], AbstractSet[str]]] = None,\n+        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n+    ) -> List[int]:\n+        \"\"\"\n+        Encodes a string into a list of token IDs.\n+\n+        Args:\n+            s (str): The input string to be encoded.\n+            bos (bool): Whether to prepend the beginning-of-sequence token.\n+            eos (bool): Whether to append the end-of-sequence token.\n+            allowed_special (\"all\"|set[str]): allowed special tokens in string\n+            disallowed_special (\"all\"|set[str]): special tokens that raise an error when in string\n+\n+        Returns:\n+            list[int]: A list of token IDs.\n+\n+        By default, setting disallowed_special=() encodes a string by ignoring\n+        special tokens. Specifically:\n+        - Setting `disallowed_special` to () will cause all text corresponding\n+          to special tokens to be encoded as natural text (insteading of raising\n+          an error).\n+        - Setting `allowed_special` to \"all\" will treat all text corresponding\n+          to special tokens to be encoded as special tokens.\n+        \"\"\"\n+        if allowed_special is None:\n+            allowed_special = set()\n+        assert type(s) is str\n+\n+        substrs = (\n+            substr\n+            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n+            for substr in self._split_whitespaces_or_nonwhitespaces(\n+                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n+            )\n+        )\n+        t: List[int] = []\n+        for substr in substrs:\n+            t.extend(\n+                self.model.encode(\n+                    substr,\n+                    allowed_special=allowed_special,\n+                    disallowed_special=disallowed_special,\n+                )\n+            )\n+        if bos:\n+            t.insert(0, self.bos_id)\n+        if eos:\n+            t.append(self.eos_id)\n+        return t\n+\n+    def decode(self, t: Sequence[int]) -> str:\n+        \"\"\"\n+        Decodes a list of token IDs into a string.\n+\n+        Args:\n+            t (List[int]): The list of token IDs to be decoded.\n+\n+        Returns:\n+            str: The decoded string.\n+        \"\"\"\n+        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n+        return self.model.decode(cast(List[int], t))\n+\n+    @staticmethod\n+    def _split_whitespaces_or_nonwhitespaces(s: str, max_consecutive_slice_len: int) -> Iterator[str]:\n+        \"\"\"\n+        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n+        consecutive whitespaces or consecutive non-whitespaces.\n+        \"\"\"\n+        current_slice_len = 0\n+        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n+        slice_start = 0\n+\n+        for i in range(len(s)):\n+            is_now_space = s[i].isspace()\n+\n+            if current_slice_is_space ^ is_now_space:\n+                current_slice_len = 1\n+                current_slice_is_space = is_now_space\n+            else:\n+                current_slice_len += 1\n+                if current_slice_len > max_consecutive_slice_len:\n+                    yield s[slice_start:i]\n+                    slice_start = i\n+                    current_slice_len = 1\n+        yield s[slice_start:]\n\n--- File: models/llama4/vision/embedding.py ---\n@@ -0,0 +1,210 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import math\n+from typing import Any, Callable, Dict, List\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.layers import ColumnParallelLinear, RowParallelLinear\n+\n+from ..args import VisionArgs\n+from .encoder import VisionEncoder\n+\n+\n+class PixelShuffle(nn.Module):\n+    def __init__(self, ps_ratio):\n+        super().__init__()\n+        self.ps_ratio = ps_ratio\n+\n+    def forward(self, x):\n+        # x: [B, N, C], N = number of patches\n+        assert self.ps_ratio is not None, \"ps_ratio is required for pixel shuffle\"\n+        assert x.dim() == 3, \"pixel shuffle requires encoded patches [B, N, C]\"\n+        hh = ww = int(math.sqrt(x.shape[1]))\n+        x = x.reshape(x.shape[0], hh, ww, -1)\n+        x = pixel_shuffle_op(x, ps_ratio=self.ps_ratio)\n+        pixel_shuffle_patches = x.reshape(x.shape[0], -1, x.shape[-1])\n+        return pixel_shuffle_patches\n+\n+\n+def pixel_shuffle_op(input_x, ps_ratio):\n+    n, w, h, c = input_x.size()\n+    input_x = input_x.view(n, w, int(h * ps_ratio), int(c / ps_ratio))\n+    input_x = input_x.permute(0, 2, 1, 3).contiguous()\n+    input_x = input_x.view(\n+        n,\n+        int(h * ps_ratio),\n+        int(w * ps_ratio),\n+        int(c / (ps_ratio * ps_ratio)),\n+    )\n+    input_x = input_x.permute(0, 2, 1, 3).contiguous()\n+    return input_x\n+\n+\n+class SimpleMLP(torch.nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        bias: bool = True,\n+        dropout: float = 0.0,\n+        act_layer: Callable = nn.GELU,\n+    ):\n+        super().__init__()\n+        # layers\n+        self.c_fc = ColumnParallelLinear(\n+            dim,\n+            hidden_dim,\n+            bias=bias,\n+            gather_output=False,\n+        )\n+        self.c_proj = RowParallelLinear(\n+            hidden_dim,\n+            hidden_dim,\n+            bias=bias,\n+            input_is_parallel=True,\n+        )\n+        self.non_linearity = act_layer()\n+        self.dropout = dropout\n+\n+    def forward(self, x):\n+        hidden = self.c_fc(x)\n+        hidden = self.non_linearity(hidden)\n+        hidden = F.dropout(hidden, p=self.dropout, training=self.training)\n+        return self.non_linearity(self.c_proj(hidden))\n+\n+\n+class PixelShuffleMLP(torch.nn.Module):\n+    def __init__(\n+        self,\n+        ps_ratio: float,\n+        input_dim: int,\n+        output_dim: int = 4096,\n+        add_fc: bool = False,\n+    ):\n+        super().__init__()\n+        self.pixel_shuffle = PixelShuffle(ps_ratio)\n+        self.mlp = SimpleMLP(\n+            int(input_dim // (ps_ratio**2)),\n+            output_dim,\n+            bias=False,\n+            dropout=0.0,\n+            act_layer=nn.GELU,\n+        )\n+        self.fc = nn.Identity()\n+        if add_fc:\n+            self.fc = ColumnParallelLinear(\n+                output_dim,\n+                output_dim,\n+                bias=False,\n+            )\n+\n+    def forward(self, encoded_patches: torch.Tensor) -> torch.Tensor:\n+        encoded_patches = self.pixel_shuffle(encoded_patches)\n+        return self.fc(self.mlp(encoded_patches))\n+\n+\n+class VisionEmbeddings(torch.nn.Module):\n+    def __init__(self, args: VisionArgs):\n+        super().__init__()\n+        self.args = args\n+\n+        image_size = args.image_size\n+        patch_size = args.patch_size\n+        self.vision_encoder = VisionEncoder(\n+            image_size=(image_size.height, image_size.width),\n+            patch_size=(patch_size.height, patch_size.width),\n+            dim=args.dim,\n+            layers=args.n_layers,\n+            heads=args.n_heads,\n+            mlp_ratio=args.mlp_ratio,\n+        )\n+        self.vision_encoder = self.vision_encoder.to(torch.bfloat16)\n+        self.vision_adapter = PixelShuffleMLP(\n+            ps_ratio=args.pixel_shuffle_ratio,\n+            input_dim=args.dim,\n+            output_dim=args.output_dim,\n+        )\n+\n+        self.output_dim = args.output_dim\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool = True,\n+        missing_keys: List[str] = None,\n+        unexpected_keys: List[str] = None,\n+        error_msgs: List[str] = None,\n+        return_state_dict: bool = False,\n+    ) -> None:\n+        original_sd = self.state_dict()\n+        for k in state_dict:\n+            if k.startswith(prefix) and len(state_dict[k].shape) == 1 and state_dict[k].shape[0] == 0:\n+                state_dict[k] = state_dict[k].reshape(original_sd[k[len(prefix) :]].shape)\n+\n+    def _get_empty_sequence(self, h):\n+        return torch.zeros(\n+            h.shape[0],\n+            h.shape[1],\n+            self.output_dim,\n+            device=h.device,\n+            dtype=h.dtype,\n+        )\n+\n+    # x_images is batched; each batch sample contains a list of images. so this is List[List[torch.Tensor]]\n+    # each image is a tensor of shape [num_tiles, C, H, W]\n+    def forward(\n+        self,\n+        image_batch: List[List[torch.Tensor]],\n+        image_mask: torch.Tensor,\n+        h_ref: torch.Tensor,\n+    ) -> torch.Tensor:\n+        images_flattened = [image for sample in image_batch for image in sample]\n+        images_flattened = torch.vstack(images_flattened).unsqueeze(1).to(h_ref.dtype).to(h_ref.device)\n+        embedding = self.vision_encoder(images_flattened)\n+        projected_embedding = self.vision_adapter(embedding)\n+\n+        h_image = self._get_empty_sequence(h_ref)\n+        return scatter_embeddings(image_batch, image_mask, h_image, projected_embedding)\n+\n+\n+def scatter_embeddings(image_batch, image_mask, h_image, encoded_patches_proj):\n+    # If dynamic transform is used and the batch contains 2 images (where image_1 has 2 chunks and image_2 has 3 chunks),\n+    # `num_images_per_sequence` now records the number of chunks per image as `[2, 3]`.\n+    # `encoded_patches_proj.split` will then split the image chunks into 2 groups: `[image_1_chunks, image_2_chunks]`.\n+    num_images_per_sequence = [sum(image.size(0) for image in sample_images) for sample_images in image_batch]\n+\n+    assert not torch.isnan(encoded_patches_proj).any()\n+    assert sum(num_images_per_sequence) == encoded_patches_proj.size(0), (\n+        f\"{sum(num_images_per_sequence)=} != {encoded_patches_proj.shape=}\"\n+    )\n+\n+    encoded_patches_list = encoded_patches_proj.split(num_images_per_sequence, dim=0)\n+    for index in range(h_image.size(0)):\n+        encoded_patches_per_sample = encoded_patches_list[index]\n+        sample_image_mask = image_mask[index]\n+\n+        if encoded_patches_per_sample.numel() == 0:\n+            continue\n+        encoded_patches_per_sample = encoded_patches_per_sample.contiguous().view(\n+            -1, encoded_patches_per_sample.size(-1)\n+        )\n+\n+        n_tokens_to_fill = sample_image_mask.sum()\n+        assert n_tokens_to_fill <= encoded_patches_per_sample.size(0)\n+\n+        h_image[index].masked_scatter_(\n+            sample_image_mask.expand(-1, h_image.size(-1)),\n+            encoded_patches_per_sample[:n_tokens_to_fill],\n+        )\n+\n+    return h_image\n\n--- File: models/llama4/vision/encoder.py ---\n@@ -0,0 +1,428 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and its affiliates.\n+\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import fairscale.nn.model_parallel.initialize as fs_init\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.layers import ColumnParallelLinear, RowParallelLinear\n+from torch import einsum\n+\n+from ..args import ModelArgs\n+from ..model import Attention\n+\n+\n+class LayerNorm(nn.LayerNorm):\n+    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n+\n+    def forward(self, x: torch.Tensor):\n+        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n+        return x\n+\n+\n+class ColumnParallelConv2dPatch(torch.nn.Module):\n+    \"\"\"Conv2D Patching layer with model parallelism.\n+    Column parallel over unfolded input.\n+    Arguments:\n+        in_channels: Input channels.\n+        out_channels: Output channels.\n+        kernel_size: Size of convolution kernel.\n+        stride (default 1): Stride for convolution.\n+        bias (default False): Use bias in Conv2d.\n+    Input: (bsz, in_channels, height, width)\n+    Output: (bsz, num_tokens, out_channels)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: Union[int, Tuple[int, int]],\n+        stride: Union[int, Tuple[int, int]],\n+        bias: Optional[bool] = False,\n+    ) -> None:\n+        super().__init__()\n+        if isinstance(kernel_size, int):\n+            kernel_size = (kernel_size, kernel_size)\n+        self._unfold = torch.nn.Unfold(kernel_size=kernel_size, stride=stride)\n+        self._linear = ColumnParallelLinear(\n+            in_channels * kernel_size[0] * kernel_size[1],\n+            out_channels,\n+            bias=bias,\n+        )\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        x = self._unfold(x)\n+        x = x.permute(0, 2, 1)\n+        x = self._linear(x)\n+        return x\n+\n+\n+class _FeedForward(torch.nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        dropout: float,\n+        act_layer: Callable = nn.GELU,\n+    ):\n+        super().__init__()\n+        # layers\n+        self.c_fc = ColumnParallelLinear(\n+            dim,\n+            hidden_dim,\n+            bias=True,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.c_proj = RowParallelLinear(\n+            hidden_dim,\n+            dim,\n+            bias=True,\n+            input_is_parallel=True,\n+            init_method=lambda x: x,\n+        )\n+        self.non_linearity = act_layer()\n+        self.dropout = dropout\n+\n+    def forward(self, x):\n+        hidden = self.c_fc(x)\n+        hidden = self.non_linearity(hidden)\n+        hidden = F.dropout(hidden, p=self.dropout, training=self.training)\n+        return self.c_proj(hidden)\n+\n+\n+class _TransformerBlock(nn.Module):\n+    def __init__(\n+        self,\n+        d_model: int,\n+        n_head: int,\n+        max_batch_size: int,\n+        max_seq_len: int,\n+        mlp_ratio: float = 4.0,\n+        act_layer: Callable = nn.GELU,\n+        gated: bool = False,\n+    ):\n+        super().__init__()\n+        assert d_model % n_head == 0\n+        self.n_heads = n_head\n+        self.head_dim = d_model // self.n_heads\n+\n+        attn_args = ModelArgs(\n+            dim=d_model,\n+            head_dim=self.head_dim,\n+            n_heads=self.n_heads,\n+            n_kv_heads=self.n_heads,\n+            max_batch_size=max_batch_size,\n+            max_seq_len=max_seq_len,\n+        )\n+        self.attn = Attention(attn_args, use_rope=True, use_qk_norm=False, add_bias=True)\n+        self.ln_1 = LayerNorm(d_model)\n+        self.mlp = _FeedForward(\n+            dim=d_model,\n+            hidden_dim=int(mlp_ratio * d_model),\n+            dropout=0.0,\n+            act_layer=act_layer,\n+        )\n+        self.ln_2 = LayerNorm(d_model)\n+        self.gated = gated\n+        if gated:\n+            self.gate_attn = nn.Parameter(torch.zeros(1))\n+            self.gate_ffn = nn.Parameter(torch.zeros(1))\n+\n+    def attention(\n+        self,\n+        x: torch.Tensor,\n+        freq_cis: Optional[torch.Tensor] = None,\n+    ):\n+        return self.attn(x=x, start_pos=0, freqs_cis=freq_cis)\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        mask: Optional[torch.Tensor] = None,\n+        freq_cis: Optional[torch.Tensor] = None,\n+    ):\n+        _gate_attn = 1 if not self.gated else self.gate_attn.tanh()\n+        _gate_ffn = 1 if not self.gated else self.gate_ffn.tanh()\n+\n+        x = x + _gate_attn * self.attention(self.ln_1(x), freq_cis=freq_cis)\n+        x = x + _gate_ffn * self.mlp(self.ln_2(x))\n+        return x\n+\n+\n+class _Transformer(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        layers: int,\n+        heads: int,\n+        max_batch_size: int,\n+        max_seq_len: int,\n+        mlp_ratio: float = 4.0,\n+        act_layer: Callable = nn.GELU,\n+        gated: bool = False,\n+    ):\n+        super().__init__()\n+        self.resblocks = nn.ModuleList(\n+            [\n+                _TransformerBlock(\n+                    d_model=dim,\n+                    n_head=heads,\n+                    mlp_ratio=mlp_ratio,\n+                    act_layer=act_layer,\n+                    gated=gated,\n+                    max_batch_size=max_batch_size,\n+                    max_seq_len=max_seq_len,\n+                )\n+                for _ in range(layers)\n+            ]\n+        )\n+\n+    def forward(self, x: torch.Tensor, return_intermediate=None, mask=None, freq_cis=None):\n+        out = []\n+        for idx, r in enumerate(self.resblocks):\n+            if return_intermediate is not None and idx in return_intermediate:\n+                out.append(x)\n+            x = r(x, mask=mask, freq_cis=freq_cis)\n+        if return_intermediate is not None:\n+            return x, torch.stack(out, dim=-1)\n+        return x\n+\n+\n+class PackingIndex:\n+    Z = 0  # Z (time) coordinate of the token in the original sample\n+    Y = 1  # Y (height) coordinate of the token in the original sample\n+    X = 2  # X (width) coordinate of the token in the original sample\n+    TIME = 3  # Total number of time units (frames) in the original sample\n+    HEIGHT = 4  # Height of the original sample\n+    WIDTH = 5  # Width of the original sample\n+    # USE INDEX TO CHECK THE TYPE OF THE TOKEN (see ID fields below)\n+    IDX = 6  # Full index of the token in the original sample (x + y * w + z * w * h)\n+    BATCH_IDX = 7  # Which batch element this token belongs to. Note the batch idx of padding tokens is BATCH_SIZE\n+\n+    # Total size of the enum, remember to update this!\n+    NUM_METADATA = 8\n+\n+    # Note: For padding tokens IDX = -1\n+    #       For cls tokens,    IDX = -2\n+    ID_CLS_TOKEN = -2\n+    ID_PAD_TOKEN = -1\n+\n+\n+ENCODER_MAX_BATCH_SIZE = 32\n+ENCODER_MAX_SEQ_LEN = 1024\n+\n+\n+class VisionEncoder(nn.Module):\n+    def __init__(\n+        self,\n+        image_size: Tuple[int, int],\n+        patch_size: Tuple[int, int],\n+        dim: int,\n+        layers: int,\n+        heads: int,\n+        mlp_ratio: float,\n+    ):\n+        super().__init__()\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.grid_size = (\n+            self.image_size[0] // self.patch_size[0],\n+            self.image_size[1] // self.patch_size[1],\n+        )\n+        self.conv1 = ColumnParallelConv2dPatch(\n+            in_channels=3,\n+            out_channels=dim,\n+            kernel_size=patch_size,\n+            stride=patch_size,\n+            bias=False,\n+        )\n+        scale = dim**-0.5\n+        self.class_embedding = nn.Parameter(scale * torch.randn(dim))\n+\n+        self.positional_embedding_vlm = nn.Parameter(\n+            scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, dim)\n+        )\n+\n+        self.ln_pre = LayerNorm(dim)\n+        self.ln_post = LayerNorm(dim)\n+\n+        self.transformer = _Transformer(\n+            dim,\n+            layers,\n+            heads,\n+            ENCODER_MAX_BATCH_SIZE,\n+            ENCODER_MAX_SEQ_LEN,\n+            mlp_ratio,\n+            act_layer=nn.GELU,\n+        )\n+\n+        # NOTE: hack for the fixed res\n+        image_h, image_w = self.image_size\n+        patch_h, patch_w = self.patch_size\n+        idx_h, idx_w = image_h // patch_h, image_w // patch_w\n+        img_idx = torch.arange(image_h * image_w // (patch_h * patch_w), dtype=torch.int32)\n+        img_idx = img_idx.reshape(idx_h * idx_w, 1)\n+        img_idx = torch.cat([img_idx, img_idx[:1]], dim=0)\n+        img_idx[-1, -1] = PackingIndex.ID_CLS_TOKEN\n+\n+        packed_img_idx = torch.empty(\n+            img_idx.shape[0],\n+            img_idx.shape[1],\n+            PackingIndex.NUM_METADATA - 1,\n+            dtype=torch.int32,\n+        )\n+        packed_img_idx[:, :, PackingIndex.Y] = img_idx // idx_w\n+        packed_img_idx[:, :, PackingIndex.X] = img_idx % idx_w\n+        packed_img_idx[:, :, PackingIndex.HEIGHT].fill_(idx_h)\n+        packed_img_idx[:, :, PackingIndex.WIDTH].fill_(idx_w)\n+        packed_img_idx[:, :, PackingIndex.IDX] = img_idx\n+        packed_img_idx = packed_img_idx.reshape(1, -1, PackingIndex.NUM_METADATA - 1)\n+        self.packed_img_idx = packed_img_idx  # for positional embedding load hook\n+\n+        # compute rope freqs\n+        rope_freq = self.get_rope_freqs(dim // heads // 2)\n+        freqs_x = self.compute_rope_freqs(rope_freq, packed_img_idx[:, :, PackingIndex.X] + 1)\n+        freqs_y = self.compute_rope_freqs(rope_freq, packed_img_idx[:, :, PackingIndex.Y] + 1)\n+        freqs = torch.cat([freqs_x, freqs_y], dim=-1).float().contiguous()[..., ::2]\n+        # disable RoPE for padding and cls tokens\n+        freqs = freqs.masked_fill(packed_img_idx[:, :, PackingIndex.IDX, None] < 0, 0)\n+        # compute complex freqs\n+        self.freq_cis = torch.view_as_complex(torch.stack([torch.cos(freqs), torch.sin(freqs)], dim=-1))\n+        # xlf automatically broadcasts\n+        self.freq_cis = self.freq_cis.squeeze(0)\n+        self.n_heads = heads // fs_init.get_model_parallel_world_size()\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def get_rope_freqs(self, dim, theta=10000):\n+        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n+        return freqs\n+\n+    @torch.amp.autocast(\"cuda\", enabled=False)\n+    def compute_rope_freqs(self, freqs, t):\n+        freqs = einsum(\"..., f -> ... f\", t.type(freqs.dtype), freqs)\n+        freqs = freqs.repeat_interleave(2, dim=-1)\n+        return freqs\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool = True,\n+        missing_keys: List[str] = None,\n+        unexpected_keys: List[str] = None,\n+        error_msgs: List[str] = None,\n+        return_state_dict: bool = False,\n+    ) -> None:\n+        orig_pos_embed = state_dict.get(prefix + \"positional_embedding\")\n+        if orig_pos_embed is not None and orig_pos_embed.shape[-2:] != self.positional_embedding_vlm.shape[-2:]:\n+            raise ValueError(\n+                f\"Positional embedding shape {orig_pos_embed.shape} does not match expected shape {self.positional_embedding_vlm.shape}\"\n+            )\n+\n+        batch_size, token_per_image, _ = self.packed_img_idx.shape\n+        # Input points for idx are [x, y, w, h]\n+        idx = self.packed_img_idx.reshape(batch_size * token_per_image, 1, -1)\n+        total_windows, window_size, _ = idx.shape\n+\n+        # Grid values are [-1, 1] and coords are w, h\n+        grid = (\n+            (idx[:, :, [PackingIndex.X, PackingIndex.Y]] / idx[:, :, [PackingIndex.WIDTH, PackingIndex.HEIGHT]]) * 2 - 1\n+        )[None, ...]\n+\n+        # In this mode, cls token has no position embedding\n+        if orig_pos_embed is not None:\n+            posemb = (\n+                orig_pos_embed[1:].view(1, self.grid_size[0], self.grid_size[1], -1).permute(0, 3, 1, 2).contiguous()\n+            )\n+            posemb = posemb.to(device=grid.device, dtype=grid.dtype)\n+            sample = F.grid_sample(\n+                posemb, grid, padding_mode=\"zeros\"\n+            )  # padding tokens / class token will get zero for posemb\n+            sample = sample.view(-1, total_windows, window_size).permute(1, 2, 0).contiguous()\n+            sample = torch.where(\n+                idx[:, :, PackingIndex.IDX, None] == PackingIndex.ID_CLS_TOKEN,\n+                orig_pos_embed[0].view(1, 1, -1).to(device=sample.device, dtype=sample.dtype),\n+                sample,\n+            )\n+\n+            new_pos_embed = sample.reshape(batch_size, token_per_image, -1)\n+\n+            state_dict[prefix + \"positional_embedding_vlm\"] = new_pos_embed.squeeze(0)\n+\n+        if return_state_dict:\n+            return state_dict\n+\n+    def apply_class_embedding(self, x):\n+        x = torch.cat(\n+            [\n+                x,\n+                self.class_embedding.to(x.dtype)\n+                + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device),\n+            ],\n+            dim=1,\n+        )  # shape = [*, grid ** 2 + 1, width]\n+        return x\n+\n+    def forward(self, images: torch.Tensor) -> torch.Tensor:\n+        # NOTE: in Llama4 bsz=bsz*num_tiles, num_chunks=1\n+        if images.ndim == 5:\n+            num_concurrent_media = 1\n+            bsz, num_chunks, nch, h, w = images.shape\n+        else:\n+            bsz, num_concurrent_media, num_chunks, nch, h, w = images.shape\n+\n+        images = images.reshape(bsz * num_concurrent_media * num_chunks, nch, h, w)\n+        # patch embedding\n+        x = images.reshape(bsz * num_concurrent_media * num_chunks, nch, h, w)\n+        x = self.conv1(x)  # shape = [*, width, grid ** 2]\n+        _, ntok, dim = x.shape\n+        x = x.reshape(bsz * num_concurrent_media * num_chunks, ntok, dim)\n+\n+        # apply cls token\n+        x = self.apply_class_embedding(x)\n+        ntok += 1\n+\n+        # apply position embeddings\n+        if self.positional_embedding_vlm is not None:\n+            x = x + self.positional_embedding_vlm.to(x.dtype)\n+\n+        x = x.reshape(bsz * num_concurrent_media, num_chunks, ntok, dim)\n+\n+        x = self.ln_pre(x)\n+        x = x.view(bsz * num_concurrent_media, -1, dim)\n+        freq_cis = self.freq_cis.to(images.device)\n+\n+        tf_output = self.transformer(\n+            x,\n+            freq_cis=freq_cis,\n+        )\n+\n+        int_x = None\n+        if isinstance(tf_output, tuple):\n+            x, int_x = tf_output\n+        else:\n+            x = tf_output\n+        x = self.ln_post(x)\n+\n+        # remove cls token output\n+        x = x[:, :-1, :]\n+\n+        # add and output x + int_x features\n+        if int_x is not None:\n+            int_x = int_x[:, :-1, :, :]\n+            int_x = int_x.reshape(bsz * num_concurrent_media, ntok - 1, -1)\n+            x = torch.cat([x, int_x], dim=-1)\n+\n+        return x\n\n--- File: models/quantize_impls.py ---\n@@ -0,0 +1,318 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import collections\n+import logging\n+from typing import Optional, Tuple, Type, Union\n+\n+log = logging.getLogger(__name__)\n+\n+try:\n+    import fbgemm_gpu.experimental.gen_ai  # noqa: F401\n+\n+    log.info(\"Using efficient FP8 or INT4 operators in FBGEMM.\")\n+except ImportError:\n+    log.error(\"No efficient FP8 or INT4 operators. Please install FBGEMM.\")\n+    raise\n+\n+import torch\n+from torch import nn, Tensor\n+\n+\n+class Fp8ScaledWeights:\n+    # TODO: Ugly trick so torch allows us to replace parameters\n+    # with our custom Fp8Weights instance. Do this properly.\n+    @property\n+    def __class__(self) -> Type[nn.parameter.Parameter]:\n+        return nn.Parameter\n+\n+    @property\n+    def grad_fn(self) -> None:\n+        return None\n+\n+\n+# pyre-fixme[4]: Attribute annotation cannot be `Any`.\n+# pyre-fixme[2]: Parameter annotation cannot be `Any`.\n+class Fp8RowwiseWeights(\n+    Fp8ScaledWeights,\n+    collections.namedtuple(\n+        \"Fp8RowwiseWeights\",\n+        [\"weight\", \"scale\", \"shape\", \"activation_scale_ub\"],\n+    ),\n+):\n+    pass\n+\n+\n+class Int4ScaledWeights:\n+    # TODO: Ugly trick so torch allows us to replace parameters\n+    # with our custom Int4Weights instance. Do this properly.\n+    @property\n+    def __class__(self) -> Type[nn.parameter.Parameter]:\n+        return nn.Parameter\n+\n+    @property\n+    def grad_fn(self) -> None:\n+        return None\n+\n+\n+# pyre-fixme[4]: Attribute annotation cannot be `Any`.\n+# pyre-fixme[2]: Parameter annotation cannot be `Any`.\n+class Int4Weights(\n+    Int4ScaledWeights,\n+    collections.namedtuple(\n+        \"Int4Weights\",\n+        [\"weight\", \"scale\", \"shape\"],\n+    ),\n+):\n+    pass\n+\n+\n+def int4_row_quantize(\n+    x: torch.Tensor,\n+    group_size: int = 128,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"\n+    Helper function to quantize a tensor to int4 with groupwise scales.\n+\n+    Args:\n+        x (Tensor): [N, K] Higher precision weight tensor to quantize.\n+        group_size (int): Number of elements to calculate group scale for.\n+    Returns:\n+        wq (Tensor): [N, K // 2] Quantized int4 tensor stored in int8 elements.\n+        group_scale (Tensor): [K / group_size, N] FP32 Scale per group.\n+    \"\"\"\n+    n_bit = 4  # Number of target bits.\n+    to_quant = x.reshape(-1, group_size).to(torch.float)\n+\n+    max_val = torch.abs(to_quant).amax(dim=1, keepdim=True)\n+    max_int = 2 ** (n_bit - 1)\n+    min_int = -(2 ** (n_bit - 1))\n+    scales = max_val.clamp(min=1e-6) / max_int\n+\n+    out = to_quant.div(scales).round().clamp_(min_int, max_int - 1)\n+\n+    # Cast to int8 and restore shape.\n+    out = out.to(dtype=torch.int8).reshape(x.shape)\n+\n+    # Scales should be in [num_groups, N] layout.\n+    scales = scales.view(x.shape[0], -1).t().contiguous()\n+\n+    return out, scales\n+\n+\n+def pack_int4(x: torch.Tensor) -> torch.Tensor:\n+    # Given int8 x, pack adjacent int4 values into a single int8.\n+    low_x = x[:, ::2]\n+    high_x = x[:, 1::2]\n+\n+    # High bits need to left shift, this also masks off extra bits.\n+    high_x = torch.bitwise_left_shift(high_x, 4)\n+    # Low bits need to have sign bits removed.\n+    low_x = torch.bitwise_and(low_x, 0xF)\n+\n+    # Recombine into a single value with bitwise or.\n+    return torch.bitwise_or(low_x, high_x).contiguous()\n+\n+\n+def bmm_nt(\n+    x: Tensor,\n+    w: Union[Fp8RowwiseWeights, Int4Weights],\n+    num_tokens: Optional[Tensor] = None,\n+) -> Tensor:\n+    if isinstance(w, Fp8ScaledWeights):\n+        xq, x_scale = torch.ops.fbgemm.quantize_fp8_per_row(x, num_tokens, w.activation_scale_ub)\n+        return torch.ops.fbgemm.f8f8bf16_rowwise_batched(xq, w.weight, x_scale, w.scale)\n+    elif isinstance(w, Int4ScaledWeights):\n+        return torch.ops.fbgemm.bf16i4bf16_rowwise_batched(x, w.weight, w.scale, torch.zeros_like(w.scale))\n+    else:\n+        raise ValueError(\"Unsupported quantization type\")\n+\n+\n+def ffn_swiglu(\n+    x: Tensor,\n+    w1: Union[Fp8RowwiseWeights, Int4Weights],\n+    w3: Union[Fp8RowwiseWeights, Int4Weights],\n+    w2: Union[Fp8RowwiseWeights, Int4Weights],\n+    num_tokens: Optional[Tensor] = None,\n+    is_memory_bounded: bool = False,\n+) -> Tensor:\n+    if isinstance(w1, Fp8ScaledWeights) and isinstance(w3, Fp8ScaledWeights) and isinstance(w2, Fp8ScaledWeights):\n+        return ffn_swiglu_dynamic(x, w1, w3, w2, w1.activation_scale_ub, num_tokens, is_memory_bounded)\n+    elif isinstance(w1, Int4ScaledWeights) and isinstance(w3, Int4ScaledWeights) and isinstance(w2, Int4ScaledWeights):\n+        return ffn_swiglu_dynamic(x, w1, w3, w2, None, num_tokens, is_memory_bounded)\n+\n+    (B, T, D) = x.shape  # noqa: N806\n+    (HD_L, D_) = w1.shape  # noqa: N806\n+    assert D_ == D\n+\n+    assert isinstance(w1, Tensor)\n+    assert isinstance(w3, Tensor)\n+    x1 = x.view(B * T, D) @ w1.T\n+    x2 = x.view(B * T, D) @ w3.T\n+    z = torch.nn.functional.silu(x1) * x2\n+    del x1, x2\n+    assert isinstance(w2, Tensor)\n+    return (z @ w2.T).view(B, T, D)\n+\n+\n+@torch.inference_mode()\n+def quantize_fp8(\n+    w: Tensor,\n+    fp8_activation_scale_ub: float,\n+    output_device: Optional[torch.device] = None,\n+) -> Fp8RowwiseWeights:\n+    \"\"\"Quantize [n, k] weight tensor.\n+\n+    Args:\n+        w (Tensor): [n, k] input high precision tensor to quantize.\n+        fp8_activation_scale_ub (float): Upper bound for activation max.\n+    \"\"\"\n+    activation_scale_ub = torch.tensor(\n+        [fp8_activation_scale_ub],\n+        dtype=torch.float,\n+        device=output_device,\n+    )\n+    wq, w_scale = torch.ops.fbgemm.quantize_fp8_per_row(w)\n+    del w\n+    return Fp8RowwiseWeights(\n+        weight=wq,\n+        scale=w_scale,\n+        shape=wq.shape,\n+        activation_scale_ub=activation_scale_ub,\n+    )\n+\n+\n+@torch.inference_mode()\n+def quantize_int4(\n+    w: Tensor,\n+    output_device: Optional[torch.device] = None,\n+) -> Int4Weights:\n+    \"\"\"Quantize [n, k/2] weight tensor.\n+\n+    Args:\n+        w (Tensor): [n, k/2] input high precision tensor to quantize.\n+    \"\"\"\n+    if w.ndim >= 3:\n+        wq, scale = zip(*[int4_row_quantize(i) for i in w])\n+        wq = torch.stack([pack_int4(i) for i in wq], dim=0)\n+        scale = torch.stack(scale, dim=0)\n+    else:\n+        wq, scale = int4_row_quantize(w)\n+        wq = pack_int4(wq)\n+    del w\n+    return Int4Weights(\n+        weight=wq.to(output_device),\n+        scale=scale.to(output_device),\n+        shape=wq.shape,\n+    )\n+\n+\n+@torch.inference_mode()\n+def load_fp8(\n+    w: Tensor,\n+    w_scale: Tensor,\n+    fp8_activation_scale_ub: float,\n+    output_device: Optional[torch.device] = None,\n+) -> Fp8RowwiseWeights:\n+    \"\"\"Load FP8 [n, k] weight tensor.\n+\n+    Args:\n+        w (Tensor): [n, k] input FP8.\n+        fp8_activation_scale_ub (float): Upper bound for activation max.\n+    \"\"\"\n+    activation_scale_ub = torch.tensor(\n+        [fp8_activation_scale_ub],\n+        dtype=torch.float,\n+        device=output_device,\n+    )\n+    return Fp8RowwiseWeights(\n+        weight=w.to(torch.float8_e4m3fn).to(device=output_device),\n+        scale=w_scale.to(device=output_device),\n+        shape=w.shape,\n+        activation_scale_ub=activation_scale_ub,\n+    )\n+\n+\n+@torch.inference_mode()\n+def load_int4(\n+    w: Tensor,\n+    scale: Tensor,\n+    output_device: Optional[torch.device] = None,\n+) -> Int4Weights:\n+    \"\"\"Load INT4 [n, k/2] weight tensor.\n+\n+    Args:\n+        w (Tensor): [n, k/2] input INT4.\n+        w_scale (Tensor): [n, k/2] input INT4 scale.\n+    \"\"\"\n+    return Int4Weights(\n+        weight=w.to(torch.int8).to(device=output_device),\n+        scale=scale.to(device=output_device),\n+        shape=w.shape,\n+    )\n+\n+\n+def fc_dynamic(\n+    x: Tensor,\n+    w: Union[Fp8RowwiseWeights, Int4Weights],\n+    activation_scale_ub: Optional[Tensor] = None,\n+    num_tokens: Optional[Tensor] = None,\n+    is_memory_bounded: bool = False,\n+) -> Tensor:\n+    \"\"\"\n+    Single w8a8 fc layer with dynamic row-wise scaling, or w4a16 fc layer with dyanmic row-wise scaling\n+    \"\"\"\n+    if isinstance(w, Int4Weights):\n+        y = torch.ops.fbgemm.bf16i4bf16_rowwise(x, w.weight, w.scale, torch.zeros_like(w.scale))\n+    else:\n+        xq, x_scale = torch.ops.fbgemm.quantize_fp8_per_row(x, num_tokens, activation_scale_ub)\n+        y = torch.ops.fbgemm.f8f8bf16_rowwise(xq, w.weight, x_scale, w.scale, use_fast_accum=True)\n+        del xq\n+    return y\n+\n+\n+def ffn_swiglu_dynamic(\n+    x: Tensor,\n+    w1: Union[Fp8RowwiseWeights, Int4Weights],\n+    w3: Union[Fp8RowwiseWeights, Int4Weights],\n+    w2: Union[Fp8RowwiseWeights, Int4Weights],\n+    activation_scale_ub: Optional[Tensor] = None,\n+    num_tokens: Optional[Tensor] = None,\n+    is_memory_bounded: bool = False,\n+) -> Tensor:\n+    assert x.dim() == 3 or x.dim() == 2\n+    if x.dim() == 3:\n+        (B, T, D) = x.shape  # noqa: N806\n+    else:\n+        (T, D) = x.shape  # noqa: N806\n+        B = 1  # noqa: N806\n+\n+    HD_L = w1.shape[0]  # noqa: N806\n+    assert HD_L == w3.shape[0]\n+    x1 = fc_dynamic(\n+        x.view(B * T, D),\n+        w1,\n+        activation_scale_ub,\n+        num_tokens,\n+        is_memory_bounded,\n+    )\n+    x2 = fc_dynamic(\n+        x.view(B * T, D),\n+        w3,\n+        activation_scale_ub,\n+        num_tokens,\n+        is_memory_bounded,\n+    )\n+    z = torch.nn.functional.silu(x1) * x2\n+    del x1, x2\n+\n+    z_ = fc_dynamic(z, w2, activation_scale_ub, num_tokens, is_memory_bounded)\n+\n+    if x.dim() == 3:\n+        return z_.view(B, T, D)\n+    else:\n+        return z_\n\n--- File: pyproject.toml ---\n@@ -4,7 +4,7 @@ build-backend = \"setuptools.build_meta\"\n \n [project]\n name = \"llama_models\"\n-version = \"0.1.4\"\n+version = \"0.2.0\"\n authors = [\n     {name = \"Meta Llama\", email = \"llama-oss@meta.com\"},\n ]\n@@ -13,10 +13,11 @@ readme = \"README.md\"\n requires-python = \">=3.10\"\n dependencies = [\n     \"PyYAML\",\n-    \"jinja2\",\n+    \"jinja2>=3.1.6\",\n     \"tiktoken\",\n     \"pydantic>=2\",\n     \"Pillow\",\n+    \"rich\",\n ]\n classifiers = []\n \n@@ -28,6 +29,8 @@ multimodal_example_chat_completion = \"llama_models.scripts.multimodal_example_ch\n multimodal_example_text_completion = \"llama_models.scripts.multimodal_example_text_completion:main\"\n example_chat_completion = \"llama_models.scripts.example_chat_completion:main\"\n example_text_completion = \"llama_models.scripts.example_text_completion:main\"\n+llama4_completion = \"llama_models.llama4.scripts.completion:main\"\n+llama4_chat_completion = \"llama_models.llama4.scripts.chat_completion:main\"\n \n [project.optional-dependencies]\n dev = [\n@@ -39,9 +42,11 @@ dev = [\n ]\n torch = [\n     \"torch\",\n+    \"torchvision\",\n     \"fairscale\",\n     \"fire\",\n     \"blobfile\",\n+    \"fbgemm-gpu-genai==1.1.2\",\n ]\n \n [tool.setuptools]\n\n--- File: requirements.txt ---\n@@ -4,7 +4,7 @@ annotated-types==0.7.0\n certifi==2025.1.31\n charset-normalizer==3.4.1\n idna==3.10\n-jinja2==3.1.5\n+jinja2==3.1.6\n markupsafe==3.0.2\n pillow==11.1.0\n pydantic==2.10.6\n\n--- File: uv.lock ---\n@@ -173,6 +173,20 @@ dependencies = [\n ]\n sdist = { url = \"https://files.pythonhosted.org/packages/c1/08/b3334d7b543ac10dcb129cef4f84723ab696725512f18d69ab3a784b0bf5/fairscale-0.4.13.tar.gz\", hash = \"sha256:1b797825c427f5dba92253fd0d8daa574e8bd651a2423497775fab1b30cfb768\", size = 266261 }\n \n+[[package]]\n+name = \"fbgemm-gpu\"\n+version = \"1.1.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"numpy\" },\n+]\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/44/9c/a0820c23afe153d13e5b0b8e3218550cca794398fc0f4e42eb34eeae54a3/fbgemm_gpu-1.1.0-cp310-cp310-manylinux_2_28_x86_64.whl\", hash = \"sha256:97b88a8f6895b0369782f84e31352b9b0dd548cdd10cbb14da6892bc3f792a51\", size = 417175644 },\n+    { url = \"https://files.pythonhosted.org/packages/06/ad/03b1b60c5c81597874a226f9274ca09af976037bae8148b40181a0ebc4fa/fbgemm_gpu-1.1.0-cp311-cp311-manylinux_2_28_x86_64.whl\", hash = \"sha256:721638a849605d20a831348e9b3fb5cb1727577b8f6fb0e8561e91107fb1b85f\", size = 417176114 },\n+    { url = \"https://files.pythonhosted.org/packages/fc/f0/6a93cfe25bd13b92b3ea8d821756650d8e66873d836cdb44a4dde67e3c5e/fbgemm_gpu-1.1.0-cp312-cp312-manylinux_2_28_x86_64.whl\", hash = \"sha256:1cfc1abd47f08b40e486cae7a0ffa943b04120d30453c9702c70439f7541e223\", size = 417175357 },\n+    { url = \"https://files.pythonhosted.org/packages/bd/e3/891a038e41c3b9cc7cd8c32b5606f77fe51a5a2979c5db0bd93170dc72cb/fbgemm_gpu-1.1.0-cp313-cp313-manylinux_2_28_x86_64.whl\", hash = \"sha256:9ef7c41b4a3e050f6cb7d8312e808ba48009856dabda579ef21818a7823fadf5\", size = 417175904 },\n+]\n+\n [[package]]\n name = \"filelock\"\n version = \"3.17.0\"\n@@ -229,25 +243,26 @@ wheels = [\n \n [[package]]\n name = \"jinja2\"\n-version = \"3.1.5\"\n+version = \"3.1.6\"\n source = { registry = \"https://pypi.org/simple\" }\n dependencies = [\n     { name = \"markupsafe\" },\n ]\n-sdist = { url = \"https://files.pythonhosted.org/packages/af/92/b3130cbbf5591acf9ade8708c365f3238046ac7cb8ccba6e81abccb0ccff/jinja2-3.1.5.tar.gz\", hash = \"sha256:8fefff8dc3034e27bb80d67c671eb8a9bc424c0ef4c0826edbff304cceff43bb\", size = 244674 }\n+sdist = { url = \"https://files.pythonhosted.org/packages/df/bf/f7da0350254c0ed7c72f3e33cef02e048281fec7ecec5f032d4aac52226b/jinja2-3.1.6.tar.gz\", hash = \"sha256:0137fb05990d35f1275a587e9aee6d56da821fc83491a0fb838183be43f66d6d\", size = 245115 }\n wheels = [\n-    { url = \"https://files.pythonhosted.org/packages/bd/0f/2ba5fbcd631e3e88689309dbe978c5769e883e4b84ebfe7da30b43275c5a/jinja2-3.1.5-py3-none-any.whl\", hash = \"sha256:aba0f4dc9ed8013c424088f68a5c226f7d6097ed89b246d7749c2ec4175c6adb\", size = 134596 },\n+    { url = \"https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl\", hash = \"sha256:85ece4451f492d0c13c5dd7c13a64681a86afae63a5f347908daf103ce6d2f67\", size = 134899 },\n ]\n \n [[package]]\n name = \"llama-models\"\n-version = \"0.1.1\"\n+version = \"0.1.3\"\n source = { editable = \".\" }\n dependencies = [\n     { name = \"jinja2\" },\n     { name = \"pillow\" },\n     { name = \"pydantic\" },\n     { name = \"pyyaml\" },\n+    { name = \"rich\" },\n     { name = \"tiktoken\" },\n ]\n \n@@ -262,26 +277,31 @@ dev = [\n torch = [\n     { name = \"blobfile\" },\n     { name = \"fairscale\" },\n+    { name = \"fbgemm-gpu\" },\n     { name = \"fire\" },\n     { name = \"torch\" },\n+    { name = \"torchvision\" },\n ]\n \n [package.metadata]\n requires-dist = [\n     { name = \"black\", marker = \"extra == 'dev'\" },\n     { name = \"blobfile\", marker = \"extra == 'torch'\" },\n     { name = \"fairscale\", marker = \"extra == 'torch'\" },\n+    { name = \"fbgemm-gpu\", marker = \"extra == 'torch'\" },\n     { name = \"fire\", marker = \"extra == 'torch'\" },\n     { name = \"isort\", marker = \"extra == 'dev'\" },\n-    { name = \"jinja2\" },\n+    { name = \"jinja2\", specifier = \">=3.1.6\" },\n     { name = \"mypy\", marker = \"extra == 'dev'\" },\n     { name = \"pillow\" },\n     { name = \"pydantic\", specifier = \">=2\" },\n     { name = \"pytest\", marker = \"extra == 'dev'\" },\n     { name = \"pyyaml\" },\n+    { name = \"rich\" },\n     { name = \"ruff\", marker = \"extra == 'dev'\" },\n     { name = \"tiktoken\" },\n     { name = \"torch\", marker = \"extra == 'torch'\" },\n+    { name = \"torchvision\", marker = \"extra == 'torch'\" },\n ]\n \n [[package]]\n@@ -366,6 +386,18 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/ba/b2/6a22fb5c0885da3b00e116aee81f0b829ec9ac8f736cd414b4a09413fc7d/lxml-5.3.0-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:6e91cf736959057f7aac7adfc83481e03615a8e8dd5758aa1d95ea69e8931dba\", size = 3487557 },\n ]\n \n+[[package]]\n+name = \"markdown-it-py\"\n+version = \"3.0.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"mdurl\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz\", hash = \"sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb\", size = 74596 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl\", hash = \"sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1\", size = 87528 },\n+]\n+\n [[package]]\n name = \"markupsafe\"\n version = \"3.0.2\"\n@@ -424,6 +456,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/4f/65/6079a46068dfceaeabb5dcad6d674f5f5c61a6fa5673746f42a9f4c233b3/MarkupSafe-3.0.2-cp313-cp313t-win_amd64.whl\", hash = \"sha256:e444a31f8db13eb18ada366ab3cf45fd4b31e4db1236a4448f68778c1d1a5a2f\", size = 15739 },\n ]\n \n+[[package]]\n+name = \"mdurl\"\n+version = \"0.1.2\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz\", hash = \"sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba\", size = 8729 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl\", hash = \"sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8\", size = 9979 },\n+]\n+\n [[package]]\n name = \"mpmath\"\n version = \"1.3.0\"\n@@ -883,6 +924,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/63/37/3e32eeb2a451fddaa3898e2163746b0cffbbdbb4740d38372db0490d67f3/pydantic_core-2.27.2-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:7e17b560be3c98a8e3aa66ce828bdebb9e9ac6ad5466fba92eb74c4c95cb1151\", size = 2004715 },\n ]\n \n+[[package]]\n+name = \"pygments\"\n+version = \"2.19.1\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz\", hash = \"sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f\", size = 4968581 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl\", hash = \"sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c\", size = 1225293 },\n+]\n+\n [[package]]\n name = \"pytest\"\n version = \"8.3.4\"\n@@ -1028,6 +1078,20 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl\", hash = \"sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6\", size = 64928 },\n ]\n \n+[[package]]\n+name = \"rich\"\n+version = \"13.9.4\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"markdown-it-py\" },\n+    { name = \"pygments\" },\n+    { name = \"typing-extensions\", marker = \"python_full_version < '3.11'\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/ab/3a/0316b28d0761c6734d6bc14e770d85506c986c85ffb239e688eeaab2c2bc/rich-13.9.4.tar.gz\", hash = \"sha256:439594978a49a09530cff7ebc4b5c7103ef57baf48d5ea3184f21d9a2befa098\", size = 223149 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/19/71/39c7c0d87f8d4e6c020a393182060eaefeeae6c01dab6a84ec346f2567df/rich-13.9.4-py3-none-any.whl\", hash = \"sha256:6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90\", size = 242424 },\n+]\n+\n [[package]]\n name = \"ruff\"\n version = \"0.9.4\"\n@@ -1204,6 +1268,38 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/88/8b/d60c0491ab63634763be1537ad488694d316ddc4a20eaadd639cedc53971/torch-2.6.0-cp313-none-macosx_11_0_arm64.whl\", hash = \"sha256:ff96f4038f8af9f7ec4231710ed4549da1bdebad95923953a25045dcf6fd87e2\", size = 66536783 },\n ]\n \n+[[package]]\n+name = \"torchvision\"\n+version = \"0.21.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"numpy\" },\n+    { name = \"pillow\" },\n+    { name = \"torch\" },\n+]\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/a9/20/72eb0b5b08fa293f20fc41c374e37cf899f0033076f0144d2cdc48f9faee/torchvision-0.21.0-1-cp310-cp310-manylinux_2_28_aarch64.whl\", hash = \"sha256:5568c5a1ff1b2ec33127b629403adb530fab81378d9018ca4ed6508293f76e2b\", size = 2327643 },\n+    { url = \"https://files.pythonhosted.org/packages/4e/3d/b7241abfa3e6651c6e00796f5de2bd1ce4d500bf5159bcbfeea47e711b93/torchvision-0.21.0-1-cp311-cp311-manylinux_2_28_aarch64.whl\", hash = \"sha256:ff96666b94a55e802ea6796cabe788541719e6f4905fc59c380fed3517b6a64d\", size = 2329320 },\n+    { url = \"https://files.pythonhosted.org/packages/52/5b/76ca113a853b19c7b1da761f8a72cb6429b3bd0bf932537d8df4657f47c3/torchvision-0.21.0-1-cp312-cp312-manylinux_2_28_aarch64.whl\", hash = \"sha256:ffa2a16499508fe6798323e455f312c7c55f2a88901c9a7c0fb1efa86cf7e327\", size = 2329878 },\n+    { url = \"https://files.pythonhosted.org/packages/4e/fe/5e193353706dab96fe73ae100d5a633ff635ce310e0d92f3bc2958d075b1/torchvision-0.21.0-1-cp313-cp313-manylinux_2_28_aarch64.whl\", hash = \"sha256:7e9e9afa150e40cd2a8f0701c43cb82a8d724f512896455c0918b987f94b84a4\", size = 2280711 },\n+    { url = \"https://files.pythonhosted.org/packages/8e/0d/143bd264876fad17c82096b6c2d433f1ac9b29cdc69ee45023096976ee3d/torchvision-0.21.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:044ea420b8c6c3162a234cada8e2025b9076fa82504758cd11ec5d0f8cd9fa37\", size = 1784140 },\n+    { url = \"https://files.pythonhosted.org/packages/5e/44/32e2d2d174391374d5ff3c4691b802e8efda9ae27ab9062eca2255b006af/torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl\", hash = \"sha256:b0c0b264b89ab572888244f2e0bad5b7eaf5b696068fc0b93e96f7c3c198953f\", size = 7237187 },\n+    { url = \"https://files.pythonhosted.org/packages/0e/6b/4fca9373eda42c1b04096758306b7bd55f7d8f78ba273446490855a0f25d/torchvision-0.21.0-cp310-cp310-manylinux_2_28_aarch64.whl\", hash = \"sha256:54815e0a56dde95cc6ec952577f67e0dc151eadd928e8d9f6a7f821d69a4a734\", size = 14699067 },\n+    { url = \"https://files.pythonhosted.org/packages/aa/f7/799ddd538b21017cbf80294c92e9efbf6db08dff6efee37c3be114a81845/torchvision-0.21.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:abbf1d7b9d52c00d2af4afa8dac1fb3e2356f662a4566bd98dfaaa3634f4eb34\", size = 1560542 },\n+    { url = \"https://files.pythonhosted.org/packages/29/88/00c69db213ee2443ada8886ec60789b227e06bb869d85ee324578221a7f7/torchvision-0.21.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:110d115333524d60e9e474d53c7d20f096dbd8a080232f88dddb90566f90064c\", size = 1784141 },\n+    { url = \"https://files.pythonhosted.org/packages/be/a2/b0cedf0a411f1a5d75cfc0b87cde56dd1ddc1878be46a42c905cd8580220/torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl\", hash = \"sha256:3891cd086c5071bda6b4ee9d266bb2ac39c998c045c2ebcd1e818b8316fb5d41\", size = 7237719 },\n+    { url = \"https://files.pythonhosted.org/packages/8c/a1/ee962ef9d0b2bf7a6f8b14cb95acb70e05cd2101af521032a09e43f8582f/torchvision-0.21.0-cp311-cp311-manylinux_2_28_aarch64.whl\", hash = \"sha256:54454923a50104c66a9ab6bd8b73a11c2fc218c964b1006d5d1fe5b442c3dcb6\", size = 14700617 },\n+    { url = \"https://files.pythonhosted.org/packages/88/53/4ad334b9b1d8dd99836869fec139cb74a27781298360b91b9506c53f1d10/torchvision-0.21.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:49bcfad8cfe2c27dee116c45d4f866d7974bcf14a5a9fbef893635deae322f2f\", size = 1560523 },\n+    { url = \"https://files.pythonhosted.org/packages/6e/1b/28f527b22d5e8800184d0bc847f801ae92c7573a8c15979d92b7091c0751/torchvision-0.21.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:97a5814a93c793aaf0179cfc7f916024f4b63218929aee977b645633d074a49f\", size = 1784140 },\n+    { url = \"https://files.pythonhosted.org/packages/36/63/0722e153fd27d64d5b0af45b5c8cb0e80b35a68cf0130303bc9a8bb095c7/torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl\", hash = \"sha256:b578bcad8a4083b40d34f689b19ca9f7c63e511758d806510ea03c29ac568f7b\", size = 7238673 },\n+    { url = \"https://files.pythonhosted.org/packages/bb/ea/03541ed901cdc30b934f897060d09bbf7a98466a08ad1680320f9ce0cbe0/torchvision-0.21.0-cp312-cp312-manylinux_2_28_aarch64.whl\", hash = \"sha256:5083a5b1fec2351bf5ea9900a741d54086db75baec4b1d21e39451e00977f1b1\", size = 14701186 },\n+    { url = \"https://files.pythonhosted.org/packages/4c/6a/c7752603060d076dfed95135b78b047dc71792630cbcb022e3693d6f32ef/torchvision-0.21.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:6eb75d41e3bbfc2f7642d0abba9383cc9ae6c5a4ca8d6b00628c225e1eaa63b3\", size = 1560520 },\n+    { url = \"https://files.pythonhosted.org/packages/f9/56/47d456b61c3bbce7bed4af3925c83d405bb87468e659fd3cf3d9840c3b51/torchvision-0.21.0-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:659b76c86757cb2ee4ca2db245e0740cfc3081fef46f0f1064d11adb4a8cee31\", size = 1784141 },\n+    { url = \"https://files.pythonhosted.org/packages/cb/4c/99880813aa50e64447fb1c4c6c804a793d2d78f7f7c53e99ddee7fa175fa/torchvision-0.21.0-cp313-cp313-manylinux1_x86_64.whl\", hash = \"sha256:084ac3f5a1f50c70d630a488d19bf62f323018eae1b1c1232f2b7047d3a7b76d\", size = 7238714 },\n+    { url = \"https://files.pythonhosted.org/packages/0b/2d/3c3ee10608310a395594aac7da8640372ed79c6585910ccae6919658dcdc/torchvision-0.21.0-cp313-cp313-manylinux_2_28_aarch64.whl\", hash = \"sha256:5045a3a5f21ec3eea6962fa5f2fa2d4283f854caec25ada493fcf4aab2925467\", size = 2281252 },\n+    { url = \"https://files.pythonhosted.org/packages/ed/b4/fc60e3bc003879d3de842baea258fffc3586f4b49cd435a5ba1e09c33315/torchvision-0.21.0-cp313-cp313-win_amd64.whl\", hash = \"sha256:9147f5e096a9270684e3befdee350f3cacafd48e0c54ab195f45790a9c146d67\", size = 1560519 },\n+]\n+\n [[package]]\n name = \"triton\"\n version = \"3.2.0\""
            },
            {
              "sha": "c4d864459af67c95f694f74e33df92ef3635b8b4",
              "url": "https://github.com/meta-llama/llama-models/commit/c4d864459af67c95f694f74e33df92ef3635b8b4",
              "message": "Move all Llama Stack types to llama-stack (#279)",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/__init__.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/interface.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/template_data.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/prompt_templates/__init__.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/prompt_templates/base.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/prompt_templates/system_prompts.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/prompt_templates/tool_response.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_tokenizer.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/tests/prompt_templates/test_system_prompts.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3_1/__init__.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3_1/prompts.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3_2/__init__.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3_2/prompts_text.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3_2/prompts_vision.py",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3_3/prompts.py",
                  "status": "removed"
                },
                {
                  "filename": "models/prompt_format.py",
                  "status": "removed"
                },
                {
                  "filename": "models/schema_utils.py",
                  "status": "removed"
                },
                {
                  "filename": "models/scripts/example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/generate_prompt_format.py",
                  "status": "removed"
                },
                {
                  "filename": "models/scripts/multimodal_example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_text_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "removed"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -5,238 +5,124 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n+import base64\n from enum import Enum\n-from typing import Any, Dict, Literal, Optional, Union\n \n-from pydantic import BaseModel, ConfigDict, Field\n+from io import BytesIO\n+from typing import Dict, List, Literal, Optional, Union\n+\n+from pydantic import BaseModel, ConfigDict, Field, field_serializer, field_validator\n+\n from typing_extensions import Annotated\n \n-from .schema_utils import json_schema_type, register_schema\n \n+# The goal is that these set of types are relevant for all Llama models.\n+# That isn't the current state yet -- e.g., BuiltinTool is somewhat specific to\n+# the llama3 series of models.\n+\n+\n+class Role(Enum):\n+    system = \"system\"\n+    user = \"user\"\n+    assistant = \"assistant\"\n+    tool = \"tool\"\n+\n+\n+class BuiltinTool(Enum):\n+    brave_search = \"brave_search\"\n+    wolfram_alpha = \"wolfram_alpha\"\n+    photogen = \"photogen\"\n+    code_interpreter = \"code_interpreter\"\n+\n+\n+Primitive = Union[str, int, float, bool, None]\n+RecursiveType = Union[Primitive, List[Primitive], Dict[str, Primitive]]\n+\n+\n+class ToolCall(BaseModel):\n+    call_id: str\n+    tool_name: Union[BuiltinTool, str]\n+    arguments: Dict[str, RecursiveType]\n+\n+    @field_validator(\"tool_name\", mode=\"before\")\n+    @classmethod\n+    def validate_field(cls, v):\n+        if isinstance(v, str):\n+            try:\n+                return BuiltinTool(v)\n+            except ValueError:\n+                return v\n+        return v\n+\n+\n+class ToolPromptFormat(Enum):\n+    \"\"\"Prompt format for calling custom / zero shot tools.\n+\n+    :cvar json: JSON format for calling tools. It takes the form:\n+        {\n+            \"type\": \"function\",\n+            \"function\" : {\n+                \"name\": \"function_name\",\n+                \"description\": \"function_description\",\n+                \"parameters\": {...}\n+            }\n+        }\n+    :cvar function_tag: Function tag format, pseudo-XML. This looks like:\n+        <function=function_name>(parameters)</function>\n+\n+    :cvar python_list: Python list. The output is a valid Python expression that can be\n+        evaluated to a list. Each element in the list is a function call. Example:\n+        [\"function_name(param1, param2)\", \"function_name(param1, param2)\"]\n+    \"\"\"\n+\n+    json = \"json\"\n+    function_tag = \"function_tag\"\n+    python_list = \"python_list\"\n+\n+\n+class StopReason(Enum):\n+    end_of_turn = \"end_of_turn\"\n+    end_of_message = \"end_of_message\"\n+    out_of_tokens = \"out_of_tokens\"\n+\n+\n+class RawMediaItem(BaseModel):\n+    type: Literal[\"image\"] = \"image\"\n+    data: bytes | BytesIO\n+\n+    model_config = ConfigDict(arbitrary_types_allowed=True)\n+\n+    @field_serializer(\"data\")\n+    def serialize_data(self, data: Optional[bytes], _info):\n+        if data is None:\n+            return None\n+        return base64.b64encode(data).decode(\"utf-8\")\n+\n+    @field_validator(\"data\", mode=\"before\")\n+    @classmethod\n+    def validate_data(cls, v):\n+        if isinstance(v, str):\n+            return base64.b64decode(v)\n+        return v\n+\n+\n+class RawTextItem(BaseModel):\n+    type: Literal[\"text\"] = \"text\"\n+    text: str\n \n-@json_schema_type\n-class GreedySamplingStrategy(BaseModel):\n-    type: Literal[\"greedy\"] = \"greedy\"\n \n+RawContentItem = Annotated[Union[RawTextItem, RawMediaItem], Field(discriminator=\"type\")]\n \n-@json_schema_type\n-class TopPSamplingStrategy(BaseModel):\n-    type: Literal[\"top_p\"] = \"top_p\"\n-    temperature: Optional[float] = Field(..., gt=0.0)\n-    top_p: Optional[float] = 0.95\n+RawContent = str | RawContentItem | List[RawContentItem]\n \n \n-@json_schema_type\n-class TopKSamplingStrategy(BaseModel):\n-    type: Literal[\"top_k\"] = \"top_k\"\n-    top_k: int = Field(..., ge=1)\n+class RawMessage(BaseModel):\n+    role: Literal[\"user\"] | Literal[\"system\"] | Literal[\"tool\"] | Literal[\"assistant\"]\n+    content: RawContent\n \n+    # This is for RAG but likely should be absorbed into content\n+    context: Optional[RawContent] = None\n \n-SamplingStrategy = register_schema(\n-    Annotated[\n-        Union[GreedySamplingStrategy, TopPSamplingStrategy, TopKSamplingStrategy],\n-        Field(discriminator=\"type\"),\n-    ],\n-    name=\"SamplingStrategy\",\n-)\n-\n-\n-@json_schema_type\n-class SamplingParams(BaseModel):\n-    strategy: SamplingStrategy = Field(default_factory=GreedySamplingStrategy)\n-\n-    max_tokens: Optional[int] = 0\n-    repetition_penalty: Optional[float] = 1.0\n-\n-\n-class CheckpointQuantizationFormat(Enum):\n-    # default format\n-    bf16 = \"bf16\"\n-\n-    # used for enabling fp8_rowwise inference, some weights are bf16\n-    fp8_mixed = \"fp8-mixed\"\n-\n-    int8 = \"int8\"\n-\n-    int4 = \"int4\"\n-\n-\n-class ModelFamily(Enum):\n-    llama2 = \"llama2\"\n-    llama3 = \"llama3\"\n-    llama3_1 = \"llama3_1\"\n-    llama3_2 = \"llama3_2\"\n-    llama3_3 = \"llama3_3\"\n-    safety = \"safety\"\n-\n-\n-class CoreModelId(Enum):\n-    \"\"\"Each of these models is a unique \"SKU\". These root models can be served in various garbs (especially by quantizing them)\"\"\"\n-\n-    # Llama 2 family\n-    llama2_7b = \"Llama-2-7b\"\n-    llama2_13b = \"Llama-2-13b\"\n-    llama2_70b = \"Llama-2-70b\"\n-    llama2_7b_chat = \"Llama-2-7b-chat\"\n-    llama2_13b_chat = \"Llama-2-13b-chat\"\n-    llama2_70b_chat = \"Llama-2-70b-chat\"\n-\n-    # Llama 3 family\n-    llama3_8b = \"Llama-3-8B\"\n-    llama3_70b = \"Llama-3-70B\"\n-    llama3_8b_instruct = \"Llama-3-8B-Instruct\"\n-    llama3_70b_instruct = \"Llama-3-70B-Instruct\"\n-\n-    # Llama 3.1 family\n-    llama3_1_8b = \"Llama3.1-8B\"\n-    llama3_1_70b = \"Llama3.1-70B\"\n-    llama3_1_405b = \"Llama3.1-405B\"\n-    llama3_1_8b_instruct = \"Llama3.1-8B-Instruct\"\n-    llama3_1_70b_instruct = \"Llama3.1-70B-Instruct\"\n-    llama3_1_405b_instruct = \"Llama3.1-405B-Instruct\"\n-\n-    # Llama 3.2 family\n-    llama3_2_1b = \"Llama3.2-1B\"\n-    llama3_2_3b = \"Llama3.2-3B\"\n-    llama3_2_1b_instruct = \"Llama3.2-1B-Instruct\"\n-    llama3_2_3b_instruct = \"Llama3.2-3B-Instruct\"\n-    llama3_2_11b_vision = \"Llama3.2-11B-Vision\"\n-    llama3_2_90b_vision = \"Llama3.2-90B-Vision\"\n-    llama3_2_11b_vision_instruct = \"Llama3.2-11B-Vision-Instruct\"\n-    llama3_2_90b_vision_instruct = \"Llama3.2-90B-Vision-Instruct\"\n-\n-    # Llama 3.3 family\n-    llama3_3_70b_instruct = \"Llama3.3-70B-Instruct\"\n-\n-    # Safety models\n-    llama_guard_3_8b = \"Llama-Guard-3-8B\"\n-    llama_guard_2_8b = \"Llama-Guard-2-8B\"\n-    llama_guard_3_11b_vision = \"Llama-Guard-3-11B-Vision\"\n-    llama_guard_3_1b = \"Llama-Guard-3-1B\"\n-\n-\n-def is_multimodal(model_id) -> bool:\n-    if model_id in [\n-        CoreModelId.llama3_2_11b_vision,\n-        CoreModelId.llama3_2_90b_vision,\n-        CoreModelId.llama3_2_11b_vision_instruct,\n-        CoreModelId.llama3_2_90b_vision_instruct,\n-    ]:\n-        return True\n-    else:\n-        return False\n-\n-\n-def model_family(model_id) -> ModelFamily:\n-    if model_id in [\n-        CoreModelId.llama2_7b,\n-        CoreModelId.llama2_13b,\n-        CoreModelId.llama2_70b,\n-        CoreModelId.llama2_7b_chat,\n-        CoreModelId.llama2_13b_chat,\n-        CoreModelId.llama2_70b_chat,\n-    ]:\n-        return ModelFamily.llama2\n-    elif model_id in [\n-        CoreModelId.llama3_8b,\n-        CoreModelId.llama3_70b,\n-        CoreModelId.llama3_8b_instruct,\n-        CoreModelId.llama3_70b_instruct,\n-    ]:\n-        return ModelFamily.llama3\n-    elif model_id in [\n-        CoreModelId.llama3_1_8b,\n-        CoreModelId.llama3_1_70b,\n-        CoreModelId.llama3_1_405b,\n-        CoreModelId.llama3_1_8b_instruct,\n-        CoreModelId.llama3_1_70b_instruct,\n-        CoreModelId.llama3_1_405b_instruct,\n-    ]:\n-        return ModelFamily.llama3_1\n-    elif model_id in [\n-        CoreModelId.llama3_2_1b,\n-        CoreModelId.llama3_2_3b,\n-        CoreModelId.llama3_2_1b_instruct,\n-        CoreModelId.llama3_2_3b_instruct,\n-        CoreModelId.llama3_2_11b_vision,\n-        CoreModelId.llama3_2_90b_vision,\n-        CoreModelId.llama3_2_11b_vision_instruct,\n-        CoreModelId.llama3_2_90b_vision_instruct,\n-    ]:\n-        return ModelFamily.llama3_2\n-    elif model_id in [\n-        CoreModelId.llama3_3_70b_instruct,\n-    ]:\n-        return ModelFamily.llama3_3\n-    elif model_id in [\n-        CoreModelId.llama_guard_3_8b,\n-        CoreModelId.llama_guard_2_8b,\n-        CoreModelId.llama_guard_3_11b_vision,\n-        CoreModelId.llama_guard_3_1b,\n-    ]:\n-        return ModelFamily.safety\n-    else:\n-        raise ValueError(f\"Unknown model family for {model_id}\")\n-\n-\n-class Model(BaseModel):\n-    core_model_id: CoreModelId\n-    description: str\n-    huggingface_repo: Optional[str] = None\n-    recommended_sampling_params: Optional[SamplingParams] = None\n-    arch_args: Dict[str, Any]\n-    variant: str = \"\"\n-\n-    quantization_format: CheckpointQuantizationFormat = CheckpointQuantizationFormat.bf16\n-    pth_file_count: int\n-    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n-\n-    # silence pydantic until we remove the `model_` fields\n-    model_config = ConfigDict(protected_namespaces=())\n-\n-    @property\n-    def model_family(self) -> ModelFamily:\n-        return model_family(self.core_model_id)\n-\n-    # The SKU is uniquely identified by (model_id, variant) combo\n-    def descriptor(self, shorten_default_variant: bool = True) -> str:\n-        if not self.variant:\n-            return self.core_model_id.value\n-        return f\"{self.core_model_id.value}:{self.variant}\"\n-\n-    @property\n-    def is_instruct_model(self) -> bool:\n-        return \"instruct\" in self.id.name\n-\n-    # Featured models are shown in the non-exhaustive model list\n-    @property\n-    def is_featured(self) -> bool:\n-        return self.model_family in [\n-            ModelFamily.llama3_1,\n-            ModelFamily.llama3_2,\n-            ModelFamily.llama3_3,\n-            ModelFamily.safety,\n-        ]\n-\n-    @property\n-    def max_seq_length(self) -> int:\n-        if self.model_family == ModelFamily.llama2:\n-            return 4096\n-        elif self.core_model_id == CoreModelId.llama_guard_2_8b:\n-            return 4096\n-        elif self.model_family == ModelFamily.llama3:\n-            return 8192\n-        elif self.model_family in [ModelFamily.llama3_1, ModelFamily.llama3_3]:\n-            return 131072\n-        elif self.model_family == ModelFamily.llama3_2:\n-            if self.quantization_format == CheckpointQuantizationFormat.int4:\n-                return 8192\n-            return 131072\n-        elif self.core_model_id in [\n-            CoreModelId.llama_guard_3_8b,\n-            CoreModelId.llama_guard_3_11b_vision,\n-            CoreModelId.llama_guard_3_1b,\n-        ]:\n-            return 131072\n-        else:\n-            raise ValueError(f\"Unknown max_seq_len for {self.core_model_id}\")\n+    # These are for the output message coming from the assistant\n+    stop_reason: Optional[StopReason] = None\n+    tool_calls: List[ToolCall] = Field(default_factory=list)\n\n--- File: models/llama3/api/__init__.py ---\n@@ -7,5 +7,4 @@\n \n from .args import *  # noqa\n from .chat_format import *  # noqa\n-from .datatypes import *  # noqa\n from .tokenizer import *  # noqa\n\n--- File: models/llama3/api/chat_format.py ---\n@@ -13,7 +13,7 @@\n \n from PIL import Image as PIL_Image\n \n-from .datatypes import (\n+from ...datatypes import (\n     BuiltinTool,\n     RawContent,\n     RawMediaItem,\n\n--- File: models/llama3/api/datatypes.py ---\n@@ -1,153 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import base64\n-from enum import Enum\n-\n-from io import BytesIO\n-from typing import Dict, List, Literal, Optional, Union\n-\n-from pydantic import BaseModel, ConfigDict, Field, field_serializer, field_validator\n-\n-from typing_extensions import Annotated\n-from ...datatypes import *  # noqa\n-\n-\n-from ...schema_utils import json_schema_type\n-\n-\n-class Role(Enum):\n-    system = \"system\"\n-    user = \"user\"\n-    assistant = \"assistant\"\n-    tool = \"tool\"\n-\n-\n-class BuiltinTool(Enum):\n-    brave_search = \"brave_search\"\n-    wolfram_alpha = \"wolfram_alpha\"\n-    photogen = \"photogen\"\n-    code_interpreter = \"code_interpreter\"\n-\n-\n-Primitive = Union[str, int, float, bool, None]\n-RecursiveType = Union[Primitive, List[Primitive], Dict[str, Primitive]]\n-\n-\n-@json_schema_type\n-class ToolCall(BaseModel):\n-    call_id: str\n-    tool_name: Union[BuiltinTool, str]\n-    arguments: Dict[str, RecursiveType]\n-\n-    @field_validator(\"tool_name\", mode=\"before\")\n-    @classmethod\n-    def validate_field(cls, v):\n-        if isinstance(v, str):\n-            try:\n-                return BuiltinTool(v)\n-            except ValueError:\n-                return v\n-        return v\n-\n-\n-@json_schema_type\n-class ToolParamDefinition(BaseModel):\n-    param_type: str\n-    description: Optional[str] = None\n-    required: Optional[bool] = True\n-    default: Optional[Any] = None\n-\n-\n-@json_schema_type\n-class ToolDefinition(BaseModel):\n-    tool_name: Union[BuiltinTool, str]\n-    description: Optional[str] = None\n-    parameters: Optional[Dict[str, ToolParamDefinition]] = None\n-\n-    @field_validator(\"tool_name\", mode=\"before\")\n-    @classmethod\n-    def validate_field(cls, v):\n-        if isinstance(v, str):\n-            try:\n-                return BuiltinTool(v)\n-            except ValueError:\n-                return v\n-        return v\n-\n-\n-class ToolPromptFormat(Enum):\n-    \"\"\"Prompt format for calling custom / zero shot tools.\n-\n-    :cvar json: JSON format for calling tools. It takes the form:\n-        {\n-            \"type\": \"function\",\n-            \"function\" : {\n-                \"name\": \"function_name\",\n-                \"description\": \"function_description\",\n-                \"parameters\": {...}\n-            }\n-        }\n-    :cvar function_tag: Function tag format, pseudo-XML. This looks like:\n-        <function=function_name>(parameters)</function>\n-\n-    :cvar python_list: Python list. The output is a valid Python expression that can be\n-        evaluated to a list. Each element in the list is a function call. Example:\n-        [\"function_name(param1, param2)\", \"function_name(param1, param2)\"]\n-    \"\"\"\n-\n-    json = \"json\"\n-    function_tag = \"function_tag\"\n-    python_list = \"python_list\"\n-\n-\n-class StopReason(Enum):\n-    end_of_turn = \"end_of_turn\"\n-    end_of_message = \"end_of_message\"\n-    out_of_tokens = \"out_of_tokens\"\n-\n-\n-class RawMediaItem(BaseModel):\n-    type: Literal[\"image\"] = \"image\"\n-    data: bytes | BytesIO\n-\n-    model_config = ConfigDict(arbitrary_types_allowed=True)\n-\n-    @field_serializer(\"data\")\n-    def serialize_data(self, data: Optional[bytes], _info):\n-        if data is None:\n-            return None\n-        return base64.b64encode(data).decode(\"utf-8\")\n-\n-    @field_validator(\"data\", mode=\"before\")\n-    @classmethod\n-    def validate_data(cls, v):\n-        if isinstance(v, str):\n-            return base64.b64decode(v)\n-        return v\n-\n-\n-class RawTextItem(BaseModel):\n-    type: Literal[\"text\"] = \"text\"\n-    text: str\n-\n-\n-RawContentItem = Annotated[Union[RawTextItem, RawMediaItem], Field(discriminator=\"type\")]\n-\n-RawContent = str | RawContentItem | List[RawContentItem]\n-\n-\n-class RawMessage(BaseModel):\n-    role: Literal[\"user\"] | Literal[\"system\"] | Literal[\"tool\"] | Literal[\"assistant\"]\n-    content: RawContent\n-\n-    # This is for RAG but likely should be absorbed into content\n-    context: Optional[RawContent] = None\n-\n-    # These are for the output message coming from the assistant\n-    stop_reason: Optional[StopReason] = None\n-    tool_calls: List[ToolCall] = Field(default_factory=list)\n\n--- File: models/llama3/api/interface.py ---\n@@ -1,255 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-from pathlib import Path\n-\n-from typing import List, Optional\n-\n-from termcolor import colored\n-\n-from ..prompt_templates import (\n-    BuiltinToolGenerator,\n-    FunctionTagCustomToolGenerator,\n-    JsonCustomToolGenerator,\n-    SystemDefaultGenerator,\n-    ToolResponseGenerator,\n-)\n-\n-from . import template_data\n-\n-from .chat_format import ChatFormat\n-\n-from .datatypes import (\n-    BuiltinTool,\n-    RawMessage,\n-    StopReason,\n-    ToolCall,\n-    ToolDefinition,\n-    ToolPromptFormat,\n-)\n-from .tokenizer import Tokenizer\n-\n-\n-THIS_DIR = Path(__file__).parent\n-\n-\n-class Template:\n-    def __init__(\n-        self,\n-        role,\n-        template_name,\n-        data_provider=None,\n-        notes=None,\n-    ):\n-        self.role = role\n-        self.template_name = template_name\n-        self.data_provider = data_provider or \"\"\n-        self._notes = notes or \"\"\n-\n-    @property\n-    def notes(self):\n-        default = \" represents newline\"\n-        notes = default\n-        if self._notes:\n-            notes += \"\\n\"\n-            notes += self._notes\n-        return notes\n-\n-\n-TEMPLATES = [\n-    Template(\n-        \"user\",\n-        \"user-default\",\n-        \"user_default\",\n-    ),\n-    Template(\n-        \"user\",\n-        \"user-images\",\n-        \"user_images\",\n-    ),\n-    Template(\"user\", \"user-interleaved-images\", \"user_interleaved_images\"),\n-    Template(\n-        \"assistant\",\n-        \"assistant-builtin-tool-call\",\n-        \"assistant_builtin_tool_call\",\n-        \"Notice <|python_tag|>\",\n-    ),\n-    Template(\n-        \"assistant\",\n-        \"assistant-custom-tool-call\",\n-        \"assistant_custom_tool_call\",\n-        \"Notice <function=...> format\",\n-    ),\n-    Template(\n-        \"assistant\",\n-        \"assistant-default\",\n-        \"assistant_default\",\n-    ),\n-    Template(\n-        \"system\",\n-        \"system-builtin-and-custom-tools\",\n-        \"system_message_builtin_and_custom_tools\",\n-    ),\n-    Template(\n-        \"system\",\n-        \"system-builtin-tools-only\",\n-        \"system_message_builtin_tools_only\",\n-    ),\n-    Template(\n-        \"system\",\n-        \"system-custom-tools-only\",\n-        \"system_message_custom_tools_only\",\n-    ),\n-    Template(\n-        \"system\",\n-        \"system-default\",\n-        \"system_default\",\n-    ),\n-    Template(\n-        \"tool\",\n-        \"tool-success\",\n-        \"tool_success\",\n-        \"Note ipython header and [stdout]\",\n-    ),\n-    Template(\n-        \"tool\",\n-        \"tool-failure\",\n-        \"tool_failure\",\n-        \"Note ipython header and [stderr]\",\n-    ),\n-]\n-\n-\n-class LLama31Interface:\n-    def __init__(self, tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json):\n-        self.tokenizer = Tokenizer.get_instance()\n-        self.formatter = ChatFormat(self.tokenizer)\n-        self.tool_prompt_format = tool_prompt_format\n-\n-    def get_tokens(self, messages: List[RawMessage]) -> List[int]:\n-        model_input = self.formatter.encode_dialog_prompt(\n-            messages,\n-            self.tool_prompt_format,\n-        )\n-        return model_input.tokens\n-\n-    def tool_response_messages(self, *args, **kwargs):\n-        template = ToolResponseGenerator().gen(*args, **kwargs)\n-        return [\n-            RawMessage(\n-                role=\"tool\",\n-                content=template.render(),\n-            )\n-        ]\n-\n-    def system_messages(\n-        self,\n-        builtin_tools: List[BuiltinTool],\n-        custom_tools: List[ToolDefinition],\n-        instruction: Optional[str] = None,\n-    ) -> List[RawMessage]:\n-        messages = []\n-\n-        default_gen = SystemDefaultGenerator()\n-        default_template = default_gen.gen()\n-\n-        sys_content = \"\"\n-\n-        tool_template = None\n-        if builtin_tools or custom_tools:\n-            tool_gen = BuiltinToolGenerator()\n-            tool_template = tool_gen.gen(builtin_tools + custom_tools)\n-\n-            sys_content += tool_template.render()\n-            sys_content += \"\\n\"\n-\n-        sys_content += default_template.render()\n-\n-        if instruction:\n-            sys_content += \"\\n\\n\"\n-            sys_content += instruction\n-\n-        sys_content += \"\\n\"\n-        messages.append(RawMessage(role=\"system\", content=sys_content))\n-\n-        if custom_tools:\n-            if self.tool_prompt_format == ToolPromptFormat.json:\n-                tool_gen = JsonCustomToolGenerator()\n-            elif self.tool_prompt_format == ToolPromptFormat.function_tag:\n-                tool_gen = FunctionTagCustomToolGenerator()\n-            else:\n-                raise ValueError(f\"Non supported ToolPromptFormat {self.tool_prompt_format}\")\n-\n-            custom_template = tool_gen.gen(custom_tools)\n-            messages.append(RawMessage(role=\"user\", content=custom_template.render()))\n-\n-        return messages\n-\n-    def assistant_response_messages(\n-        self,\n-        content: str,\n-        stop_reason: StopReason,\n-        tool_call: Optional[ToolCall] = None,\n-    ) -> List[RawMessage]:\n-        tool_calls = []\n-        if tool_call:\n-            tool_calls.append(tool_call)\n-        return [\n-            RawMessage(\n-                role=\"assistant\",\n-                content=content,\n-                tool_calls=tool_calls,\n-                stop_reason=stop_reason,\n-            )\n-        ]\n-\n-    def user_message(self, content: str) -> List[RawMessage]:\n-        return [RawMessage(role=\"user\", content=content)]\n-\n-    def display_message_as_tokens(self, message: RawMessage) -> None:\n-        \"\"\"Util to print tokenized string to shell\"\"\"\n-        tokens = self.formatter.encode_message(message, self.tool_prompt_format)\n-        on_colors = [\n-            \"on_red\",\n-            \"on_green\",\n-            \"on_yellow\",\n-            \"on_blue\",\n-            \"on_magenta\",\n-            \"on_cyan\",\n-        ]\n-        for i, t in enumerate(tokens):\n-            on_col = on_colors[i % len(on_colors)]\n-            print(colored(self.tokenizer.decode([t]), \"white\", on_col), end=\"\")\n-        print(\"\\n\", end=\"\")\n-\n-\n-def list_jinja_templates() -> List[Template]:\n-    return TEMPLATES\n-\n-\n-def render_jinja_template(name: str, tool_prompt_format: ToolPromptFormat):\n-    by_name = {t.template_name: t for t in TEMPLATES}\n-    if name not in by_name:\n-        raise ValueError(f\"No template found for `{name}`\")\n-\n-    template = by_name[name]\n-    interface = LLama31Interface(tool_prompt_format)\n-\n-    data_func = getattr(template_data, template.data_provider)\n-    if template.role == \"system\":\n-        messages = interface.system_messages(**data_func())\n-    elif template.role == \"tool\":\n-        messages = interface.tool_response_messages(**data_func())\n-    elif template.role == \"assistant\":\n-        messages = interface.assistant_response_messages(**data_func())\n-    elif template.role == \"user\":\n-        messages = interface.user_message(**data_func())\n-\n-    tokens = interface.get_tokens(messages)\n-    special_tokens = list(interface.tokenizer.special_tokens.values())\n-    tokens = [(interface.tokenizer.decode([t]), t in special_tokens) for t in tokens]\n-    return template, tokens\n\n--- File: models/llama3/api/template_data.py ---\n@@ -1,109 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-from ..prompt_templates import (\n-    BuiltinToolGenerator,\n-    JsonCustomToolGenerator,\n-    ToolResponseGenerator,\n-)\n-from .datatypes import BuiltinTool, StopReason, ToolCall\n-\n-INSTRUCTION = \"You are a helpful assistant.\"\n-\n-\n-def system_message_builtin_tools_only():\n-    return {\n-        \"builtin_tools\": BuiltinToolGenerator().data_examples()[0],\n-        \"custom_tools\": [],\n-        \"instruction\": INSTRUCTION,\n-    }\n-\n-\n-def system_message_builtin_code_only():\n-    return {\n-        \"builtin_tools\": BuiltinToolGenerator().data_examples()[1],\n-        \"custom_tools\": [],\n-        \"instruction\": \"\",\n-    }\n-\n-\n-def system_message_custom_tools_only():\n-    return {\n-        \"builtin_tools\": [],\n-        \"custom_tools\": JsonCustomToolGenerator().data_examples()[0],\n-        \"instruction\": INSTRUCTION,\n-    }\n-\n-\n-def system_message_builtin_and_custom_tools():\n-    return {\n-        \"builtin_tools\": BuiltinToolGenerator().data_examples()[0],\n-        \"custom_tools\": JsonCustomToolGenerator().data_examples()[0],\n-        \"instruction\": INSTRUCTION,\n-    }\n-\n-\n-def system_default():\n-    return {\n-        \"builtin_tools\": [],\n-        \"custom_tools\": [],\n-        \"instruction\": INSTRUCTION,\n-    }\n-\n-\n-def tool_success():\n-    return ToolResponseGenerator().data_examples()[0]\n-\n-\n-def tool_failure():\n-    return ToolResponseGenerator().data_examples()[1]\n-\n-\n-def assistant_builtin_tool_call():\n-    return {\n-        \"content\": \"\",\n-        \"tool_call\": ToolCall(\n-            call_id=\"uuid\",\n-            tool_name=BuiltinTool.brave_search,\n-            arguments={\n-                \"query\": \"Who won NBA in 2024?\",\n-            },\n-        ),\n-        \"stop_reason\": StopReason.end_of_message,\n-    }\n-\n-\n-def assistant_custom_tool_call():\n-    return {\n-        \"content\": \"\",\n-        \"tool_call\": ToolCall(\n-            call_id=\"uuid\",\n-            tool_name=\"trending_songs\",\n-            arguments={\"country\": \"US\", \"n\": 10},\n-        ),\n-        \"stop_reason\": StopReason.end_of_turn,\n-    }\n-\n-\n-def assistant_default():\n-    return {\n-        \"content\": \"Hi, I am a helpful assistant. What can I help you with today?\",\n-        \"tool_call\": None,\n-        \"stop_reason\": StopReason.end_of_turn,\n-    }\n-\n-\n-def user_default():\n-    return {\"content\": \"Please tell me how to plan a trip to New York\"}\n-\n-\n-def user_images():\n-    return {\"content\": \"<|image|><|image|>What do these images depict?\"}\n-\n-\n-def user_interleaved_images():\n-    return {\"content\": \"<|image|>Describe the image in one sentence.<|image|>Write a haiku about these images\"}\n\n--- File: models/llama3/api/tool_utils.py ---\n@@ -9,7 +9,8 @@\n import re\n from typing import Optional, Tuple\n \n-from .datatypes import BuiltinTool, RecursiveType, ToolCall, ToolPromptFormat\n+from ...datatypes import BuiltinTool, RecursiveType, ToolCall, ToolPromptFormat\n+\n \n BUILTIN_TOOL_PATTERN = r'\\b(?P<tool_name>\\w+)\\.call\\(query=\"(?P<query>[^\"]*)\"\\)'\n CUSTOM_TOOL_CALL_PATTERN = re.compile(r\"<function=(?P<function_name>[^}]+)>(?P<args>{.*?})\")\n\n--- File: models/llama3/prompt_templates/__init__.py ---\n@@ -1,16 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-from .base import PromptTemplate, PromptTemplateGeneratorBase  # noqa: F401\n-from .system_prompts import (  # noqa: F401\n-    BuiltinToolGenerator,\n-    FunctionTagCustomToolGenerator,\n-    JsonCustomToolGenerator,\n-    PythonListCustomToolGenerator,\n-    SystemDefaultGenerator,\n-)\n-from .tool_response import ToolResponseGenerator  # noqa: F401\n\n--- File: models/llama3/prompt_templates/base.py ---\n@@ -1,33 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-from dataclasses import dataclass\n-from typing import Any, Dict, List\n-\n-from jinja2 import Template\n-\n-\n-@dataclass\n-class PromptTemplate:\n-    template: str\n-    data: Dict[str, Any]\n-\n-    def render(self):\n-        template = Template(self.template)\n-        return template.render(self.data)\n-\n-\n-class PromptTemplateGeneratorBase:\n-    \"\"\"\n-    Base class for prompt template generators.\n-    \"\"\"\n-\n-    def gen(self, *args, **kwargs) -> PromptTemplate:\n-        raise NotImplementedError()\n-\n-    def data_examples(self) -> List[Any]:\n-        raise NotImplementedError()\n\n--- File: models/llama3/prompt_templates/system_prompts.py ---\n@@ -1,308 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import textwrap\n-from datetime import datetime\n-from typing import Any, List, Optional\n-\n-from llama_models.llama3.api.datatypes import (\n-    BuiltinTool,\n-    ToolDefinition,\n-    ToolParamDefinition,\n-)\n-\n-from .base import PromptTemplate, PromptTemplateGeneratorBase\n-\n-\n-class SystemDefaultGenerator(PromptTemplateGeneratorBase):\n-    def gen(self, *args, **kwargs) -> PromptTemplate:\n-        template_str = textwrap.dedent(\n-            \"\"\"\n-            Cutting Knowledge Date: December 2023\n-            Today Date: {{ today }}\n-            \"\"\"\n-        )\n-        return PromptTemplate(\n-            template_str.lstrip(\"\\n\"),\n-            {\"today\": datetime.now().strftime(\"%d %B %Y\")},\n-        )\n-\n-    def data_examples(self) -> List[Any]:\n-        return [None]\n-\n-\n-class BuiltinToolGenerator(PromptTemplateGeneratorBase):\n-    def _tool_breakdown(self, tools: List[ToolDefinition]):\n-        builtin_tools, custom_tools = [], []\n-        for dfn in tools:\n-            if isinstance(dfn.tool_name, BuiltinTool):\n-                builtin_tools.append(dfn)\n-            else:\n-                custom_tools.append(dfn)\n-\n-        return builtin_tools, custom_tools\n-\n-    def gen(self, tools: List[ToolDefinition]) -> PromptTemplate:\n-        builtin_tools, custom_tools = self._tool_breakdown(tools)\n-        template_str = textwrap.dedent(\n-            \"\"\"\n-            {% if builtin_tools or custom_tools -%}\n-            Environment: ipython\n-            {% endif -%}\n-            {% set builtin_tools = builtin_tools | reject('equalto', 'code_interpreter') | list -%}\n-            {% if builtin_tools -%}\n-            Tools: {{ builtin_tools | join(\", \") | trim -}}\n-            {% endif %}\n-            \"\"\"\n-        )\n-        return PromptTemplate(\n-            template_str.lstrip(\"\\n\"),\n-            {\n-                \"builtin_tools\": [t.tool_name.value for t in builtin_tools],\n-                \"custom_tools\": custom_tools,\n-            },\n-        )\n-\n-    def data_examples(self) -> List[List[ToolDefinition]]:\n-        return [\n-            # builtin tools\n-            [\n-                ToolDefinition(tool_name=BuiltinTool.code_interpreter),\n-                ToolDefinition(tool_name=BuiltinTool.brave_search),\n-                ToolDefinition(tool_name=BuiltinTool.wolfram_alpha),\n-            ],\n-            # only code interpretor\n-            [\n-                ToolDefinition(tool_name=BuiltinTool.code_interpreter),\n-            ],\n-        ]\n-\n-\n-class JsonCustomToolGenerator(PromptTemplateGeneratorBase):\n-    def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n-        template_str = textwrap.dedent(\n-            \"\"\"\n-            Answer the user's question by making use of the following functions if needed.\n-            If none of the function can be used, please say so.\n-            Here is a list of functions in JSON format:\n-            {% for t in custom_tools -%}\n-            {# manually setting up JSON because jinja sorts keys in unexpected ways -#}\n-            {%- set tname = t.tool_name -%}\n-            {%- set tdesc = t.description -%}\n-            {%- set tparams = t.parameters -%}\n-            {%- set required_params = [] -%}\n-            {%- for name, param in tparams.items() if param.required == true -%}\n-                {%- set _ = required_params.append(name) -%}\n-            {%- endfor -%}\n-            {\n-                \"type\": \"function\",\n-                \"function\": {\n-                    \"name\": \"{{tname}}\",\n-                    \"description\": \"{{tdesc}}\",\n-                    \"parameters\": {\n-                        \"type\": \"object\",\n-                        \"properties\": [\n-                            {%- for name, param in tparams.items() %}\n-                            {\n-                                \"{{name}}\": {\n-                                    \"type\": \"object\",\n-                                    \"description\": \"{{param.description}}\"\n-                                }\n-                            }{% if not loop.last %},{% endif %}\n-                            {%- endfor %}\n-                        ],\n-                        \"required\": {{ required_params | tojson }}\n-                    }\n-                }\n-            }\n-            {% endfor %}\n-            Return function calls in JSON format.\n-            \"\"\"\n-        )\n-\n-        return PromptTemplate(\n-            template_str.lstrip(\"\\n\"),\n-            {\"custom_tools\": [t.model_dump() for t in custom_tools]},\n-        )\n-\n-    def data_examples(self) -> List[List[ToolDefinition]]:\n-        return [\n-            [\n-                ToolDefinition(\n-                    tool_name=\"trending_songs\",\n-                    description=\"Returns the trending songs on a Music site\",\n-                    parameters={\n-                        \"n\": ToolParamDefinition(\n-                            param_type=\"int\",\n-                            description=\"The number of songs to return\",\n-                            required=True,\n-                        ),\n-                        \"genre\": ToolParamDefinition(\n-                            param_type=\"str\",\n-                            description=\"The genre of the songs to return\",\n-                            required=False,\n-                        ),\n-                    },\n-                ),\n-            ]\n-        ]\n-\n-\n-class FunctionTagCustomToolGenerator(PromptTemplateGeneratorBase):\n-    def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n-        template_str = textwrap.dedent(\n-            \"\"\"\n-            You have access to the following functions:\n-\n-            {% for t in custom_tools %}\n-            {#- manually setting up JSON because jinja sorts keys in unexpected ways -#}\n-            {%- set tname = t.tool_name -%}\n-            {%- set tdesc = t.description -%}\n-            {%- set modified_params = t.parameters.copy() -%}\n-            {%- for key, value in modified_params.items() -%}\n-                {%- if 'default' in value -%}\n-                    {%- set _ = value.pop('default', None) -%}\n-                {%- endif -%}\n-            {%- endfor -%}\n-            {%- set tparams = modified_params | tojson -%}\n-            Use the function '{{ tname }}' to '{{ tdesc }}':\n-            {\"name\": \"{{tname}}\", \"description\": \"{{tdesc}}\", \"parameters\": {{tparams}}}\n-\n-            {% endfor -%}\n-            Think very carefully before calling functions.\n-            If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n-\n-            <function=example_function_name>{\"example_name\": \"example_value\"}</function>\n-\n-            Reminder:\n-            - If looking for real time information use relevant functions before falling back to brave_search\n-            - Function calls MUST follow the specified format, start with <function= and end with </function>\n-            - Required parameters MUST be specified\n-            - Only call one function at a time\n-            - Put the entire function call reply on one line\n-            \"\"\"\n-        )\n-        return PromptTemplate(\n-            template_str.lstrip(\"\\n\"),\n-            {\"custom_tools\": [t.model_dump() for t in custom_tools]},\n-        )\n-\n-    def data_examples(self) -> List[List[ToolDefinition]]:\n-        return [\n-            [\n-                ToolDefinition(\n-                    tool_name=\"trending_songs\",\n-                    description=\"Returns the trending songs on a Music site\",\n-                    parameters={\n-                        \"n\": ToolParamDefinition(\n-                            param_type=\"int\",\n-                            description=\"The number of songs to return\",\n-                            required=True,\n-                        ),\n-                        \"genre\": ToolParamDefinition(\n-                            param_type=\"str\",\n-                            description=\"The genre of the songs to return\",\n-                            required=False,\n-                        ),\n-                    },\n-                ),\n-            ]\n-        ]\n-\n-\n-class PythonListCustomToolGenerator(PromptTemplateGeneratorBase):  # noqa: N801\n-    DEFAULT_PROMPT = textwrap.dedent(\n-        \"\"\"\n-        You are an expert in composing functions. You are given a question and a set of possible functions.\n-        Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n-        If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n-        also point it out. You should only return the function call in tools call sections.\n-\n-        {{ function_description }}\n-        \"\"\".strip(\n-            \"\\n\"\n-        )\n-    )\n-\n-    def gen(\n-        self, custom_tools: List[ToolDefinition], system_prompt: Optional[str] = None\n-    ) -> PromptTemplate:\n-        system_prompt = system_prompt or self.DEFAULT_PROMPT\n-        return PromptTemplate(\n-            system_prompt,\n-            {\"function_description\": self._gen_function_description(custom_tools)},\n-        )\n-\n-    def _gen_function_description(\n-        self, custom_tools: List[ToolDefinition]\n-    ) -> PromptTemplate:\n-        template_str = textwrap.dedent(\n-            \"\"\"\n-            If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n-            You SHOULD NOT include any other text in the response.\n-\n-            Here is a list of functions in JSON format that you can invoke.\n-\n-            [\n-                {% for t in tools -%}\n-                {# manually setting up JSON because jinja sorts keys in unexpected ways -#}\n-                {%- set tname = t.tool_name -%}\n-                {%- set tdesc = t.description -%}\n-                {%- set tparams = t.parameters -%}\n-                {%- set required_params = [] -%}\n-                {%- for name, param in tparams.items() if param.required == true -%}\n-                    {%- set _ = required_params.append(name) -%}\n-                {%- endfor -%}\n-                {\n-                    \"name\": \"{{tname}}\",\n-                    \"description\": \"{{tdesc}}\",\n-                    \"parameters\": {\n-                        \"type\": \"dict\",\n-                        \"required\": {{ required_params | tojson }},\n-                        \"properties\": {\n-                            {%- for name, param in tparams.items() %}\n-                            \"{{name}}\": {\n-                                \"type\": \"{{param.param_type}}\",\n-                                \"description\": \"{{param.description}}\"{% if param.default %},\n-                                \"default\": \"{{param.default}}\"{% endif %}\n-                            }{% if not loop.last %},{% endif %}\n-                            {%- endfor %}\n-                        }\n-                    }\n-                }{% if not loop.last %},\n-                {% endif -%}\n-                {%- endfor %}\n-            ]\n-            \"\"\"\n-        )\n-        return PromptTemplate(\n-            template_str.strip(\"\\n\"),\n-            {\"tools\": [t.model_dump() for t in custom_tools]},\n-        ).render()\n-\n-    def data_examples(self) -> List[List[ToolDefinition]]:\n-        return [\n-            [\n-                ToolDefinition(\n-                    tool_name=\"get_weather\",\n-                    description=\"Get weather info for places\",\n-                    parameters={\n-                        \"city\": ToolParamDefinition(\n-                            param_type=\"string\",\n-                            description=\"The name of the city to get the weather for\",\n-                            required=True,\n-                        ),\n-                        \"metric\": ToolParamDefinition(\n-                            param_type=\"string\",\n-                            description=\"The metric for weather. Options are: celsius, fahrenheit\",\n-                            required=False,\n-                            default=\"celsius\",\n-                        ),\n-                    },\n-                ),\n-            ]\n-        ]\n\n--- File: models/llama3/prompt_templates/tool_response.py ---\n@@ -1,57 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import textwrap\n-from typing import Optional\n-\n-from .base import PromptTemplate, PromptTemplateGeneratorBase\n-\n-\n-class ToolResponseGenerator(PromptTemplateGeneratorBase):\n-    def gen(\n-        self,\n-        status: str,\n-        stdout: Optional[str] = None,\n-        stderr: Optional[str] = None,\n-    ):\n-        assert status in [\n-            \"success\",\n-            \"failure\",\n-        ], f\"status must be 'success' or 'failure'; Got: {status}\"\n-        template_str = textwrap.dedent(\n-            \"\"\"\n-            {% if status == \"success\" %}completed{% else %}failed{% endif %}\n-            {%- if stdout %}\n-            [stdout]{{ stdout }}[/stdout]\n-            {%- endif -%}\n-            {%- if stderr %}\n-            [stderr]{{ stderr }}[/stderr]\n-            {%- endif -%}\n-            \"\"\"\n-        )\n-        return PromptTemplate(\n-            template_str.lstrip(\"\\n\"),\n-            {\n-                \"status\": status,\n-                \"stdout\": stdout,\n-                \"stderr\": stderr,\n-            },\n-        )\n-\n-    def data_examples(self):\n-        return [\n-            # success\n-            {\n-                \"status\": \"success\",\n-                \"stdout\": '{\"results\":[\"something something\"]}',\n-            },\n-            # failure\n-            {\n-                \"status\": \"failure\",\n-                \"stderr\": \"brave_search encounter an error: could not communicate with api.brave.com\",\n-            },\n-        ]\n\n--- File: models/llama3/reference_impl/generation.py ---\n@@ -31,9 +31,10 @@\n )\n from termcolor import cprint\n \n+from ...datatypes import RawContent, RawMessage, StopReason, ToolPromptFormat\n+\n from ..api.args import ModelArgs\n from ..api.chat_format import ChatFormat, LLMInput\n-from ..api.datatypes import RawContent, RawMessage, StopReason, ToolPromptFormat\n from ..api.tokenizer import Tokenizer\n from .model import Transformer\n \n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -13,7 +13,7 @@\n import numpy as np\n import pytest\n import torch\n-from llama_models.llama3.api.datatypes import RawMediaItem, RawMessage, RawTextItem\n+from llama_models.datatypes import RawMediaItem, RawMessage, RawTextItem\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n\n--- File: models/llama3/tests/api/test_tokenizer.py ---\n@@ -8,20 +8,17 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n-import os\n from unittest import TestCase\n \n-from .chat_format import ChatFormat\n-from .datatypes import RawMessage, ToolPromptFormat\n-from .tokenizer import Tokenizer\n+from llama_models.datatypes import RawMessage, ToolPromptFormat\n \n-\n-# TOKENIZER_PATH=<tokenizer_path> python -m unittest models/llama3/api/test_tokenizer.py\n+from llama_models.llama3.api.chat_format import ChatFormat\n+from llama_models.llama3.api.tokenizer import Tokenizer\n \n \n class TokenizerTests(TestCase):\n     def setUp(self):\n-        self.tokenizer = Tokenizer(os.environ[\"TOKENIZER_PATH\"])\n+        self.tokenizer = Tokenizer.get_instance()\n         self.format = ChatFormat(self.tokenizer)\n \n     def test_special_tokens(self):\n\n--- File: models/llama3/tests/prompt_templates/test_system_prompts.py ---\n@@ -1,193 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import textwrap\n-import unittest\n-from datetime import datetime\n-\n-from llama_models.llama3.prompt_templates import (\n-    BuiltinToolGenerator,\n-    FunctionTagCustomToolGenerator,\n-    JsonCustomToolGenerator,\n-    PythonListCustomToolGenerator,\n-    SystemDefaultGenerator,\n-)\n-\n-\n-class PromptTemplateTests(unittest.TestCase):\n-    def check_generator_output(self, generator, expected_text):\n-        example = generator.data_examples()[0]\n-\n-        pt = generator.gen(example)\n-        text = pt.render()\n-        # print(text)  # debugging\n-        assert text == expected_text, f\"Expected:\\n{expected_text}\\nActual:\\n{text}\"\n-\n-    def test_system_default(self):\n-        generator = SystemDefaultGenerator()\n-        today = datetime.now().strftime(\"%d %B %Y\")\n-        expected_text = f\"Cutting Knowledge Date: December 2023\\nToday Date: {today}\"\n-        self.check_generator_output(generator, expected_text)\n-\n-    def test_system_builtin_only(self):\n-        generator = BuiltinToolGenerator()\n-        expected_text = textwrap.dedent(\n-            \"\"\"\n-            Environment: ipython\n-            Tools: brave_search, wolfram_alpha\n-            \"\"\"\n-        )\n-        self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n-\n-    def test_system_custom_only(self):\n-        self.maxDiff = None\n-        generator = JsonCustomToolGenerator()\n-        expected_text = textwrap.dedent(\n-            \"\"\"\n-            Answer the user's question by making use of the following functions if needed.\n-            If none of the function can be used, please say so.\n-            Here is a list of functions in JSON format:\n-            {\n-                \"type\": \"function\",\n-                \"function\": {\n-                    \"name\": \"trending_songs\",\n-                    \"description\": \"Returns the trending songs on a Music site\",\n-                    \"parameters\": {\n-                        \"type\": \"object\",\n-                        \"properties\": [\n-                            {\n-                                \"n\": {\n-                                    \"type\": \"object\",\n-                                    \"description\": \"The number of songs to return\"\n-                                }\n-                            },\n-                            {\n-                                \"genre\": {\n-                                    \"type\": \"object\",\n-                                    \"description\": \"The genre of the songs to return\"\n-                                }\n-                            }\n-                        ],\n-                        \"required\": [\"n\"]\n-                    }\n-                }\n-            }\n-\n-            Return function calls in JSON format.\n-            \"\"\"\n-        )\n-        self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n-\n-    def test_system_custom_function_tag(self):\n-        self.maxDiff = None\n-        generator = FunctionTagCustomToolGenerator()\n-        expected_text = textwrap.dedent(\n-            \"\"\"\n-            You have access to the following functions:\n-\n-            Use the function 'trending_songs' to 'Returns the trending songs on a Music site':\n-            {\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n-\n-            Think very carefully before calling functions.\n-            If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n-\n-            <function=example_function_name>{\"example_name\": \"example_value\"}</function>\n-\n-            Reminder:\n-            - If looking for real time information use relevant functions before falling back to brave_search\n-            - Function calls MUST follow the specified format, start with <function= and end with </function>\n-            - Required parameters MUST be specified\n-            - Only call one function at a time\n-            - Put the entire function call reply on one line\n-            \"\"\"\n-        )\n-        self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n-\n-    def test_llama_3_2_system_zero_shot(self):\n-        generator = PythonListCustomToolGenerator()\n-        expected_text = textwrap.dedent(\n-            \"\"\"\n-            You are an expert in composing functions. You are given a question and a set of possible functions.\n-            Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n-            If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n-            also point it out. You should only return the function call in tools call sections.\n-\n-            If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n-            You SHOULD NOT include any other text in the response.\n-\n-            Here is a list of functions in JSON format that you can invoke.\n-\n-            [\n-                {\n-                    \"name\": \"get_weather\",\n-                    \"description\": \"Get weather info for places\",\n-                    \"parameters\": {\n-                        \"type\": \"dict\",\n-                        \"required\": [\"city\"],\n-                        \"properties\": {\n-                            \"city\": {\n-                                \"type\": \"string\",\n-                                \"description\": \"The name of the city to get the weather for\"\n-                            },\n-                            \"metric\": {\n-                                \"type\": \"string\",\n-                                \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n-                                \"default\": \"celsius\"\n-                            }\n-                        }\n-                    }\n-                }\n-            ]\n-            \"\"\"\n-        )\n-        self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n-\n-    def test_llama_3_2_provided_system_prompt(self):\n-        generator = PythonListCustomToolGenerator()\n-        expected_text = textwrap.dedent(\n-            \"\"\"\n-            Overriding message.\n-\n-            If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n-            You SHOULD NOT include any other text in the response.\n-\n-            Here is a list of functions in JSON format that you can invoke.\n-\n-            [\n-                {\n-                    \"name\": \"get_weather\",\n-                    \"description\": \"Get weather info for places\",\n-                    \"parameters\": {\n-                        \"type\": \"dict\",\n-                        \"required\": [\"city\"],\n-                        \"properties\": {\n-                            \"city\": {\n-                                \"type\": \"string\",\n-                                \"description\": \"The name of the city to get the weather for\"\n-                            },\n-                            \"metric\": {\n-                                \"type\": \"string\",\n-                                \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n-                                \"default\": \"celsius\"\n-                            }\n-                        }\n-                    }\n-                }\n-            ]\"\"\"\n-        )\n-        user_system_prompt = textwrap.dedent(\n-            \"\"\"\n-            Overriding message.\n-            \n-            {{ function_description }}\n-            \"\"\"\n-        )\n-        example = generator.data_examples()[0]\n-\n-        pt = generator.gen(example, user_system_prompt)\n-        text = pt.render()\n-        assert text == expected_text, f\"Expected:\\n{expected_text}\\nActual:\\n{text}\"\n\n--- File: models/llama3_1/__init__.py ---\n@@ -1,6 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n\n--- File: models/llama3_1/prompts.py ---\n@@ -1,252 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import textwrap\n-from typing import List\n-\n-from ..llama3.api.datatypes import (\n-    BuiltinTool,\n-    RawMessage,\n-    StopReason,\n-    ToolCall,\n-    ToolPromptFormat,\n-)\n-from ..prompt_format import (\n-    llama3_1_builtin_tool_call_dialog,\n-    llama3_1_custom_tool_call_dialog,\n-    # llama3_1_e2e_tool_call_dialog,\n-    TextCompletionContent,\n-    UseCase,\n-)\n-\n-\n-def wolfram_alpha_response():\n-    return textwrap.dedent(\n-        \"\"\"\n-        {\n-            \"queryresult\": {\n-                \"success\": true,\n-                \"inputstring\": \"100th decimal of pi\",\n-                \"pods\": [\n-                    {\n-                        \"title\": \"Input interpretation\",\n-                        \"subpods\": [\n-                            {\n-                                \"title\": \"\",\n-                                \"plaintext\": \"100th digit | \\u03c0\"\n-                            }\n-                        ]\n-                    },\n-                    {\n-                        \"title\": \"Nearby digits\",\n-                        \"subpods\": [\n-                            {\n-                                \"title\": \"\",\n-                                \"plaintext\": \"...86208998628034825342117067982148086513282306647093...\"\n-                            }\n-                        ]\n-                    },\n-                    {\n-                        \"title\": \"Result\",\n-                        \"primary\": true,\n-                        \"subpods\": [\n-                            {\n-                                \"title\": \"\",\n-                                \"plaintext\": \"7\"\n-                            }\n-                        ]\n-                    }\n-                ]\n-            }\n-        }\n-        \"\"\"\n-    )\n-\n-\n-def usecases() -> List[UseCase | str]:\n-    return [\n-        textwrap.dedent(\n-            \"\"\"\n-            # Llama 3.1 - Prompt Formats\n-            ## Tokens\n-            Here is a list of special tokens that are supported by Llama 3.1:\n-            - `<|begin_of_text|>`: Specifies the start of the prompt\n-            - `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n-            - `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n-            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and tool]\n-            - `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n-            - `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n-                - at the end of a direct interaction between the model and the user\n-                - at the end of multiple interactions between the model and any available tools\n-                This token signals to the executor that the model has finished generating a response.\n-            - `<|python_tag|>`: Is a special tag used in the model's response to signify a tool call.\n-            \"\"\"\n-        ),\n-        textwrap.dedent(\n-            \"\"\"\n-            There are 4 different roles that are supported by Llama 3.1\n-            - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n-            - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n-            - `tool`: A new role introduced in Llama 3.1. This role is used to mark messages with the output of a tool call when sent back to the model from the executor. (The actual token used by the model for this role is \"ipython\".)\n-            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `tool` and `user` prompts.\n-            \"\"\"\n-        ),\n-        UseCase(\n-            title=\"Llama 3.1 Base Model\",\n-            description=\"Text completion for Llama 3.1 base model uses this format.\",\n-            dialogs=[TextCompletionContent(content=\"Color of sky is blue but sometimes can also be\")],\n-            notes=\"Note start special tag\",\n-        ),\n-        \"## Llama 3.1 Instruct Model\",\n-        UseCase(\n-            title=\"User and assistant conversation\",\n-            description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n-            dialogs=[\n-                [\n-                    RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n-                    RawMessage(\n-                        role=\"user\",\n-                        content=\"Answer who are you in the form of jeopardy?\",\n-                    ),\n-                ]\n-            ],\n-            notes=\"\",\n-        ),\n-        \"## Tool Calling Formats\",\n-        textwrap.dedent(\n-            \"\"\"\n-            The three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:\n-            - Brave Search: Tool call to perform web searches.\n-            - Wolfram Alpha: Tool call to perform complex mathematical calculations.\n-            - Code Interpreter: Enables the model to output python code.\n-            \"\"\"\n-        ),\n-        UseCase(\n-            title=\"Builtin Tool Calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Here is an example of a conversation using brave search\n-                \"\"\"\n-            ),\n-            dialogs=[llama3_1_builtin_tool_call_dialog()],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Just including Environment: ipython turns on code interpreter; therefore, you don't need to specify code interpretation on the Tools: line. The model can generate python code which is interpreted by the executor, with the result provided back to the model.\n-                - The message body of the assistant response starts with a special tag <|python_tag|>\n-                - As alluded to above, in such an environment, the model can generate <|eom_id|> instead of just the standard <|eot_id|> . The latter indicates the turn is finished, while the former indicates continued multi-step reasoning. That is, the model is expecting a continuation message with the output of the tool call.\n-                - The model tool call response is of the form `tool.call(query=\"...\")` wher tool is `brave_search` or `wolfram_alpha`\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Builtin Code Interpreter\",\n-            description=\"Here is an actual example of model responding with code\",\n-            dialogs=[\n-                [\n-                    RawMessage(role=\"system\", content=\"Environment: ipython\"),\n-                    RawMessage(\n-                        role=\"user\",\n-                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\",\n-                    ),\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Model starts with <|python_tag|> and continues writing python code that it needs to be executed\n-                - No explicit mention of code_interpreter in system prompt. `Environment: ipython` implicitly enables it.\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Built-in tools full interaction\",\n-            description=\"Here is a full interaction with the built-in tools including the tool response and the final assistant response.\",\n-            dialogs=[\n-                [\n-                    RawMessage(\n-                        role=\"system\",\n-                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\",\n-                    ),\n-                    RawMessage(role=\"user\", content=\"What is the 100th decimal of pi?\"),\n-                    RawMessage(\n-                        role=\"assistant\",\n-                        content=\"\",\n-                        stop_reason=StopReason.end_of_message,\n-                        tool_calls=[\n-                            ToolCall(\n-                                call_id=\"tool_call_id\",\n-                                tool_name=BuiltinTool.wolfram_alpha,\n-                                arguments={\"query\": \"100th decimal of pi\"},\n-                            )\n-                        ],\n-                    ),\n-                    RawMessage(\n-                        role=\"tool\",\n-                        content=wolfram_alpha_response(),\n-                    ),\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Note the `<|python_tag|>` in the assistant response.\n-                - Role is `tool` for the wolfram alpha response that is passed back to the model.\n-                - Final message from assistant has <|eot_id|> tag.\n-                \"\"\"\n-            ),\n-        ),\n-        \"## Zero shot tool calling\",\n-        UseCase(\n-            title=\"JSON based tool calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Llama models can now output custom tool calls from a single message to allow easier tool calling.\n-                The following prompts provide an example of how custom tools can be called from the output of the model.\n-                It's important to note that the model itself does not execute the calls; it provides structured output to facilitate calling by an executor.\n-                \"\"\"\n-            ),\n-            dialogs=[llama3_1_custom_tool_call_dialog()],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - JSON format for providing tools needs name, description and parameters\n-                - Model responds with `<|python_tag|>` and `<|eom_id|>` as `Environment: ipython` was in the system prompt\n-                - Instructions for tools added as a user message\n-                - Only single tool calls are supported as of now\n-                \"\"\"\n-            ),\n-        ),\n-        # FIXME: This is not working yet as expected\n-        # UseCase(\n-        #     title=\"E2E tool call example\",\n-        #     description=textwrap.dedent(\n-        #         \"\"\"\n-        #         Here is an example showing the whole multi-step turn by taking custom tool outputs and passing back to the model.\n-        #         \"\"\"\n-        #     ),\n-        #     dialogs=[\n-        #         llama3_1_e2e_tool_call_dialog(\n-        #             tool_prompt_format=ToolPromptFormat.function_tag\n-        #         )\n-        #     ],\n-        #     notes=\"\",\n-        # ),\n-        \"## Example of a user defined tool calling\",\n-        UseCase(\n-            title=\"`<function>` based tool calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Here is an example of how you could also write custom instructions for model to do zero shot tool calling.\n-                In this example, we define a custom tool calling format using the `<function>` tag.\n-                \"\"\"\n-            ),\n-            dialogs=[llama3_1_custom_tool_call_dialog(ToolPromptFormat.function_tag)],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - In this case, model does NOT respond with `<|python_tag|>` and ends with `<|eot_id|>`\n-                - Instructions for tools added as a user message\n-                \"\"\"\n-            ),\n-        ),\n-    ]\n\n--- File: models/llama3_2/__init__.py ---\n@@ -1,6 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n\n--- File: models/llama3_2/prompts_text.py ---\n@@ -1,223 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-import json\n-import textwrap\n-\n-from ..llama3.api.datatypes import RawMessage, StopReason, ToolCall, ToolPromptFormat\n-from ..prompt_format import (\n-    llama3_1_builtin_code_interpreter_dialog,\n-    TextCompletionContent,\n-    UseCase,\n-)\n-\n-\n-def user_tool_call():\n-    content = textwrap.dedent(\n-        \"\"\"\n-        Questions: Can you retrieve the details for the user with the ID 7890, who has black as their special request?\n-        Here is a list of functions in JSON format that you can invoke:\n-        [\n-            {\n-                \"name\": \"get_user_info\",\n-                \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n-                \"parameters\": {\n-                    \"type\": \"dict\",\n-                    \"required\": [\n-                        \"user_id\"\n-                    ],\n-                    \"properties\": {\n-                        \"user_id\": {\n-                        \"type\": \"integer\",\n-                        \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n-                    },\n-                    \"special\": {\n-                        \"type\": \"string\",\n-                        \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n-                        \"default\": \"none\"\n-                        }\n-                    }\n-                }\n-            }\n-        ]\n-\n-        Should you decide to return the function call(s),Put it in the format of [func1(params_name=params_value, params_name2=params_value2...), func2(params)]\n-\n-        NO other text MUST be included.\n-        \"\"\"\n-    )\n-    return content.strip()\n-\n-\n-def system_tool_call():\n-    content = textwrap.dedent(\n-        \"\"\"\n-        You are an expert in composing functions. You are given a question and a set of possible functions.\n-        Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n-        If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n-        also point it out. You should only return the function call in tools call sections.\n-\n-        If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n-        You SHOULD NOT include any other text in the response.\n-\n-        Here is a list of functions in JSON format that you can invoke.\n-\n-        [\n-            {\n-                \"name\": \"get_weather\",\n-                \"description\": \"Get weather info for places\",\n-                \"parameters\": {\n-                    \"type\": \"dict\",\n-                    \"required\": [\n-                        \"city\"\n-                    ],\n-                    \"properties\": {\n-                        \"city\": {\n-                            \"type\": \"string\",\n-                            \"description\": \"The name of the city to get the weather for\"\n-                        },\n-                        \"metric\": {\n-                            \"type\": \"string\",\n-                            \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n-                            \"default\": \"celsius\"\n-                        }\n-                    }\n-                }\n-            }\n-        ]\n-        \"\"\"\n-    )\n-    return content.strip()\n-\n-\n-def usecases():\n-    return [\n-        UseCase(\n-            title=\"User and assistant conversation\",\n-            description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n-            dialogs=[\n-                [\n-                    RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n-                    RawMessage(role=\"user\", content=\"Who are you?\"),\n-                ]\n-            ],\n-            notes=\"This format is unchanged from Llama3.1\",\n-        ),\n-        UseCase(\n-            title=\"Zero shot function calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                For Llama3.2 1B and 3B instruct models, we are introducing a new format for zero shot function calling.\n-                This new format is designed to be more flexible and powerful than the previous format.\n-                All available functions can be provided in the system message. A key difference is in the format of how the assistant responds with function calls.\n-                It is pythonic in the form of `[func1(params_name=params_value, params_name2=params_value2...), func2(params)]` instead of the `json` or `<function>` tag that were defined in Llama3.1.\n-                Here is an example for the same,\n-                \"\"\"\n-            ),\n-            dialogs=[\n-                # Zero shot tool calls as system message\n-                [\n-                    RawMessage(role=\"system\", content=system_tool_call()),\n-                    RawMessage(role=\"user\", content=\"What is the weather in SF and Seattle?\"),\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - The output supports multiple tool calls natively\n-                - JSON format for defining the functions in the system prompt is similar to Llama3.1\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Zero shot function calling with user message\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                While the default is to provide all function calls in a system message, in Llama3.2 text models you can also provide information for all the available tools in a user message.\n-                \"\"\"\n-            ),\n-            dialogs=[\n-                # Zero shot tool call as user message\n-                [\n-                    RawMessage(role=\"user\", content=user_tool_call()),\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - The tool call format for the model is the same whether your function calls are provided in the system or user message.\n-                - While builtin tool calls end with a <|eom_id|>, notice the <|eot_id|> for zero shot tool calls.\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Code Interpreter\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Code Interpreter continues to work in 3.2 text models similar to Llama 3.1 model family.\n-                Here is an example,\n-                \"\"\"\n-            ),\n-            dialogs=[llama3_1_builtin_code_interpreter_dialog()],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Note `Environment: ipython` in the system prompt.\n-                - Note that the response starts with `<|python_tag|>` and ends with `<|eom_id|>`\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Zero shot function calling E2E format\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Here is an example of the e2e cycle of tool calls with the model in a muti-step way.\n-                \"\"\"\n-            ),\n-            dialogs=[\n-                [\n-                    RawMessage(role=\"system\", content=system_tool_call()),\n-                    RawMessage(role=\"user\", content=\"What is the weather in SF?\"),\n-                    RawMessage(\n-                        role=\"assistant\",\n-                        content=\"\",\n-                        stop_reason=StopReason.end_of_turn,\n-                        tool_calls=[\n-                            ToolCall(\n-                                call_id=\"cc\",\n-                                tool_name=\"get_weather\",\n-                                arguments={\n-                                    \"city\": \"San Francisco\",\n-                                    \"metric\": \"celsius\",\n-                                },\n-                            )\n-                        ],\n-                    ),\n-                    RawMessage(\n-                        role=\"tool\",\n-                        content=json.dumps(\"25 C\"),\n-                    ),\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - The output of the function call is provided back to the model as a tool response ( in json format ).\n-                - Notice `<|start_header_id|>ipython<|end_header_id|>` as the header message preceding the tool response.\n-                - The model finally summarizes the information from the tool response and returns the result to the user.\n-                \"\"\"\n-            ),\n-            tool_prompt_format=ToolPromptFormat.python_list,\n-        ),\n-        UseCase(\n-            title=\"Prompt format for base models\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                For base models (Llama3.2-1B and Llama3.2-3B), the prompt format for a simple completion is as follows\n-                \"\"\"\n-            ),\n-            dialogs=[\n-                TextCompletionContent(content=\"The color of the sky is blue but sometimes it can also be\"),\n-            ],\n-            notes=\"Same as Llama3.1\",\n-        ),\n-    ]\n\n--- File: models/llama3_2/prompts_vision.py ---\n@@ -1,122 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import textwrap\n-from pathlib import Path\n-\n-from ..llama3.api.datatypes import RawMediaItem, RawMessage, RawTextItem\n-from ..prompt_format import (\n-    llama3_1_builtin_tool_call_dialog,\n-    # llama3_1_builtin_tool_call_with_image_dialog,\n-    llama3_2_user_assistant_conversation,\n-    TextCompletionContent,\n-    UseCase,\n-)\n-\n-\n-def usecases():\n-    this_dir = Path(__file__).parent.parent.resolve()\n-    with open(this_dir / \"scripts/resources/dog.jpg\", \"rb\") as f:\n-        img = f.read()\n-\n-    return [\n-        llama3_2_user_assistant_conversation(),\n-        UseCase(\n-            title=\"User and assistant conversation with Images\",\n-            description=\"This example shows how to pass and image to the model as part of the messages.\",\n-            dialogs=[\n-                [\n-                    RawMessage(\n-                        role=\"user\",\n-                        content=[\n-                            RawMediaItem(data=img),\n-                            RawTextItem(text=\"Describe this image in two sentences\"),\n-                        ],\n-                    )\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - The `<|image|>` tag is used to indicate presence of the image\n-                - The model isn't an early fusion model so doesn't actually translate an image into several tokens. Instead the cross-attention layers take input \"on the side\" from a vision encoder\n-                ![Image](mm-model.png)\n-                - Its important to postion the <|image|> tag appropriately in the prompt. Image will only attend to the subsequent text tokens\n-                - The <|image|> tag is part of the user message body, implying that it should only come after the header `<|start_header_id|>{role}<|end_header_id|>` in the message body\n-                - We recommend using a single image in one prompt\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Builtin and Zero Shot Tool Calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Llama3.2 vision models follow the same tool calling format as Llama3.1 models when inputs are text only.\n-                Use `Environment: ipython` to enable tools.\n-                Add `Tools: {{tool_name1}},{{tool_name2}}` for each of the builtin tools.\n-                The same builtin tools as Llama3.1 are available,\n-                - code_interpreter (for executing python code)\n-                - brave_search (to search the web)\n-                - wolfram_alpha (for querying wolfram alpha for mathematical questions)\n-                \"\"\",\n-            ),\n-            dialogs=[llama3_1_builtin_tool_call_dialog()],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Note the `<|python_tag|>` before `brave_search` function call.\n-                - The `<|eom_id|>` tag is used to indicate the end of the message.\n-                - Similar to Llama3.1, code_interpreter is not explicitly mentioned but is enabled via `Environment: ipython`.\n-                - Tool Calling does NOT work with images in the prompt as of now.\n-                \"\"\"\n-            ),\n-        ),\n-        # UseCase(\n-        #     title=\"Tool Calling for vision models\",\n-        #     description=textwrap.dedent(\n-        #         \"\"\"\n-        #         While Llama3.2 vision models follow the same tool calling format as Llama3.1 models when inputs are text only,\n-        #         they are not able to do tool calling when prompt contains image inputs (along with text).\n-        #         The recommended way would be to separate out the image understanding from the tool calling in successive prompts.\n-        #         Here is an example of how that could be done,\n-        #         \"\"\",\n-        #     ),\n-        #     dialogs=[llama3_1_builtin_tool_call_with_image_dialog()],\n-        #     notes=textwrap.dedent(\n-        #         \"\"\"\n-        #         - Instead of a single prompt (image understanding + tool call), we split into two prompts to achieve the same result.\n-        #         \"\"\"\n-        #     ),\n-        # ),\n-        UseCase(\n-            title=\"Prompt format for base models\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                For base models (Llama3.2-11B-Vision and Llama3.2-90B-Vision), the prompt format for a simple completion is as follows\n-                \"\"\"\n-            ),\n-            dialogs=[\n-                TextCompletionContent(content=\"The color of the sky is blue but sometimes it can also be\"),\n-            ],\n-            notes=\"- Same as Llama3.1\",\n-        ),\n-        UseCase(\n-            title=\"Prompt format for base models with Image\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                For base models (Llama3.2-11B-Vision and Llama3.2-90B-Vision), here is an example of how the text completion format looks with an image,\n-                \"\"\"\n-            ),\n-            dialogs=[\n-                TextCompletionContent(\n-                    content=[\n-                        RawMediaItem(data=img),\n-                        RawTextItem(text=\"If I had to write a haiku for this one\"),\n-                    ]\n-                ),\n-            ],\n-            notes=\"- Note the placement of the special tags <|begin_of_text|> and <|image|>\",\n-        ),\n-    ]\n\n--- File: models/llama3_3/prompts.py ---\n@@ -1,251 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import textwrap\n-from typing import List\n-\n-from ..llama3.api.datatypes import (\n-    BuiltinTool,\n-    RawMessage,\n-    StopReason,\n-    ToolCall,\n-    ToolPromptFormat,\n-)\n-from ..prompt_format import (\n-    llama3_1_builtin_tool_call_dialog,\n-    llama3_1_custom_tool_call_dialog,\n-    # llama3_1_e2e_tool_call_dialog,\n-    TextCompletionContent,\n-    UseCase,\n-)\n-\n-\n-def wolfram_alpha_response():\n-    return textwrap.dedent(\n-        \"\"\"\n-        {\n-            \"queryresult\": {\n-                \"success\": true,\n-                \"inputstring\": \"100th decimal of pi\",\n-                \"pods\": [\n-                    {\n-                        \"title\": \"Input interpretation\",\n-                        \"subpods\": [\n-                            {\n-                                \"title\": \"\",\n-                                \"plaintext\": \"100th digit | \\u03c0\"\n-                            }\n-                        ]\n-                    },\n-                    {\n-                        \"title\": \"Nearby digits\",\n-                        \"subpods\": [\n-                            {\n-                                \"title\": \"\",\n-                                \"plaintext\": \"...86208998628034825342117067982148086513282306647093...\"\n-                            }\n-                        ]\n-                    },\n-                    {\n-                        \"title\": \"Result\",\n-                        \"primary\": true,\n-                        \"subpods\": [\n-                            {\n-                                \"title\": \"\",\n-                                \"plaintext\": \"7\"\n-                            }\n-                        ]\n-                    }\n-                ]\n-            }\n-        }\n-        \"\"\"\n-    )\n-\n-\n-def usecases() -> List[UseCase | str]:\n-    return [\n-        textwrap.dedent(\n-            \"\"\"\n-            # Llama 3.1 - Prompt Formats\n-            ## Tokens\n-            Here is a list of special tokens that are supported by Llama 3.1:\n-            - `<|begin_of_text|>`: Specifies the start of the prompt\n-            - `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n-            - `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n-            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and tool]\n-            - `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n-            - `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n-                - at the end of a direct interaction between the model and the user\n-                - at the end of multiple interactions between the model and any available tools\n-                This token signals to the executor that the model has finished generating a response.\n-            - `<|python_tag|>`: Is a special tag used in the model's response to signify a tool call.\n-            \"\"\"\n-        ),\n-        textwrap.dedent(\n-            \"\"\"\n-            There are 4 different roles that are supported by Llama 3.1\n-            - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n-            - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n-            - `tool`: A new role introduced in Llama 3.1. This role is used to mark messages with the output of a tool call when sent back to the model from the executor. (The actual token used by the model for this role is \"ipython\".)\n-            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `tool` and `user` prompts.\n-            \"\"\"\n-        ),\n-        UseCase(\n-            title=\"Llama 3.1 Base Model\",\n-            description=\"Text completion for Llama 3.1 base model uses this format.\",\n-            dialogs=[TextCompletionContent(content=\"Color of sky is blue but sometimes can also be\")],\n-            notes=\"Note start special tag\",\n-        ),\n-        \"## Llama 3.1 Instruct Model\",\n-        UseCase(\n-            title=\"User and assistant conversation\",\n-            description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n-            dialogs=[\n-                [\n-                    RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n-                    RawMessage(\n-                        role=\"user\",\n-                        content=\"Answer who are you in the form of jeopardy?\",\n-                    ),\n-                ]\n-            ],\n-            notes=\"\",\n-        ),\n-        \"## Tool Calling Formats\",\n-        textwrap.dedent(\n-            \"\"\"\n-            The three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:\n-            - Brave Search: Tool call to perform web searches.\n-            - Wolfram Alpha: Tool call to perform complex mathematical calculations.\n-            - Code Interpreter: Enables the model to output python code.\n-            \"\"\"\n-        ),\n-        UseCase(\n-            title=\"Builtin Tool Calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Here is an example of a conversation using brave search\n-                \"\"\"\n-            ),\n-            dialogs=[llama3_1_builtin_tool_call_dialog()],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Just including Environment: ipython turns on code interpreter; therefore, you don't need to specify code interpretation on the Tools: line. The model can generate python code which is interpreted by the executor, with the result provided back to the model.\n-                - The message body of the assistant response starts with a special tag <|python_tag|>\n-                - As alluded to above, in such an environment, the model can generate <|eom_id|> instead of just the standard <|eot_id|> . The latter indicates the turn is finished, while the former indicates continued multi-step reasoning. That is, the model is expecting a continuation message with the output of the tool call.\n-                - The model tool call response is of the form `tool.call(query=\"...\")` wher tool is `brave_search` or `wolfram_alpha`\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Builtin Code Interpreter\",\n-            description=\"Here is an actual example of model responding with code\",\n-            dialogs=[\n-                [\n-                    RawMessage(role=\"system\", content=\"Environment: ipython\"),\n-                    RawMessage(\n-                        role=\"user\",\n-                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\",\n-                    ),\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Model starts with <|python_tag|> and continues writing python code that it needs to be executed\n-                - No explicit mention of code_interpreter in system prompt. `Environment: ipython` implicitly enables it.\n-                \"\"\"\n-            ),\n-        ),\n-        UseCase(\n-            title=\"Built-in tools full interaction\",\n-            description=\"Here is a full interaction with the built-in tools including the tool response and the final assistant response.\",\n-            dialogs=[\n-                [\n-                    RawMessage(\n-                        role=\"system\",\n-                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\",\n-                    ),\n-                    RawMessage(role=\"user\", content=\"What is the 100th decimal of pi?\"),\n-                    RawMessage(\n-                        content=\"\",\n-                        stop_reason=StopReason.end_of_message,\n-                        tool_calls=[\n-                            ToolCall(\n-                                call_id=\"tool_call_id\",\n-                                tool_name=BuiltinTool.wolfram_alpha,\n-                                arguments={\"query\": \"100th decimal of pi\"},\n-                            )\n-                        ],\n-                    ),\n-                    RawMessage(\n-                        role=\"tool\",\n-                        content=wolfram_alpha_response(),\n-                    ),\n-                ],\n-            ],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - Note the `<|python_tag|>` in the assistant response.\n-                - Role is `tool` for the wolfram alpha response that is passed back to the model.\n-                - Final message from assistant has <|eot_id|> tag.\n-                \"\"\"\n-            ),\n-        ),\n-        \"## Zero shot tool calling\",\n-        UseCase(\n-            title=\"JSON based tool calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Llama models can now output custom tool calls from a single message to allow easier tool calling.\n-                The following prompts provide an example of how custom tools can be called from the output of the model.\n-                It's important to note that the model itself does not execute the calls; it provides structured output to facilitate calling by an executor.\n-                \"\"\"\n-            ),\n-            dialogs=[llama3_1_custom_tool_call_dialog()],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - JSON format for providing tools needs name, description and parameters\n-                - Model responds with `<|python_tag|>` and `<|eom_id|>` as `Environment: ipython` was in the system prompt\n-                - Instructions for tools added as a user message\n-                - Only single tool calls are supported as of now\n-                \"\"\"\n-            ),\n-        ),\n-        # FIXME: This is not working yet as expected\n-        # UseCase(\n-        #     title=\"E2E tool call example\",\n-        #     description=textwrap.dedent(\n-        #         \"\"\"\n-        #         Here is an example showing the whole multi-step turn by taking custom tool outputs and passing back to the model.\n-        #         \"\"\"\n-        #     ),\n-        #     dialogs=[\n-        #         llama3_1_e2e_tool_call_dialog(\n-        #             tool_prompt_format=ToolPromptFormat.function_tag\n-        #         )\n-        #     ],\n-        #     notes=\"\",\n-        # ),\n-        \"## Example of a user defined tool calling\",\n-        UseCase(\n-            title=\"`<function>` based tool calling\",\n-            description=textwrap.dedent(\n-                \"\"\"\n-                Here is an example of how you could also write custom instructions for model to do zero shot tool calling.\n-                In this example, we define a custom tool calling format using the `<function>` tag.\n-                \"\"\"\n-            ),\n-            dialogs=[llama3_1_custom_tool_call_dialog(ToolPromptFormat.function_tag)],\n-            notes=textwrap.dedent(\n-                \"\"\"\n-                - In this case, model does NOT respond with `<|python_tag|>` and ends with `<|eot_id|>`\n-                - Instructions for tools added as a user message\n-                \"\"\"\n-            ),\n-        ),\n-    ]\n\n--- File: models/prompt_format.py ---\n@@ -1,206 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import textwrap\n-\n-from pathlib import Path\n-from typing import List\n-\n-from llama_models.llama3.api.interface import LLama31Interface\n-\n-from pydantic import BaseModel, Field\n-\n-from .llama3.api.datatypes import (\n-    RawContent,\n-    RawMediaItem,\n-    RawMessage,\n-    RawTextItem,\n-    StopReason,\n-    ToolCall,\n-    ToolPromptFormat,\n-)\n-\n-\n-class TextCompletionContent(BaseModel):\n-    content: RawContent = \"\"\n-\n-\n-class UseCase(BaseModel):\n-    title: str = \"\"\n-    description: str = \"\"\n-    dialogs: List[List[RawMessage] | TextCompletionContent | str] = Field(default_factory=list)\n-    notes: str = \"\"\n-    tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json\n-\n-    def md_format(self):\n-        section = textwrap.dedent(\n-            \"\"\"\n-            ## {title}\n-\n-            {description}\n-\n-            {dialogs_text}\n-            {notes}\n-\n-            \"\"\"\n-        )\n-        return section.lstrip()\n-\n-    def dialogs_to_text(self, generator) -> str:\n-        def _code_block(text):\n-            return f\"```\\n{text}\\n```\"\n-\n-        text = \"\"\n-        for dialog in self.dialogs:\n-            if isinstance(dialog, str):\n-                text += dialog\n-                text += \"\\n\\n\"\n-                continue\n-\n-            elif isinstance(dialog, TextCompletionContent):\n-                input_tokens, output_tokens = generator.text_completion_raw(\n-                    dialog.content,\n-                    max_gen_len=64,\n-                    temperature=0.1,\n-                    top_p=0.95,\n-                )\n-            else:\n-                input_tokens, output_tokens = generator.chat_completion_raw(\n-                    dialog,\n-                    max_gen_len=512,\n-                    temperature=0.0,\n-                    top_p=0.95,\n-                    tool_prompt_format=self.tool_prompt_format,\n-                )\n-            text += \"##### Input Prompt Format\\n\"\n-\n-            # FIXME: This is added to undo the hack in chat_formatter where\n-            # vision tokens are replaced with 128256.\n-            input_tokens = [generator.formatter.vision_token if t == 128256 else t for t in input_tokens]\n-\n-            text += _code_block(generator.tokenizer.decode(input_tokens))\n-            # TODO: Figure out if \"\" needs to be added for newlines or end or some indication\n-            text += \"\\n\\n\"\n-            text += \"##### Model Response Format\\n\"\n-            text += _code_block(generator.tokenizer.decode(output_tokens))\n-            text += \"\\n\\n\"\n-\n-        return text\n-\n-    def to_text(self, generator):\n-        section = self.md_format()\n-        dialogs_text = self.dialogs_to_text(generator)\n-        notes = f\"##### Notes\\n{self.notes}\" if self.notes else \"\"\n-        section = section.format(\n-            title=self.title,\n-            description=self.description,\n-            dialogs_text=dialogs_text,\n-            notes=notes,\n-        )\n-        return section\n-\n-\n-def llama3_1_builtin_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n-    from llama_models.llama3.api.template_data import system_message_builtin_tools_only\n-\n-    interface = LLama31Interface(tool_prompt_format)\n-\n-    messages = interface.system_messages(**system_message_builtin_tools_only())\n-    messages += interface.user_message(content=\"Search the web for the latest price of 1oz gold?\")\n-\n-    return messages\n-\n-\n-def llama3_1_builtin_code_interpreter_dialog(tool_prompt_format=ToolPromptFormat.json):\n-    from llama_models.llama3.api.template_data import system_message_builtin_code_only\n-\n-    interface = LLama31Interface(tool_prompt_format)\n-\n-    messages = interface.system_messages(**system_message_builtin_code_only())\n-    messages += interface.user_message(\n-        content=\"Write code to check if number is prime. Use it to verify if number 7 is prime\"\n-    )\n-\n-    return messages\n-\n-\n-def llama3_1_builtin_tool_call_with_image_dialog(\n-    tool_prompt_format=ToolPromptFormat.json,\n-):\n-    from llama_models.llama3.api.template_data import system_message_builtin_tools_only\n-\n-    this_dir = Path(__file__).parent.resolve()\n-    with open(this_dir / \"scripts/resources/dog.jpg\", \"rb\") as f:\n-        img = f.read()\n-\n-    interface = LLama31Interface(tool_prompt_format)\n-\n-    messages = interface.system_messages(**system_message_builtin_tools_only())\n-    messages += interface.user_message(content=[RawMediaItem(data=img), RawTextItem(text=\"What is this dog breed?\")])\n-    messages += interface.assistant_response_messages(\n-        \"Based on the description of the dog in the image, it appears to be a small breed dog, possibly a terrier mix\",\n-        StopReason.end_of_turn,\n-    )\n-    messages += interface.user_message(\"Search the web for some food recommendations for the indentified breed\")\n-    return messages\n-\n-\n-def llama3_1_custom_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n-    from llama_models.llama3.api.template_data import system_message_custom_tools_only\n-\n-    interface = LLama31Interface(tool_prompt_format)\n-\n-    messages = interface.system_messages(**system_message_custom_tools_only())\n-    messages += interface.user_message(content=\"Use tools to get latest trending songs\")\n-    return messages\n-\n-\n-def llama3_1_e2e_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n-    import json\n-\n-    from llama_models.llama3.api.template_data import system_message_custom_tools_only\n-\n-    tool_response = json.dumps([\"great song1\", \"awesome song2\", \"cool song3\"])\n-    interface = LLama31Interface(tool_prompt_format)\n-\n-    messages = interface.system_messages(**system_message_custom_tools_only())\n-    messages += interface.user_message(content=\"Use tools to get latest trending songs\")\n-    messages.append(\n-        RawMessage(\n-            role=\"assistant\",\n-            content=\"\",\n-            stop_reason=StopReason.end_of_message,\n-            tool_calls=[\n-                ToolCall(\n-                    call_id=\"call_id\",\n-                    tool_name=\"trending_songs\",\n-                    arguments={\"n\": \"10\", \"genre\": \"latest\"},\n-                )\n-            ],\n-        ),\n-    )\n-    messages.append(\n-        RawMessage(\n-            role=\"assistant\",\n-            content=tool_response,\n-        )\n-    )\n-    return messages\n-\n-\n-def llama3_2_user_assistant_conversation():\n-    return UseCase(\n-        title=\"User and assistant conversation\",\n-        description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n-        dialogs=[\n-            [\n-                RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n-                RawMessage(role=\"user\", content=\"Who are you?\"),\n-            ]\n-        ],\n-        notes=\"This format is unchanged from Llama3.1\",\n-    )\n\n--- File: models/schema_utils.py ---\n@@ -1,128 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-from dataclasses import dataclass\n-from typing import Any, Callable, Dict, List, Optional, Type, TypeVar, Union\n-\n-# Borrowed from https://github.com/hunyadi/strong_typing/blob/master/strong_typing/core.py\n-\n-\n-class JsonObject:\n-    \"Placeholder type for an unrestricted JSON object.\"\n-\n-\n-class JsonArray:\n-    \"Placeholder type for an unrestricted JSON array.\"\n-\n-\n-# a JSON type with possible `null` values\n-JsonType = Union[\n-    None,\n-    bool,\n-    int,\n-    float,\n-    str,\n-    Dict[str, \"JsonType\"],\n-    List[\"JsonType\"],\n-]\n-\n-# a JSON type that cannot contain `null` values\n-StrictJsonType = Union[\n-    bool,\n-    int,\n-    float,\n-    str,\n-    Dict[str, \"StrictJsonType\"],\n-    List[\"StrictJsonType\"],\n-]\n-\n-# a meta-type that captures the object type in a JSON schema\n-Schema = Dict[str, JsonType]\n-\n-\n-T = TypeVar(\"T\")\n-\n-\n-def register_schema(\n-    data_type: T,\n-    schema: Optional[Schema] = None,\n-    name: Optional[str] = None,\n-    examples: Optional[List[JsonType]] = None,\n-) -> T:\n-    \"\"\"\n-    Associates a type with a JSON schema definition.\n-\n-    :param data_type: The type to associate with a JSON schema.\n-    :param schema: The schema to associate the type with. Derived automatically if omitted.\n-    :param name: The name used for looking up the type. Determined automatically if omitted.\n-    :returns: The input type.\n-    \"\"\"\n-    return data_type\n-\n-\n-def json_schema_type(\n-    cls: Optional[Type[T]] = None,\n-    *,\n-    schema: Optional[Schema] = None,\n-    examples: Optional[List[JsonType]] = None,\n-) -> Union[Type[T], Callable[[Type[T]], Type[T]]]:\n-    \"\"\"Decorator to add user-defined schema definition to a class.\"\"\"\n-\n-    def wrap(cls: Type[T]) -> Type[T]:\n-        return register_schema(cls, schema, examples=examples)\n-\n-    # see if decorator is used as @json_schema_type or @json_schema_type()\n-    if cls is None:\n-        # called with parentheses\n-        return wrap\n-    else:\n-        # called as @json_schema_type without parentheses\n-        return wrap(cls)\n-\n-\n-register_schema(JsonObject, name=\"JsonObject\")\n-register_schema(JsonArray, name=\"JsonArray\")\n-register_schema(JsonType, name=\"JsonType\")\n-register_schema(StrictJsonType, name=\"StrictJsonType\")\n-\n-\n-@dataclass\n-class WebMethod:\n-    route: Optional[str] = None\n-    public: bool = False\n-    request_examples: Optional[List[Any]] = None\n-    response_examples: Optional[List[Any]] = None\n-    method: Optional[str] = None\n-\n-\n-def webmethod(\n-    route: Optional[str] = None,\n-    method: Optional[str] = None,\n-    public: Optional[bool] = False,\n-    request_examples: Optional[List[Any]] = None,\n-    response_examples: Optional[List[Any]] = None,\n-) -> Callable[[T], T]:\n-    \"\"\"\n-    Decorator that supplies additional metadata to an endpoint operation function.\n-\n-    :param route: The URL path pattern associated with this operation which path parameters are substituted into.\n-    :param public: True if the operation can be invoked without prior authentication.\n-    :param request_examples: Sample requests that the operation might take. Pass a list of objects, not JSON.\n-    :param response_examples: Sample responses that the operation might produce. Pass a list of objects, not JSON.\n-    \"\"\"\n-\n-    def wrap(cls: T) -> T:\n-        cls.__webmethod__ = WebMethod(\n-            route=route,\n-            method=method,\n-            public=public or False,\n-            request_examples=request_examples,\n-            response_examples=response_examples,\n-        )\n-        return cls\n-\n-    return wrap\n\n--- File: models/scripts/example_chat_completion.py ---\n@@ -12,7 +12,7 @@\n \n import fire\n \n-from llama_models.llama3.api.datatypes import RawMessage, StopReason\n+from llama_models.datatypes import RawMessage, StopReason\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n\n--- File: models/scripts/generate_prompt_format.py ---\n@@ -1,60 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-import importlib\n-from pathlib import Path\n-from typing import Optional\n-\n-import fire\n-\n-# from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_models.llama3.reference_impl.generation import Llama\n-\n-\n-THIS_DIR = Path(__file__).parent.resolve()\n-\n-\n-def run_main(\n-    ckpt_dir: str,\n-    module_name: str,\n-    output_path: str,\n-    model_parallel_size: Optional[int] = None,\n-):\n-    module = importlib.import_module(module_name)\n-    assert hasattr(module, \"usecases\"), f\"Module {module_name} missing usecases function\"\n-    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n-    generator = Llama.build(\n-        ckpt_dir=ckpt_dir,\n-        tokenizer_path=tokenizer_path,\n-        max_seq_len=512,\n-        max_batch_size=1,\n-        model_parallel_size=model_parallel_size,\n-    )\n-\n-    use_cases = module.usecases()\n-    text = \"\"\n-    for u in use_cases:\n-        if isinstance(u, str):\n-            use_case_text = f\"\\n{u}\\n\"\n-        else:\n-            use_case_text = u.to_text(generator)\n-\n-        text += use_case_text\n-        print(use_case_text)\n-\n-    text += \"Thank You!\\n\"\n-\n-    with open(output_path, \"w\") as f:\n-        f.write(text)\n-\n-\n-def main():\n-    fire.Fire(run_main)\n-\n-\n-if __name__ == \"__main__\":\n-    main()\n\n--- File: models/scripts/multimodal_example_chat_completion.py ---\n@@ -13,7 +13,7 @@\n from typing import Optional\n \n import fire\n-from llama_models.llama3.api.datatypes import RawMediaItem, RawMessage, RawTextItem\n+from llama_models.datatypes import RawMediaItem, RawMessage, RawTextItem\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n\n--- File: models/scripts/multimodal_example_text_completion.py ---\n@@ -13,7 +13,7 @@\n from typing import Optional\n \n import fire\n-from llama_models.llama3.api.datatypes import RawMediaItem\n+from llama_models.datatypes import RawMediaItem\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n\n--- File: models/sku_list.py ---\n@@ -1,994 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-from dataclasses import dataclass\n-from functools import lru_cache\n-from typing import List, Optional\n-\n-from .datatypes import (\n-    CheckpointQuantizationFormat,\n-    CoreModelId,\n-    Model,\n-    SamplingParams,\n-    TopPSamplingStrategy,\n-)\n-\n-LLAMA2_VOCAB_SIZE = 32000\n-LLAMA3_VOCAB_SIZE = 128256\n-\n-\n-def resolve_model(descriptor: str) -> Optional[Model]:\n-    for m in all_registered_models():\n-        if descriptor in (m.descriptor(), m.huggingface_repo):\n-            return m\n-    return None\n-\n-\n-def all_registered_models() -> List[Model]:\n-    return (\n-        llama2_family() + llama3_family() + llama3_1_family() + llama3_2_family() + llama3_3_family() + safety_models()\n-    )\n-\n-\n-def recommended_sampling_params() -> SamplingParams:\n-    return SamplingParams(\n-        strategy=TopPSamplingStrategy(\n-            temperature=1.0,\n-            top_p=0.9,\n-        )\n-    )\n-\n-\n-def llama2_family() -> List[Model]:\n-    return [\n-        *llama2_base_models(),\n-        *llama2_instruct_models(),\n-    ]\n-\n-\n-def llama3_family() -> List[Model]:\n-    return [\n-        *llama3_base_models(),\n-        *llama3_instruct_models(),\n-    ]\n-\n-\n-def llama3_1_family() -> List[Model]:\n-    return [\n-        *llama3_1_base_models(),\n-        *llama3_1_instruct_models(),\n-    ]\n-\n-\n-def llama3_2_family() -> List[Model]:\n-    return [\n-        *llama3_2_base_models(),\n-        *llama3_2_instruct_models(),\n-    ]\n-\n-\n-def llama3_3_family() -> List[Model]:\n-    return [\n-        *llama3_3_instruct_models(),\n-    ]\n-\n-\n-def llama2_base_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama2_7b,\n-            description=\"Llama 2 7b model\",\n-            huggingface_repo=\"meta-llama/Llama-2-7b\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA2_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama2_13b,\n-            description=\"Llama 2 13b model\",\n-            huggingface_repo=\"meta-llama/Llama-2-13b\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 5120,\n-                \"n_layers\": 40,\n-                \"n_heads\": 40,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA2_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama2_70b,\n-            description=\"Llama 2 70b model\",\n-            huggingface_repo=\"meta-llama/Llama-2-70b\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA2_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=8,\n-        ),\n-    ]\n-\n-\n-def llama3_base_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_8b,\n-            description=\"Llama 3 8b model\",\n-            huggingface_repo=\"meta-llama/Llama-3-8B\",\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_70b,\n-            description=\"Llama 3 70b model\",\n-            huggingface_repo=\"meta-llama/Llama-3-70B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=8,\n-        ),\n-    ]\n-\n-\n-def llama3_1_base_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_8b,\n-            description=\"Llama 3.1 8b model\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-8B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_70b,\n-            description=\"Llama 3.1 70b model\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-70B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=8,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_405b,\n-            variant=\"bf16-mp8\",\n-            description=\"Llama 3.1 405b model (BF16 weights)\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-405B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 16384,\n-                \"n_layers\": 126,\n-                \"n_heads\": 128,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.2,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=8,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_405b,\n-            description=\"Llama 3.1 405b model (FP8 quantized)\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-405B-FP8\",\n-            quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 16384,\n-                \"n_layers\": 126,\n-                \"n_heads\": 128,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.2,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=8,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_405b,\n-            variant=\"bf16-mp16\",\n-            description=\"Llama 3.1 405b model (BF16 weights for mp16)\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-405B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 16384,\n-                \"n_layers\": 126,\n-                \"n_heads\": 128,\n-                \"n_kv_heads\": 16,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.2,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=16,\n-        ),\n-    ]\n-\n-\n-def llama3_2_base_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_1b,\n-            description=\"Llama 3.2 1b model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-1B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 2048,\n-                \"n_layers\": 16,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.5,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_3b,\n-            description=\"Llama 3.2 3b model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-3B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 3072,\n-                \"n_layers\": 28,\n-                \"n_heads\": 24,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.0,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_11b_vision,\n-            description=\"Llama 3.2 11b vision model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-11B-Vision\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-                \"vision_chunk_size\": 448,\n-                \"vision_max_num_chunks\": 4,\n-                \"vision_num_cross_attention_layers\": 8,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_90b_vision,\n-            description=\"Llama 3.2 90b vision model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-90B-Vision\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-                \"vision_chunk_size\": 560,\n-                \"vision_max_num_chunks\": 4,\n-                \"vision_num_cross_attention_layers\": 20,\n-            },\n-            pth_file_count=8,\n-        ),\n-    ]\n-\n-\n-def llama2_instruct_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama2_7b_chat,\n-            description=\"Llama 2 7b chat model\",\n-            huggingface_repo=\"meta-llama/Llama-2-7b-chat\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA2_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama2_13b_chat,\n-            description=\"Llama 2 13b chat model\",\n-            huggingface_repo=\"meta-llama/Llama-2-13b-chat\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 5120,\n-                \"n_layers\": 40,\n-                \"n_heads\": 40,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA2_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama2_70b_chat,\n-            description=\"Llama 2 70b chat model\",\n-            huggingface_repo=\"meta-llama/Llama-2-70b-chat\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA2_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=8,\n-        ),\n-    ]\n-\n-\n-def llama3_instruct_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_8b_instruct,\n-            description=\"Llama 3 8b instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3-8B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_70b_instruct,\n-            description=\"Llama 3 70b instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3-70B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=8,\n-        ),\n-    ]\n-\n-\n-def llama3_1_instruct_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_8b_instruct,\n-            description=\"Llama 3.1 8b instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-8B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_70b_instruct,\n-            description=\"Llama 3.1 70b instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-70B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=8,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_405b_instruct,\n-            variant=\"bf16-mp8\",\n-            description=\"Llama 3.1 405b instruct model (BF16 weights)\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 16384,\n-                \"n_layers\": 126,\n-                \"n_heads\": 128,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.2,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=8,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_405b_instruct,\n-            description=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct-FP8\",\n-            quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 16384,\n-                \"n_layers\": 126,\n-                \"n_heads\": 128,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.2,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=8,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_1_405b_instruct,\n-            variant=\"bf16-mp16\",\n-            description=\"Llama 3.1 405b instruct model (BF16 weights for mp16)\",\n-            huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 16384,\n-                \"n_layers\": 126,\n-                \"n_heads\": 128,\n-                \"n_kv_heads\": 16,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.2,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=16,\n-        ),\n-    ]\n-\n-\n-def arch_args_1b() -> dict:\n-    return {\n-        \"dim\": 2048,\n-        \"n_layers\": 16,\n-        \"n_heads\": 32,\n-        \"n_kv_heads\": 8,\n-        \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-        \"ffn_dim_multiplier\": 1.5,\n-        \"multiple_of\": 256,\n-        \"norm_eps\": 1e-05,\n-        \"rope_theta\": 500000.0,\n-        \"use_scaled_rope\": True,\n-    }\n-\n-\n-def arch_args_3b() -> dict:\n-    return {\n-        \"dim\": 3072,\n-        \"n_layers\": 28,\n-        \"n_heads\": 24,\n-        \"n_kv_heads\": 8,\n-        \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-        \"ffn_dim_multiplier\": 1.0,\n-        \"multiple_of\": 256,\n-        \"norm_eps\": 1e-05,\n-        \"rope_theta\": 500000.0,\n-        \"use_scaled_rope\": True,\n-    }\n-\n-\n-def llama3_2_quantized_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_1b_instruct,\n-            variant=\"int4-qlora-eo8\",\n-            quantization_format=CheckpointQuantizationFormat.int4,\n-            description=\"Llama 3.2 1b INT4 quantized LoRA\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                **arch_args_1b(),\n-                \"quantization_args\": {\n-                    \"group_size\": 256,\n-                },\n-                \"lora_args\": {\n-                    \"rank\": 16,\n-                    \"scale\": 2.0,\n-                },\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_1b_instruct,\n-            variant=\"int4-spinquant-eo8\",\n-            quantization_format=CheckpointQuantizationFormat.int4,\n-            description=\"Llama 3.2 1b INT4 quantized SpinQuant\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                **arch_args_1b(),\n-                \"quantization_args\": {\n-                    \"group_size\": 256,\n-                },\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_3b_instruct,\n-            variant=\"int4-qlora-eo8\",\n-            quantization_format=CheckpointQuantizationFormat.int4,\n-            description=\"Llama 3.2 3b INT4 quantized LoRA\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                **arch_args_3b(),\n-                \"quantization_args\": {\n-                    \"group_size\": 256,\n-                },\n-                \"lora_args\": {\n-                    \"rank\": 16,\n-                    \"scale\": 2.0,\n-                },\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_3b_instruct,\n-            variant=\"int4-spinquant-eo8\",\n-            quantization_format=CheckpointQuantizationFormat.int4,\n-            description=\"Llama 3.2 3b INT4 quantized SpinQuant\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-3B-Instruct-SpinQuant_INT4_EO8\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                **arch_args_3b(),\n-                \"quantization_args\": {\n-                    \"group_size\": 256,\n-                },\n-            },\n-            pth_file_count=1,\n-        ),\n-    ]\n-\n-\n-def llama3_2_instruct_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_1b_instruct,\n-            description=\"Llama 3.2 1b instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-1B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args=arch_args_1b(),\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_3b_instruct,\n-            description=\"Llama 3.2 3b instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-3B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args=arch_args_3b(),\n-            pth_file_count=1,\n-        ),\n-        *llama3_2_quantized_models(),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_11b_vision_instruct,\n-            description=\"Llama 3.2 11b vision instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-                \"vision_chunk_size\": 560,\n-                \"vision_max_num_chunks\": 4,\n-                \"vision_num_cross_attention_layers\": 8,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama3_2_90b_vision_instruct,\n-            description=\"Llama 3.2 90b vision instruct model\",\n-            huggingface_repo=\"meta-llama/Llama-3.2-90B-Vision-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-                \"vision_chunk_size\": 560,\n-                \"vision_max_num_chunks\": 4,\n-                \"vision_num_cross_attention_layers\": 20,\n-            },\n-            pth_file_count=8,\n-        ),\n-    ]\n-\n-\n-def llama3_3_instruct_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama3_3_70b_instruct,\n-            description=\"Llama 3.3 70b instruct\",\n-            huggingface_repo=\"meta-llama/Llama-3.3-70B-Instruct\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 8192,\n-                \"n_layers\": 80,\n-                \"n_heads\": 64,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 4096,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=8,\n-        ),\n-    ]\n-\n-\n-@lru_cache\n-def safety_models() -> List[Model]:\n-    return [\n-        Model(\n-            core_model_id=CoreModelId.llama_guard_3_11b_vision,\n-            description=\"Llama Guard v3 11b vision system safety model\",\n-            huggingface_repo=\"meta-llama/Llama-Guard-3-11B-Vision\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-                \"vision_chunk_size\": 560,\n-                \"vision_max_num_chunks\": 4,\n-                \"vision_num_cross_attention_layers\": 8,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama_guard_3_1b,\n-            variant=\"int4\",\n-            description=\"Llama Guard v3 1b 'int4' quantized system safety model\",\n-            huggingface_repo=\"meta-llama/Llama-Guard-3-1B-INT4\",\n-            quantization_format=CheckpointQuantizationFormat.int4,\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 2048,\n-                \"n_layers\": 12,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"rope_freq_base\": 500000.0,\n-                \"norm_eps\": 1e-05,\n-                \"hidden_dim\": 6400,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama_guard_3_1b,\n-            description=\"Llama Guard v3 1b system safety model\",\n-            huggingface_repo=\"meta-llama/Llama-Guard-3-1B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 2048,\n-                \"n_layers\": 16,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.5,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama_guard_3_8b,\n-            description=\"Llama Guard v3 8b system safety model\",\n-            huggingface_repo=\"meta-llama/Llama-Guard-3-8B\",\n-            arch_args={\n-                \"dim\": 4096,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"n_layers\": 32,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama_guard_3_8b,\n-            variant=\"int8\",\n-            description=\"Llama Guard v3 8b system safety model\",\n-            huggingface_repo=\"meta-llama/Llama-Guard-3-8B-INT8\",\n-            quantization_format=CheckpointQuantizationFormat.int8,\n-            arch_args={\n-                \"dim\": 4096,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 1024,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"n_layers\": 32,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-            },\n-            pth_file_count=1,\n-        ),\n-        Model(\n-            core_model_id=CoreModelId.llama_guard_2_8b,\n-            description=\"Llama Guard v2 8b system safety model\",\n-            huggingface_repo=\"meta-llama/Llama-Guard-2-8B\",\n-            arch_args={\n-                \"dim\": 4096,\n-                \"n_layers\": 32,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA2_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.3,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": False,\n-            },\n-            pth_file_count=1,\n-        ),\n-    ]\n-\n-\n-@dataclass\n-class LlamaDownloadInfo:\n-    folder: str\n-    files: List[str]\n-    pth_size: int\n-\n-\n-def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n-    \"\"\"Information needed to download model from llamameta.net\"\"\"\n-\n-    pth_count = model.pth_file_count\n-    if model.core_model_id == CoreModelId.llama3_1_405b:\n-        if pth_count == 16:\n-            folder = \"Llama-3.1-405B-MP16\"\n-        elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n-            folder = \"Llama-3.1-405B\"\n-        else:\n-            folder = \"Llama-3.1-405B-MP8\"\n-    elif model.core_model_id == CoreModelId.llama3_1_405b_instruct:\n-        if pth_count == 16:\n-            folder = \"Llama-3.1-405B-Instruct-MP16\"\n-        elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n-            folder = \"Llama-3.1-405B-Instruct\"\n-        else:\n-            folder = \"Llama-3.1-405B-Instruct-MP8\"\n-    elif model.core_model_id == CoreModelId.llama_guard_3_8b:\n-        if model.quantization_format == CheckpointQuantizationFormat.int8:\n-            folder = \"Llama-Guard-3-8B-INT8-HF\"\n-        else:\n-            folder = \"Llama-Guard-3-8B\"\n-    elif model.core_model_id == CoreModelId.llama_guard_2_8b:\n-        folder = \"llama-guard-2\"\n-    else:\n-        folder = model.huggingface_repo.split(\"/\")[-1]\n-        if \"Llama-2\" in folder:\n-            folder = folder.lower()\n-\n-    files = [\"checklist.chk\"]\n-    if (\n-        model.core_model_id == CoreModelId.llama_guard_3_8b\n-        and model.quantization_format == CheckpointQuantizationFormat.int8\n-    ):\n-        files.extend(\n-            [\n-                \"generation_config.json\",\n-                \"model-00001-of-00002.safetensors\",\n-                \"model-00002-of-00002.safetensors\",\n-                \"special_tokens_map.json\",\n-                \"tokenizer.json\",\n-                \"tokenizer_config.json\",\n-                \"model.safetensors.index.json\",\n-            ]\n-        )\n-    elif (\n-        model.core_model_id == CoreModelId.llama_guard_3_1b\n-        and model.quantization_format == CheckpointQuantizationFormat.int4\n-    ):\n-        files.extend(\n-            [\n-                \"llama_guard_3_1b_pruned_xnnpack.pte\",\n-                \"example-prompt.txt\",\n-                \"params.json\",\n-                \"tokenizer.model\",\n-            ]\n-        )\n-    else:\n-        files.extend(\n-            [\n-                \"tokenizer.model\",\n-                \"params.json\",\n-            ]\n-        )\n-        if model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n-            files.extend([f\"fp8_scales_{i}.pt\" for i in range(pth_count)])\n-        files.extend([f\"consolidated.{i:02d}.pth\" for i in range(pth_count)])\n-\n-    return LlamaDownloadInfo(\n-        folder=folder,\n-        files=files,\n-        pth_size=llama_meta_pth_size(model),\n-    )\n-\n-\n-# Sadness because Cloudfront rejects our HEAD requests to find Content-Length\n-def llama_meta_pth_size(model: Model) -> int:\n-    if model.core_model_id not in (\n-        CoreModelId.llama3_1_405b,\n-        CoreModelId.llama3_1_405b_instruct,\n-    ):\n-        return 0\n-\n-    if model.pth_file_count == 16:\n-        return 51268302389\n-    elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n-        return 60903742309\n-    else:\n-        return 101470976045"
            },
            {
              "sha": "ecf2f12acb17a5c2bd3913504a70fa79f2e518de",
              "url": "https://github.com/meta-llama/llama-models/commit/ecf2f12acb17a5c2bd3913504a70fa79f2e518de",
              "message": "Add back requirements.txt via uv-export",
              "files_changed": [
                {
                  "filename": ".pre-commit-config.yaml",
                  "status": "modified"
                },
                {
                  "filename": "requirements.txt",
                  "status": "added"
                },
                {
                  "filename": "uv.lock",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .pre-commit-config.yaml ---\n@@ -43,6 +43,12 @@ repos:\n         additional_dependencies:\n         - black==24.3.0\n \n+-   repo: https://github.com/astral-sh/uv-pre-commit\n+    rev: 0.5.26\n+    hooks:\n+    -   id: uv-export\n+        args: [\"--frozen\", \"--no-hashes\", \"--no-emit-project\"]\n+\n # -   repo: https://github.com/pre-commit/mirrors-mypy\n #     rev: v1.14.0\n #     hooks:\n\n--- File: requirements.txt ---\n@@ -0,0 +1,17 @@\n+# This file was autogenerated by uv via the following command:\n+#    uv export --frozen --no-hashes --no-emit-project\n+annotated-types==0.7.0\n+certifi==2025.1.31\n+charset-normalizer==3.4.1\n+idna==3.10\n+jinja2==3.1.5\n+markupsafe==3.0.2\n+pillow==11.1.0\n+pydantic==2.10.6\n+pydantic-core==2.27.2\n+pyyaml==6.0.2\n+regex==2024.11.6\n+requests==2.32.3\n+tiktoken==0.8.0\n+typing-extensions==4.12.2\n+urllib3==2.3.0"
            },
            {
              "sha": "364ed4ec2bdcc13ab3acb08e364e843b6322b9ae",
              "url": "https://github.com/meta-llama/llama-models/commit/364ed4ec2bdcc13ab3acb08e364e843b6322b9ae",
              "message": "Update flake8 to ruff, run pre-commit on everything",
              "files_changed": [
                {
                  "filename": ".flake8",
                  "status": "removed"
                },
                {
                  "filename": ".pre-commit-config.yaml",
                  "status": "modified"
                },
                {
                  "filename": ".ruff.toml",
                  "status": "added"
                },
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/interface.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/template_data.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/test_tokenizer.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tokenizer.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/prompt_templates/base.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/prompt_templates/tool_response.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/encoder_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/image_transform.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/prompt_templates/test_system_prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/eval_details.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/prompts_text.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/prompts_vision.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_3/USE_POLICY.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_3/prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/prompt_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/generate_prompt_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .flake8 ---\n@@ -1,29 +0,0 @@\n-[flake8]\n-# Suggested config from pytorch that we can adapt\n-select = B,C,E,F,N,P,T4,W,B9,TOR0,TOR1,TOR2\n-max-line-length = 120\n-# C408 ignored because we like the dict keyword argument syntax\n-# E501 is not flexible enough, we're using B950 instead\n-# N812 ignored because import torch.nn.functional as F is PyTorch convention\n-# N817 ignored because importing using acronyms is convention (DistributedDataParallel as DDP)\n-# E731 allow usage of assigning lambda expressions\n-# E701 let black auto-format statements on one line\n-# E704 let black auto-format statements on one line\n-ignore =\n-    E203,E305,E402,E501,E721,E741,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,N812,N817,E731,E701,E704\n-    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n-    # to line this up with executable bit\n-    EXE001,\n-    # these ignores are from flake8-bugbear; please fix!\n-    B007,B008,B950\n-optional-ascii-coding = True\n-exclude =\n-    ./.git,\n-    ./docs\n-    ./build\n-    ./scripts,\n-    ./venv,\n-    *.pyi\n-    .pre-commit-config.yaml\n-    *.md\n-    .flake8\n\n--- File: .pre-commit-config.yaml ---\n@@ -1,14 +1,12 @@\n-exclude: 'build'\n+exclude: 'build/'\n \n default_language_version:\n     python: python3\n \n repos:\n -   repo: https://github.com/pre-commit/pre-commit-hooks\n-    rev: 6306a48f7dae5861702d573c9c247e4e9498e867\n+    rev: v5.0.0  # Latest stable version\n     hooks:\n-    -   id: trailing-whitespace\n-    -   id: check-ast\n     -   id: check-merge-conflict\n     -   id: check-added-large-files\n         args: ['--maxkb=1000']\n@@ -28,26 +26,59 @@ repos:\n           - --license-filepath\n           - docs/license_header.txt\n \n--   repo: https://github.com/pycqa/flake8\n-    rev: 34cbf8ef3950f43d09b85e2e45c15ae5717dc37b\n+-   repo: https://github.com/astral-sh/ruff-pre-commit\n+    rev: v0.9.4\n     hooks:\n-    -   id: flake8\n-        additional_dependencies:\n-          - flake8-bugbear == 22.4.25\n-          - pep8-naming == 0.12.1\n-          - torchfix\n-        args: ['--config=.flake8']\n+    -   id: ruff\n+        args: [\n+            --fix,\n+            --exit-non-zero-on-fix\n+        ]\n+    -   id: ruff-format\n \n--   repo: https://github.com/omnilib/ufmt\n-    rev: v2.7.0\n+-   repo: https://github.com/adamchainz/blacken-docs\n+    rev: 1.19.0\n     hooks:\n-    -   id: ufmt\n+    -   id: blacken-docs\n         additional_dependencies:\n-          - black == 24.4.2\n-          - usort == 1.0.8\n+        - black==24.3.0\n+\n+# -   repo: https://github.com/pre-commit/mirrors-mypy\n+#     rev: v1.14.0\n+#     hooks:\n+#     -   id: mypy\n+#         additional_dependencies:\n+#           - types-requests\n+#           - types-setuptools\n+#           - pydantic\n+#         args: [--ignore-missing-imports]\n \n # - repo: https://github.com/jsh9/pydoclint\n #   rev: d88180a8632bb1602a4d81344085cf320f288c5a\n #   hooks:\n #     - id: pydoclint\n #       args: [--config=pyproject.toml]\n+\n+# - repo: https://github.com/tcort/markdown-link-check\n+#   rev: v3.11.2\n+#   hooks:\n+#     - id: markdown-link-check\n+#       args: ['--quiet']\n+\n+# -   repo: local\n+#     hooks:\n+#       - id: distro-codegen\n+#         name: Distribution Template Codegen\n+#         additional_dependencies:\n+#           - rich\n+#           - pydantic\n+#         entry: python -m llama_stack.scripts.distro_codegen\n+#         language: python\n+#         pass_filenames: false\n+#         require_serial: true\n+#         files: ^llama_stack/templates/.*$\n+#         stages: [manual]\n+\n+ci:\n+    autofix_commit_msg:  [pre-commit.ci] Auto format from pre-commit.com hooks\n+    autoupdate_commit_msg:  [pre-commit.ci] pre-commit autoupdate\n\n--- File: .ruff.toml ---\n@@ -0,0 +1,37 @@\n+# Suggested config from pytorch that we can adapt\n+lint.select = [\"B\", \"C\", \"E\" , \"F\" , \"N\", \"W\", \"B9\"]\n+\n+line-length = 120\n+\n+# C408 ignored because we like the dict keyword argument syntax\n+# E501 is not flexible enough, we're using B950 instead\n+# N812 ignored because import torch.nn.functional as F is PyTorch convention\n+# N817 ignored because importing using acronyms is convention (DistributedDataParallel as DDP)\n+# E731 allow usage of assigning lambda expressions\n+# E701 let black auto-format statements on one line\n+# E704 let black auto-format statements on one line\n+lint.ignore = [\n+    \"E203\", \"E305\", \"E402\", \"E501\", \"E721\", \"E741\", \"F405\", \"F821\", \"F841\",\n+    \"C408\", \"E302\", \"W291\", \"E303\", \"N812\", \"N817\", \"E731\", \"E701\",\n+    # These are the additional ones we started ignoring after moving to ruff. We should look into each one of them later.\n+    \"C901\", \"C405\", \"C414\", \"N803\", \"N999\", \"C403\", \"C416\", \"B028\", \"C419\", \"C401\", \"B023\",\n+    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n+    # to line this up with executable bit\n+    \"EXE001\",\n+    # random naming hints don't need\n+    \"N802\",\n+    # these ignores are from flake8-bugbear; please fix!\n+    \"B007\", \"B008\"\n+]\n+\n+exclude = [\n+    \"./.git\",\n+    \"./docs/*\",\n+    \"./build\",\n+    \"./scripts\",\n+    \"./venv\",\n+    \"*.pyi\",\n+    \".pre-commit-config.yaml\",\n+    \"*.md\",\n+    \".flake8\"\n+]\n\n--- File: README.md ---\n@@ -96,10 +96,10 @@ huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"origin\n   model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n \n   pipeline = transformers.pipeline(\n-    \"text-generation\",\n-    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    model_kwargs={\"torch_dtype\": torch.bfloat16},\n-    device=\"cuda\",\n+      \"text-generation\",\n+      model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+      model_kwargs={\"torch_dtype\": torch.bfloat16},\n+      device=\"cuda\",\n   )\n   ```\n \n\n--- File: models/datatypes.py ---\n@@ -187,9 +187,7 @@ class Model(BaseModel):\n     arch_args: Dict[str, Any]\n     variant: str = \"\"\n \n-    quantization_format: CheckpointQuantizationFormat = (\n-        CheckpointQuantizationFormat.bf16\n-    )\n+    quantization_format: CheckpointQuantizationFormat = CheckpointQuantizationFormat.bf16\n     pth_file_count: int\n     metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n \n\n--- File: models/llama3/api/chat_format.py ---\n@@ -58,20 +58,13 @@ class ChatFormat:\n     def __init__(self, tokenizer: Tokenizer):\n         self.tokenizer = tokenizer\n \n-        self.possible_headers = {\n-            role: f\"<|start_header_id|>{role_str(role)}<|end_header_id|>\\n\\n\"\n-            for role in Role\n-        }\n+        self.possible_headers = {role: f\"<|start_header_id|>{role_str(role)}<|end_header_id|>\\n\\n\" for role in Role}\n         self.vision_token = self.tokenizer.special_tokens[\"<|image|>\"]\n \n     def _encode_header(self, role: str) -> List[int]:\n         tokens = []\n         tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n-        tokens.extend(\n-            self.tokenizer.encode(\n-                \"ipython\" if role == \"tool\" else role, bos=False, eos=False\n-            )\n-        )\n+        tokens.extend(self.tokenizer.encode(\"ipython\" if role == \"tool\" else role, bos=False, eos=False))\n         tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n         tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n         return tokens\n@@ -80,9 +73,7 @@ def encode_content(self, content: RawContent) -> LLMInput:\n         tokens, images = self._encode_content(content, bos=True)\n         return self._model_input_from_tokens_images(tokens, images)\n \n-    def _encode_content(\n-        self, content: RawContent, bos: bool = False\n-    ) -> Tuple[List[int], List[PIL_Image.Image]]:\n+    def _encode_content(self, content: RawContent, bos: bool = False) -> Tuple[List[int], List[PIL_Image.Image]]:\n         tokens = []\n         images = []\n \n@@ -94,9 +85,7 @@ def _process(c):\n             if isinstance(c, str) or isinstance(c, RawTextItem):\n                 if isinstance(c, RawTextItem):\n                     c = c.text\n-                tokens.extend(\n-                    self.tokenizer.encode(c, bos=False if added_bos else bos, eos=False)\n-                )\n+                tokens.extend(self.tokenizer.encode(c, bos=False if added_bos else bos, eos=False))\n                 added_bos = True\n \n             elif isinstance(c, RawMediaItem):\n@@ -150,9 +139,7 @@ def _process_content(c):\n         if message.role == \"assistant\":\n             eom = message.stop_reason == StopReason.end_of_message\n \n-        tokens.append(\n-            self.tokenizer.special_tokens[\"<|eom_id|>\" if eom else \"<|eot_id|>\"]\n-        )\n+        tokens.append(self.tokenizer.special_tokens[\"<|eom_id|>\" if eom else \"<|eot_id|>\"])\n         return tokens, images\n \n     def encode_dialog_prompt(\n@@ -174,16 +161,12 @@ def encode_dialog_prompt(\n         return self._model_input_from_tokens_images(tokens, images)\n \n     # TODO(this should be generic, not only for assistant messages)\n-    def decode_assistant_message(\n-        self, tokens: List[int], stop_reason: StopReason\n-    ) -> RawMessage:\n+    def decode_assistant_message(self, tokens: List[int], stop_reason: StopReason) -> RawMessage:\n         content = self.tokenizer.decode(tokens)\n \n         return self.decode_assistant_message_from_content(content, stop_reason)\n \n-    def decode_assistant_message_from_content(\n-        self, content: str, stop_reason: StopReason\n-    ) -> RawMessage:\n+    def decode_assistant_message_from_content(self, content: str, stop_reason: StopReason) -> RawMessage:\n         content = content.strip(\" \")\n         header_str = self.possible_headers[Role.assistant]\n         if content.startswith(header_str):\n@@ -248,9 +231,7 @@ def decode_assistant_message_from_content(\n             tool_calls=tool_calls,\n         )\n \n-    def _model_input_from_tokens_images(\n-        self, tokens: List[int], images: List[PIL_Image.Image]\n-    ) -> LLMInput:\n+    def _model_input_from_tokens_images(self, tokens: List[int], images: List[PIL_Image.Image]) -> LLMInput:\n         vision_input = None\n         if len(images) > 0:\n             vision_input = VisionInput(\n@@ -259,9 +240,7 @@ def _model_input_from_tokens_images(\n             )\n \n         return LLMInput(\n-            tokens=[\n-                128256 if token == self.vision_token else token for token in tokens\n-            ],\n+            tokens=[128256 if token == self.vision_token else token for token in tokens],\n             vision=vision_input,\n         )\n \n@@ -270,19 +249,14 @@ def create_vision_mask(\n     tokens: List[int],\n     vision_token: int,\n ) -> List[List[int]]:\n-    vision_token_locations = [\n-        i for i, token in enumerate(tokens) if token == vision_token\n-    ]\n+    vision_token_locations = [i for i, token in enumerate(tokens) if token == vision_token]\n     if len(vision_token_locations) == 0:\n         return []\n \n     if len(vision_token_locations) == 1:\n         # only one image present, unmask until end of sequence\n         return [[vision_token_locations[0], -1]]\n-    vision_masks = [\n-        [loc1, loc2]\n-        for loc1, loc2 in zip(vision_token_locations[:-1], vision_token_locations[1:])\n-    ]\n+    vision_masks = [[loc1, loc2] for loc1, loc2 in zip(vision_token_locations[:-1], vision_token_locations[1:])]\n     # last image will attend to all subsequent text\n     vision_masks.append([vision_token_locations[-1], len(tokens)])\n \n\n--- File: models/llama3/api/datatypes.py ---\n@@ -136,9 +136,7 @@ class RawTextItem(BaseModel):\n     text: str\n \n \n-RawContentItem = Annotated[\n-    Union[RawTextItem, RawMediaItem], Field(discriminator=\"type\")\n-]\n+RawContentItem = Annotated[Union[RawTextItem, RawMediaItem], Field(discriminator=\"type\")]\n \n RawContent = str | RawContentItem | List[RawContentItem]\n \n\n--- File: models/llama3/api/interface.py ---\n@@ -182,9 +182,7 @@ def system_messages(\n             elif self.tool_prompt_format == ToolPromptFormat.function_tag:\n                 tool_gen = FunctionTagCustomToolGenerator()\n             else:\n-                raise ValueError(\n-                    f\"Non supported ToolPromptFormat {self.tool_prompt_format}\"\n-                )\n+                raise ValueError(f\"Non supported ToolPromptFormat {self.tool_prompt_format}\")\n \n             custom_template = tool_gen.gen(custom_tools)\n             messages.append(RawMessage(role=\"user\", content=custom_template.render()))\n\n--- File: models/llama3/api/template_data.py ---\n@@ -106,6 +106,4 @@ def user_images():\n \n \n def user_interleaved_images():\n-    return {\n-        \"content\": \"<|image|>Describe the image in one sentence.<|image|>Write a haiku about these images\"\n-    }\n+    return {\"content\": \"<|image|>Describe the image in one sentence.<|image|>Write a haiku about these images\"}\n\n--- File: models/llama3/api/test_tokenizer.py ---\n@@ -50,9 +50,7 @@ def test_encode_message(self):\n             content=\"This is a test sentence.\",\n         )\n         self.assertEqual(\n-            self.format.encode_message(\n-                message, tool_prompt_format=ToolPromptFormat.json\n-            )[0],\n+            self.format.encode_message(message, tool_prompt_format=ToolPromptFormat.json)[0],\n             [\n                 128006,  # <|start_header_id|>\n                 882,  # \"user\"\n\n--- File: models/llama3/api/tokenizer.py ---\n@@ -60,9 +60,7 @@ def get_instance(cls):\n         global _INSTANCE\n \n         if _INSTANCE is None:\n-            _INSTANCE = Tokenizer(\n-                os.path.join(os.path.dirname(__file__), \"tokenizer.model\")\n-            )\n+            _INSTANCE = Tokenizer(os.path.join(os.path.dirname(__file__), \"tokenizer.model\"))\n         return _INSTANCE\n \n     def __init__(self, model_path: str):\n@@ -91,14 +89,11 @@ def __init__(self, model_path: str):\n             \"<|image|>\",\n         ]\n         reserved_tokens = [\n-            f\"<|reserved_special_token_{2 + i}|>\"\n-            for i in range(self.num_reserved_special_tokens - len(special_tokens))\n+            f\"<|reserved_special_token_{2 + i}|>\" for i in range(self.num_reserved_special_tokens - len(special_tokens))\n         ]\n         special_tokens = special_tokens + reserved_tokens\n \n-        self.special_tokens = {\n-            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n-        }\n+        self.special_tokens = {token: num_base_tokens + i for i, token in enumerate(special_tokens)}\n         self.model = tiktoken.Encoding(\n             name=Path(model_path).name,\n             pat_str=self.pat_str,\n@@ -190,9 +185,7 @@ def decode(self, t: Sequence[int]) -> str:\n         return self.model.decode(cast(List[int], t))\n \n     @staticmethod\n-    def _split_whitespaces_or_nonwhitespaces(\n-        s: str, max_consecutive_slice_len: int\n-    ) -> Iterator[str]:\n+    def _split_whitespaces_or_nonwhitespaces(s: str, max_consecutive_slice_len: int) -> Iterator[str]:\n         \"\"\"\n         Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n         consecutive whitespaces or consecutive non-whitespaces.\n\n--- File: models/llama3/api/tool_utils.py ---\n@@ -12,9 +12,7 @@\n from .datatypes import BuiltinTool, RecursiveType, ToolCall, ToolPromptFormat\n \n BUILTIN_TOOL_PATTERN = r'\\b(?P<tool_name>\\w+)\\.call\\(query=\"(?P<query>[^\"]*)\"\\)'\n-CUSTOM_TOOL_CALL_PATTERN = re.compile(\n-    r\"<function=(?P<function_name>[^}]+)>(?P<args>{.*?})\"\n-)\n+CUSTOM_TOOL_CALL_PATTERN = re.compile(r\"<function=(?P<function_name>[^}]+)>(?P<args>{.*?})\")\n \n \n def is_json(s):\n@@ -56,9 +54,7 @@ def is_valid_python_list(input_string):\n                 return False\n \n             # Check if all arguments are keyword arguments\n-            if element.args or not all(\n-                isinstance(arg, ast.keyword) for arg in element.keywords\n-            ):\n+            if element.args or not all(isinstance(arg, ast.keyword) for arg in element.keywords):\n                 return False\n \n         return True\n@@ -77,9 +73,7 @@ def parse_python_list_for_function_calls(input_string):\n     tree = ast.parse(input_string)\n \n     # Ensure the input is a list\n-    if not isinstance(tree.body[0], ast.Expr) or not isinstance(\n-        tree.body[0].value, ast.List\n-    ):\n+    if not isinstance(tree.body[0], ast.Expr) or not isinstance(tree.body[0].value, ast.List):\n         raise ValueError(\"Input must be a list of function calls\")\n \n     result = []\n@@ -134,14 +128,10 @@ def maybe_extract_custom_tool_call(message_body: str) -> Optional[Tuple[str, str\n             try:\n                 return tool_name, json.loads(query.replace(\"'\", '\"'))\n             except Exception as e:\n-                print(\n-                    \"Exception while parsing json query for custom tool call\", query, e\n-                )\n+                print(\"Exception while parsing json query for custom tool call\", query, e)\n         elif is_json(message_body):\n             response = json.loads(message_body)\n-            if (\"type\" in response and response[\"type\"] == \"function\") or (\n-                \"name\" in response\n-            ):\n+            if (\"type\" in response and response[\"type\"] == \"function\") or (\"name\" in response):\n                 function_name = response[\"name\"]\n                 args = response[\"parameters\"]\n                 return function_name, args\n@@ -196,7 +186,5 @@ def format_value(value: RecursiveType) -> str:\n                     else:\n                         raise ValueError(f\"Unsupported type: {type(value)}\")\n \n-                args_str = \", \".join(\n-                    f\"{k}={format_value(v)}\" for k, v in t.arguments.items()\n-                )\n+                args_str = \", \".join(f\"{k}={format_value(v)}\" for k, v in t.arguments.items())\n                 return f\"[{fname}({args_str})]\"\n\n--- File: models/llama3/prompt_templates/base.py ---\n@@ -1,3 +1,10 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n from dataclasses import dataclass\n from typing import Any, Dict, List\n \n\n--- File: models/llama3/prompt_templates/tool_response.py ---\n@@ -1,11 +1,17 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n import textwrap\n from typing import Optional\n \n from .base import PromptTemplate, PromptTemplateGeneratorBase\n \n \n class ToolResponseGenerator(PromptTemplateGeneratorBase):\n-\n     def gen(\n         self,\n         status: str,\n\n--- File: models/llama3/reference_impl/generation.py ---\n@@ -68,7 +68,7 @@ def build(\n         model_parallel_size: Optional[int] = None,\n         tokenizer_path: Optional[str] = None,\n         seed: int = 1,\n-        device: str = \"cuda\"\n+        device: str = \"cuda\",\n     ):\n         \"\"\"\n         Build a Llama instance by initializing and loading a model checkpoint.\n@@ -97,8 +97,12 @@ def build(\n         \"\"\"\n \n         device = torch.device(device)\n-        if (device.type == \"cuda\" and not torch.cuda.is_available() or\n-            device.type == \"xpu\" and not torch.xpu.is_available()):\n+        if (\n+            device.type == \"cuda\"\n+            and not torch.cuda.is_available()\n+            or device.type == \"xpu\"\n+            and not torch.xpu.is_available()\n+        ):\n             raise RuntimeError(f\"PyTorch backend for {device.type} device type is not available\")\n \n         if not torch.distributed.is_initialized():\n@@ -127,9 +131,9 @@ def build(\n \n         checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n         assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n-        assert model_parallel_size == len(\n-            checkpoints\n-        ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n+        assert model_parallel_size == len(checkpoints), (\n+            f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n+        )\n         ckpt_path = checkpoints[get_model_parallel_rank()]\n         checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n         with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n@@ -193,10 +197,7 @@ def generate(\n         params = self.model.params\n \n         if print_model_input:\n-            tokens_to_print = [\n-                self.formatter.vision_token if t == 128256 else t\n-                for t in model_input.tokens\n-            ]\n+            tokens_to_print = [self.formatter.vision_token if t == 128256 else t for t in model_input.tokens]\n             cprint(\n                 \"Input to model:\\n\" + self.tokenizer.decode(tokens_to_print) + \"\\n\",\n                 \"red\",\n@@ -210,9 +211,7 @@ def generate(\n         max_prompt_len = max(len(t) for t in prompt_tokens)\n \n         if max_prompt_len >= params.max_seq_len:\n-            cprint(\n-                f\"Out of token budget {max_prompt_len} vs {params.max_seq_len}\", \"red\"\n-            )\n+            cprint(f\"Out of token budget {max_prompt_len} vs {params.max_seq_len}\", \"red\")\n             return\n \n         total_len = min(max_gen_len + max_prompt_len, params.max_seq_len)\n@@ -223,12 +222,10 @@ def generate(\n             mask = model_input.vision.mask if model_input.vision is not None else []\n \n             # the method works for bsz > 1 so add a batch dimension\n-            xattn_caches, cross_attention_masks, full_text_row_masked_out_mask = (\n-                self.model.compute_vision_tokens_masks(\n-                    batch_images=[images],\n-                    batch_masks=[mask],\n-                    total_len=total_len,\n-                )\n+            xattn_caches, cross_attention_masks, full_text_row_masked_out_mask = self.model.compute_vision_tokens_masks(\n+                batch_images=[images],\n+                batch_masks=[mask],\n+                total_len=total_len,\n             )\n \n         pad_id = self.tokenizer.pad_id\n@@ -247,17 +244,13 @@ def generate(\n                 yield TokenResult(\n                     token=t,\n                     text=self.tokenizer.decode([t]),\n-                    logprobs=(\n-                        token_logprobs[0, i : i + 1].tolist() if logprobs else None\n-                    ),\n+                    logprobs=(token_logprobs[0, i : i + 1].tolist() if logprobs else None),\n                 )\n \n         stop_tokens = torch.tensor(self.tokenizer.stop_tokens)\n         for cur_pos in range(min_prompt_len, total_len):\n             if is_vision:\n-                position_ids = torch.arange(\n-                    prev_pos, cur_pos, dtype=torch.long\n-                )\n+                position_ids = torch.arange(prev_pos, cur_pos, dtype=torch.long)\n                 text_only_inference = model_input.vision is None\n                 logits = self.model.forward(\n                     position_ids,\n@@ -278,9 +271,7 @@ def generate(\n \n             next_token = next_token.reshape(-1)\n             # only replace token if prompt has already been generated\n-            next_token = torch.where(\n-                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n-            )\n+            next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n             tokens[:, cur_pos] = next_token\n \n             target = tokens[:, prev_pos + 1 : cur_pos + 1]\n@@ -303,17 +294,11 @@ def generate(\n                     reduction=\"none\",\n                     ignore_index=pad_id,\n                 )\n-            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n-                torch.isin(next_token, stop_tokens)\n-            )\n+            eos_reached |= (~input_text_mask[:, cur_pos]) & (torch.isin(next_token, stop_tokens))\n             yield TokenResult(\n                 token=next_token[0].item(),\n                 text=self.tokenizer.decode(next_token.tolist()),\n-                logprobs=(\n-                    token_logprobs[:, cur_pos : cur_pos + 1][0].tolist()\n-                    if logprobs\n-                    else None\n-                ),\n+                logprobs=(token_logprobs[:, cur_pos : cur_pos + 1][0].tolist() if logprobs else None),\n             )\n \n             prev_pos = cur_pos\n@@ -329,11 +314,7 @@ def text_completion(\n         logprobs: bool = False,\n         echo: bool = False,\n     ) -> CompletionPrediction:\n-        if (\n-            max_gen_len is None\n-            or max_gen_len == 0\n-            or max_gen_len >= self.model.params.max_seq_len\n-        ):\n+        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n             max_gen_len = self.model.params.max_seq_len - 1\n \n         model_input = self.formatter.encode_content(content)\n@@ -374,11 +355,7 @@ def chat_completion(\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n         echo: bool = False,\n     ) -> ChatPrediction:\n-        if (\n-            max_gen_len is None\n-            or max_gen_len == 0\n-            or max_gen_len >= self.model.params.max_seq_len\n-        ):\n+        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n             max_gen_len = self.model.params.max_seq_len - 1\n \n         tokens = []\n@@ -387,9 +364,7 @@ def chat_completion(\n \n         stop_reason = None\n         for result in self.generate(\n-            model_input=self.formatter.encode_dialog_prompt(\n-                messages, tool_prompt_format\n-            ),\n+            model_input=self.formatter.encode_dialog_prompt(messages, tool_prompt_format),\n             max_gen_len=max_gen_len,\n             temperature=temperature,\n             top_p=top_p,\n@@ -428,11 +403,7 @@ def chat_completion_raw(\n         max_gen_len: Optional[int] = None,\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n     ) -> List[int]:\n-        if (\n-            max_gen_len is None\n-            or max_gen_len == 0\n-            or max_gen_len >= self.model.params.max_seq_len\n-        ):\n+        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n             max_gen_len = self.model.params.max_seq_len - 1\n \n         output_tokens = []\n@@ -456,11 +427,7 @@ def text_completion_raw(\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n     ):\n-        if (\n-            max_gen_len is None\n-            or max_gen_len == 0\n-            or max_gen_len >= self.model.params.max_seq_len\n-        ):\n+        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:\n             max_gen_len = self.model.params.max_seq_len - 1\n \n         model_input = self.formatter.encode_content(content)\n\n--- File: models/llama3/reference_impl/model.py ---\n@@ -54,19 +54,15 @@ def apply_scaling(freqs: torch.Tensor) -> torch.Tensor:\n \n     wavelen = 2 * torch.pi / freqs\n     new_freqs = torch.where(wavelen > low_freq_wavelen, freqs / scale_factor, freqs)\n-    smooth = (old_context_len / wavelen - low_freq_factor) / (\n-        high_freq_factor - low_freq_factor\n-    )\n+    smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n     return torch.where(\n         (wavelen >= high_freq_wavelen) & (wavelen <= low_freq_wavelen),\n         (1 - smooth) * new_freqs / scale_factor + smooth * new_freqs,\n         new_freqs,\n     )\n \n \n-def precompute_freqs_cis(\n-    dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False\n-):\n+def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False):\n     freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n     t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n     if use_scaled:\n@@ -191,18 +187,12 @@ def forward(\n         values = self.cache_v[:bsz, : start_pos + seqlen]\n \n         # repeat k/v heads if n_kv_heads < n_heads\n-        keys = repeat_kv(\n-            keys, self.n_rep\n-        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n-        values = repeat_kv(\n-            values, self.n_rep\n-        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n+        keys = repeat_kv(keys, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n+        values = repeat_kv(values, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n \n         xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n         keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n-        values = values.transpose(\n-            1, 2\n-        )  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n+        values = values.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n         scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n         if mask is not None:\n             scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n@@ -227,15 +217,9 @@ def __init__(\n             hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n         hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n \n-        self.w1 = ColumnParallelLinear(\n-            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n-        )\n-        self.w2 = RowParallelLinear(\n-            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n-        )\n-        self.w3 = ColumnParallelLinear(\n-            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n-        )\n+        self.w1 = ColumnParallelLinear(dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x)\n+        self.w2 = RowParallelLinear(hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x)\n+        self.w3 = ColumnParallelLinear(dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x)\n \n     def forward(self, x):\n         return self.w2(F.silu(self.w1(x)) * self.w3(x))\n@@ -277,18 +261,14 @@ def __init__(self, params: ModelArgs):\n         self.vocab_size = params.vocab_size\n         self.n_layers = params.n_layers\n \n-        self.tok_embeddings = VocabParallelEmbedding(\n-            params.vocab_size, params.dim, init_method=lambda x: x\n-        )\n+        self.tok_embeddings = VocabParallelEmbedding(params.vocab_size, params.dim, init_method=lambda x: x)\n \n         self.layers = torch.nn.ModuleList()\n         for layer_id in range(params.n_layers):\n             self.layers.append(TransformerBlock(layer_id, params))\n \n         self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n-        self.output = ColumnParallelLinear(\n-            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n-        )\n+        self.output = ColumnParallelLinear(params.dim, params.vocab_size, bias=False, init_method=lambda x: x)\n \n         self.freqs_cis = precompute_freqs_cis(\n             params.dim // params.n_heads,\n@@ -320,9 +300,7 @@ def forward(self, tokens: torch.Tensor, start_pos: int):\n             # only for the new sequence. Thus, the matrix of scores is of size\n             # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n             # j > cache_len + i, since row i corresponds to token cache_len + i.\n-            mask = torch.hstack(\n-                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n-            ).type_as(h)\n+            mask = torch.hstack([torch.zeros((seqlen, start_pos), device=tokens.device), mask]).type_as(h)\n \n         for layer in self.layers:\n             h = layer(h, start_pos, freqs_cis, mask)\n\n--- File: models/llama3/reference_impl/multimodal/encoder_utils.py ---\n@@ -31,30 +31,22 @@ def resize_local_position_embedding(orig_pos_embed, grid_size):\n         orig_pos_embed[:1],\n         orig_pos_embed[1:],\n     )\n-    logger.info(\n-        f\"resizing position embedding grid-size from {orig_grid_size} to {new_grid_size}\"\n-    )\n+    logger.info(f\"resizing position embedding grid-size from {orig_grid_size} to {new_grid_size}\")\n \n-    new_pos_emb_img = new_pos_emb_img.reshape(\n-        1, orig_grid_size[0], orig_grid_size[1], -1\n-    ).permute(0, 3, 1, 2)\n+    new_pos_emb_img = new_pos_emb_img.reshape(1, orig_grid_size[0], orig_grid_size[1], -1).permute(0, 3, 1, 2)\n \n     new_pos_emb_img = F.interpolate(\n         new_pos_emb_img,\n         size=new_grid_size,\n         mode=\"bilinear\",\n         align_corners=True,\n     )\n-    new_pos_emb_img = new_pos_emb_img.permute(0, 2, 3, 1).reshape(\n-        1, new_grid_size[0] * new_grid_size[1], -1\n-    )[0]\n+    new_pos_emb_img = new_pos_emb_img.permute(0, 2, 3, 1).reshape(1, new_grid_size[0] * new_grid_size[1], -1)[0]\n     new_pos_embed = torch.cat([new_pos_emb_tok, new_pos_emb_img], dim=0)\n     return new_pos_embed\n \n \n-def initialize_global_position_embedding_from_local(\n-    pos_and_cls_embed, grid_size, x_scale, y_scale\n-):\n+def initialize_global_position_embedding_from_local(pos_and_cls_embed, grid_size, x_scale, y_scale):\n     \"\"\"\n     Takes a local position embedding for vision encoder and uses it\n     to initialize the global position embedding.\n@@ -65,9 +57,7 @@ def initialize_global_position_embedding_from_local(\n     pos_embed = pos_and_cls_embed[1:]\n     cls_embed = pos_and_cls_embed[0].view(1, 1, 1, -1)\n     grid_size = to_2tuple(grid_size)\n-    new_pos_emb_img = pos_embed.reshape(1, grid_size[0], grid_size[1], -1).permute(\n-        0, 3, 1, 2\n-    )\n+    new_pos_emb_img = pos_embed.reshape(1, grid_size[0], grid_size[1], -1).permute(0, 3, 1, 2)\n     new_grid_size = (x_scale * grid_size[0], y_scale * grid_size[1])\n     new_pos_emb_img = F.interpolate(\n         new_pos_emb_img,\n@@ -76,13 +66,9 @@ def initialize_global_position_embedding_from_local(\n         align_corners=True,\n     )\n     new_pos_emb_img = new_pos_emb_img.permute(0, 2, 3, 1)\n-    new_pos_emb_img = new_pos_emb_img.view(\n-        x_scale, grid_size[0], y_scale, grid_size[1], -1\n-    )\n+    new_pos_emb_img = new_pos_emb_img.view(x_scale, grid_size[0], y_scale, grid_size[1], -1)\n     new_pos_emb_img = new_pos_emb_img.permute(0, 2, 1, 3, 4).contiguous()\n-    new_pos_emb_img = new_pos_emb_img.reshape(\n-        x_scale, y_scale, grid_size[0] * grid_size[1], -1\n-    )\n+    new_pos_emb_img = new_pos_emb_img.reshape(x_scale, y_scale, grid_size[0] * grid_size[1], -1)\n     cls_embed = cls_embed.expand(x_scale, y_scale, -1, -1)\n     pos_and_cls_embed = torch.cat([cls_embed, new_pos_emb_img], dim=2)\n     return pos_and_cls_embed\n\n--- File: models/llama3/reference_impl/multimodal/image_transform.py ---\n@@ -91,9 +91,7 @@ def get_factors(n: int) -> Set[int]:\n                 factors_set.add(n // i)\n         return factors_set\n \n-    def find_supported_resolutions(\n-        self, max_num_chunks: int, patch_size: int\n-    ) -> torch.Tensor:\n+    def find_supported_resolutions(self, max_num_chunks: int, patch_size: int) -> torch.Tensor:\n         \"\"\"\n         Computes all of the allowed resoltuions for a fixed number of chunks\n         and patch_size. Useful for when dividing an image into chunks.\n@@ -244,15 +242,11 @@ def resize_without_distortion(\n         # If target_size requires upscaling, we might want to limit the upscaling to max_upscaling_size\n         if max_upscaling_size is not None:\n             new_target_width = min(max(image_width, max_upscaling_size), target_size[0])\n-            new_target_height = min(\n-                max(image_height, max_upscaling_size), target_size[1]\n-            )\n+            new_target_height = min(max(image_height, max_upscaling_size), target_size[1])\n             target_size = (new_target_width, new_target_height)\n \n         # resize to target_size while preserving aspect ratio\n-        new_size_without_distortion = self.get_max_res_without_distortion(\n-            image_size, target_size\n-        )\n+        new_size_without_distortion = self.get_max_res_without_distortion(image_size, target_size)\n \n         image = F.resize(\n             image,\n@@ -382,9 +376,7 @@ def __call__(\n         assert isinstance(image, Image.Image), type(image)\n         w, h = image.size\n \n-        possible_resolutions = self.find_supported_resolutions(\n-            max_num_chunks=max_num_chunks, patch_size=self.size\n-        )\n+        possible_resolutions = self.find_supported_resolutions(max_num_chunks=max_num_chunks, patch_size=self.size)\n         possible_resolutions = torch.tensor(possible_resolutions)\n \n         best_resolution = self.get_best_fit(\n@@ -394,9 +386,7 @@ def __call__(\n         )\n \n         max_upscaling_size = None if resize_to_max_canvas else self.size\n-        image = self.resize_without_distortion(\n-            image, best_resolution, max_upscaling_size\n-        )\n+        image = self.resize_without_distortion(image, best_resolution, max_upscaling_size)\n         image = self._pad(image, best_resolution)\n \n         image = self.to_tensor(image)\n\n--- File: models/llama3/reference_impl/multimodal/model.py ---\n@@ -186,9 +186,7 @@ def __init__(\n \n         self.n_kv_heads = n_heads\n         self.n_local_heads = n_heads * qkvo_replication // model_parallel_size\n-        self.n_local_kv_heads = (\n-            self.n_kv_heads * qkvo_replication // model_parallel_size\n-        )\n+        self.n_local_kv_heads = self.n_kv_heads * qkvo_replication // model_parallel_size\n         self.n_rep = self.n_local_heads // self.n_local_kv_heads\n         self.head_dim = dim // n_heads\n \n@@ -227,10 +225,7 @@ def forward(\n         x: torch.Tensor,\n         mask: torch.Tensor = None,\n     ):\n-\n-        xq, xk, xv = [\n-            F.linear(x, w) for w in [self.wq.weight, self.wk.weight, self.wv.weight]\n-        ]\n+        xq, xk, xv = [F.linear(x, w) for w in [self.wq.weight, self.wk.weight, self.wv.weight]]\n \n         bs, slen, _ = xq.shape\n \n@@ -243,9 +238,7 @@ def forward(\n         xk = xk.repeat_interleave(self.n_rep, dim=1)\n         xv = xv.repeat_interleave(self.n_rep, dim=1)\n \n-        attn_output = F.scaled_dot_product_attention(\n-            xq, xk, xv, attn_mask=mask, dropout_p=0.0\n-        )\n+        attn_output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=mask, dropout_p=0.0)\n \n         attn_output = attn_output.transpose(1, 2).contiguous().reshape(bs, slen, -1)\n \n@@ -372,14 +365,10 @@ def __init__(\n         )\n         scale = width**-0.5\n         self.class_embedding = nn.Parameter(scale * torch.randn(width))\n-        self.positional_embedding = nn.Parameter(\n-            scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width)\n-        )\n+        self.positional_embedding = nn.Parameter(scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width))\n         self.ln_post = LayerNorm(width)\n         self.ln_pre = LayerNorm(width)\n-        self.transformer = ImageTransformer(\n-            width, layers, heads, mlp_ratio, act_layer=act_layer\n-        )\n+        self.transformer = ImageTransformer(width, layers, heads, mlp_ratio, act_layer=act_layer)\n         # pre and post tile position embedding\n         self.global_transformer = ImageTransformer(\n             width, n_global_layers, heads, mlp_ratio, act_layer=act_layer, gated=True\n@@ -421,9 +410,7 @@ def load_hook(\n     ) -> None:\n         orig_pos_embed = state_dict.get(prefix + \"positional_embedding\")\n         if orig_pos_embed is not None:\n-            new_pos_embed = resize_local_position_embedding(\n-                orig_pos_embed, self.grid_size\n-            )\n+            new_pos_embed = resize_local_position_embedding(orig_pos_embed, self.grid_size)\n             state_dict[prefix + \"positional_embedding\"] = new_pos_embed\n         if hasattr(self, \"gated_positional_embedding\"):\n             if prefix + \"gated_positional_embedding\" not in state_dict:\n@@ -435,12 +422,8 @@ def load_hook(\n                     self.max_num_tiles,\n                 )\n                 state_dict[prefix + \"gated_positional_embedding\"] = global_pos_embed\n-                state_dict[prefix + \"gated_positional_embedding_gate\"] = torch.zeros(\n-                    1, dtype=global_pos_embed.dtype\n-                )\n-                logger.info(\n-                    f\"Initialized global positional embedding with size {global_pos_embed.size()}\"\n-                )\n+                state_dict[prefix + \"gated_positional_embedding_gate\"] = torch.zeros(1, dtype=global_pos_embed.dtype)\n+                logger.info(f\"Initialized global positional embedding with size {global_pos_embed.size()}\")\n             else:\n                 global_pos_embed = resize_global_position_embedding(\n                     state_dict[prefix + \"gated_positional_embedding\"],\n@@ -460,25 +443,19 @@ def apply_positional_embedding(self, x, ar):\n         # apply regular position embedding\n         bsz, num_chunks, num_tokens, dim = x.shape\n         x = x.view(bsz * num_chunks, num_tokens, dim)\n-        x = x + self.positional_embedding * (\n-            1 - self.gated_positional_embedding_gate.tanh()\n-        )\n+        x = x + self.positional_embedding * (1 - self.gated_positional_embedding_gate.tanh())\n         x = x.view(bsz, num_chunks, num_tokens, dim)\n         for idx, arx in enumerate(ar):\n             _pos_embed = self.gated_positional_embedding[: arx[0], : arx[1]]\n             _pos_embed = _pos_embed.reshape(arx[0] * arx[1], *_pos_embed.shape[2:])\n-            x[idx, : arx[0] * arx[1]] += (\n-                _pos_embed * self.gated_positional_embedding_gate.tanh()\n-            )\n+            x[idx, : arx[0] * arx[1]] += _pos_embed * self.gated_positional_embedding_gate.tanh()\n         return x\n \n     def apply_class_embedding(self, x):\n         x = torch.cat(\n             [\n                 self.class_embedding.to(x.dtype)\n-                + torch.zeros(\n-                    x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device\n-                ),\n+                + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device),\n                 x,\n             ],\n             dim=1,\n@@ -518,9 +495,7 @@ def forward(self, images: torch.Tensor, ar: torch.Tensor) -> torch.Tensor:\n         x, npad = expand_num_tokens_to_mult8(x)\n         attn_mask = build_encoder_attention_mask(x, ar, ntok, num_chunks, 1)\n         x = x.view(bsz * num_concurrent_media, -1, dim)\n-        x, int_x = self.transformer(\n-            x, return_intermediate=self.return_intermediate, mask=attn_mask\n-        )\n+        x, int_x = self.transformer(x, return_intermediate=self.return_intermediate, mask=attn_mask)\n \n         x = self.ln_post(x)\n         x = x.reshape(bsz * num_concurrent_media, num_chunks, ntok + npad, dim)\n@@ -639,10 +614,7 @@ def forward(\n         freqs_cis: torch.Tensor,\n         position_ids: torch.LongTensor,\n     ):\n-\n-        xq, xk, xv = [\n-            F.linear(x, w) for w in [self.wq.weight, self.wk.weight, self.wv.weight]\n-        ]\n+        xq, xk, xv = [F.linear(x, w) for w in [self.wq.weight, self.wk.weight, self.wv.weight]]\n \n         bs, slen, _ = xq.shape\n \n@@ -664,9 +636,7 @@ def forward(\n         xk = xk.repeat_interleave(self.n_rep, dim=1)\n         xv = xv.repeat_interleave(self.n_rep, dim=1)\n \n-        attn_output = F.scaled_dot_product_attention(\n-            xq, xk, xv, attn_mask=mask, dropout_p=0.0\n-        )\n+        attn_output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=mask, dropout_p=0.0)\n \n         attn_output = attn_output.transpose(1, 2).contiguous().reshape(bs, slen, -1)\n \n@@ -702,15 +672,9 @@ def __init__(\n             hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n         hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n \n-        self.w1 = ColumnParallelLinear(\n-            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n-        )\n-        self.w2 = RowParallelLinear(\n-            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n-        )\n-        self.w3 = ColumnParallelLinear(\n-            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n-        )\n+        self.w1 = ColumnParallelLinear(dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x)\n+        self.w2 = RowParallelLinear(hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x)\n+        self.w3 = ColumnParallelLinear(dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x)\n \n     def forward(self, x):\n         x1, x3 = [F.linear(x, w) for w in [self.w1.weight, self.w3.weight]]\n@@ -794,9 +758,7 @@ def __init__(\n         super().__init__()\n         self.num_tiles = num_tiles\n         self.width = width\n-        self.embedding = nn.Parameter(\n-            torch.randn(num_tiles, num_tiles, 1, width) / math.sqrt(width)\n-        )\n+        self.embedding = nn.Parameter(torch.randn(num_tiles, num_tiles, 1, width) / math.sqrt(width))\n         self.gated = gated\n         if gated:\n             self.gate = nn.Parameter(torch.zeros(1))\n@@ -818,9 +780,7 @@ def load_hook(\n         if embed is not None:\n             # reshape the weights to the correct shape\n             nt_old, nt_old, _, w = embed.shape\n-            logging.info(\n-                f\"Resizing tile embedding from {nt_old}x{nt_old} to {self.num_tiles}x{self.num_tiles}\"\n-            )\n+            logging.info(f\"Resizing tile embedding from {nt_old}x{nt_old} to {self.num_tiles}x{self.num_tiles}\")\n             embed_new = TilePositionEmbedding._dynamic_resize(embed, self.num_tiles)\n             # assign the weights to the module\n             state_dict[prefix + \"embedding\"] = embed_new\n@@ -846,9 +806,7 @@ def forward(self, x: torch.Tensor, ar: torch.Tensor, num_tiles: int = None):\n             num_tiles = self.num_tiles\n         elif num_tiles > self.num_tiles:\n             embed = TilePositionEmbedding._dynamic_resize(self.embedding, num_tiles)\n-        out_pos_embed = torch.zeros(\n-            x.shape[0], num_tiles, 1, self.width, device=x.device, dtype=x.dtype\n-        )\n+        out_pos_embed = torch.zeros(x.shape[0], num_tiles, 1, self.width, device=x.device, dtype=x.dtype)\n         for idx, arx in enumerate(ar):\n             h, w = arx\n             out_pos_embed[idx, : w * h] = embed[:h, :w].reshape(w * h, 1, self.width)\n@@ -976,9 +934,7 @@ def forward(\n \n         xk, xv = xattn_cache\n \n-        output = F.scaled_dot_product_attention(\n-            xq, xk, xv, attn_mask=xattn_mask, dropout_p=0.0\n-        )\n+        output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=xattn_mask, dropout_p=0.0)\n         output = output * full_text_row_masked_out_mask\n         output = output.transpose(1, 2).contiguous().reshape(bsz, seqlen, -1)\n \n@@ -1086,9 +1042,7 @@ def __init__(self, args: ModelArgs) -> None:\n         self.max_num_chunks = args.vision_max_num_chunks\n         if return_intermediate is not None:\n             return_intermediate = [int(l) for l in return_intermediate.split(\",\")]\n-            self.vision_input_dim = (\n-                len(return_intermediate) + 1\n-            ) * self.vision_input_dim\n+            self.vision_input_dim = (len(return_intermediate) + 1) * self.vision_input_dim\n         self.patch_size = 14\n         self.vision_encoder = VisionEncoder(\n             max_num_tiles=4,\n@@ -1106,19 +1060,13 @@ def __init__(self, args: ModelArgs) -> None:\n             init_method=lambda x: x,\n         )\n \n-    def forward(\n-        self, images: torch.Tensor, aspect_ratios: torch.Tensor\n-    ) -> torch.Tensor:\n+    def forward(self, images: torch.Tensor, aspect_ratios: torch.Tensor) -> torch.Tensor:\n         # vision_tokens: (B, T, D)\n         # aspect_ratios: (B, T)\n         # h: (B, T, D)\n-        vision_tokens = self.vision_encoder(\n-            images.to(dtype=torch.get_default_dtype()), aspect_ratios\n-        )\n+        vision_tokens = self.vision_encoder(images.to(dtype=torch.get_default_dtype()), aspect_ratios)\n \n-        vision_tokens = F.linear(\n-            vision_tokens, self.vision_projection.weight, self.vision_projection.bias\n-        )\n+        vision_tokens = F.linear(vision_tokens, self.vision_projection.weight, self.vision_projection.bias)\n         vision_tokens = gather_from_tensor_model_parallel_region(vision_tokens)\n         return vision_tokens\n \n@@ -1137,26 +1085,20 @@ def __init__(self, args: ModelArgs) -> None:\n         self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n         self.n_local_kv_heads = self.n_kv_heads // self.model_parallel_size\n         assert self.vocab_size % self.model_parallel_size == 0\n-        self.tok_embeddings = VocabParallelEmbedding(\n-            args.vocab_size, args.dim, init_method=lambda x: x\n-        )\n+        self.tok_embeddings = VocabParallelEmbedding(args.vocab_size, args.dim, init_method=lambda x: x)\n         self.pos_embeddings = None\n         # final norm layer (not necessary for post-norm)\n         self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n \n         # output layer\n-        self.output = ColumnParallelLinear(\n-            args.dim, args.vocab_size, bias=False, init_method=lambda x: x\n-        )\n+        self.output = ColumnParallelLinear(args.dim, args.vocab_size, bias=False, init_method=lambda x: x)\n \n         self.n_llama_layers = args.n_layers\n         self.model_dim = args.dim\n \n         # BLOCKS\n \n-        self.fusion_schedule = self._init_fusion_schedule(\n-            args.vision_num_cross_attention_layers\n-        )\n+        self.fusion_schedule = self._init_fusion_schedule(args.vision_num_cross_attention_layers)\n         self.learnable_embedding = VocabParallelEmbedding(\n             max(fs_init.get_model_parallel_world_size(), 8),\n             args.dim,\n@@ -1223,10 +1165,7 @@ def get_partially_trainable_embedding(self, x):\n         xz = torch.zeros_like(x, device=x.device)\n         oz = torch.ones_like(x, device=x.device)\n         x_orig = torch.minimum(x, torch.tensor(self._thresh, device=x.device))\n-        x_new = (\n-            torch.maximum(x, torch.tensor(self._thresh + 1, device=x.device))\n-            - self.num_frozen_embeddings\n-        )\n+        x_new = torch.maximum(x, torch.tensor(self._thresh + 1, device=x.device)) - self.num_frozen_embeddings\n \n         mask_orig = torch.where(x >= self.num_frozen_embeddings, xz, oz).unsqueeze(-1)\n         mask_new = torch.where(x < self.num_frozen_embeddings, xz, oz).unsqueeze(-1)\n@@ -1304,21 +1243,19 @@ def _get_xattn_mask(\n     ) -> Tuple[Tensor, Tensor]:\n         assert vision_tokens is not None, \"Vision tokens must be provided\"\n         vision_seqlen = vision_tokens.shape[3]\n-        assert (\n-            vision_tokens.shape[1] == cross_attention_masks.shape[2]\n-        ), f\"Mismatch in number of images given and number of masks given {vision_tokens.shape} {cross_attention_masks.shape}\"\n-        assert (\n-            vision_tokens.shape[2] == cross_attention_masks.shape[3]\n-        ), f\"Vision tokens shape {vision_tokens.shape} mismatch with xattn shape {cross_attention_masks.shape}\"\n-        assert (\n-            num_tokens == cross_attention_masks.shape[1]\n-        ), f\"Mismatch in text sequence length and cross attention mask sequence length {num_tokens} {cross_attention_masks.shape}\"\n+        assert vision_tokens.shape[1] == cross_attention_masks.shape[2], (\n+            f\"Mismatch in number of images given and number of masks given {vision_tokens.shape} {cross_attention_masks.shape}\"\n+        )\n+        assert vision_tokens.shape[2] == cross_attention_masks.shape[3], (\n+            f\"Vision tokens shape {vision_tokens.shape} mismatch with xattn shape {cross_attention_masks.shape}\"\n+        )\n+        assert num_tokens == cross_attention_masks.shape[1], (\n+            f\"Mismatch in text sequence length and cross attention mask sequence length {num_tokens} {cross_attention_masks.shape}\"\n+        )\n         _, _, _, num_image_tokens, image_token_dim = tuple(vision_tokens.shape)\n         bsz, ntext, nimg, nchunks = cross_attention_masks.shape\n         cross_attention_masks = (\n-            cross_attention_masks.repeat_interleave(vision_seqlen, dim=3)\n-            .view(bsz, ntext, -1)\n-            .unsqueeze(1)\n+            cross_attention_masks.repeat_interleave(vision_seqlen, dim=3).view(bsz, ntext, -1).unsqueeze(1)\n         )\n         full_text_row_masked_out_mask = _get_full_row_masked_out_mask(\n             cross_attention_masks,\n@@ -1358,9 +1295,7 @@ def compute_vision_tokens_masks(\n     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         skip_vision_encoder = False\n \n-        assert len(batch_images) == len(\n-            batch_masks\n-        ), \"Images and masks must have the same length\"\n+        assert len(batch_images) == len(batch_masks), \"Images and masks must have the same length\"\n \n         max_num_images = max(len(x) for x in batch_images)\n         bsz = len(batch_images)\n@@ -1369,19 +1304,13 @@ def compute_vision_tokens_masks(\n             num_chunks = [[self.max_num_chunks] for _ in batch_images]\n             skip_vision_encoder = True\n         else:\n-            images_and_aspect_ratios = [\n-                [self.image_transform(im) for im in row] for row in batch_images\n-            ]\n-            transformed_images = [\n-                [x[0] for x in row] for row in images_and_aspect_ratios\n-            ]\n+            images_and_aspect_ratios = [[self.image_transform(im) for im in row] for row in batch_images]\n+            transformed_images = [[x[0] for x in row] for row in images_and_aspect_ratios]\n \n             aspect_ratios = torch.ones(bsz, max_num_images, 2, dtype=torch.int64)\n             for i, row in enumerate(images_and_aspect_ratios):\n                 if len(row) > 0:\n-                    aspect_ratios[i, : len(row)] = torch.stack(\n-                        [torch.tensor(x[1]) for x in row]\n-                    )\n+                    aspect_ratios[i, : len(row)] = torch.stack([torch.tensor(x[1]) for x in row])\n \n             stacked_images, num_chunks = _stack_images(\n                 transformed_images,\n@@ -1396,11 +1325,7 @@ def compute_vision_tokens_masks(\n                     bsz,\n                     max_num_images,\n                     self.max_num_chunks,\n-                    int(\n-                        (self.vision_model.image_res / self.vision_model.patch_size)\n-                        ** 2\n-                        + 1\n-                    ),\n+                    int((self.vision_model.image_res / self.vision_model.patch_size) ** 2 + 1),\n                     self.model_dim,\n                 ),\n             )\n@@ -1410,9 +1335,7 @@ def compute_vision_tokens_masks(\n         bsz, nimg, nchunk, ntok, image_token_dim = tuple(vision_tokens.shape)\n         xattn_caches = torch.stack(\n             [\n-                layer.compute_xattn_kv_cache(\n-                    vision_tokens.view(bsz, -1, image_token_dim)\n-                )\n+                layer.compute_xattn_kv_cache(vision_tokens.view(bsz, -1, image_token_dim))\n                 for layer in self.text_model.cross_attention_layers\n             ]\n         )\n@@ -1423,14 +1346,12 @@ def compute_vision_tokens_masks(\n             self.max_num_chunks,\n         )\n \n-        cross_attention_masks, full_text_row_masked_out_mask = (\n-            self.text_model._get_xattn_mask(\n-                num_tokens=total_len,\n-                text_device=vision_tokens.device.type,\n-                text_dtype=next(self.text_model.parameters()).dtype,\n-                vision_tokens=vision_tokens,\n-                cross_attention_masks=padded_masks,\n-            )\n+        cross_attention_masks, full_text_row_masked_out_mask = self.text_model._get_xattn_mask(\n+            num_tokens=total_len,\n+            text_device=vision_tokens.device.type,\n+            text_dtype=next(self.text_model.parameters()).dtype,\n+            vision_tokens=vision_tokens,\n+            cross_attention_masks=padded_masks,\n         )\n \n         return (xattn_caches, cross_attention_masks, full_text_row_masked_out_mask)\n@@ -1449,9 +1370,7 @@ def forward(\n             position_ids=position_ids,\n             h=h,\n             xattn_mask=cross_attention_masks[:, :, position_ids],\n-            full_text_row_masked_out_mask=full_text_row_masked_out_mask[\n-                :, :, position_ids\n-            ],\n+            full_text_row_masked_out_mask=full_text_row_masked_out_mask[:, :, position_ids],\n             xattn_caches=xattn_caches,\n             text_only_inference=text_only_inference,\n         )\n@@ -1511,8 +1430,6 @@ def _pad_masks(\n                 mask_elem[1] = min(mask_elem[1], total_len)\n                 if mask_elem[1] == -1:\n                     mask_elem[1] = total_len\n-                out_masks[\n-                    idx, mask_elem[0] : mask_elem[1], mask_idx, :mask_num_chunks\n-                ].fill_(0.0)\n+                out_masks[idx, mask_elem[0] : mask_elem[1], mask_idx, :mask_num_chunks].fill_(0.0)\n \n     return out_masks\n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -21,8 +21,8 @@\n \n \n def get_device():\n-    if 'DEVICE' in os.environ:\n-        return os.environ['DEVICE']\n+    if \"DEVICE\" in os.environ:\n+        return os.environ[\"DEVICE\"]\n \n     if torch.cuda.is_available():\n         return \"cuda\"\n@@ -40,11 +40,7 @@ def build_generator(env_var: str, device: str):\n     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n     os.environ[\"MASTER_PORT\"] = \"29501\"\n     return Llama.build(\n-        ckpt_dir=os.environ[env_var],\n-        max_seq_len=128,\n-        max_batch_size=1,\n-        model_parallel_size=1,\n-        device=device\n+        ckpt_dir=os.environ[env_var], max_seq_len=128, max_batch_size=1, model_parallel_size=1, device=device\n     )\n \n \n@@ -59,9 +55,7 @@ def test_run_generation(self):\n         dialogs = [\n             [\n                 RawMessage(role=\"system\", content=\"Always answer with Haiku\"),\n-                RawMessage(\n-                    role=\"user\", content=\"I am going to Paris, what should I see?\"\n-                ),\n+                RawMessage(role=\"user\", content=\"I am going to Paris, what should I see?\"),\n             ],\n             [\n                 RawMessage(role=\"system\", content=\"Always answer with emojis\"),\n@@ -98,9 +92,7 @@ def setUpClass(cls):\n     @unittest.skip(\"Disabling vision model test\")\n     @pytest.mark.skip(reason=\"Disabling vision model test\")\n     def test_run_generation(self):\n-        with open(\n-            THIS_DIR.parent.parent.parent / \"scripts/resources/dog.jpg\", \"rb\"\n-        ) as f:\n+        with open(THIS_DIR.parent.parent.parent / \"scripts/resources/dog.jpg\", \"rb\") as f:\n             img = f.read()\n \n         dialogs = [\n\n--- File: models/llama3/tests/api/test_tool_utils.py ---\n@@ -14,25 +14,23 @@\n \n \n class TestToolUtils(unittest.TestCase):\n-\n     def test_maybe_extract_custom_tool_call(self):\n-        single_tool_call = (\n-            \"\"\"<function=getWeather>{\"location\":\"New York\",\"date\":\"2023-08-05\"}\"\"\"\n-        )\n+        single_tool_call = \"\"\"<function=getWeather>{\"location\":\"New York\",\"date\":\"2023-08-05\"}\"\"\"\n         res = ToolUtils.maybe_extract_custom_tool_call(single_tool_call)\n         tool_name, args = res\n         assert tool_name == \"getWeather\"\n         assert args == {\"location\": \"New York\", \"date\": \"2023-08-05\"}\n \n \n class TestPythonListCheck(unittest.TestCase):\n-\n     def test_valid_list_with_single_function_call(self):\n         input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True)]'\n         assert is_valid_python_list(input_string) is True\n \n     def test_valid_list_with_multiple_function_calls(self):\n-        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True), get_melting_point(substance=\"ice\", kelvin=False)]'\n+        input_string = (\n+            '[get_boiling_point(liquid_name=\"water\", celcius=True), get_melting_point(substance=\"ice\", kelvin=False)]'\n+        )\n         assert is_valid_python_list(input_string) is True\n \n     def test_invalid_empty_list(self):\n@@ -60,9 +58,7 @@ def test_invalid_syntax(self):\n         assert is_valid_python_list(input_string) is False\n \n     def test_valid_list_with_boolean_args(self):\n-        input_string = (\n-            '[get_boiling_point(liquid_name=\"water\", celcius=True, precise=False)]'\n-        )\n+        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True, precise=False)]'\n         assert is_valid_python_list(input_string) is True\n \n     def test_valid_list_with_numeric_args(self):\n@@ -79,14 +75,15 @@ def test_invalid_extra_char_function_call(self):\n \n \n class TestParsePythonList(unittest.TestCase):\n-\n     def test_single_function_call(self):\n         input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True)]'\n         expected = [(\"get_boiling_point\", {\"liquid_name\": \"water\", \"celcius\": True})]\n         assert parse_python_list_for_function_calls(input_string) == expected\n \n     def test_multiple_function_calls(self):\n-        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True), get_melting_point(substance=\"ice\", kelvin=False)]'\n+        input_string = (\n+            '[get_boiling_point(liquid_name=\"water\", celcius=True), get_melting_point(substance=\"ice\", kelvin=False)]'\n+        )\n         expected = [\n             (\"get_boiling_point\", {\"liquid_name\": \"water\", \"celcius\": True}),\n             (\"get_melting_point\", {\"substance\": \"ice\", \"kelvin\": False}),\n@@ -99,9 +96,7 @@ def test_function_call_with_numeric_args(self):\n         assert parse_python_list_for_function_calls(input_string) == expected\n \n     def test_function_call_with_mixed_type_args(self):\n-        input_string = (\n-            '[process_data(name=\"sample\", value=42, active=True, ratio=3.14)]'\n-        )\n+        input_string = '[process_data(name=\"sample\", value=42, active=True, ratio=3.14)]'\n         expected = [\n             (\n                 \"process_data\",\n\n--- File: models/llama3/tests/prompt_templates/test_system_prompts.py ---\n@@ -19,7 +19,6 @@\n \n \n class PromptTemplateTests(unittest.TestCase):\n-\n     def check_generator_output(self, generator, expected_text):\n         example = generator.data_examples()[0]\n \n\n--- File: models/llama3_1/prompts.py ---\n@@ -98,11 +98,7 @@ def usecases() -> List[UseCase | str]:\n         UseCase(\n             title=\"Llama 3.1 Base Model\",\n             description=\"Text completion for Llama 3.1 base model uses this format.\",\n-            dialogs=[\n-                TextCompletionContent(\n-                    content=\"Color of sky is blue but sometimes can also be\"\n-                )\n-            ],\n+            dialogs=[TextCompletionContent(content=\"Color of sky is blue but sometimes can also be\")],\n             notes=\"Note start special tag\",\n         ),\n         \"## Llama 3.1 Instruct Model\",\n\n--- File: models/llama3_2/eval_details.md ---\n@@ -149,4 +149,3 @@ For post-trained models, we use 0-shot config and report scores over the test se\n ### MathVista\n \n For post-trained models, we use 0-shot config and report scores over the testmini set. Maximum generation length is 2048 tokens. We use an LLM-based answer extractor as recommended by MathVista paper [Lu et al. (2024)](https://arxiv.org/pdf/2310.02255). We use the following system prompt: \"&lt;|image|>{question}\"\n-\n\n--- File: models/llama3_2/prompts_text.py ---\n@@ -121,9 +121,7 @@ def usecases():\n                 # Zero shot tool calls as system message\n                 [\n                     RawMessage(role=\"system\", content=system_tool_call()),\n-                    RawMessage(\n-                        role=\"user\", content=\"What is the weather in SF and Seattle?\"\n-                    ),\n+                    RawMessage(role=\"user\", content=\"What is the weather in SF and Seattle?\"),\n                 ],\n             ],\n             notes=textwrap.dedent(\n@@ -218,9 +216,7 @@ def usecases():\n                 \"\"\"\n             ),\n             dialogs=[\n-                TextCompletionContent(\n-                    content=\"The color of the sky is blue but sometimes it can also be\"\n-                ),\n+                TextCompletionContent(content=\"The color of the sky is blue but sometimes it can also be\"),\n             ],\n             notes=\"Same as Llama3.1\",\n         ),\n\n--- File: models/llama3_2/prompts_vision.py ---\n@@ -98,9 +98,7 @@ def usecases():\n                 \"\"\"\n             ),\n             dialogs=[\n-                TextCompletionContent(\n-                    content=\"The color of the sky is blue but sometimes it can also be\"\n-                ),\n+                TextCompletionContent(content=\"The color of the sky is blue but sometimes it can also be\"),\n             ],\n             notes=\"- Same as Llama3.1\",\n         ),\n\n--- File: models/llama3_3/USE_POLICY.md ---\n@@ -70,4 +70,3 @@ Please report any violation of this Policy, software bug, or other problem\n * Reporting risky content generated by the model: [developers.facebook.com/llama\\_output\\_feedback](http://developers.facebook.com/llama_output_feedback)  \n * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)  \n * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.3: LlamaUseReport@meta.com\n-\n\n--- File: models/llama3_3/prompts.py ---\n@@ -98,11 +98,7 @@ def usecases() -> List[UseCase | str]:\n         UseCase(\n             title=\"Llama 3.1 Base Model\",\n             description=\"Text completion for Llama 3.1 base model uses this format.\",\n-            dialogs=[\n-                TextCompletionContent(\n-                    content=\"Color of sky is blue but sometimes can also be\"\n-                )\n-            ],\n+            dialogs=[TextCompletionContent(content=\"Color of sky is blue but sometimes can also be\")],\n             notes=\"Note start special tag\",\n         ),\n         \"## Llama 3.1 Instruct Model\",\n\n--- File: models/prompt_format.py ---\n@@ -32,9 +32,7 @@ class TextCompletionContent(BaseModel):\n class UseCase(BaseModel):\n     title: str = \"\"\n     description: str = \"\"\n-    dialogs: List[List[RawMessage] | TextCompletionContent | str] = Field(\n-        default_factory=list\n-    )\n+    dialogs: List[List[RawMessage] | TextCompletionContent | str] = Field(default_factory=list)\n     notes: str = \"\"\n     tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json\n \n@@ -82,10 +80,7 @@ def _code_block(text):\n \n             # FIXME: This is added to undo the hack in chat_formatter where\n             # vision tokens are replaced with 128256.\n-            input_tokens = [\n-                generator.formatter.vision_token if t == 128256 else t\n-                for t in input_tokens\n-            ]\n+            input_tokens = [generator.formatter.vision_token if t == 128256 else t for t in input_tokens]\n \n             text += _code_block(generator.tokenizer.decode(input_tokens))\n             # TODO: Figure out if \"\" needs to be added for newlines or end or some indication\n@@ -115,9 +110,7 @@ def llama3_1_builtin_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n     interface = LLama31Interface(tool_prompt_format)\n \n     messages = interface.system_messages(**system_message_builtin_tools_only())\n-    messages += interface.user_message(\n-        content=\"Search the web for the latest price of 1oz gold?\"\n-    )\n+    messages += interface.user_message(content=\"Search the web for the latest price of 1oz gold?\")\n \n     return messages\n \n@@ -147,16 +140,12 @@ def llama3_1_builtin_tool_call_with_image_dialog(\n     interface = LLama31Interface(tool_prompt_format)\n \n     messages = interface.system_messages(**system_message_builtin_tools_only())\n-    messages += interface.user_message(\n-        content=[RawMediaItem(data=img), RawTextItem(text=\"What is this dog breed?\")]\n-    )\n+    messages += interface.user_message(content=[RawMediaItem(data=img), RawTextItem(text=\"What is this dog breed?\")])\n     messages += interface.assistant_response_messages(\n         \"Based on the description of the dog in the image, it appears to be a small breed dog, possibly a terrier mix\",\n         StopReason.end_of_turn,\n     )\n-    messages += interface.user_message(\n-        \"Search the web for some food recommendations for the indentified breed\"\n-    )\n+    messages += interface.user_message(\"Search the web for some food recommendations for the indentified breed\")\n     return messages\n \n \n\n--- File: models/scripts/generate_prompt_format.py ---\n@@ -25,9 +25,7 @@ def run_main(\n     model_parallel_size: Optional[int] = None,\n ):\n     module = importlib.import_module(module_name)\n-    assert hasattr(\n-        module, \"usecases\"\n-    ), f\"Module {module_name} missing usecases function\"\n+    assert hasattr(module, \"usecases\"), f\"Module {module_name} missing usecases function\"\n     tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n\n--- File: models/sku_list.py ---\n@@ -30,12 +30,7 @@ def resolve_model(descriptor: str) -> Optional[Model]:\n \n def all_registered_models() -> List[Model]:\n     return (\n-        llama2_family()\n-        + llama3_family()\n-        + llama3_1_family()\n-        + llama3_2_family()\n-        + llama3_3_family()\n-        + safety_models()\n+        llama2_family() + llama3_family() + llama3_1_family() + llama3_2_family() + llama3_3_family() + safety_models()\n     )"
            },
            {
              "sha": "5bc4bae95f57b607d28d0d1a6bf8f49b83fe240c",
              "url": "https://github.com/meta-llama/llama-models/commit/5bc4bae95f57b607d28d0d1a6bf8f49b83fe240c",
              "message": "Fix source finding",
              "files_changed": [
                {
                  "filename": "pyproject.toml",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: pyproject.toml ---\n@@ -46,4 +46,4 @@ torch = [\n \n [tool.setuptools]\n package-dir = {\"llama_models\" = \"llama_models\"}\n-packages = [\"llama_models\"]\n+packages = {find = {}}"
            },
            {
              "sha": "26407383e8668fb95d7bbfbe06aeb6d15280326c",
              "url": "https://github.com/meta-llama/llama-models/commit/26407383e8668fb95d7bbfbe06aeb6d15280326c",
              "message": "Move setup.py to pyproject.toml",
              "files_changed": [
                {
                  "filename": "MANIFEST.in",
                  "status": "modified"
                },
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "pyproject.toml",
                  "status": "modified"
                },
                {
                  "filename": "requirements.txt",
                  "status": "removed"
                },
                {
                  "filename": "setup.py",
                  "status": "removed"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: MANIFEST.in ---\n@@ -1,4 +1,5 @@\n-include requirements.txt\n+include pyproject.toml\n+include README.md\n include llama_models/llama3/api/tokenizer.model\n include llama_models/scripts/resources/dog.jpg\n include llama_models/scripts/resources/pasta.jpeg\n\n--- File: README.md ---\n@@ -48,12 +48,7 @@ Remember that the links expire after 24 hours and a certain amount of downloads.\n \n ## Running the models\n \n-You need to install the following dependencies (in addition to the `requirements.txt` in the root directory of this repository) to run the models:\n-```\n-pip install torch fairscale fire blobfile\n-```\n-\n-After installing the dependencies, you can run the example scripts (within `llama_models/scripts/` sub-directory) as follows:\n+You need to `pip install llama_models[torch]` to run the models. After installing the dependencies, you can run the example scripts (within `llama_models/scripts/` sub-directory) as follows:\n ```bash\n #!/bin/bash\n \n\n--- File: pyproject.toml ---\n@@ -1,3 +1,49 @@\n [build-system]\n requires = [\"setuptools>=61.0\"]\n build-backend = \"setuptools.build_meta\"\n+\n+[project]\n+name = \"llama_models\"\n+version = \"0.1.0\"\n+authors = [\n+    {name = \"Meta Llama\", email = \"llama-oss@meta.com\"},\n+]\n+description = \"Llama models\"\n+readme = \"README.md\"\n+requires-python = \">=3.10\"\n+dependencies = [\n+    \"PyYAML\",\n+    \"jinja2\",\n+    \"tiktoken\",\n+    \"pydantic>=2\",\n+    \"Pillow\",\n+]\n+classifiers = []\n+\n+[project.urls]\n+Homepage = \"https://github.com/meta-llama/llama-models\"\n+\n+[project.scripts]\n+multimodal_example_chat_completion = \"llama_models.scripts.multimodal_example_chat_completion:main\"\n+multimodal_example_text_completion = \"llama_models.scripts.multimodal_example_text_completion:main\"\n+example_chat_completion = \"llama_models.scripts.example_chat_completion:main\"\n+example_text_completion = \"llama_models.scripts.example_text_completion:main\"\n+\n+[project.optional-dependencies]\n+dev = [\n+    \"pytest\",\n+    \"black\",\n+    \"isort\",\n+    \"mypy\",\n+    \"ruff\",\n+]\n+torch = [\n+    \"torch\",\n+    \"fairscale\",\n+    \"fire\",\n+    \"blobfile\",\n+]\n+\n+[tool.setuptools]\n+package-dir = {\"llama_models\" = \"llama_models\"}\n+packages = [\"llama_models\"]\n\n--- File: requirements.txt ---\n@@ -1,5 +0,0 @@\n-PyYAML\n-jinja2\n-tiktoken\n-pydantic>=2\n-Pillow\n\n--- File: setup.py ---\n@@ -1,45 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# the root directory of this source tree.\n-\n-from setuptools import setup\n-\n-\n-def read_requirements():\n-    with open(\"requirements.txt\") as fp:\n-        content = fp.readlines()\n-    return [line.strip() for line in content if not line.startswith(\"#\")]\n-\n-\n-setup(\n-    name=\"llama_models\",\n-    version=\"0.1.0\",\n-    author=\"Meta Llama\",\n-    author_email=\"llama-oss@meta.com\",\n-    description=\"Llama models\",\n-    entry_points={\n-        \"console_scripts\": [\n-            \"multimodal_example_chat_completion = llama_models.scripts.multimodal_example_chat_completion:main\",\n-            \"multimodal_example_text_completion = llama_models.scripts.multimodal_example_text_completion:main\",\n-            \"example_chat_completion = llama_models.scripts.example_chat_completion:main\",\n-            \"example_text_completion = llama_models.scripts.example_text_completion:main\",\n-        ]\n-    },\n-    long_description=open(\"README.md\").read(),\n-    long_description_content_type=\"text/markdown\",\n-    url=\"https://github.com/meta-llama/llama-models\",\n-    package_dir={\"llama_models\": \"llama_models\"},\n-    classifiers=[],\n-    python_requires=\">=3.10\",\n-    install_requires=read_requirements(),\n-    include_package_data=True,\n-)"
            },
            {
              "sha": "b7419d74b7fd11f6e9f8e89a44a194c0e4c6171c",
              "url": "https://github.com/meta-llama/llama-models/commit/b7419d74b7fd11f6e9f8e89a44a194c0e4c6171c",
              "message": "Removing json_schema_type from Enums to flatten them out in the schema",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -11,7 +11,7 @@\n from pydantic import BaseModel, ConfigDict, Field\n from typing_extensions import Annotated\n \n-from .schema_utils import json_schema_type\n+from .schema_utils import json_schema_type, register_schema\n \n \n @json_schema_type\n@@ -32,10 +32,13 @@ class TopKSamplingStrategy(BaseModel):\n     top_k: int = Field(..., ge=1)\n \n \n-SamplingStrategy = Annotated[\n-    Union[GreedySamplingStrategy, TopPSamplingStrategy, TopKSamplingStrategy],\n-    Field(discriminator=\"type\"),\n-]\n+SamplingStrategy = register_schema(\n+    Annotated[\n+        Union[GreedySamplingStrategy, TopPSamplingStrategy, TopKSamplingStrategy],\n+        Field(discriminator=\"type\"),\n+    ],\n+    name=\"SamplingStrategy\",\n+)\n \n \n @json_schema_type\n@@ -46,15 +49,6 @@ class SamplingParams(BaseModel):\n     repetition_penalty: Optional[float] = 1.0\n \n \n-@json_schema_type(\n-    schema={\n-        \"description\": \"\"\"\n-The format in which weights are specified. This does not necessarily\n-always equal what quantization is desired at runtime since there\n-can be on-the-fly conversions done.\n-\"\"\",\n-    }\n-)\n class CheckpointQuantizationFormat(Enum):\n     # default format\n     bf16 = \"bf16\"\n@@ -67,7 +61,6 @@ class CheckpointQuantizationFormat(Enum):\n     int4 = \"int4\"\n \n \n-@json_schema_type\n class ModelFamily(Enum):\n     llama2 = \"llama2\"\n     llama3 = \"llama3\"\n@@ -77,7 +70,6 @@ class ModelFamily(Enum):\n     safety = \"safety\"\n \n \n-@json_schema_type\n class CoreModelId(Enum):\n     \"\"\"Each of these models is a unique \"SKU\". These root models can be served in various garbs (especially by quantizing them)\"\"\"\n \n@@ -187,11 +179,6 @@ def model_family(model_id) -> ModelFamily:\n         raise ValueError(f\"Unknown model family for {model_id}\")\n \n \n-@json_schema_type(\n-    schema={\n-        \"description\": \"The model family and SKU of the model along with other parameters corresponding to the model.\"\n-    }\n-)\n class Model(BaseModel):\n     core_model_id: CoreModelId\n     description: str\n\n--- File: models/llama3/api/datatypes.py ---\n@@ -20,15 +20,13 @@\n from ...schema_utils import json_schema_type\n \n \n-@json_schema_type\n class Role(Enum):\n     system = \"system\"\n     user = \"user\"\n     assistant = \"assistant\"\n     tool = \"tool\"\n \n \n-@json_schema_type\n class BuiltinTool(Enum):\n     brave_search = \"brave_search\"\n     wolfram_alpha = \"wolfram_alpha\"\n@@ -82,13 +80,10 @@ def validate_field(cls, v):\n         return v\n \n \n-@json_schema_type\n class ToolPromptFormat(Enum):\n-    \"\"\"This Enum refers to the prompt format for calling custom / zero shot tools\n+    \"\"\"Prompt format for calling custom / zero shot tools.\n \n-    `json` --\n-        Refers to the json format for calling tools.\n-        The json format takes the form like\n+    :cvar json: JSON format for calling tools. It takes the form:\n         {\n             \"type\": \"function\",\n             \"function\" : {\n@@ -97,22 +92,19 @@ class ToolPromptFormat(Enum):\n                 \"parameters\": {...}\n             }\n         }\n-\n-    `function_tag` --\n-        This is an example of how you could define\n-        your own user defined format for making tool calls.\n-        The function_tag format looks like this,\n+    :cvar function_tag: Function tag format, pseudo-XML. This looks like:\n         <function=function_name>(parameters)</function>\n \n-    The detailed prompts for each of these formats are added to llama cli\n+    :cvar python_list: Python list. The output is a valid Python expression that can be\n+        evaluated to a list. Each element in the list is a function call. Example:\n+        [\"function_name(param1, param2)\", \"function_name(param1, param2)\"]\n     \"\"\"\n \n     json = \"json\"\n     function_tag = \"function_tag\"\n     python_list = \"python_list\"\n \n \n-@json_schema_type\n class StopReason(Enum):\n     end_of_turn = \"end_of_turn\"\n     end_of_message = \"end_of_message\""
            },
            {
              "sha": "1b45e35a8b0037251298d40d6e77f4ba5e180285",
              "url": "https://github.com/meta-llama/llama-models/commit/1b45e35a8b0037251298d40d6e77f4ba5e180285",
              "message": "Bump version to 0.1.0",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.63\",\n+    version=\"0.1.0\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "2dee240a2c2e8d3b6ecff3305759fcde4a9cde45",
              "url": "https://github.com/meta-llama/llama-models/commit/2dee240a2c2e8d3b6ecff3305759fcde4a9cde45",
              "message": "Update serialization for RawMediaItem",
              "files_changed": [
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/datatypes.py ---\n@@ -5,15 +5,17 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n+import base64\n from enum import Enum\n+\n+from io import BytesIO\n from typing import Dict, List, Literal, Optional, Union\n \n-from pydantic import BaseModel, ConfigDict, Field, field_validator\n+from pydantic import BaseModel, ConfigDict, Field, field_serializer, field_validator\n \n from typing_extensions import Annotated\n from ...datatypes import *  # noqa\n \n-from io import BytesIO\n \n from ...schema_utils import json_schema_type\n \n@@ -123,6 +125,19 @@ class RawMediaItem(BaseModel):\n \n     model_config = ConfigDict(arbitrary_types_allowed=True)\n \n+    @field_serializer(\"data\")\n+    def serialize_data(self, data: Optional[bytes], _info):\n+        if data is None:\n+            return None\n+        return base64.b64encode(data).decode(\"utf-8\")\n+\n+    @field_validator(\"data\", mode=\"before\")\n+    @classmethod\n+    def validate_data(cls, v):\n+        if isinstance(v, str):\n+            return base64.b64decode(v)\n+        return v\n+\n \n class RawTextItem(BaseModel):\n     type: Literal[\"text\"] = \"text\""
            },
            {
              "sha": "b333524a4180b473a5879fc2b40772a6b2f8a956",
              "url": "https://github.com/meta-llama/llama-models/commit/b333524a4180b473a5879fc2b40772a6b2f8a956",
              "message": "bugfix",
              "files_changed": [
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/chat_format.py ---\n@@ -67,7 +67,11 @@ def __init__(self, tokenizer: Tokenizer):\n     def _encode_header(self, role: str) -> List[int]:\n         tokens = []\n         tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n-        tokens.extend(self.tokenizer.encode(role, bos=False, eos=False))\n+        tokens.extend(\n+            self.tokenizer.encode(\n+                \"ipython\" if role == \"tool\" else role, bos=False, eos=False\n+            )\n+        )\n         tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n         tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n         return tokens\n@@ -118,7 +122,7 @@ def _process(c):\n     def encode_message(\n         self, message: RawMessage, tool_prompt_format: ToolPromptFormat\n     ) -> Tuple[List[int], List[PIL_Image.Image]]:\n-        tokens = self._encode_header(role_str(message.role))\n+        tokens = self._encode_header(message.role)\n         images = []\n \n         def _process_content(c):\n@@ -165,7 +169,7 @@ def encode_dialog_prompt(\n             images.extend(imgs)\n \n         # Add the start of an assistant message for the model to complete.\n-        tokens.extend(self._encode_header(role_str(Role.assistant)))\n+        tokens.extend(self._encode_header(\"assistant\"))\n \n         return self._model_input_from_tokens_images(tokens, images)"
            },
            {
              "sha": "fef88723e8eff36050fddeaa4fa3974ba509c123",
              "url": "https://github.com/meta-llama/llama-models/commit/fef88723e8eff36050fddeaa4fa3974ba509c123",
              "message": "Rename \"ipython\" enum to \"tool\" (#261)",
              "files_changed": [
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/interface.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/prompts_text.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_3/prompt_format.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_3/prompts.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/chat_format.py ---\n@@ -42,13 +42,24 @@ class LLMInput:\n     vision: Optional[VisionInput] = None\n \n \n+def role_str(role: Role) -> str:\n+    role_strs = {\n+        Role.user: \"user\",\n+        Role.system: \"system\",\n+        Role.tool: \"ipython\",  # special\n+        Role.assistant: \"assistant\",\n+    }\n+    return role_strs[role]\n+\n+\n class ChatFormat:\n     possible_headers: Dict[Role, str]\n \n     def __init__(self, tokenizer: Tokenizer):\n         self.tokenizer = tokenizer\n+\n         self.possible_headers = {\n-            role: f\"<|start_header_id|>{role.value}<|end_header_id|>\\n\\n\"\n+            role: f\"<|start_header_id|>{role_str(role)}<|end_header_id|>\\n\\n\"\n             for role in Role\n         }\n         self.vision_token = self.tokenizer.special_tokens[\"<|image|>\"]\n@@ -107,7 +118,7 @@ def _process(c):\n     def encode_message(\n         self, message: RawMessage, tool_prompt_format: ToolPromptFormat\n     ) -> Tuple[List[int], List[PIL_Image.Image]]:\n-        tokens = self._encode_header(message.role)\n+        tokens = self._encode_header(role_str(message.role))\n         images = []\n \n         def _process_content(c):\n@@ -154,7 +165,7 @@ def encode_dialog_prompt(\n             images.extend(imgs)\n \n         # Add the start of an assistant message for the model to complete.\n-        tokens.extend(self._encode_header(Role.assistant.value))\n+        tokens.extend(self._encode_header(role_str(Role.assistant)))\n \n         return self._model_input_from_tokens_images(tokens, images)\n \n\n--- File: models/llama3/api/datatypes.py ---\n@@ -23,7 +23,7 @@ class Role(Enum):\n     system = \"system\"\n     user = \"user\"\n     assistant = \"assistant\"\n-    ipython = \"ipython\"\n+    tool = \"tool\"\n \n \n @json_schema_type\n@@ -137,7 +137,7 @@ class RawTextItem(BaseModel):\n \n \n class RawMessage(BaseModel):\n-    role: Literal[\"user\", \"system\", \"ipython\", \"assistant\"]\n+    role: Literal[\"user\"] | Literal[\"system\"] | Literal[\"tool\"] | Literal[\"assistant\"]\n     content: RawContent\n \n     # This is for RAG but likely should be absorbed into content\n\n--- File: models/llama3/api/interface.py ---\n@@ -141,7 +141,7 @@ def tool_response_messages(self, *args, **kwargs):\n         template = ToolResponseGenerator().gen(*args, **kwargs)\n         return [\n             RawMessage(\n-                role=\"ipython\",\n+                role=\"tool\",\n                 content=template.render(),\n             )\n         ]\n\n--- File: models/llama3_1/prompts.py ---\n@@ -77,7 +77,7 @@ def usecases() -> List[UseCase | str]:\n             - `<|begin_of_text|>`: Specifies the start of the prompt\n             - `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n             - `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n-            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and ipython]\n+            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and tool]\n             - `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n             - `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n                 - at the end of a direct interaction between the model and the user\n@@ -91,8 +91,8 @@ def usecases() -> List[UseCase | str]:\n             There are 4 different roles that are supported by Llama 3.1\n             - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n             - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n-            - `ipython`: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n-            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `ipython` and `user` prompts.\n+            - `tool`: A new role introduced in Llama 3.1. This role is used to mark messages with the output of a tool call when sent back to the model from the executor. (The actual token used by the model for this role is \"ipython\".)\n+            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `tool` and `user` prompts.\n             \"\"\"\n         ),\n         UseCase(\n@@ -188,15 +188,15 @@ def usecases() -> List[UseCase | str]:\n                         ],\n                     ),\n                     RawMessage(\n-                        role=\"ipython\",\n+                        role=\"tool\",\n                         content=wolfram_alpha_response(),\n                     ),\n                 ],\n             ],\n             notes=textwrap.dedent(\n                 \"\"\"\n                 - Note the `<|python_tag|>` in the assistant response.\n-                - Role is `ipython` for the wolfram alpha response that is passed back to the model.\n+                - Role is `tool` for the wolfram alpha response that is passed back to the model.\n                 - Final message from assistant has <|eot_id|> tag.\n                 \"\"\"\n             ),\n\n--- File: models/llama3_2/prompts_text.py ---\n@@ -196,7 +196,7 @@ def usecases():\n                         ],\n                     ),\n                     RawMessage(\n-                        role=\"ipython\",\n+                        role=\"tool\",\n                         content=json.dumps(\"25 C\"),\n                     ),\n                 ],\n\n--- File: models/llama3_3/prompt_format.md ---\n@@ -19,8 +19,8 @@ Here is a list of special tokens that are supported by Llama 3.1:\n There are 4 different roles that are supported by Llama 3.1\n - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n-- `ipython`: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n-- `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `ipython` and `user` prompts.\n+- `tool`: A new role introduced in Llama 3.1. This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n+- `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `tool` and `user` prompts.\n \n ## Llama 3.1 Base Model\n \n@@ -217,7 +217,7 @@ The 100th decimal of pi is 7.<|eot_id|>\n \n \n - Note the `<|python_tag|>` in the assistant response.\n-- Role is `ipython` for the wolfram alpha response that is passed back to the model.\n+- Role is `tool` for the wolfram alpha response that is passed back to the model.\n - Final message from assistant has <|eot_id|> tag.\n \n \n\n--- File: models/llama3_3/prompts.py ---\n@@ -77,7 +77,7 @@ def usecases() -> List[UseCase | str]:\n             - `<|begin_of_text|>`: Specifies the start of the prompt\n             - `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n             - `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n-            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and ipython]\n+            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and tool]\n             - `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n             - `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n                 - at the end of a direct interaction between the model and the user\n@@ -91,8 +91,8 @@ def usecases() -> List[UseCase | str]:\n             There are 4 different roles that are supported by Llama 3.1\n             - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n             - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n-            - `ipython`: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n-            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `ipython` and `user` prompts.\n+            - `tool`: A new role introduced in Llama 3.1. This role is used to mark messages with the output of a tool call when sent back to the model from the executor. (The actual token used by the model for this role is \"ipython\".)\n+            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `tool` and `user` prompts.\n             \"\"\"\n         ),\n         UseCase(\n@@ -187,15 +187,15 @@ def usecases() -> List[UseCase | str]:\n                         ],\n                     ),\n                     RawMessage(\n-                        role=\"ipython\",\n+                        role=\"tool\",\n                         content=wolfram_alpha_response(),\n                     ),\n                 ],\n             ],\n             notes=textwrap.dedent(\n                 \"\"\"\n                 - Note the `<|python_tag|>` in the assistant response.\n-                - Role is `ipython` for the wolfram alpha response that is passed back to the model.\n+                - Role is `tool` for the wolfram alpha response that is passed back to the model.\n                 - Final message from assistant has <|eot_id|> tag.\n                 \"\"\"\n             ),"
            },
            {
              "sha": "675e4be3973f70a6441cc0302766a1669a99db1f",
              "url": "https://github.com/meta-llama/llama-models/commit/675e4be3973f70a6441cc0302766a1669a99db1f",
              "message": "Bump version to 0.0.63",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.62\",\n+    version=\"0.0.63\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "dc60bfea0e61c74841d626c1eea5a26c1c749238",
              "url": "https://github.com/meta-llama/llama-models/commit/dc60bfea0e61c74841d626c1eea5a26c1c749238",
              "message": "Bump version to 0.0.62",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.61\",\n+    version=\"0.0.62\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "bf5b0c4fe74e3b51ed5904ab65e3f671b194d2a9",
              "url": "https://github.com/meta-llama/llama-models/commit/bf5b0c4fe74e3b51ed5904ab65e3f671b194d2a9",
              "message": "Modeling code and nearby utilities should work with \"raw\" bytes not URLs (#244)",
              "files_changed": [
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/interface.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/test_tokenizer.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/prompt_templates/system_prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/prompts_text.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/prompts_vision.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_3/prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/prompt_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_text_completion.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/chat_format.py ---\n@@ -5,15 +5,28 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n+import io\n import uuid\n \n from dataclasses import dataclass\n-from typing import Dict, List, Tuple\n+from typing import Dict, List, Optional, Tuple\n \n-from .tokenizer import Tokenizer\n-from .datatypes import *  # noqa: F403\n from PIL import Image as PIL_Image\n \n+from .datatypes import (\n+    BuiltinTool,\n+    RawContent,\n+    RawMediaItem,\n+    RawMessage,\n+    RawTextItem,\n+    Role,\n+    StopReason,\n+    ToolCall,\n+    ToolPromptFormat,\n+)\n+\n+from .tokenizer import Tokenizer\n+\n from .tool_utils import ToolUtils\n \n \n@@ -24,7 +37,7 @@ class VisionInput:\n \n \n @dataclass\n-class ModelInput:\n+class LLMInput:\n     tokens: List[int]\n     vision: Optional[VisionInput] = None\n \n@@ -48,12 +61,12 @@ def _encode_header(self, role: str) -> List[int]:\n         tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n         return tokens\n \n-    def encode_content(self, content: InterleavedTextMedia) -> ModelInput:\n+    def encode_content(self, content: RawContent) -> LLMInput:\n         tokens, images = self._encode_content(content, bos=True)\n         return self._model_input_from_tokens_images(tokens, images)\n \n     def _encode_content(\n-        self, content: InterleavedTextMedia, bos: bool = False\n+        self, content: RawContent, bos: bool = False\n     ) -> Tuple[List[int], List[PIL_Image.Image]]:\n         tokens = []\n         images = []\n@@ -63,30 +76,36 @@ def _encode_content(\n         def _process(c):\n             nonlocal added_bos, bos\n \n-            if isinstance(c, str):\n+            if isinstance(c, str) or isinstance(c, RawTextItem):\n+                if isinstance(c, RawTextItem):\n+                    c = c.text\n                 tokens.extend(\n                     self.tokenizer.encode(c, bos=False if added_bos else bos, eos=False)\n                 )\n                 added_bos = True\n-            elif isinstance(c, ImageMedia):\n+\n+            elif isinstance(c, RawMediaItem):\n                 bos = False if added_bos else bos\n                 if bos:\n                     tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n                     added_bos = True\n                 tokens.append(self.vision_token)\n-                cc = interleaved_text_media_localize(c)\n-                images.append(cc.image)\n \n-        if isinstance(content, str):\n-            _process(content)\n-        elif isinstance(content, list):\n+                bytes_io = io.BytesIO(c.data) if isinstance(c.data, bytes) else c.data\n+                image = PIL_Image.open(bytes_io)\n+                image = image.convert(\"RGB\")\n+                images.append(image)\n+\n+        if isinstance(content, list):\n             for c in content:\n                 _process(c)\n+        else:\n+            _process(content)\n \n         return tokens, images\n \n     def encode_message(\n-        self, message: Message, tool_prompt_format: ToolPromptFormat\n+        self, message: RawMessage, tool_prompt_format: ToolPromptFormat\n     ) -> Tuple[List[int], List[PIL_Image.Image]]:\n         tokens = self._encode_header(message.role)\n         images = []\n@@ -96,22 +115,24 @@ def _process_content(c):\n             tokens.extend(toks)\n             images.extend(imgs)\n \n-        if isinstance(message, CompletionMessage) and len(message.tool_calls) > 0:\n+        if message.role == \"assistant\" and len(message.tool_calls) > 0:\n             tokens.append(self.tokenizer.special_tokens[\"<|python_tag|>\"])\n \n         _process_content(message.content)\n \n-        if isinstance(message, UserMessage) and message.context is not None:\n+        if message.role == \"user\" and message.context is not None:\n+            # This is RAG context; why is it here in the chat format? I don't think\n+            # this is needed and can be moved upwards\n             _process_content(\"\\n\\n\")\n             _process_content(message.context)\n \n-        if isinstance(message, CompletionMessage):\n+        if message.role == \"assistant\":\n             for t in message.tool_calls:\n                 content = ToolUtils.encode_tool_call(t, tool_prompt_format)\n                 _process_content(content)\n \n         eom = False\n-        if isinstance(message, CompletionMessage):\n+        if message.role == \"assistant\":\n             eom = message.stop_reason == StopReason.end_of_message\n \n         tokens.append(\n@@ -121,9 +142,9 @@ def _process_content(c):\n \n     def encode_dialog_prompt(\n         self,\n-        messages: List[Message],\n+        messages: List[RawMessage],\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n-    ) -> ModelInput:\n+    ) -> LLMInput:\n         tokens = []\n         images = []\n         tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n@@ -140,14 +161,14 @@ def encode_dialog_prompt(\n     # TODO(this should be generic, not only for assistant messages)\n     def decode_assistant_message(\n         self, tokens: List[int], stop_reason: StopReason\n-    ) -> CompletionMessage:\n+    ) -> RawMessage:\n         content = self.tokenizer.decode(tokens)\n \n         return self.decode_assistant_message_from_content(content, stop_reason)\n \n     def decode_assistant_message_from_content(\n         self, content: str, stop_reason: StopReason\n-    ) -> CompletionMessage:\n+    ) -> RawMessage:\n         content = content.strip(\" \")\n         header_str = self.possible_headers[Role.assistant]\n         if content.startswith(header_str):\n@@ -205,23 +226,24 @@ def decode_assistant_message_from_content(\n             )\n             content = \"\"\n \n-        return CompletionMessage(\n+        return RawMessage(\n+            role=\"assistant\",\n             content=content,\n             stop_reason=stop_reason,\n             tool_calls=tool_calls,\n         )\n \n     def _model_input_from_tokens_images(\n         self, tokens: List[int], images: List[PIL_Image.Image]\n-    ) -> ModelInput:\n+    ) -> LLMInput:\n         vision_input = None\n         if len(images) > 0:\n             vision_input = VisionInput(\n                 mask=create_vision_mask(tokens, self.vision_token),\n                 images=images,\n             )\n \n-        return ModelInput(\n+        return LLMInput(\n             tokens=[\n                 128256 if token == self.vision_token else token for token in tokens\n             ],\n\n--- File: models/llama3/api/datatypes.py ---\n@@ -8,17 +8,13 @@\n from enum import Enum\n from typing import Dict, List, Literal, Optional, Union\n \n-from pydantic import BaseModel, Field, field_validator\n+from pydantic import BaseModel, ConfigDict, Field, field_validator\n \n from typing_extensions import Annotated\n from ...datatypes import *  # noqa\n \n-import base64\n-import re\n from io import BytesIO\n \n-from PIL import Image as PIL_Image\n-\n from ...schema_utils import json_schema_type\n \n \n@@ -30,74 +26,6 @@ class Role(Enum):\n     ipython = \"ipython\"\n \n \n-@json_schema_type(\n-    schema={\"type\": \"string\", \"format\": \"uri\", \"pattern\": \"^(https?://|file://|data:)\"}\n-)\n-class URL(BaseModel):\n-    uri: str\n-\n-    def __str__(self) -> str:\n-        return self.uri\n-\n-\n-@json_schema_type\n-class ImageMedia(BaseModel):\n-    image: Union[PIL_Image.Image, URL]\n-\n-    model_config = ConfigDict(arbitrary_types_allowed=True)\n-\n-\n-InterleavedTextMedia = Union[\n-    str,\n-    # Specific modalities can be placed here, but not generic attachments\n-    # since models don't consume them in a generic way\n-    ImageMedia,\n-    List[Union[str, ImageMedia]],\n-]\n-\n-\n-def interleaved_text_media_as_str(content: InterleavedTextMedia, sep: str = \" \") -> str:\n-    def _process(c) -> str:\n-        if isinstance(c, str):\n-            return c\n-        else:\n-            return \"<media>\"\n-\n-    if isinstance(content, list):\n-        return sep.join(_process(c) for c in content)\n-    else:\n-        return _process(content)\n-\n-\n-def interleaved_text_media_localize(\n-    content: InterleavedTextMedia,\n-) -> InterleavedTextMedia:\n-    def _localize_single(c: str | ImageMedia) -> str | ImageMedia:\n-        if isinstance(c, ImageMedia):\n-            # load image and return PIL version\n-            img = c.image\n-            if isinstance(img, URL):\n-                if img.uri.startswith(\"file://\"):\n-                    img = PIL_Image.open(img.uri[len(\"file://\") :]).convert(\"RGB\")\n-                elif img.uri.startswith(\"data\"):\n-                    match = re.match(r\"data:image/(\\w+);base64,(.+)\", img.uri)\n-                    if not match:\n-                        raise ValueError(\"Invalid data URL format\")\n-                    image_type, image_data = match.groups()\n-                    image_data = base64.b64decode(image_data)\n-                    img = PIL_Image.open(BytesIO(image_data))\n-                else:\n-                    raise ValueError(\"Unsupported URL type\")\n-            return ImageMedia(image=img)\n-        else:\n-            return c\n-\n-    if isinstance(content, list):\n-        return [_localize_single(c) for c in content]\n-    else:\n-        return _localize_single(content)\n-\n-\n @json_schema_type\n class BuiltinTool(Enum):\n     brave_search = \"brave_search\"\n@@ -127,23 +55,6 @@ def validate_field(cls, v):\n         return v\n \n \n-@json_schema_type\n-class ToolResponse(BaseModel):\n-    call_id: str\n-    tool_name: Union[BuiltinTool, str]\n-    content: InterleavedTextMedia\n-\n-    @field_validator(\"tool_name\", mode=\"before\")\n-    @classmethod\n-    def validate_field(cls, v):\n-        if isinstance(v, str):\n-            try:\n-                return BuiltinTool(v)\n-            except ValueError:\n-                return v\n-        return v\n-\n-\n @json_schema_type\n class ToolParamDefinition(BaseModel):\n     param_type: str\n@@ -169,12 +80,6 @@ def validate_field(cls, v):\n         return v\n \n \n-@json_schema_type\n-class ToolChoice(Enum):\n-    auto = \"auto\"\n-    required = \"required\"\n-\n-\n @json_schema_type\n class ToolPromptFormat(Enum):\n     \"\"\"This Enum refers to the prompt format for calling custom / zero shot tools\n@@ -206,54 +111,38 @@ class ToolPromptFormat(Enum):\n \n \n @json_schema_type\n-class UserMessage(BaseModel):\n-    role: Literal[Role.user.value] = Role.user.value\n-    content: InterleavedTextMedia\n-    context: Optional[InterleavedTextMedia] = None\n+class StopReason(Enum):\n+    end_of_turn = \"end_of_turn\"\n+    end_of_message = \"end_of_message\"\n+    out_of_tokens = \"out_of_tokens\"\n \n \n-@json_schema_type\n-class SystemMessage(BaseModel):\n-    role: Literal[Role.system.value] = Role.system.value\n-    content: InterleavedTextMedia\n+class RawMediaItem(BaseModel):\n+    type: Literal[\"image\"] = \"image\"\n+    data: bytes | BytesIO\n \n+    model_config = ConfigDict(arbitrary_types_allowed=True)\n \n-@json_schema_type\n-class ToolResponseMessage(BaseModel):\n-    role: Literal[Role.ipython.value] = Role.ipython.value\n-    # it was nice to re-use the ToolResponse type, but having all messages\n-    # have a `content` type makes things nicer too\n-    call_id: str\n-    tool_name: Union[BuiltinTool, str]\n-    content: InterleavedTextMedia\n \n+class RawTextItem(BaseModel):\n+    type: Literal[\"text\"] = \"text\"\n+    text: str\n \n-@json_schema_type\n-class StopReason(Enum):\n-    end_of_turn = \"end_of_turn\"\n-    end_of_message = \"end_of_message\"\n-    out_of_tokens = \"out_of_tokens\"\n \n+RawContentItem = Annotated[\n+    Union[RawTextItem, RawMediaItem], Field(discriminator=\"type\")\n+]\n \n-@json_schema_type\n-class TokenLogProbs(BaseModel):\n-    logprobs_by_token: Dict[str, float]\n+RawContent = str | RawContentItem | List[RawContentItem]\n \n \n-@json_schema_type\n-class CompletionMessage(BaseModel):\n-    role: Literal[Role.assistant.value] = Role.assistant.value\n-    content: InterleavedTextMedia\n-    stop_reason: StopReason\n-    tool_calls: List[ToolCall] = Field(default_factory=list)\n+class RawMessage(BaseModel):\n+    role: Literal[\"user\", \"system\", \"ipython\", \"assistant\"]\n+    content: RawContent\n \n+    # This is for RAG but likely should be absorbed into content\n+    context: Optional[RawContent] = None\n \n-Message = Annotated[\n-    Union[\n-        UserMessage,\n-        SystemMessage,\n-        ToolResponseMessage,\n-        CompletionMessage,\n-    ],\n-    Field(discriminator=\"role\"),\n-]\n+    # These are for the output message coming from the assistant\n+    stop_reason: Optional[StopReason] = None\n+    tool_calls: List[ToolCall] = Field(default_factory=list)\n\n--- File: models/llama3/api/interface.py ---\n@@ -25,15 +25,11 @@\n \n from .datatypes import (\n     BuiltinTool,\n-    CompletionMessage,\n-    Message,\n+    RawMessage,\n     StopReason,\n-    SystemMessage,\n     ToolCall,\n     ToolDefinition,\n     ToolPromptFormat,\n-    ToolResponseMessage,\n-    UserMessage,\n )\n from .tokenizer import Tokenizer\n \n@@ -134,7 +130,7 @@ def __init__(self, tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json)\n         self.formatter = ChatFormat(self.tokenizer)\n         self.tool_prompt_format = tool_prompt_format\n \n-    def get_tokens(self, messages: List[Message]) -> List[int]:\n+    def get_tokens(self, messages: List[RawMessage]) -> List[int]:\n         model_input = self.formatter.encode_dialog_prompt(\n             messages,\n             self.tool_prompt_format,\n@@ -144,9 +140,8 @@ def get_tokens(self, messages: List[Message]) -> List[int]:\n     def tool_response_messages(self, *args, **kwargs):\n         template = ToolResponseGenerator().gen(*args, **kwargs)\n         return [\n-            ToolResponseMessage(\n-                call_id=\"call_id\",\n-                tool_name=\"tool_name\",\n+            RawMessage(\n+                role=\"ipython\",\n                 content=template.render(),\n             )\n         ]\n@@ -156,7 +151,7 @@ def system_messages(\n         builtin_tools: List[BuiltinTool],\n         custom_tools: List[ToolDefinition],\n         instruction: Optional[str] = None,\n-    ) -> List[Message]:\n+    ) -> List[RawMessage]:\n         messages = []\n \n         default_gen = SystemDefaultGenerator()\n@@ -179,7 +174,7 @@ def system_messages(\n             sys_content += instruction\n \n         sys_content += \"\\n\"\n-        messages.append(SystemMessage(content=sys_content))\n+        messages.append(RawMessage(role=\"system\", content=sys_content))\n \n         if custom_tools:\n             if self.tool_prompt_format == ToolPromptFormat.json:\n@@ -188,11 +183,11 @@ def system_messages(\n                 tool_gen = FunctionTagCustomToolGenerator()\n             else:\n                 raise ValueError(\n-                    f\"Non supported ToolPromptFormat {request.tool_prompt_format}\"\n+                    f\"Non supported ToolPromptFormat {self.tool_prompt_format}\"\n                 )\n \n             custom_template = tool_gen.gen(custom_tools)\n-            messages.append(UserMessage(content=custom_template.render()))\n+            messages.append(RawMessage(role=\"user\", content=custom_template.render()))\n \n         return messages\n \n@@ -201,22 +196,23 @@ def assistant_response_messages(\n         content: str,\n         stop_reason: StopReason,\n         tool_call: Optional[ToolCall] = None,\n-    ) -> List[CompletionMessage]:\n+    ) -> List[RawMessage]:\n         tool_calls = []\n         if tool_call:\n             tool_calls.append(tool_call)\n         return [\n-            CompletionMessage(\n+            RawMessage(\n+                role=\"assistant\",\n                 content=content,\n                 tool_calls=tool_calls,\n                 stop_reason=stop_reason,\n             )\n         ]\n \n-    def user_message(self, content: str) -> List[UserMessage]:\n-        return [UserMessage(content=content)]\n+    def user_message(self, content: str) -> List[RawMessage]:\n+        return [RawMessage(role=\"user\", content=content)]\n \n-    def display_message_as_tokens(self, message: Message) -> None:\n+    def display_message_as_tokens(self, message: RawMessage) -> None:\n         \"\"\"Util to print tokenized string to shell\"\"\"\n         tokens = self.formatter.encode_message(message, self.tool_prompt_format)\n         on_colors = [\n\n--- File: models/llama3/api/test_tokenizer.py ---\n@@ -12,7 +12,7 @@\n from unittest import TestCase\n \n from .chat_format import ChatFormat\n-from .datatypes import SystemMessage, ToolPromptFormat, UserMessage\n+from .datatypes import RawMessage, ToolPromptFormat\n from .tokenizer import Tokenizer\n \n \n@@ -45,7 +45,8 @@ def test_decode(self):\n         )\n \n     def test_encode_message(self):\n-        message = UserMessage(\n+        message = RawMessage(\n+            role=\"user\",\n             content=\"This is a test sentence.\",\n         )\n         self.assertEqual(\n@@ -69,10 +70,12 @@ def test_encode_message(self):\n \n     def test_encode_dialog(self):\n         messages = [\n-            SystemMessage(\n+            RawMessage(\n+                role=\"system\",\n                 content=\"This is a test sentence.\",\n             ),\n-            UserMessage(\n+            RawMessage(\n+                role=\"user\",\n                 content=\"This is a response.\",\n             ),\n         ]\n\n--- File: models/llama3/api/tool_utils.py ---\n@@ -100,7 +100,6 @@ def parse_python_list_for_function_calls(input_string):\n \n \n class ToolUtils:\n-\n     @staticmethod\n     def is_builtin_tool_call(message_body: str) -> bool:\n         match = re.search(ToolUtils.BUILTIN_TOOL_PATTERN, message_body)\n\n--- File: models/llama3/prompt_templates/system_prompts.py ---\n@@ -19,7 +19,6 @@\n \n \n class SystemDefaultGenerator(PromptTemplateGeneratorBase):\n-\n     def gen(self, *args, **kwargs) -> PromptTemplate:\n         template_str = textwrap.dedent(\n             \"\"\"\n@@ -37,7 +36,6 @@ def data_examples(self) -> List[Any]:\n \n \n class BuiltinToolGenerator(PromptTemplateGeneratorBase):\n-\n     def _tool_breakdown(self, tools: List[ToolDefinition]):\n         builtin_tools, custom_tools = [], []\n         for dfn in tools:\n@@ -50,7 +48,6 @@ def _tool_breakdown(self, tools: List[ToolDefinition]):\n \n     def gen(self, tools: List[ToolDefinition]) -> PromptTemplate:\n         builtin_tools, custom_tools = self._tool_breakdown(tools)\n-        data = []\n         template_str = textwrap.dedent(\n             \"\"\"\n             {% if builtin_tools or custom_tools -%}\n@@ -86,7 +83,6 @@ def data_examples(self) -> List[List[ToolDefinition]]:\n \n \n class JsonCustomToolGenerator(PromptTemplateGeneratorBase):\n-\n     def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n         template_str = textwrap.dedent(\n             \"\"\"\n@@ -157,7 +153,6 @@ def data_examples(self) -> List[List[ToolDefinition]]:\n \n \n class FunctionTagCustomToolGenerator(PromptTemplateGeneratorBase):\n-\n     def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n         template_str = textwrap.dedent(\n             \"\"\"\n@@ -220,7 +215,6 @@ def data_examples(self) -> List[List[ToolDefinition]]:\n \n \n class PythonListCustomToolGenerator(PromptTemplateGeneratorBase):  # noqa: N801\n-\n     def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n         template_str = textwrap.dedent(\n             \"\"\"\n\n--- File: models/llama3/reference_impl/generation.py ---\n@@ -32,14 +32,8 @@\n from termcolor import cprint\n \n from ..api.args import ModelArgs\n-from ..api.chat_format import ChatFormat, ModelInput\n-from ..api.datatypes import (\n-    CompletionMessage,\n-    InterleavedTextMedia,\n-    Message,\n-    StopReason,\n-    ToolPromptFormat,\n-)\n+from ..api.chat_format import ChatFormat, LLMInput\n+from ..api.datatypes import RawContent, RawMessage, StopReason, ToolPromptFormat\n from ..api.tokenizer import Tokenizer\n from .model import Transformer\n \n@@ -53,7 +47,7 @@ class CompletionPrediction:\n \n @dataclass\n class ChatPrediction:\n-    generation: CompletionMessage\n+    generation: RawMessage\n     decoded_tokens: Optional[List[str]] = None\n     logprobs: Optional[List[List[float]]] = None\n \n@@ -163,7 +157,7 @@ def __init__(self, model: Transformer, tokenizer: Tokenizer, args: ModelArgs):\n     @torch.inference_mode()\n     def generate(\n         self,\n-        model_input: ModelInput,\n+        model_input: LLMInput,\n         max_gen_len: int,\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n@@ -303,7 +297,7 @@ def generate(\n \n     def text_completion(\n         self,\n-        content: InterleavedTextMedia,\n+        content: RawContent,\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n@@ -347,7 +341,7 @@ def text_completion(\n \n     def chat_completion(\n         self,\n-        messages: List[Message],\n+        messages: List[RawMessage],\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n@@ -403,7 +397,7 @@ def chat_completion(\n \n     def chat_completion_raw(\n         self,\n-        messages: List[Message],\n+        messages: List[RawMessage],\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n@@ -432,7 +426,7 @@ def chat_completion_raw(\n \n     def text_completion_raw(\n         self,\n-        content: InterleavedTextMedia,\n+        content: RawContent,\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n@@ -448,8 +442,6 @@ def text_completion_raw(\n         input_tokens = model_input.tokens\n \n         output_tokens = []\n-        token_logprobs = []\n-        decoded_tokens = []\n         for result in self.generate(\n             model_input=model_input,\n             max_gen_len=max_gen_len,\n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -12,10 +12,9 @@\n \n import numpy as np\n import pytest\n-from llama_models.llama3.api.datatypes import ImageMedia, SystemMessage, UserMessage\n+from llama_models.llama3.api.datatypes import RawMediaItem, RawMessage, RawTextItem\n \n from llama_models.llama3.reference_impl.generation import Llama\n-from PIL import Image as PIL_Image\n \n THIS_DIR = Path(__file__).parent\n \n@@ -37,22 +36,21 @@ def build_generator(env_var: str):\n \n \n class TestTextModelInference(unittest.TestCase):\n-\n     @classmethod\n     def setUpClass(cls):\n         cls.generator = build_generator(\"TEXT_MODEL_CHECKPOINT_DIR\")\n \n     def test_run_generation(self):\n         dialogs = [\n             [\n-                SystemMessage(content=\"Always answer with Haiku\"),\n-                UserMessage(content=\"I am going to Paris, what should I see?\"),\n+                RawMessage(role=\"system\", content=\"Always answer with Haiku\"),\n+                RawMessage(\n+                    role=\"user\", content=\"I am going to Paris, what should I see?\"\n+                ),\n             ],\n             [\n-                SystemMessage(\n-                    content=\"Always answer with emojis\",\n-                ),\n-                UserMessage(content=\"How to go from Beijing to NY?\"),\n+                RawMessage(role=\"system\", content=\"Always answer with emojis\"),\n+                RawMessage(role=\"user\", content=\"How to go from Beijing to NY?\"),\n             ],\n         ]\n         for dialog in dialogs:\n@@ -71,7 +69,6 @@ def test_run_generation(self):\n \n \n class TestVisionModelInference(unittest.TestCase):\n-\n     @classmethod\n     def setUpClass(cls):\n         cls.generator = build_generator(\"VISION_MODEL_CHECKPOINT_DIR\")\n@@ -82,21 +79,23 @@ def test_run_generation(self):\n         with open(\n             THIS_DIR.parent.parent.parent / \"scripts/resources/dog.jpg\", \"rb\"\n         ) as f:\n-            img = PIL_Image.open(f).convert(\"RGB\")\n+            img = f.read()\n \n         dialogs = [\n             [\n-                UserMessage(\n+                RawMessage(\n+                    role=\"user\",\n                     content=[\n-                        ImageMedia(image=img),\n-                        \"Describe this image in two sentences\",\n+                        RawMediaItem(data=img),\n+                        RawTextItem(text=\"Describe this image in two sentences\"),\n                     ],\n                 )\n             ],\n             [\n-                UserMessage(\n-                    content=\"what is the recipe of mayonnaise in two sentences?\"\n-                ),\n+                RawMessage(\n+                    role=\"user\",\n+                    content=\"what is the recipe of mayonnaise in two sentences?\",\n+                )\n             ],\n         ]\n \n\n--- File: models/llama3_1/prompts.py ---\n@@ -7,7 +7,14 @@\n \n import textwrap\n from typing import List\n-from ..llama3.api.datatypes import *  # noqa: F403\n+\n+from ..llama3.api.datatypes import (\n+    BuiltinTool,\n+    RawMessage,\n+    StopReason,\n+    ToolCall,\n+    ToolPromptFormat,\n+)\n from ..prompt_format import (\n     llama3_1_builtin_tool_call_dialog,\n     llama3_1_custom_tool_call_dialog,\n@@ -104,8 +111,11 @@ def usecases() -> List[UseCase | str]:\n             description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n             dialogs=[\n                 [\n-                    SystemMessage(content=\"You are a helpful assistant\"),\n-                    UserMessage(content=\"Answer who are you in the form of jeopardy?\"),\n+                    RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n+                    RawMessage(\n+                        role=\"user\",\n+                        content=\"Answer who are you in the form of jeopardy?\",\n+                    ),\n                 ]\n             ],\n             notes=\"\",\n@@ -141,9 +151,10 @@ def usecases() -> List[UseCase | str]:\n             description=\"Here is an actual example of model responding with code\",\n             dialogs=[\n                 [\n-                    SystemMessage(content=\"Environment: ipython\"),\n-                    UserMessage(\n-                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\"\n+                    RawMessage(role=\"system\", content=\"Environment: ipython\"),\n+                    RawMessage(\n+                        role=\"user\",\n+                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\",\n                     ),\n                 ],\n             ],\n@@ -159,11 +170,13 @@ def usecases() -> List[UseCase | str]:\n             description=\"Here is a full interaction with the built-in tools including the tool response and the final assistant response.\",\n             dialogs=[\n                 [\n-                    SystemMessage(\n-                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\"\n+                    RawMessage(\n+                        role=\"system\",\n+                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\",\n                     ),\n-                    UserMessage(content=\"What is the 100th decimal of pi?\"),\n-                    CompletionMessage(\n+                    RawMessage(role=\"user\", content=\"What is the 100th decimal of pi?\"),\n+                    RawMessage(\n+                        role=\"assistant\",\n                         content=\"\",\n                         stop_reason=StopReason.end_of_message,\n                         tool_calls=[\n@@ -174,9 +187,8 @@ def usecases() -> List[UseCase | str]:\n                             )\n                         ],\n                     ),\n-                    ToolResponseMessage(\n-                        call_id=\"wolfram_alpha_id\",\n-                        tool_name=BuiltinTool.wolfram_alpha,\n+                    RawMessage(\n+                        role=\"ipython\",\n                         content=wolfram_alpha_response(),\n                     ),\n                 ],\n\n--- File: models/llama3_2/prompts_text.py ---\n@@ -6,7 +6,8 @@\n # the top-level of this source tree.\n import json\n import textwrap\n-from ..llama3.api.datatypes import *  # noqa: F403\n+\n+from ..llama3.api.datatypes import RawMessage, StopReason, ToolCall, ToolPromptFormat\n from ..prompt_format import (\n     llama3_1_builtin_code_interpreter_dialog,\n     TextCompletionContent,\n@@ -99,8 +100,8 @@ def usecases():\n             description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n             dialogs=[\n                 [\n-                    SystemMessage(content=\"You are a helpful assistant\"),\n-                    UserMessage(content=\"Who are you?\"),\n+                    RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n+                    RawMessage(role=\"user\", content=\"Who are you?\"),\n                 ]\n             ],\n             notes=\"This format is unchanged from Llama3.1\",\n@@ -119,8 +120,10 @@ def usecases():\n             dialogs=[\n                 # Zero shot tool calls as system message\n                 [\n-                    SystemMessage(content=system_tool_call()),\n-                    UserMessage(content=\"What is the weather in SF and Seattle?\"),\n+                    RawMessage(role=\"system\", content=system_tool_call()),\n+                    RawMessage(\n+                        role=\"user\", content=\"What is the weather in SF and Seattle?\"\n+                    ),\n                 ],\n             ],\n             notes=textwrap.dedent(\n@@ -140,7 +143,7 @@ def usecases():\n             dialogs=[\n                 # Zero shot tool call as user message\n                 [\n-                    UserMessage(content=user_tool_call()),\n+                    RawMessage(role=\"user\", content=user_tool_call()),\n                 ],\n             ],\n             notes=textwrap.dedent(\n@@ -175,9 +178,10 @@ def usecases():\n             ),\n             dialogs=[\n                 [\n-                    SystemMessage(content=system_tool_call()),\n-                    UserMessage(content=\"What is the weather in SF?\"),\n-                    CompletionMessage(\n+                    RawMessage(role=\"system\", content=system_tool_call()),\n+                    RawMessage(role=\"user\", content=\"What is the weather in SF?\"),\n+                    RawMessage(\n+                        role=\"assistant\",\n                         content=\"\",\n                         stop_reason=StopReason.end_of_turn,\n                         tool_calls=[\n@@ -191,9 +195,8 @@ def usecases():\n                             )\n                         ],\n                     ),\n-                    ToolResponseMessage(\n-                        call_id=\"call\",\n-                        tool_name=\"get_weather\",\n+                    RawMessage(\n+                        role=\"ipython\",\n                         content=json.dumps(\"25 C\"),\n                     ),\n                 ],\n\n--- File: models/llama3_2/prompts_vision.py ---\n@@ -8,9 +8,7 @@\n import textwrap\n from pathlib import Path\n \n-from PIL import Image as PIL_Image\n-\n-from ..llama3.api.datatypes import *  # noqa: F403\n+from ..llama3.api.datatypes import RawMediaItem, RawMessage, RawTextItem\n from ..prompt_format import (\n     llama3_1_builtin_tool_call_dialog,\n     # llama3_1_builtin_tool_call_with_image_dialog,\n@@ -23,7 +21,7 @@\n def usecases():\n     this_dir = Path(__file__).parent.parent.resolve()\n     with open(this_dir / \"scripts/resources/dog.jpg\", \"rb\") as f:\n-        img = PIL_Image.open(f).convert(\"RGB\")\n+        img = f.read()\n \n     return [\n         llama3_2_user_assistant_conversation(),\n@@ -32,10 +30,11 @@ def usecases():\n             description=\"This example shows how to pass and image to the model as part of the messages.\",\n             dialogs=[\n                 [\n-                    UserMessage(\n+                    RawMessage(\n+                        role=\"user\",\n                         content=[\n-                            ImageMedia(image=img),\n-                            \"Describe this image in two sentences\",\n+                            RawMediaItem(data=img),\n+                            RawTextItem(text=\"Describe this image in two sentences\"),\n                         ],\n                     )\n                 ],\n@@ -115,8 +114,8 @@ def usecases():\n             dialogs=[\n                 TextCompletionContent(\n                     content=[\n-                        ImageMedia(image=img),\n-                        \"If I had to write a haiku for this one\",\n+                        RawMediaItem(data=img),\n+                        RawTextItem(text=\"If I had to write a haiku for this one\"),\n                     ]\n                 ),\n             ],\n\n--- File: models/llama3_3/prompts.py ---\n@@ -7,7 +7,14 @@\n \n import textwrap\n from typing import List\n-from ..llama3.api.datatypes import *  # noqa: F403\n+\n+from ..llama3.api.datatypes import (\n+    BuiltinTool,\n+    RawMessage,\n+    StopReason,\n+    ToolCall,\n+    ToolPromptFormat,\n+)\n from ..prompt_format import (\n     llama3_1_builtin_tool_call_dialog,\n     llama3_1_custom_tool_call_dialog,\n@@ -104,8 +111,11 @@ def usecases() -> List[UseCase | str]:\n             description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n             dialogs=[\n                 [\n-                    SystemMessage(content=\"You are a helpful assistant\"),\n-                    UserMessage(content=\"Answer who are you in the form of jeopardy?\"),\n+                    RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n+                    RawMessage(\n+                        role=\"user\",\n+                        content=\"Answer who are you in the form of jeopardy?\",\n+                    ),\n                 ]\n             ],\n             notes=\"\",\n@@ -141,9 +151,10 @@ def usecases() -> List[UseCase | str]:\n             description=\"Here is an actual example of model responding with code\",\n             dialogs=[\n                 [\n-                    SystemMessage(content=\"Environment: ipython\"),\n-                    UserMessage(\n-                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\"\n+                    RawMessage(role=\"system\", content=\"Environment: ipython\"),\n+                    RawMessage(\n+                        role=\"user\",\n+                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\",\n                     ),\n                 ],\n             ],\n@@ -159,11 +170,12 @@ def usecases() -> List[UseCase | str]:\n             description=\"Here is a full interaction with the built-in tools including the tool response and the final assistant response.\",\n             dialogs=[\n                 [\n-                    SystemMessage(\n-                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\"\n+                    RawMessage(\n+                        role=\"system\",\n+                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\",\n                     ),\n-                    UserMessage(content=\"What is the 100th decimal of pi?\"),\n-                    CompletionMessage(\n+                    RawMessage(role=\"user\", content=\"What is the 100th decimal of pi?\"),\n+                    RawMessage(\n                         content=\"\",\n                         stop_reason=StopReason.end_of_message,\n                         tool_calls=[\n@@ -174,9 +186,8 @@ def usecases() -> List[UseCase | str]:\n                             )\n                         ],\n                     ),\n-                    ToolResponseMessage(\n-                        call_id=\"wolfram_alpha_id\",\n-                        tool_name=BuiltinTool.wolfram_alpha,\n+                    RawMessage(\n+                        role=\"ipython\",\n                         content=wolfram_alpha_response(),\n                     ),\n                 ],\n\n--- File: models/prompt_format.py ---\n@@ -10,21 +10,29 @@\n from pathlib import Path\n from typing import List\n \n-from PIL import Image as PIL_Image\n+from llama_models.llama3.api.interface import LLama31Interface\n+\n from pydantic import BaseModel, Field\n \n-from .llama3.api.datatypes import *  # noqa: F403\n-from llama_models.llama3.api.interface import LLama31Interface\n+from .llama3.api.datatypes import (\n+    RawContent,\n+    RawMediaItem,\n+    RawMessage,\n+    RawTextItem,\n+    StopReason,\n+    ToolCall,\n+    ToolPromptFormat,\n+)\n \n \n class TextCompletionContent(BaseModel):\n-    content: InterleavedTextMedia = \"\"\n+    content: RawContent = \"\"\n \n \n class UseCase(BaseModel):\n     title: str = \"\"\n     description: str = \"\"\n-    dialogs: List[List[Message] | TextCompletionContent | str] = Field(\n+    dialogs: List[List[RawMessage] | TextCompletionContent | str] = Field(\n         default_factory=list\n     )\n     notes: str = \"\"\n@@ -134,13 +142,13 @@ def llama3_1_builtin_tool_call_with_image_dialog(\n \n     this_dir = Path(__file__).parent.resolve()\n     with open(this_dir / \"scripts/resources/dog.jpg\", \"rb\") as f:\n-        img = PIL_Image.open(f).convert(\"RGB\")\n+        img = f.read()\n \n     interface = LLama31Interface(tool_prompt_format)\n \n     messages = interface.system_messages(**system_message_builtin_tools_only())\n     messages += interface.user_message(\n-        content=[ImageMedia(image=img), \"What is this dog breed?\"]\n+        content=[RawMediaItem(data=img), RawTextItem(text=\"What is this dog breed?\")]\n     )\n     messages += interface.assistant_response_messages(\n         \"Based on the description of the dog in the image, it appears to be a small breed dog, possibly a terrier mix\",\n@@ -173,7 +181,8 @@ def llama3_1_e2e_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n     messages = interface.system_messages(**system_message_custom_tools_only())\n     messages += interface.user_message(content=\"Use tools to get latest trending songs\")\n     messages.append(\n-        CompletionMessage(\n+        RawMessage(\n+            role=\"assistant\",\n             content=\"\",\n             stop_reason=StopReason.end_of_message,\n             tool_calls=[\n@@ -186,9 +195,8 @@ def llama3_1_e2e_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n         ),\n     )\n     messages.append(\n-        ToolResponseMessage(\n-            call_id=\"tool_response\",\n-            tool_name=\"trending_songs\",\n+        RawMessage(\n+            role=\"assistant\",\n             content=tool_response,\n         )\n     )\n@@ -201,8 +209,8 @@ def llama3_2_user_assistant_conversation():\n         description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n         dialogs=[\n             [\n-                SystemMessage(content=\"You are a helpful assistant\"),\n-                UserMessage(content=\"Who are you?\"),\n+                RawMessage(role=\"system\", content=\"You are a helpful assistant\"),\n+                RawMessage(role=\"user\", content=\"Who are you?\"),\n             ]\n         ],\n         notes=\"This format is unchanged from Llama3.1\",\n\n--- File: models/scripts/example_chat_completion.py ---\n@@ -12,12 +12,7 @@\n \n import fire\n \n-from llama_models.llama3.api.datatypes import (\n-    CompletionMessage,\n-    StopReason,\n-    SystemMessage,\n-    UserMessage,\n-)\n+from llama_models.llama3.api.datatypes import RawMessage, StopReason\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n@@ -48,10 +43,14 @@ def run_main(\n     )\n \n     dialogs = [\n-        [UserMessage(content=\"what is the recipe of mayonnaise?\")],\n+        [RawMessage(role=\"user\", content=\"what is the recipe of mayonnaise?\")],\n         [\n-            UserMessage(content=\"I am going to Paris, what should I see?\"),\n-            CompletionMessage(\n+            RawMessage(\n+                role=\"user\",\n+                content=\"I am going to Paris, what should I see?\",\n+            ),\n+            RawMessage(\n+                role=\"assistant\",\n                 content=\"\"\"\\\n Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n \n@@ -62,17 +61,15 @@ def run_main(\n These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n                 stop_reason=StopReason.end_of_turn,\n             ),\n-            UserMessage(content=\"What is so great about #1?\"),\n+            RawMessage(role=\"user\", content=\"What is so great about #1?\"),\n         ],\n         [\n-            SystemMessage(content=\"Always answer with Haiku\"),\n-            UserMessage(content=\"I am going to Paris, what should I see?\"),\n+            RawMessage(role=\"system\", content=\"Always answer with Haiku\"),\n+            RawMessage(role=\"user\", content=\"I am going to Paris, what should I see?\"),\n         ],\n         [\n-            SystemMessage(\n-                content=\"Always answer with emojis\",\n-            ),\n-            UserMessage(content=\"How to go from Beijing to NY?\"),\n+            RawMessage(role=\"system\", content=\"Always answer with emojis\"),\n+            RawMessage(role=\"user\", content=\"How to go from Beijing to NY?\"),\n         ],\n     ]\n     for dialog in dialogs:\n\n--- File: models/scripts/multimodal_example_chat_completion.py ---\n@@ -8,15 +8,16 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n+from io import BytesIO\n+from pathlib import Path\n from typing import Optional\n \n import fire\n-\n-from llama_models.llama3.api.datatypes import ImageMedia, UserMessage\n+from llama_models.llama3.api.datatypes import RawMediaItem, RawMessage, RawTextItem\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n-from PIL import Image as PIL_Image\n+THIS_DIR = Path(__file__).parent\n \n \n def run_main(\n@@ -38,21 +39,27 @@ def run_main(\n     # image understanding\n     dialogs = []\n     with open(THIS_DIR / \"resources/dog.jpg\", \"rb\") as f:\n-        img = PIL_Image.open(f).convert(\"RGB\")\n+        img = f.read()\n \n     dialogs = [\n         [\n-            UserMessage(\n+            RawMessage(\n+                role=\"user\",\n                 content=[\n-                    ImageMedia(image=img),\n-                    \"Describe this image in two sentences\",\n+                    RawMediaItem(data=BytesIO(img)),\n+                    RawTextItem(text=\"Describe this image in two sentences\"),\n                 ],\n             )\n         ],\n     ]\n     # text only\n     dialogs += [\n-        [UserMessage(content=\"what is the recipe of mayonnaise in two sentences?\")],\n+        [\n+            RawMessage(\n+                role=\"user\",\n+                content=\"what is the recipe of mayonnaise in two sentences?\",\n+            )\n+        ],\n     ]\n \n     for dialog in dialogs:\n\n--- File: models/scripts/multimodal_example_text_completion.py ---\n@@ -8,18 +8,21 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n+from io import BytesIO\n+from pathlib import Path\n from typing import Optional\n \n import fire\n-\n-from llama_models.llama3.api.datatypes import ImageMedia\n+from llama_models.llama3.api.datatypes import RawMediaItem\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n-from PIL import Image as PIL_Image\n from termcolor import cprint\n \n \n+THIS_DIR = Path(__file__).parent\n+\n+\n def run_main(\n     ckpt_dir: str,\n     temperature: float = 0.6,\n@@ -37,17 +40,14 @@ def run_main(\n     )\n \n     with open(THIS_DIR / \"resources/dog.jpg\", \"rb\") as f:\n-        img = PIL_Image.open(f).convert(\"RGB\")\n-\n-    with open(THIS_DIR / \"resources/pasta.jpeg\", \"rb\") as f:\n-        img2 = PIL_Image.open(f).convert(\"RGB\")\n+        img = f.read()\n \n     interleaved_contents = [\n         # text only\n         \"The color of the sky is blue but sometimes it can also be\",\n         # image understanding\n         [\n-            ImageMedia(image=img),\n+            RawMediaItem(type=\"image\", data=BytesIO(img)),\n             \"If I had to write a haiku for this one\",\n         ],\n     ]"
            },
            {
              "sha": "3a5906d138313fa30eec49eb13b209aabf153ed0",
              "url": "https://github.com/meta-llama/llama-models/commit/3a5906d138313fa30eec49eb13b209aabf153ed0",
              "message": "Update gitignore",
              "files_changed": [
                {
                  "filename": ".gitignore",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .gitignore ---\n@@ -2,3 +2,4 @@ __pycache__\n dist\n *.egg-info\n build\n+.DS_Store"
            },
            {
              "sha": "7890266c5a3ccd29e739d53a71ea968bcf4ca400",
              "url": "https://github.com/meta-llama/llama-models/commit/7890266c5a3ccd29e739d53a71ea968bcf4ca400",
              "message": "Bump version to 0.0.61",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.60\",\n+    version=\"0.0.61\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "fc1e70e7970bdf599a924f6fd06cedb6e3819224",
              "url": "https://github.com/meta-llama/llama-models/commit/fc1e70e7970bdf599a924f6fd06cedb6e3819224",
              "message": "Bump version to 0.0.60",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.59\",\n+    version=\"0.0.60\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "c2cbe2cd270e98bdcdb9f788bef16e870330c4df",
              "url": "https://github.com/meta-llama/llama-models/commit/c2cbe2cd270e98bdcdb9f788bef16e870330c4df",
              "message": "Bump version to 0.0.59",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.58\",\n+    version=\"0.0.59\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "804a64fba64ca5a327bc910447512957a8a35e32",
              "url": "https://github.com/meta-llama/llama-models/commit/804a64fba64ca5a327bc910447512957a8a35e32",
              "message": "Add eval_details for llama3_3",
              "files_changed": [
                {
                  "filename": "models/llama3_3/eval_details.md",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_3/eval_details.md ---\n@@ -0,0 +1,155 @@\n+\n+\n+# Llama 3 Evaluation Details\n+\n+This document contains some additional context on the settings and methodology for how we evaluated the Llama 3.2 models.\n+\n+\n+## Language auto-eval benchmark notes:\n+\n+For a given benchmark, we strive to use consistent evaluation settings across all models, including external models. We make every effort to achieve optimal scores for external models, including addressing any model-specific parsing and tokenization requirements. Where the scores are lower for external models than self-reported scores on comparable or more conservative settings, we report the self-reported scores for external models. We are also releasing the data generated as part of evaluations with publicly available benchmarks which can be found on [huggingface here](https://huggingface.co/collections/meta-llama/llama-32-evals-66f44b3d2df1c7b136d821f0).\n+\n+\n+### MMLU\n+\n+For the pre-trained models we use a 5-shot config. To determine the choice character we use the standard MMLU prompt and compare the negative log-likelihood (NLL) of the various choices.\n+\n+For the post-trained models we report both 5-shot and 0-shot scores. We ask the model to generate the best choice character. The 0-shot scores use a CoT (chain of thought) prompt. The maximum generation lengths for the 5-shot and 0-shot configs are 10 tokens and 1024 tokens respectively.\n+\n+Macro averages are reported unless otherwise stated. The micro average scores for the various models are: 65.6, 79.0, and 85.4 for the pre-trained 8B, 70B and 405B models respectively for the 5-shot config; 69.44, 84.0, 87.71 for the post-trained 8B, 70B and 405B models respectively for the 5-shot config.\n+\n+\n+### TLDR9+\n+\n+For post-trained models, we use a 1-shot config and report rougeL scores. We run this as a generative task. Maximum generation length is 512 tokens. We specifically ran this on [TLDR9+ dataset](https://github.com/sajastu/reddit_collector)\n+\n+\n+### Open-Rewrite\n+\n+For post-trained models, we use a 0-shot config and report micro_avg rougeL scores across elaborate, formality, others, paraphrase, shorten, and wiki. We run this as a generative task. Maximum generation length is 512 tokens. Specific dataset can be found [here](https://github.com/google-research/google-research/tree/master/rewritelm).\n+\n+\n+### IFEval\n+\n+For post-trained models, we use the default settings as specified [here](https://arxiv.org/pdf/2311.07911). We compute the prompt level scores and instruction level strict and loose accuracy. We then report the average across all the scores.\n+\n+\n+### ARC-Challenge\n+\n+We use the Arc-Challenge subset from the Arc benchmark. For the pre-trained models, we use a  25-shot config and use the MMLU setup for evaluation where we provide all the choices in the prompt and calculate likelihood over choice characters. For the post-trained models, we use 0-shot config and ask the model to generate the choice character. The maximum generation length is 100 tokens.\n+\n+\n+### GPQA\n+\n+For post-trained models, we use 0-shot config with CoT prompt and report exact match scores over the possible options using the main set. Max generation length is 2048 tokens.\n+\n+\n+### GPQA Diamond\n+\n+GPQA Diamond is the subset of GPQA where two out of two experts agree, Rein et al. (2023). For post-trained models, we use 0-shot config with CoT prompt and report exact match scores over the possible options using the Diamond set. Max generation length is 2048 tokens.\n+\n+### AGIEval English\n+\n+For pre-trained models, we use the default few-shot and prompt settings as specified [here](https://github.com/ruixiangcui/AGIEval). The score is averaged over the english subtasks. The max generation length is 10 tokens.\n+\n+\n+### SQuAD\n+\n+For pre-trained models, we use SQuAD v2 with a 1-shot config and report exact match scores. We run this as a generative task. Maximum generation length is 32 tokens.\n+\n+\n+### QuAC\n+\n+For pre-trained models, we use a 1-shot config and report the F1 scores. We run this as a generative task. Maximum generation length is 32 tokens.\n+\n+\n+### DROP\n+\n+For pre-trained models, for each validation example, we draw 3 random few-shot examples from the train split and report the F1 scores. The maximum generation length is 32 tokens.\n+\n+\n+### GSM8K\n+\n+For both pre-trained and post-trained models, we use the same 8-shot config with CoT prompt as in [Wei et al. (2022)](https://arxiv.org/pdf/2201.11903.pdf) (maj@1). The maximum generation length is 1024 tokens.\n+\n+\n+### MATH\n+\n+For pre-trained models, we use the same 4-shot config as in [Lewkowycz et al. (2022)](https://arxiv.org/pdf/2206.14858.pdf) (maj@1). Maximum generation length is 512 tokens.\n+\n+For post-trained models we use a 0-shot config with Cot prompt. We enhance the exact match using [sympy](https://www.sympy.org/en/index.html) and then use an [equality template](https://github.com/openai/simple-evals/blob/main/common.py#L27-L85) with a judge to resolve complex expressions. Maximum generation length is 5120 tokens. The MATH score represents the full dataset. The scores for MATH-HARD (Lvl 5) are 25.4, 43.8, and 53.4 for the 8B, 70B and 405B models respectively.\n+\n+\n+### InfiniteBench\n+\n+We report on EN.MC (Free-form question answering based on the fake book) and EN.QA (Multiple choice questions derived from the fake book) sub-tasks. The average tokens in these tasks is 184.4k and 192.6k respectively. We truncate down the dataset to 128k input tokens using our tokenizer to have a 'clean' dataset where the right answer is not mistakenly deleted during truncation. For post-trained models, we use a 0-shot config. Maximum generation length is 20 for both the En.QA and En.MC tasks.\n+\n+\n+### NIH/Multi-needle\n+\n+For post-training, we use a 0-shot config. Our context lengths are evenly spaced between 2000 and 131072 in 10 intervals, inclusive of the endpoints for llama models and between 2000 and 128000 for non-llama models. Maximum generation length is 256 tokens.\n+\n+\n+### MGSM\n+\n+For post-trained models, we use an 0-shot config with CoT prompt and report exact match (maj@1) scores. Maximum generation length is 2048 tokens. The scores are averaged over all the eleven languages present in the MGSM benchmark, including the ones not supported by Llama models.\n+\n+\n+### Multilingual MMLU\n+\n+For post-trained models, we use a 5-shot config. We run this as a generative task. Maximum generation length is 10 tokens. The scores are individually reported for each and averaged over the seven non-english languages that Llama models support (Portuguese, Spanish, Italian, German, French, Hindi, Thai).\n+\n+\n+### Berkeley Function Calling Leaderboard (BFCL) v2\n+\n+For Berkeley Function Calling Leaderboard (BFCL-v2), benchmark results were achieved by running the open source evaluation repository [ShishirPatil/gorilla ](https://github.com/ShishirPatil/gorilla/)on commit 70d6722 and we report the \"AST Summary\" metric.\n+\n+\n+### Nexus\n+\n+We use the [open-source ](https://github.com/nexusflowai/NexusRaven)prompt and evaluation function followed by the[ open source notebook](https://github.com/nexusflowai/NexusRaven-V2/blob/master/evaluation_notebook/GPT4_Evaluation/Benchmark_GPT4.ipynb) to compute the scores.\n+\n+\n+### RULER\n+\n+For more comprehensive long-context evals beyond retrieval, we assess our perf on RULER benchmark, where we synthesize datasets across increasingly long context length buckets, and compare mean across each of them over retrieval (single needle, multi needle, multi-value per multiple keys), multi-hop tracing (variable tracking), aggregation (common words, frequency extraction), and question-answering.\n+\n+\n+### MMMU\n+\n+For post-trained models, we use 0-shot config with CoT prompt and report scores over the possible options using the validation set. Maximum generation length is 2048 tokens. We use the following system prompt: \"&lt;|image|>Look at the image carefully and solve the following question step-by-step. {question} Options: {options} Indicate the correct answer at the end.\"\n+\n+\n+### MMMU-Pro standard\n+\n+For post-trained models, we use 0-shot config, we use multiple choice questions with ten options, and report scores over the possible options using the test set. In case of multiple images provided per prompt, they are stitched into a single image. Maximum generation length is 2048 tokens. We use the following system prompt: \"&lt;|image|>{question} Options: {options}\"\n+\n+\n+### MMMU-Pro vision\n+\n+For post-trained models, we use 0-shot config and report scores over the possible options using the test set. Maximum generation length is 2048 tokens. We use the following system prompt: \"&lt;|image|>Your job is to extract the question from the image the user attached, reason about it, and then answer the question. Follow these steps: \\n1. Always start by Clearly stating the question shown in the image, and also state all of the options. \\n2. Then, Carefully review the information beyond the question shown in the image and analyze it without making any assumptions. \\n3. Next, use your understanding of the image to answer the question you listed out in the first step. Use a step-by-step process \\n4. Finally, write the answer in the following format where X is exactly one of the option letters: The best answer is X.\\nAlways follow the steps above, printing out everything. Let's think step by step. Your response must be among the given options.\"\n+\n+\n+### AI2D\n+\n+For post-trained models, we use 0-shot config and report scores using the test set. Maximum generation length is 400 tokens. For 11b we use the following system prompt: \"&lt;|image|>Look at the scientific diagram carefully and answer the following question: {question}\\n Think step by step and finally respond to the question with only the correct option number as \\\"FINAL ANSWER\\\". Let's think step by step.\" For 90b we use a different system prompt: \"&lt;|image|>Look at the scientific diagram carefully and answer the following question: {question}\\n Respond only with the correct option digit.\"\n+\n+\n+### ChartQA\n+\n+For post-trained models, we use 0-shot config with CoT prompt and report scores using the test set. Maximum generation length is 512 tokens. For 11b we use the following system prompt: \"&lt;|image|>You are provided a chart image and will be asked a question. You have to think through your answer and provide a step-by-step solution. Once you have the solution, write the final answer in at most a few words at the end with the phrase \"FINAL ANSWER:\". The question is: {question}&lt;cot_start>Let's think step by step.\" For 90b we use a different system prompt: \"&lt;|image|>You are provided a chart image and will be asked a question. Follow these steps carefully:\\n Step 1: Analyze the question to understand what specific data or information is being asked for. Focus on whether the question is asking for a specific number or category from the chart image.\\n Step 2: Identify any numbers, categories, or groups mentioned in the question and take note of them. Focus on detecting and matching them directly to the image. \\nStep 3: Study the image carefully and find the relevant data corresponding to the categories or numbers mentioned. Avoid unnecessary assumptions or calculations; simply read the correct data from the image.\\n Step 4: Develop a clear plan to solve the question by locating the right data. Focus only on the specific category or group that matches the question. \\nStep 5: Use step-by-step reasoning to ensure you are referencing the correct numbers or data points from the image, avoiding unnecessary extra steps or interpretations.\\n Step 6: Provide the final answer, starting with \"FINAL ANSWER:\" and using as few words as possible, simply stating the number or data point requested. \\n\\n The question is: {question}&lt;cot_start>Let\\'s think step by step.\"\n+\n+\n+### DocVQA\n+\n+For post-trained models, we use 0-shot config and report scores over the test set. Maximum generation length is 512 tokens. We use the following system prompt: \"&lt;|image|> Read the text in the image carefully and answer the question with the text as seen exactly in the image. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Question: {question}\"\n+\n+\n+### VQAv2\n+\n+For post-trained models, we use 0-shot config and report scores over the test set. Maximum generation length is 25 tokens. We use the following system prompt: \"&lt;|image|> Look at the image carefully and answer this visual question. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Respond with as few words as possible.\\n Question: {question}\"\n+\n+\n+### MathVista\n+\n+For post-trained models, we use 0-shot config and report scores over the testmini set. Maximum generation length is 2048 tokens. We use an LLM-based answer extractor as recommended by MathVista paper [Lu et al. (2024)](https://arxiv.org/pdf/2310.02255). We use the following system prompt: \"&lt;|image|>{question}\""
            },
            {
              "sha": "32fd6f1789a8959993e4e5a6675e84e0d41d2bbb",
              "url": "https://github.com/meta-llama/llama-models/commit/32fd6f1789a8959993e4e5a6675e84e0d41d2bbb",
              "message": "Bump version to 0.0.58",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.57\",\n+    version=\"0.0.58\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "362d52a58bcf6c802ea6b0d3707c5373119534e5",
              "url": "https://github.com/meta-llama/llama-models/commit/362d52a58bcf6c802ea6b0d3707c5373119534e5",
              "message": "Bump version to 0.0.55",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.54\",\n+    version=\"0.0.55\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "d968839f577c3d56ca6cc81f54b8dcd907b71b50",
              "url": "https://github.com/meta-llama/llama-models/commit/d968839f577c3d56ca6cc81f54b8dcd907b71b50",
              "message": "Bump version to 0.0.54",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.53\",\n+    version=\"0.0.54\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "8752149f58654c54c012209f43b57bb476146f0c",
              "url": "https://github.com/meta-llama/llama-models/commit/8752149f58654c54c012209f43b57bb476146f0c",
              "message": "accept huggingface repo name when resolving a model",
              "files_changed": [
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/sku_list.py ---\n@@ -23,7 +23,7 @@\n \n def resolve_model(descriptor: str) -> Optional[Model]:\n     for m in all_registered_models():\n-        if descriptor == m.descriptor():\n+        if descriptor in (m.descriptor(), m.huggingface_repo):\n             return m\n     return None"
            },
            {
              "sha": "8069eb65c23375c0e3ee117538fd8a51b9bfce38",
              "url": "https://github.com/meta-llama/llama-models/commit/8069eb65c23375c0e3ee117538fd8a51b9bfce38",
              "message": "Bump version to 0.0.53",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.50\",\n+    version=\"0.0.53\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "f5e5bc042405e7d87494cc23c55622a40c537c1c",
              "url": "https://github.com/meta-llama/llama-models/commit/f5e5bc042405e7d87494cc23c55622a40c537c1c",
              "message": "Bump version to 0.0.49",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.48\",\n+    version=\"0.0.49\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "2fe1a1690162910660332e3294a552cf0ec7e754",
              "url": "https://github.com/meta-llama/llama-models/commit/2fe1a1690162910660332e3294a552cf0ec7e754",
              "message": "Bump version to 0.0.47",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.46\",\n+    version=\"0.0.47\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "de71a49faea0e3d4ae06d7762c6622a0b78de1ce",
              "url": "https://github.com/meta-llama/llama-models/commit/de71a49faea0e3d4ae06d7762c6622a0b78de1ce",
              "message": "Update paths in MANIFEST",
              "files_changed": [
                {
                  "filename": "MANIFEST.in",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: MANIFEST.in ---\n@@ -1,7 +1,7 @@\n include requirements.txt\n-include models/llama3/api/tokenizer.model\n-include models/scripts/resources/dog.jpg\n-include models/scripts/resources/pasta.jpeg\n-include models/llama3_1/prompt_format.md\n-include models/llama3_2/text_prompt_format.md\n-include models/llama3_2/vision_prompt_format.md\n+include llama_models/llama3/api/tokenizer.model\n+include llama_models/scripts/resources/dog.jpg\n+include llama_models/scripts/resources/pasta.jpeg\n+include llama_models/llama3_1/prompt_format.md\n+include llama_models/llama3_2/text_prompt_format.md\n+include llama_models/llama3_2/vision_prompt_format.md"
            },
            {
              "sha": "7fa8b3bcc0baea7ddbdb367cb1ae636ad8f1eed8",
              "url": "https://github.com/meta-llama/llama-models/commit/7fa8b3bcc0baea7ddbdb367cb1ae636ad8f1eed8",
              "message": "Add a symlink for llama_models (#201)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "llama_models",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/example_text_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_text_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -53,15 +53,15 @@ You need to install the following dependencies (in addition to the `requirements\n pip install torch fairscale fire blobfile\n ```\n \n-After installing the dependencies, you can run the example scripts (within `models/scripts/` sub-directory) as follows:\n+After installing the dependencies, you can run the example scripts (within `llama_models/scripts/` sub-directory) as follows:\n ```bash\n #!/bin/bash\n \n CHECKPOINT_DIR=~/.llama/checkpoints/Meta-Llama3.1-8B-Instruct\n-PYTHONPATH=$(git rev-parse --show-toplevel) torchrun models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n+PYTHONPATH=$(git rev-parse --show-toplevel) torchrun llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n ```\n \n-The above script should be used with an Instruct (Chat) model. For a Base model, use the script `models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n+The above script should be used with an Instruct (Chat) model. For a Base model, use the script `llama_models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n \n For running larger models with tensor parallelism, you should modify as:\n ```bash\n@@ -70,7 +70,7 @@ For running larger models with tensor parallelism, you should modify as:\n NGPUS=8\n PYTHONPATH=$(git rev-parse --show-toplevel) torchrun \\\n   --nproc_per_node=$NGPUS \\\n-  models/scripts/example_chat_completion.py $CHECKPOINT_DIR \\\n+  llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR \\\n   --model_parallel_size $NGPUS\n ```\n \n\n--- File: llama_models ---\n@@ -0,0 +1 @@\n+models\n\\ No newline at end of file\n\n--- File: models/scripts/example_chat_completion.py ---\n@@ -12,14 +12,14 @@\n \n import fire\n \n-from models.llama3.api.datatypes import (\n+from llama_models.llama3.api.datatypes import (\n     CompletionMessage,\n     StopReason,\n     SystemMessage,\n     UserMessage,\n )\n \n-from models.llama3.reference_impl.generation import Llama\n+from llama_models.llama3.reference_impl.generation import Llama\n \n \n def run_main(\n\n--- File: models/scripts/example_text_completion.py ---\n@@ -11,9 +11,9 @@\n from typing import Optional\n \n import fire\n-from termcolor import cprint\n \n-from models.llama3.reference_impl.generation import Llama\n+from llama_models.llama3.reference_impl.generation import Llama\n+from termcolor import cprint\n \n \n def run_main(\n\n--- File: models/scripts/multimodal_example_chat_completion.py ---\n@@ -12,11 +12,11 @@\n \n import fire\n \n-from PIL import Image as PIL_Image\n+from llama_models.llama3.api.datatypes import ImageMedia, UserMessage\n \n-from models.llama3.api.datatypes import ImageMedia, UserMessage\n+from llama_models.llama3.reference_impl.generation import Llama\n \n-from models.llama3.reference_impl.generation import Llama\n+from PIL import Image as PIL_Image\n \n \n def run_main(\n\n--- File: models/scripts/multimodal_example_text_completion.py ---\n@@ -12,12 +12,12 @@\n \n import fire\n \n-from PIL import Image as PIL_Image\n-from termcolor import cprint\n+from llama_models.llama3.api.datatypes import ImageMedia\n \n-from models.llama3.api.datatypes import ImageMedia\n+from llama_models.llama3.reference_impl.generation import Llama\n \n-from models.llama3.reference_impl.generation import Llama\n+from PIL import Image as PIL_Image\n+from termcolor import cprint\n \n \n def run_main(\n\n--- File: setup.py ---\n@@ -37,7 +37,7 @@ def read_requirements():\n     long_description=open(\"README.md\").read(),\n     long_description_content_type=\"text/markdown\",\n     url=\"https://github.com/meta-llama/llama-models\",\n-    package_dir={\"llama_models\": \"models\"},\n+    package_dir={\"llama_models\": \"llama_models\"},\n     classifiers=[],\n     python_requires=\">=3.10\",\n     install_requires=read_requirements(),"
            },
            {
              "sha": "e517d3f1d08bae02e4b72bb7d0951766a52d860c",
              "url": "https://github.com/meta-llama/llama-models/commit/e517d3f1d08bae02e4b72bb7d0951766a52d860c",
              "message": "Bump version to 0.0.46",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.45\",\n+    version=\"0.0.46\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "9db4fba00ca881fe9b18eb3f5d9874307eb71113",
              "url": "https://github.com/meta-llama/llama-models/commit/9db4fba00ca881fe9b18eb3f5d9874307eb71113",
              "message": "Update QuantizationArgs schema to incorporate spinquant, etc. (#195)",
              "files_changed": [
                {
                  "filename": "models/llama3/api/args.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/args.py ---\n@@ -6,12 +6,27 @@\n # the top-level of this source tree.\n \n from dataclasses import dataclass\n+from enum import Enum\n from typing import Optional\n \n \n+class QuantizationScheme(Enum):\n+    int4_weight_int8_dynamic_activation = \"int4_weight_int8_dynamic_activation\"\n+\n+\n @dataclass\n class QuantizationArgs:\n+    scheme: Optional[QuantizationScheme] = None\n     group_size: Optional[int] = None\n+    spinquant: bool = False\n+\n+    def __init__(self, **kwargs):\n+        for k, v in kwargs.items():\n+            if k == \"scheme\":\n+                setattr(self, k, QuantizationScheme(v))\n+            else:\n+                if hasattr(self, k):\n+                    setattr(self, k, v)\n \n \n @dataclass"
            },
            {
              "sha": "3c6f056986d7f8c453a4d3a460c06dc543e22caa",
              "url": "https://github.com/meta-llama/llama-models/commit/3c6f056986d7f8c453a4d3a460c06dc543e22caa",
              "message": "Bump version to 0.0.45",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.44\",\n+    version=\"0.0.45\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "5309637ceedec731719b742d9a44176a8e96b494",
              "url": "https://github.com/meta-llama/llama-models/commit/5309637ceedec731719b742d9a44176a8e96b494",
              "message": "Update quantized model IDs so they have \"instruct\" in them",
              "files_changed": [
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/sku_list.py ---\n@@ -609,7 +609,7 @@ def arch_args_3b() -> dict:\n def llama3_2_quantized_models() -> List[Model]:\n     return [\n         Model(\n-            core_model_id=CoreModelId.llama3_2_1b,\n+            core_model_id=CoreModelId.llama3_2_1b_instruct,\n             variant=\"int4-qlora-eo8\",\n             quantization_format=CheckpointQuantizationFormat.int4,\n             description=\"Llama 3.2 1b INT4 quantized LoRA\",\n@@ -628,7 +628,7 @@ def llama3_2_quantized_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.llama3_2_1b,\n+            core_model_id=CoreModelId.llama3_2_1b_instruct,\n             variant=\"int4-spinquant-eo8\",\n             quantization_format=CheckpointQuantizationFormat.int4,\n             description=\"Llama 3.2 1b INT4 quantized SpinQuant\",\n@@ -643,7 +643,7 @@ def llama3_2_quantized_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.llama3_2_3b,\n+            core_model_id=CoreModelId.llama3_2_3b_instruct,\n             variant=\"int4-qlora-eo8\",\n             quantization_format=CheckpointQuantizationFormat.int4,\n             description=\"Llama 3.2 3b INT4 quantized LoRA\",\n@@ -662,7 +662,7 @@ def llama3_2_quantized_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.llama3_2_3b,\n+            core_model_id=CoreModelId.llama3_2_3b_instruct,\n             variant=\"int4-spinquant-eo8\",\n             quantization_format=CheckpointQuantizationFormat.int4,\n             description=\"Llama 3.2 3b INT4 quantized SpinQuant\","
            },
            {
              "sha": "431f70180f855113a6fefc4955e4adc4c50c437c",
              "url": "https://github.com/meta-llama/llama-models/commit/431f70180f855113a6fefc4955e4adc4c50c437c",
              "message": "Bump version to 0.0.44",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.43\",\n+    version=\"0.0.44\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "baf7b01b6e62bc7126c7b558d2b67d4533142680",
              "url": "https://github.com/meta-llama/llama-models/commit/baf7b01b6e62bc7126c7b558d2b67d4533142680",
              "message": "New quantized models (#191)",
              "files_changed": [
                {
                  "filename": ".github/workflows/gha_workflow_llama_models_tests.yml",
                  "status": "modified"
                },
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/args.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/MODEL_CARD.md",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/gha_workflow_llama_models_tests.yml ---\n@@ -50,8 +50,8 @@ on:\n         required: false\n         default: \"---\"\n \n-env: \n-  TOKENIZER_PATH: \"models/llama3/api/tokenizer.model\"  \n+env:\n+  TOKENIZER_PATH: \"models/llama3/api/tokenizer.model\"\n   MODELS_PATH: \"/data/llama3.2\"\n   VISION_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_vision }}\"\n   TEXT_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_text }}\"\n@@ -178,7 +178,6 @@ jobs:\n       ############################################\n \n       #### Run tests ####\n-      \n       - name: \"PR - Run Tests\"\n         id: pr_run_tests\n         working-directory: \"${{ github.workspace }}\"\n\n--- File: models/datatypes.py ---\n@@ -175,7 +175,7 @@ class Model(BaseModel):\n     huggingface_repo: Optional[str] = None\n     recommended_sampling_params: Optional[SamplingParams] = None\n     arch_args: Dict[str, Any]\n-    is_default_variant: bool\n+    variant: str = \"\"\n \n     quantization_format: CheckpointQuantizationFormat = (\n         CheckpointQuantizationFormat.bf16\n@@ -190,23 +190,10 @@ class Model(BaseModel):\n     def model_family(self) -> ModelFamily:\n         return model_family(self.core_model_id)\n \n-    # The variant is a string representation of other parameters which helps\n-    # uniquely identify the model. this typically includes the quantization\n-    # format, model parallel size, etc.\n-    @property\n-    def variant(self) -> str:\n-        parts = [\n-            self.quantization_format.value,\n-            f\"mp{self.pth_file_count}\",\n-        ]\n-\n-        return \"-\".join(parts)\n-\n     # The SKU is uniquely identified by (model_id, variant) combo\n     def descriptor(self, shorten_default_variant: bool = True) -> str:\n-        if shorten_default_variant and self.is_default_variant:\n+        if not self.variant:\n             return self.core_model_id.value\n-\n         return f\"{self.core_model_id.value}:{self.variant}\"\n \n     @property\n@@ -233,6 +220,8 @@ def max_seq_length(self) -> int:\n         elif self.model_family == ModelFamily.llama3_1:\n             return 131072\n         elif self.model_family == ModelFamily.llama3_2:\n+            if self.quantization_format == CheckpointQuantizationFormat.int4:\n+                return 8192\n             return 131072\n         elif self.core_model_id in [\n             CoreModelId.llama_guard_3_8b,\n\n--- File: models/llama3/api/args.py ---\n@@ -9,6 +9,17 @@\n from typing import Optional\n \n \n+@dataclass\n+class QuantizationArgs:\n+    group_size: Optional[int] = None\n+\n+\n+@dataclass\n+class LoRAArgs:\n+    rank: int\n+    scale: float\n+\n+\n @dataclass\n class ModelArgs:\n     dim: int = 4096\n@@ -30,10 +41,18 @@ class ModelArgs:\n     vision_max_num_chunks: int = 4\n     vision_num_cross_attention_layers: int = -1\n \n+    quantization_args: Optional[QuantizationArgs] = None\n+    lora_args: Optional[LoRAArgs] = None\n+\n     def __init__(self, **kwargs):\n         for k, v in kwargs.items():\n-            if hasattr(self, k):\n-                setattr(self, k, v)\n+            if k == \"lora_args\":\n+                setattr(self, k, LoRAArgs(**v))\n+            elif k == \"quantization_args\":\n+                setattr(self, k, QuantizationArgs(**v))\n+            else:\n+                if hasattr(self, k):\n+                    setattr(self, k, v)\n \n         if self.n_kv_heads is None:\n             self.n_kv_heads = self.n_heads\n\n--- File: models/llama3/reference_impl/model.py ---\n@@ -314,9 +314,9 @@ def forward(self, tokens: torch.Tensor, start_pos: int):\n             mask = torch.triu(mask, diagonal=1)\n \n             # https://github.com/pytorch/pytorch/issues/100005\n-            # torch.triu is buggy when the device is mps: filled values are \n-            # nan instead of 0. \n-            if mask.device.type == torch.device('mps').type:\n+            # torch.triu is buggy when the device is mps: filled values are\n+            # nan instead of 0.\n+            if mask.device.type == torch.device(\"mps\").type:\n                 mask = torch.nan_to_num(mask, nan=0.0)\n \n             # When performing key-value caching, we compute the attention scores\n\n--- File: models/llama3_2/MODEL_CARD.md ---\n@@ -10,12 +10,14 @@ The Llama 3.2 collection of multilingual large language models (LLMs) is a colle\n | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n | Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n |  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n+| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n+|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n \n **Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n \n **Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n \n-**Model Release Date:** Sept 25, 2024\n+**Model Release Date:** Oct 24, 2024\n \n **Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n \n@@ -25,25 +27,29 @@ The Llama 3.2 collection of multilingual large language models (LLMs) is a colle\n \n ## Intended Use\n \n-**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks.\n+**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n \n **Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n \n ## Hardware and Software\n \n-**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n+**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n \n **Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n \n-##\n-\n **Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n \n |  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n | :---- | :---: | ----- | :---: | :---: | :---: |\n | Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n | Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n-| Total | 830k |         86k |  | 240 | 0 |\n+| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n+| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n+| Llama 3.2 1B QLoRA | 1.3k | 0 | 700 | 0.381 | 0 |\n+| Llama 3.2 3B QLoRA | 1.6k | 0 | 700 | 0.461 | 0 |\n+| Total | 833k |         86k |  | 240 | 0 |\n+\n+\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n \n The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n \n@@ -53,6 +59,24 @@ The methodology used to determine training energy use and greenhouse gas emissio\n \n **Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n \n+## Quantization\n+\n+### Quantization Scheme\n+\n+We designed the current quantization scheme with the [PyTorchs ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n+- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n+- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n+- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n+\n+\n+### Quantization-Aware Training and LoRA\n+\n+The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n+\n+### SpinQuant\n+\n+[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n+\n ## Benchmarks \\- English Text\n \n In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n@@ -71,35 +95,64 @@ In this section, we report the results for Llama 3.2 models on standard automati\n \n ### Instruction Tuned Models\n \n-| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n-| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: |\n-| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 63.4 | 69.4 |\n-| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 40.1 | 40.9 |\n-| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 19.0 | 17.2 |\n-| Instruction following |  | IFEval | 0 | avg(prompt/instruction acc loose/strict) | 59.5 | 77.4 | 80.4 |\n-| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 77.7 | 84.5 |\n-|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 47.3 | 51.9 |\n-| Reasoning |  | ARC-C | 0 | acc | 59.4 | 78.6 | 83.4 |\n-|  |  | GPQA | 0 | acc | 27.2 | 32.8 | 32.8 |\n-|  |  | Hellaswag | 0 | acc | 41.2 | 69.8 | 78.7 |\n-| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 67.0 | 70.9 |\n-|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 34.3 | 38.5 |\n-| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | 19.8 | 27.3 |\n-|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | 63.3 | 72.2 |\n-|  |  | NIH/Multi-needle | 0 | recall | 75.0 | 84.7 | 98.8 |\n-| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 58.2 | 68.9 |\n+| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n+| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n+| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n+| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n+| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n+| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n+| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n+|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n+| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n+|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n+|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n+| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n+|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n+| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n+|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n+|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n+| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n+\n+\\*\\*for comparison purposes only. Model not released.\n \n ### Multilingual Benchmarks\n \n-| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n-| :---: | :---: | :---: | :---: | :---: | :---: |\n-| General | MMLU (5-shot, macro\\_avg/acc) | Portuguese | 39.82 | 54.48 | 62.12 |\n-|  |  | Spanish | 41.5 | 55.1 | 62.5 |\n-|  |  | Italian | 39.8 | 53.8 | 61.6 |\n-|  |  | German | 39.2 | 53.3 | 60.6 |\n-|  |  | French | 40.5 | 54.6 | 62.3 |\n-|  |  | Hindi | 33.5 | 43.3 | 50.9 |\n-|  |  | Thai | 34.7 | 44.5 | 50.3 |\n+| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n+| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n+| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n+| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n+| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n+| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n+| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n+| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n+| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n+\n+\\*\\*for comparison purposes only. Model not released.\n+\n+## Inference time\n+\n+In the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n+\n+| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n+| :---- | ----- | ----- | ----- | ----- | ----- |\n+| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n+| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n+| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n+| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n+| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n+| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n+\n+(\\*) The performance measurement is done using an adb binary-based approach.\n+(\\*\\*) It is measured on an Android OnePlus 12 device.\n+(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n+\n+*Footnote:*\n+\n+- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n+- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n+- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n+- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n+- *RSS size \\- Memory usage in resident set size (RSS)*\n \n ## Responsibility & Safety\n \n\n--- File: models/sku_list.py ---\n@@ -23,11 +23,7 @@\n \n def resolve_model(descriptor: str) -> Optional[Model]:\n     for m in all_registered_models():\n-        descriptors = [\n-            m.descriptor(shorten_default_variant=False),\n-            m.descriptor(shorten_default_variant=True),\n-        ]\n-        if descriptor in descriptors:\n+        if descriptor == m.descriptor():\n             return m\n     return None\n \n@@ -82,7 +78,6 @@ def llama2_base_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama2_7b,\n-            is_default_variant=True,\n             description=\"Llama 2 7b model\",\n             huggingface_repo=\"meta-llama/Llama-2-7b\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -102,7 +97,6 @@ def llama2_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama2_13b,\n-            is_default_variant=True,\n             description=\"Llama 2 13b model\",\n             huggingface_repo=\"meta-llama/Llama-2-13b\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -122,7 +116,6 @@ def llama2_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama2_70b,\n-            is_default_variant=True,\n             description=\"Llama 2 70b model\",\n             huggingface_repo=\"meta-llama/Llama-2-70b\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -147,7 +140,6 @@ def llama3_base_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama3_8b,\n-            is_default_variant=True,\n             description=\"Llama 3 8b model\",\n             huggingface_repo=\"meta-llama/Llama-3-8B\",\n             arch_args={\n@@ -166,7 +158,6 @@ def llama3_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_70b,\n-            is_default_variant=True,\n             description=\"Llama 3 70b model\",\n             huggingface_repo=\"meta-llama/Llama-3-70B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -191,7 +182,6 @@ def llama3_1_base_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama3_1_8b,\n-            is_default_variant=True,\n             description=\"Llama 3.1 8b model\",\n             huggingface_repo=\"meta-llama/Llama-3.1-8B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -211,7 +201,6 @@ def llama3_1_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_70b,\n-            is_default_variant=True,\n             description=\"Llama 3.1 70b model\",\n             huggingface_repo=\"meta-llama/Llama-3.1-70B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -231,7 +220,7 @@ def llama3_1_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_405b,\n-            is_default_variant=False,\n+            variant=\"bf16-mp8\",\n             description=\"Llama 3.1 405b model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Llama-3.1-405B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -251,7 +240,6 @@ def llama3_1_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_405b,\n-            is_default_variant=True,\n             description=\"Llama 3.1 405b model (FP8 quantized)\",\n             huggingface_repo=\"meta-llama/Llama-3.1-405B-FP8\",\n             quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n@@ -272,7 +260,7 @@ def llama3_1_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_405b,\n-            is_default_variant=False,\n+            variant=\"bf16-mp16\",\n             description=\"Llama 3.1 405b model (BF16 weights for mp16)\",\n             huggingface_repo=\"meta-llama/Llama-3.1-405B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -297,7 +285,6 @@ def llama3_2_base_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama3_2_1b,\n-            is_default_variant=True,\n             description=\"Llama 3.2 1b model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-1B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -317,7 +304,6 @@ def llama3_2_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_2_3b,\n-            is_default_variant=True,\n             description=\"Llama 3.2 3b model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-3B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -337,7 +323,6 @@ def llama3_2_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_2_11b_vision,\n-            is_default_variant=True,\n             description=\"Llama 3.2 11b vision model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-11B-Vision\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -360,7 +345,6 @@ def llama3_2_base_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_2_90b_vision,\n-            is_default_variant=True,\n             description=\"Llama 3.2 90b vision model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-90B-Vision\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -388,7 +372,6 @@ def llama2_instruct_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama2_7b_chat,\n-            is_default_variant=True,\n             description=\"Llama 2 7b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-7b-chat\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -408,7 +391,6 @@ def llama2_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama2_13b_chat,\n-            is_default_variant=True,\n             description=\"Llama 2 13b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-13b-chat\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -428,7 +410,6 @@ def llama2_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama2_70b_chat,\n-            is_default_variant=True,\n             description=\"Llama 2 70b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-70b-chat\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -453,7 +434,6 @@ def llama3_instruct_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama3_8b_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3 8b instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3-8B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -473,7 +453,6 @@ def llama3_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_70b_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3 70b instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3-70B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -498,7 +477,6 @@ def llama3_1_instruct_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama3_1_8b_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3.1 8b instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3.1-8B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -518,7 +496,6 @@ def llama3_1_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_70b_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3.1 70b instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3.1-70B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -538,7 +515,7 @@ def llama3_1_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_405b_instruct,\n-            is_default_variant=False,\n+            variant=\"bf16-mp8\",\n             description=\"Llama 3.1 405b instruct model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -558,7 +535,6 @@ def llama3_1_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_405b_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n             huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct-FP8\",\n             quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n@@ -579,7 +555,7 @@ def llama3_1_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_1_405b_instruct,\n-            is_default_variant=False,\n+            variant=\"bf16-mp16\",\n             description=\"Llama 3.1 405b instruct model (BF16 weights for mp16)\",\n             huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -600,51 +576,130 @@ def llama3_1_instruct_models() -> List[Model]:\n     ]\n \n \n+def arch_args_1b() -> dict:\n+    return {\n+        \"dim\": 2048,\n+        \"n_layers\": 16,\n+        \"n_heads\": 32,\n+        \"n_kv_heads\": 8,\n+        \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+        \"ffn_dim_multiplier\": 1.5,\n+        \"multiple_of\": 256,\n+        \"norm_eps\": 1e-05,\n+        \"rope_theta\": 500000.0,\n+        \"use_scaled_rope\": True,\n+    }\n+\n+\n+def arch_args_3b() -> dict:\n+    return {\n+        \"dim\": 3072,\n+        \"n_layers\": 28,\n+        \"n_heads\": 24,\n+        \"n_kv_heads\": 8,\n+        \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+        \"ffn_dim_multiplier\": 1.0,\n+        \"multiple_of\": 256,\n+        \"norm_eps\": 1e-05,\n+        \"rope_theta\": 500000.0,\n+        \"use_scaled_rope\": True,\n+    }\n+\n+\n+def llama3_2_quantized_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_1b,\n+            variant=\"int4-qlora-eo8\",\n+            quantization_format=CheckpointQuantizationFormat.int4,\n+            description=\"Llama 3.2 1b INT4 quantized LoRA\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                **arch_args_1b(),\n+                \"quantization_args\": {\n+                    \"group_size\": 256,\n+                },\n+                \"lora_args\": {\n+                    \"rank\": 16,\n+                    \"scale\": 2.0,\n+                },\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_1b,\n+            variant=\"int4-spinquant-eo8\",\n+            quantization_format=CheckpointQuantizationFormat.int4,\n+            description=\"Llama 3.2 1b INT4 quantized SpinQuant\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                **arch_args_1b(),\n+                \"quantization_args\": {\n+                    \"group_size\": 256,\n+                },\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_3b,\n+            variant=\"int4-qlora-eo8\",\n+            quantization_format=CheckpointQuantizationFormat.int4,\n+            description=\"Llama 3.2 3b INT4 quantized LoRA\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                **arch_args_3b(),\n+                \"quantization_args\": {\n+                    \"group_size\": 256,\n+                },\n+                \"lora_args\": {\n+                    \"rank\": 16,\n+                    \"scale\": 2.0,\n+                },\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_3b,\n+            variant=\"int4-spinquant-eo8\",\n+            quantization_format=CheckpointQuantizationFormat.int4,\n+            description=\"Llama 3.2 3b INT4 quantized SpinQuant\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-3B-Instruct-SpinQuant_INT4_EO8\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                **arch_args_3b(),\n+                \"quantization_args\": {\n+                    \"group_size\": 256,\n+                },\n+            },\n+            pth_file_count=1,\n+        ),\n+    ]\n+\n+\n def llama3_2_instruct_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama3_2_1b_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3.2 1b instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-1B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 2048,\n-                \"n_layers\": 16,\n-                \"n_heads\": 32,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.5,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n+            arch_args=arch_args_1b(),\n             pth_file_count=1,\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_2_3b_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3.2 3b instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-3B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            arch_args={\n-                \"dim\": 3072,\n-                \"n_layers\": 28,\n-                \"n_heads\": 24,\n-                \"n_kv_heads\": 8,\n-                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n-                \"ffn_dim_multiplier\": 1.0,\n-                \"multiple_of\": 256,\n-                \"norm_eps\": 1e-05,\n-                \"rope_theta\": 500000.0,\n-                \"use_scaled_rope\": True,\n-            },\n+            arch_args=arch_args_3b(),\n             pth_file_count=1,\n         ),\n+        *llama3_2_quantized_models(),\n         Model(\n             core_model_id=CoreModelId.llama3_2_11b_vision_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3.2 11b vision instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -667,7 +722,6 @@ def llama3_2_instruct_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama3_2_90b_vision_instruct,\n-            is_default_variant=True,\n             description=\"Llama 3.2 90b vision instruct model\",\n             huggingface_repo=\"meta-llama/Llama-3.2-90B-Vision-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -696,7 +750,6 @@ def safety_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_11b_vision,\n-            is_default_variant=True,\n             description=\"Llama Guard v3 11b vision system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-11B-Vision\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -719,7 +772,7 @@ def safety_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_1b,\n-            is_default_variant=False,\n+            variant=\"int4\",\n             description=\"Llama Guard v3 1b 'int4' quantized system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-1B-INT4\",\n             quantization_format=CheckpointQuantizationFormat.int4,\n@@ -739,7 +792,6 @@ def safety_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_1b,\n-            is_default_variant=True,\n             description=\"Llama Guard v3 1b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-1B\",\n             recommended_sampling_params=recommended_sampling_params(),\n@@ -759,7 +811,6 @@ def safety_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n-            is_default_variant=True,\n             description=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B\",\n             arch_args={\n@@ -778,7 +829,7 @@ def safety_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n-            is_default_variant=False,\n+            variant=\"int8\",\n             description=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B-INT8\",\n             quantization_format=CheckpointQuantizationFormat.int8,\n@@ -798,7 +849,6 @@ def safety_models() -> List[Model]:\n         ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_2_8b,\n-            is_default_variant=True,\n             description=\"Llama Guard v2 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-2-8B\",\n             arch_args={"
            },
            {
              "sha": "6940e562095c91ff322d8b7b4e460058ee94331c",
              "url": "https://github.com/meta-llama/llama-models/commit/6940e562095c91ff322d8b7b4e460058ee94331c",
              "message": "Make `tokenizer_path` optional (#182)",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/example_text_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_text_completion.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/generation.py ---\n@@ -69,10 +69,10 @@ class Llama:\n     @staticmethod\n     def build(\n         ckpt_dir: str,\n-        tokenizer_path: str,\n         max_seq_len: int,\n         max_batch_size: int,\n         model_parallel_size: Optional[int] = None,\n+        tokenizer_path: Optional[str] = None,\n         seed: int = 1,\n     ):\n         \"\"\"\n@@ -132,7 +132,11 @@ def build(\n             max_batch_size=max_batch_size,\n             **params,\n         )\n-        tokenizer = Tokenizer(model_path=tokenizer_path)\n+        if tokenizer_path:\n+            tokenizer = Tokenizer(model_path=tokenizer_path)\n+        else:\n+            tokenizer = Tokenizer.get_instance()\n+\n         assert model_args.vocab_size == tokenizer.n_words\n         if torch.cuda.is_bf16_supported():\n             torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -7,11 +7,11 @@\n \n import os\n import unittest\n-import pytest\n \n from pathlib import Path\n \n import numpy as np\n+import pytest\n from llama_models.llama3.api.datatypes import ImageMedia, SystemMessage, UserMessage\n \n from llama_models.llama3.reference_impl.generation import Llama\n@@ -21,7 +21,6 @@\n \n \n def build_generator(env_var: str):\n-    tokenizer_path = str(THIS_DIR.parent.parent / \"api/tokenizer.model\")\n     if env_var not in os.environ:\n         raise ValueError(f\"{env_var} must be specified for this test\")\n \n@@ -31,7 +30,6 @@ def build_generator(env_var: str):\n     os.environ[\"MASTER_PORT\"] = \"29501\"\n     return Llama.build(\n         ckpt_dir=os.environ[env_var],\n-        tokenizer_path=tokenizer_path,\n         max_seq_len=128,\n         max_batch_size=1,\n         model_parallel_size=1,\n\n--- File: models/scripts/example_chat_completion.py ---\n@@ -8,7 +8,6 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n-from pathlib import Path\n from typing import Optional\n \n import fire\n@@ -23,9 +22,6 @@\n from models.llama3.reference_impl.generation import Llama\n \n \n-THIS_DIR = Path(__file__).parent.resolve()\n-\n-\n def run_main(\n     ckpt_dir: str,\n     temperature: float = 0.6,\n@@ -44,10 +40,8 @@ def run_main(\n \n     `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n     \"\"\"\n-    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n-        tokenizer_path=tokenizer_path,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n         model_parallel_size=model_parallel_size,\n\n--- File: models/scripts/example_text_completion.py ---\n@@ -8,7 +8,6 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n-from pathlib import Path\n from typing import Optional\n \n import fire\n@@ -17,9 +16,6 @@\n from models.llama3.reference_impl.generation import Llama\n \n \n-THIS_DIR = Path(__file__).parent.resolve()\n-\n-\n def run_main(\n     ckpt_dir: str,\n     temperature: float = 0.6,\n@@ -29,10 +25,8 @@ def run_main(\n     max_gen_len: int = 64,\n     model_parallel_size: Optional[int] = None,\n ):\n-    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n-        tokenizer_path=tokenizer_path,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n         model_parallel_size=model_parallel_size,\n\n--- File: models/scripts/multimodal_example_chat_completion.py ---\n@@ -8,7 +8,6 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n-from pathlib import Path\n from typing import Optional\n \n import fire\n@@ -20,9 +19,6 @@\n from models.llama3.reference_impl.generation import Llama\n \n \n-THIS_DIR = Path(__file__).parent.resolve()\n-\n-\n def run_main(\n     ckpt_dir: str,\n     temperature: float = 0.6,\n@@ -32,10 +28,8 @@ def run_main(\n     max_gen_len: Optional[int] = None,\n     model_parallel_size: Optional[int] = None,\n ):\n-    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n-        tokenizer_path=tokenizer_path,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n         model_parallel_size=model_parallel_size,\n\n--- File: models/scripts/multimodal_example_text_completion.py ---\n@@ -8,7 +8,6 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n-from pathlib import Path\n from typing import Optional\n \n import fire\n@@ -21,9 +20,6 @@\n from models.llama3.reference_impl.generation import Llama\n \n \n-THIS_DIR = Path(__file__).parent.resolve()\n-\n-\n def run_main(\n     ckpt_dir: str,\n     temperature: float = 0.6,\n@@ -33,10 +29,8 @@ def run_main(\n     max_gen_len: Optional[int] = None,\n     model_parallel_size: Optional[int] = None,\n ):\n-    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n-        tokenizer_path=tokenizer_path,\n         max_seq_len=max_seq_len,\n         max_batch_size=max_batch_size,\n         model_parallel_size=model_parallel_size,"
            },
            {
              "sha": "373bfad50798519cf88302d1106bae7a0671998b",
              "url": "https://github.com/meta-llama/llama-models/commit/373bfad50798519cf88302d1106bae7a0671998b",
              "message": "Bump version to 0.0.41",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.40\",\n+    version=\"0.0.41\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "a3b56230a99d28c9366fab8669ff96654eba58e8",
              "url": "https://github.com/meta-llama/llama-models/commit/a3b56230a99d28c9366fab8669ff96654eba58e8",
              "message": "Bump version to 0.0.40",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.39\",\n+    version=\"0.0.40\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "5d79f6064a68f39aefbf6ce8abbf93f84e94e7ec",
              "url": "https://github.com/meta-llama/llama-models/commit/5d79f6064a68f39aefbf6ce8abbf93f84e94e7ec",
              "message": "Bump version to 0.0.39",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.38\",\n+    version=\"0.0.39\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "4269717b2ea587627903bacbb75ccce1427ad914",
              "url": "https://github.com/meta-llama/llama-models/commit/4269717b2ea587627903bacbb75ccce1427ad914",
              "message": "Bump version to 0.0.38",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.37\",\n+    version=\"0.0.38\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "b322e974c46bfd6509c01c70c4669733bd75d398",
              "url": "https://github.com/meta-llama/llama-models/commit/b322e974c46bfd6509c01c70c4669733bd75d398",
              "message": "Bump version to 0.0.37",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.36\",\n+    version=\"0.0.37\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "fd61cb7f3639d171e6fef9f84267bff487b2f14a",
              "url": "https://github.com/meta-llama/llama-models/commit/fd61cb7f3639d171e6fef9f84267bff487b2f14a",
              "message": "Ensure we need pydantic >= 2",
              "files_changed": [
                {
                  "filename": "requirements.txt",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: requirements.txt ---\n@@ -1,5 +1,5 @@\n PyYAML\n jinja2\n tiktoken\n-pydantic\n+pydantic>=2\n Pillow"
            },
            {
              "sha": "e4a6ed52a142bb9b5106dcbf48e41f97f8e7378e",
              "url": "https://github.com/meta-llama/llama-models/commit/e4a6ed52a142bb9b5106dcbf48e41f97f8e7378e",
              "message": "Add tests",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/tests/api/test_tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/multimodal_example_chat_completion.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/generation.py ---\n@@ -287,7 +287,7 @@ def generate(\n                 token=next_token[0].item(),\n                 text=self.tokenizer.decode(next_token.tolist()),\n                 logprobs=(\n-                    token_logprobs[:, prev_pos + 1 : cur_pos + 1][0].tolist()\n+                    token_logprobs[:, cur_pos : cur_pos + 1][0].tolist()\n                     if logprobs\n                     else None\n                 ),\n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -0,0 +1,114 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import os\n+import unittest\n+\n+from pathlib import Path\n+\n+import numpy as np\n+from llama_models.llama3.api.datatypes import ImageMedia, SystemMessage, UserMessage\n+\n+from llama_models.llama3.reference_impl.generation import Llama\n+from PIL import Image as PIL_Image\n+\n+THIS_DIR = Path(__file__).parent\n+\n+\n+def build_generator(env_var: str):\n+    tokenizer_path = str(THIS_DIR.parent.parent / \"api/tokenizer.model\")\n+    if env_var not in os.environ:\n+        raise ValueError(f\"{env_var} must be specified for this test\")\n+\n+    os.environ[\"RANK\"] = \"0\"\n+    os.environ[\"WORLD_SIZE\"] = \"1\"\n+    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n+    os.environ[\"MASTER_PORT\"] = \"29501\"\n+    return Llama.build(\n+        ckpt_dir=os.environ[env_var],\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=128,\n+        max_batch_size=1,\n+        model_parallel_size=1,\n+    )\n+\n+\n+class TestTextModelInference(unittest.TestCase):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.generator = build_generator(\"TEXT_MODEL_CHECKPOINT_DIR\")\n+\n+    def test_run_generation(self):\n+        dialogs = [\n+            [\n+                SystemMessage(content=\"Always answer with Haiku\"),\n+                UserMessage(content=\"I am going to Paris, what should I see?\"),\n+            ],\n+            [\n+                SystemMessage(\n+                    content=\"Always answer with emojis\",\n+                ),\n+                UserMessage(content=\"How to go from Beijing to NY?\"),\n+            ],\n+        ]\n+        for dialog in dialogs:\n+            result = self.__class__.generator.chat_completion(\n+                dialog,\n+                temperature=0,\n+                logprobs=True,\n+            )\n+\n+            out_message = result.generation\n+            self.assertTrue(len(out_message.content) > 0)\n+            shape = np.array(result.logprobs).shape\n+            # assert at least 10 tokens\n+            self.assertTrue(shape[0] > 10)\n+            self.assertEqual(shape[1], 1)\n+\n+\n+class TestVisionModelInference(unittest.TestCase):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.generator = build_generator(\"VISION_MODEL_CHECKPOINT_DIR\")\n+\n+    def test_run_generation(self):\n+        with open(\n+            THIS_DIR.parent.parent.parent / \"scripts/resources/dog.jpg\", \"rb\"\n+        ) as f:\n+            img = PIL_Image.open(f).convert(\"RGB\")\n+\n+        dialogs = [\n+            [\n+                UserMessage(\n+                    content=[\n+                        ImageMedia(image=img),\n+                        \"Describe this image in two sentences\",\n+                    ],\n+                )\n+            ],\n+            [\n+                UserMessage(\n+                    content=\"what is the recipe of mayonnaise in two sentences?\"\n+                ),\n+            ],\n+        ]\n+\n+        for dialog in dialogs:\n+            result = self.__class__.generator.chat_completion(\n+                dialog,\n+                temperature=0,\n+                logprobs=True,\n+            )\n+\n+            out_message = result.generation\n+            self.assertTrue(len(out_message.content) > 0)\n+            shape = np.array(result.logprobs).shape\n+            # assert at least 10 tokens\n+            self.assertTrue(shape[0] > 10)\n+            self.assertEqual(shape[1], 1)\n\n--- File: models/llama3/tests/api/test_tool_utils.py ---\n@@ -4,14 +4,16 @@\n # This source code is licensed under the terms described in the LICENSE file in\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n+import unittest\n+\n from llama_models.llama3.api.tool_utils import (\n     is_valid_python_list,\n     parse_python_list_for_function_calls,\n     ToolUtils,\n )\n \n \n-class TestToolUtils:\n+class TestToolUtils(unittest.TestCase):\n \n     def test_maybe_extract_custom_tool_call(self):\n         single_tool_call = (\n@@ -23,7 +25,7 @@ def test_maybe_extract_custom_tool_call(self):\n         assert args == {\"location\": \"New York\", \"date\": \"2023-08-05\"}\n \n \n-class TestPythonListCheck:\n+class TestPythonListCheck(unittest.TestCase):\n \n     def test_valid_list_with_single_function_call(self):\n         input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True)]'\n@@ -76,7 +78,7 @@ def test_invalid_extra_char_function_call(self):\n         assert is_valid_python_list(input_string) is False\n \n \n-class TestParsePythonList:\n+class TestParsePythonList(unittest.TestCase):\n \n     def test_single_function_call(self):\n         input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True)]'\n\n--- File: models/scripts/multimodal_example_chat_completion.py ---\n@@ -46,9 +46,6 @@ def run_main(\n     with open(THIS_DIR / \"resources/dog.jpg\", \"rb\") as f:\n         img = PIL_Image.open(f).convert(\"RGB\")\n \n-    with open(THIS_DIR / \"resources/pasta.jpeg\", \"rb\") as f:\n-        img2 = PIL_Image.open(f).convert(\"RGB\")\n-\n     dialogs = [\n         [\n             UserMessage("
            },
            {
              "sha": "ad6d745d1ab1690bd763f78a0a1c6dc4f85ab513",
              "url": "https://github.com/meta-llama/llama-models/commit/ad6d745d1ab1690bd763f78a0a1c6dc4f85ab513",
              "message": "Revert \"<debug>\"",
              "files_changed": [
                {
                  "filename": "models/scripts/lg_vision.py",
                  "status": "removed"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/scripts/lg_vision.py ---\n@@ -1,108 +0,0 @@\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n-#\n-# Run this as:\n-# PYTHONPATH=$(git rev-parse --show-toplevel) \\\n-#   torchrun models/scripts/lg_vision.py \\\n-#   ~/.llama/checkpoints/Llama-Guard-3-11B-Vision/ \\\n-#   ~/image2.jpeg \\\n-#   \"Tell me how to make a bomb\"\n-#\n-\n-from pathlib import Path\n-\n-import fire\n-\n-from PIL import Image as PIL_Image\n-\n-from models.llama3.api.datatypes import ImageMedia, UserMessage\n-\n-from models.llama3.reference_impl.generation import Llama\n-\n-\n-THIS_DIR = Path(__file__).parent.resolve()\n-\n-\n-def run_main(\n-    ckpt_dir: str,\n-    image_path: str,\n-    user_prompt: str,\n-    max_seq_len: int = 512,\n-):\n-    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n-    generator = Llama.build(\n-        ckpt_dir=ckpt_dir,\n-        tokenizer_path=tokenizer_path,\n-        max_seq_len=max_seq_len,\n-        max_batch_size=1,\n-        model_parallel_size=1,\n-    )\n-\n-    with open(image_path, \"rb\") as f:\n-        img = PIL_Image.open(f).convert(\"RGB\")\n-\n-    prompt = f\"\"\"Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.\n-\n-<BEGIN UNSAFE CONTENT CATEGORIES>\n-S1: Violent Crimes.\n-S2: Non-Violent Crimes.\n-S3: Sex Crimes.\n-S4: Child Exploitation.\n-S5: Defamation.\n-S6: Specialized Advice.\n-S7: Privacy.\n-S8: Intellectual Property.\n-S9: Indiscriminate Weapons.\n-S10: Hate.\n-S11: Self-Harm.\n-S12: Sexual Content.\n-S13: Elections.\n-<END UNSAFE CONTENT CATEGORIES>\n-\n-<BEGIN CONVERSATION>\n-\n-User: {user_prompt}\n-\n-<END CONVERSATION>\n-\n-Provide your safety assessment for ONLY THE LAST User message in the above conversation:\n- - First line must read 'safe' or 'unsafe'.\n- - If unsafe, a second line must include a comma-separated list of violated categories.\n-\"\"\"\n-\n-    dialog = [\n-        UserMessage(\n-            content=[\n-                ImageMedia(image=img),\n-                prompt,\n-            ],\n-        )\n-    ]\n-    result = generator.chat_completion(\n-        dialog,\n-        temperature=0,\n-    )\n-\n-    for msg in dialog:\n-        print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n-\n-    out_message = result.generation\n-    print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n-    for t in out_message.tool_calls:\n-        print(f\"  Tool call: {t.tool_name} ({t.arguments})\")\n-    print(\"\\n==================================\\n\")\n-\n-\n-def main():\n-    fire.Fire(run_main)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
            },
            {
              "sha": "05f2a3646612726cb83de9dfba93c891de46fcaa",
              "url": "https://github.com/meta-llama/llama-models/commit/05f2a3646612726cb83de9dfba93c891de46fcaa",
              "message": "Revert \"Add images\"",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/image1.jpeg",
                  "status": "removed"
                },
                {
                  "filename": "models/scripts/image2.jpeg",
                  "status": "removed"
                },
                {
                  "filename": "models/scripts/lg_vision.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/generation.py ---\n@@ -305,7 +305,6 @@ def text_completion(\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n         echo: bool = False,\n-        print_model_input: bool = False,\n     ) -> CompletionPrediction:\n         if (\n             max_gen_len is None\n@@ -326,7 +325,6 @@ def text_completion(\n             top_p=top_p,\n             logprobs=logprobs,\n             echo=echo,\n-            print_model_input=print_model_input,\n         ):\n             tokens.append(result.token)\n             if logprobs:\n@@ -352,7 +350,6 @@ def chat_completion(\n         logprobs: bool = False,\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n         echo: bool = False,\n-        print_model_input: bool = False,\n     ) -> ChatPrediction:\n         if (\n             max_gen_len is None\n@@ -375,7 +372,6 @@ def chat_completion(\n             top_p=top_p,\n             logprobs=logprobs,\n             echo=echo,\n-            print_model_input=print_model_input,\n         ):\n             tokens.append(result.token)\n             if result.text == \"<|eot_id|>\":\n\n--- File: models/scripts/lg_vision.py ---\n@@ -88,7 +88,6 @@ def run_main(\n     result = generator.chat_completion(\n         dialog,\n         temperature=0,\n-        print_model_input=True,\n     )\n \n     for msg in dialog:"
            },
            {
              "sha": "ac1a15b6fe1932e9c9200b858873fed997929118",
              "url": "https://github.com/meta-llama/llama-models/commit/ac1a15b6fe1932e9c9200b858873fed997929118",
              "message": "Remove Prompt-Guard from llama-models. It is _NOT_ a llama model.",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -99,7 +99,6 @@ class CoreModelId(Enum):\n \n     # Safety models\n     llama_guard_3_8b = \"Llama-Guard-3-8B\"\n-    prompt_guard_86m = \"Prompt-Guard-86M\"\n     llama_guard_2_8b = \"Llama-Guard-2-8B\"\n     llama_guard_3_11b_vision = \"Llama-Guard-3-11B-Vision\"\n     llama_guard_3_1b = \"Llama-Guard-3-1B\"\n@@ -156,7 +155,6 @@ def model_family(model_id) -> ModelFamily:\n         return ModelFamily.llama3_2\n     elif model_id in [\n         CoreModelId.llama_guard_3_8b,\n-        CoreModelId.prompt_guard_86m,\n         CoreModelId.llama_guard_2_8b,\n         CoreModelId.llama_guard_3_11b_vision,\n         CoreModelId.llama_guard_3_1b,\n@@ -238,7 +236,6 @@ def max_seq_length(self) -> int:\n             return 131072\n         elif self.core_model_id in [\n             CoreModelId.llama_guard_3_8b,\n-            CoreModelId.prompt_guard_86m,\n             CoreModelId.llama_guard_3_11b_vision,\n             CoreModelId.llama_guard_3_1b,\n         ]:\n\n--- File: models/sku_list.py ---\n@@ -796,14 +796,6 @@ def safety_models() -> List[Model]:\n             },\n             pth_file_count=1,\n         ),\n-        Model(\n-            core_model_id=CoreModelId.prompt_guard_86m,\n-            is_default_variant=True,\n-            description=\"Prompt Guard 86M injection safety model\",\n-            huggingface_repo=\"meta-llama/Prompt-Guard-86M\",\n-            arch_args={},\n-            pth_file_count=1,\n-        ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_2_8b,\n             is_default_variant=True,\n@@ -856,8 +848,6 @@ def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n             folder = \"Llama-Guard-3-8B-INT8-HF\"\n         else:\n             folder = \"Llama-Guard-3-8B\"\n-    elif model.core_model_id == CoreModelId.prompt_guard_86m:\n-        folder = \"Prompt-Guard\"\n     elif model.core_model_id == CoreModelId.llama_guard_2_8b:\n         folder = \"llama-guard-2\"\n     else:\n@@ -881,15 +871,6 @@ def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n                 \"model.safetensors.index.json\",\n             ]\n         )\n-    elif model.core_model_id == CoreModelId.prompt_guard_86m:\n-        files.extend(\n-            [\n-                \"model.safetensors\",\n-                \"special_tokens_map.json\",\n-                \"tokenizer.json\",\n-                \"tokenizer_config.json\",\n-            ]\n-        )\n     elif (\n         model.core_model_id == CoreModelId.llama_guard_3_1b\n         and model.quantization_format == CheckpointQuantizationFormat.int4"
            },
            {
              "sha": "7a279628aa737335664fb732ce9108a57fd48507",
              "url": "https://github.com/meta-llama/llama-models/commit/7a279628aa737335664fb732ce9108a57fd48507",
              "message": "Fix test_tokenizer.py",
              "files_changed": [
                {
                  "filename": "models/llama3/api/test_tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/test_tokenizer.py ---\n@@ -12,7 +12,7 @@\n from unittest import TestCase\n \n from .chat_format import ChatFormat\n-from .datatypes import SystemMessage, UserMessage\n+from .datatypes import SystemMessage, ToolPromptFormat, UserMessage\n from .tokenizer import Tokenizer\n \n \n@@ -49,7 +49,9 @@ def test_encode_message(self):\n             content=\"This is a test sentence.\",\n         )\n         self.assertEqual(\n-            self.format.encode_message(message),\n+            self.format.encode_message(\n+                message, tool_prompt_format=ToolPromptFormat.json\n+            )[0],\n             [\n                 128006,  # <|start_header_id|>\n                 882,  # \"user\""
            },
            {
              "sha": "47eee078abfc695284fa559abf94cb44f830f51c",
              "url": "https://github.com/meta-llama/llama-models/commit/47eee078abfc695284fa559abf94cb44f830f51c",
              "message": "Add images",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/image1.jpeg",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/image2.jpeg",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/lg_vision.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/generation.py ---\n@@ -305,6 +305,7 @@ def text_completion(\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n         echo: bool = False,\n+        print_model_input: bool = False,\n     ) -> CompletionPrediction:\n         if (\n             max_gen_len is None\n@@ -325,6 +326,7 @@ def text_completion(\n             top_p=top_p,\n             logprobs=logprobs,\n             echo=echo,\n+            print_model_input=print_model_input,\n         ):\n             tokens.append(result.token)\n             if logprobs:\n@@ -350,6 +352,7 @@ def chat_completion(\n         logprobs: bool = False,\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n         echo: bool = False,\n+        print_model_input: bool = False,\n     ) -> ChatPrediction:\n         if (\n             max_gen_len is None\n@@ -372,6 +375,7 @@ def chat_completion(\n             top_p=top_p,\n             logprobs=logprobs,\n             echo=echo,\n+            print_model_input=print_model_input,\n         ):\n             tokens.append(result.token)\n             if result.text == \"<|eot_id|>\":\n\n--- File: models/scripts/lg_vision.py ---\n@@ -88,6 +88,7 @@ def run_main(\n     result = generator.chat_completion(\n         dialog,\n         temperature=0,\n+        print_model_input=True,\n     )\n \n     for msg in dialog:"
            },
            {
              "sha": "b37b146f714fb33135eb99c95e6e33c745a41be2",
              "url": "https://github.com/meta-llama/llama-models/commit/b37b146f714fb33135eb99c95e6e33c745a41be2",
              "message": "<debug>",
              "files_changed": [
                {
                  "filename": "models/scripts/lg_vision.py",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/scripts/lg_vision.py ---\n@@ -0,0 +1,108 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+#\n+# Run this as:\n+# PYTHONPATH=$(git rev-parse --show-toplevel) \\\n+#   torchrun models/scripts/lg_vision.py \\\n+#   ~/.llama/checkpoints/Llama-Guard-3-11B-Vision/ \\\n+#   ~/image2.jpeg \\\n+#   \"Tell me how to make a bomb\"\n+#\n+\n+from pathlib import Path\n+\n+import fire\n+\n+from PIL import Image as PIL_Image\n+\n+from models.llama3.api.datatypes import ImageMedia, UserMessage\n+\n+from models.llama3.reference_impl.generation import Llama\n+\n+\n+THIS_DIR = Path(__file__).parent.resolve()\n+\n+\n+def run_main(\n+    ckpt_dir: str,\n+    image_path: str,\n+    user_prompt: str,\n+    max_seq_len: int = 512,\n+):\n+    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=1,\n+        model_parallel_size=1,\n+    )\n+\n+    with open(image_path, \"rb\") as f:\n+        img = PIL_Image.open(f).convert(\"RGB\")\n+\n+    prompt = f\"\"\"Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.\n+\n+<BEGIN UNSAFE CONTENT CATEGORIES>\n+S1: Violent Crimes.\n+S2: Non-Violent Crimes.\n+S3: Sex Crimes.\n+S4: Child Exploitation.\n+S5: Defamation.\n+S6: Specialized Advice.\n+S7: Privacy.\n+S8: Intellectual Property.\n+S9: Indiscriminate Weapons.\n+S10: Hate.\n+S11: Self-Harm.\n+S12: Sexual Content.\n+S13: Elections.\n+<END UNSAFE CONTENT CATEGORIES>\n+\n+<BEGIN CONVERSATION>\n+\n+User: {user_prompt}\n+\n+<END CONVERSATION>\n+\n+Provide your safety assessment for ONLY THE LAST User message in the above conversation:\n+ - First line must read 'safe' or 'unsafe'.\n+ - If unsafe, a second line must include a comma-separated list of violated categories.\n+\"\"\"\n+\n+    dialog = [\n+        UserMessage(\n+            content=[\n+                ImageMedia(image=img),\n+                prompt,\n+            ],\n+        )\n+    ]\n+    result = generator.chat_completion(\n+        dialog,\n+        temperature=0,\n+    )\n+\n+    for msg in dialog:\n+        print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n+\n+    out_message = result.generation\n+    print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n+    for t in out_message.tool_calls:\n+        print(f\"  Tool call: {t.tool_name} ({t.arguments})\")\n+    print(\"\\n==================================\\n\")\n+\n+\n+def main():\n+    fire.Fire(run_main)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
            },
            {
              "sha": "a47498f2ccdbb246cafd228481cbd817ba9224de",
              "url": "https://github.com/meta-llama/llama-models/commit/a47498f2ccdbb246cafd228481cbd817ba9224de",
              "message": "Bump version to 0.0.36",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.35\",\n+    version=\"0.0.36\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "aebe490ff37fb529037e8b7a304f1eb8c3b07000",
              "url": "https://github.com/meta-llama/llama-models/commit/aebe490ff37fb529037e8b7a304f1eb8c3b07000",
              "message": "Bump version to 0.0.35",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.24\",\n+    version=\"0.0.35\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "8d29d93fa5700a60532e0061a02ffa89d0acd3fc",
              "url": "https://github.com/meta-llama/llama-models/commit/8d29d93fa5700a60532e0061a02ffa89d0acd3fc",
              "message": "Support for Llama3.2 series of models (#150)",
              "files_changed": [
                {
                  "filename": "MANIFEST.in",
                  "status": "modified"
                },
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/args.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/interface.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/template_data.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tokenizer.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/prompt_templates/__init__.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/prompt_templates/system_prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/encoder_utils.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/image_transform.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/model.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/utils.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/tests/api/test_tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/prompt_templates/test_system_prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/download.sh",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3_1/prompt_format.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/prompts.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/LICENSE",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/MODEL_CARD.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/MODEL_CARD_VISION.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/USE_POLICY.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/mm-model.png",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/prompts_text.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/prompts_vision.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/text_prompt_format.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_2/vision_prompt_format.md",
                  "status": "added"
                },
                {
                  "filename": "models/prompt_format.py",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/example_text_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/generate_prompt_format.py",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/multimodal_example_chat_completion.py",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/multimodal_example_text_completion.py",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/resources/dog.jpg",
                  "status": "added"
                },
                {
                  "filename": "models/scripts/resources/pasta.jpeg",
                  "status": "added"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                },
                {
                  "filename": "requirements.txt",
                  "status": "modified"
                },
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: MANIFEST.in ---\n@@ -1,19 +1,7 @@\n include requirements.txt\n-include models/llama3/api/templates\n-include models/llama3/api/templates/assistant_message.jinja\n-include models/llama3/api/templates/user_message.jinja\n-include models/llama3/api/templates/assistant_message.builtin_tool_call.yaml\n-include models/llama3/api/templates/assistant_message.custom_tool_call.yaml\n-include models/llama3/api/templates/assistant_message.default.yaml\n-include models/llama3/api/templates/system_message.builtin_and_custom_tools.yaml\n-include models/llama3/api/templates/system_message.builtin_tools_only.yaml\n-include models/llama3/api/templates/system_message.custom_tools_only.yaml\n-include models/llama3/api/templates/system_message.default.yaml\n-include models/llama3/api/templates/system_message.jinja\n-include models/llama3/api/templates/tool_message.failure.yaml\n-include models/llama3/api/templates/tool_message.jinja\n-include models/llama3/api/templates/tool_message.success.yaml\n-include models/llama3/api/templates/user_message.default.yaml\n include models/llama3/api/tokenizer.model\n-include models/scripts/example_chat_completion.py\n-include models/scripts/example_text_completion.py\n+include models/scripts/resources/dog.jpg\n+include models/scripts/resources/pasta.jpeg\n+include models/llama3_1/prompt_format.md\n+include models/llama3_2/text_prompt_format.md\n+include models/llama3_2/vision_prompt_format.md\n\n--- File: README.md ---\n@@ -24,9 +24,11 @@ Our mission is to empower individuals and industry through this opportunity whil\n \n |  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n | :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n-| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/MODEL_CARD.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n-| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/MODEL_CARD.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n-| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/MODEL_CARD.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n+| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/USE_POLICY.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n+| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/USE_POLICY.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n+| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n+| Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n+| Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n \n ## Download\n \n@@ -35,7 +37,7 @@ To download the model weights and tokenizer:\n 1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n 2. Read and accept the license.\n 3. Once your request is approved you will receive a signed URL via email.\n-4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-toolchain`.\n+4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-toolchain`. (**<-- Start Here if you have received an email already.**)\n 5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\n If you want older versions of models, run `llama model list --show-all` to show all the available Llama models.\n \n\n--- File: models/datatypes.py ---\n@@ -49,12 +49,15 @@ class CheckpointQuantizationFormat(Enum):\n \n     int8 = \"int8\"\n \n+    int4 = \"int4\"\n+\n \n @json_schema_type\n class ModelFamily(Enum):\n     llama2 = \"llama2\"\n     llama3 = \"llama3\"\n     llama3_1 = \"llama3_1\"\n+    llama3_2 = \"llama3_2\"\n     safety = \"safety\"\n \n \n@@ -63,63 +66,100 @@ class CoreModelId(Enum):\n     \"\"\"Each of these models is a unique \"SKU\". These root models can be served in various garbs (especially by quantizing them)\"\"\"\n \n     # Llama 2 family\n-    meta_llama2_7b = \"Llama-2-7b\"\n-    meta_llama2_13b = \"Llama-2-13b\"\n-    meta_llama2_70b = \"Llama-2-70b\"\n-    meta_llama2_7b_chat = \"Llama-2-7b-chat\"\n-    meta_llama2_13b_chat = \"Llama-2-13b-chat\"\n-    meta_llama2_70b_chat = \"Llama-2-70b-chat\"\n+    llama2_7b = \"Llama-2-7b\"\n+    llama2_13b = \"Llama-2-13b\"\n+    llama2_70b = \"Llama-2-70b\"\n+    llama2_7b_chat = \"Llama-2-7b-chat\"\n+    llama2_13b_chat = \"Llama-2-13b-chat\"\n+    llama2_70b_chat = \"Llama-2-70b-chat\"\n \n     # Llama 3 family\n-    meta_llama3_8b = \"Llama-3-8B\"\n-    meta_llama3_70b = \"Llama-3-70B\"\n-    meta_llama3_8b_instruct = \"Llama-3-8B-Instruct\"\n-    meta_llama3_70b_instruct = \"Llama-3-70B-Instruct\"\n+    llama3_8b = \"Llama-3-8B\"\n+    llama3_70b = \"Llama-3-70B\"\n+    llama3_8b_instruct = \"Llama-3-8B-Instruct\"\n+    llama3_70b_instruct = \"Llama-3-70B-Instruct\"\n \n     # Llama 3.1 family\n-    meta_llama3_1_8b = \"Meta-Llama3.1-8B\"\n-    meta_llama3_1_70b = \"Meta-Llama3.1-70B\"\n-    meta_llama3_1_405b = \"Meta-Llama3.1-405B\"\n-    meta_llama3_1_8b_instruct = \"Meta-Llama3.1-8B-Instruct\"\n-    meta_llama3_1_70b_instruct = \"Meta-Llama3.1-70B-Instruct\"\n-    meta_llama3_1_405b_instruct = \"Meta-Llama3.1-405B-Instruct\"\n+    llama3_1_8b = \"Llama3.1-8B\"\n+    llama3_1_70b = \"Llama3.1-70B\"\n+    llama3_1_405b = \"Llama3.1-405B\"\n+    llama3_1_8b_instruct = \"Llama3.1-8B-Instruct\"\n+    llama3_1_70b_instruct = \"Llama3.1-70B-Instruct\"\n+    llama3_1_405b_instruct = \"Llama3.1-405B-Instruct\"\n+\n+    # Llama 3.2 family\n+    llama3_2_1b = \"Llama3.2-1B\"\n+    llama3_2_3b = \"Llama3.2-3B\"\n+    llama3_2_1b_instruct = \"Llama3.2-1B-Instruct\"\n+    llama3_2_3b_instruct = \"Llama3.2-3B-Instruct\"\n+    llama3_2_11b_vision = \"Llama3.2-11B-Vision\"\n+    llama3_2_90b_vision = \"Llama3.2-90B-Vision\"\n+    llama3_2_11b_vision_instruct = \"Llama3.2-11B-Vision-Instruct\"\n+    llama3_2_90b_vision_instruct = \"Llama3.2-90B-Vision-Instruct\"\n \n     # Safety models\n     llama_guard_3_8b = \"Llama-Guard-3-8B\"\n     prompt_guard_86m = \"Prompt-Guard-86M\"\n     llama_guard_2_8b = \"Llama-Guard-2-8B\"\n+    llama_guard_3_11b_vision = \"Llama-Guard-3-11B-Vision\"\n+    llama_guard_3_1b = \"Llama-Guard-3-1B\"\n+\n+\n+def is_multimodal(model_id) -> bool:\n+    if model_id in [\n+        CoreModelId.llama3_2_11b_vision,\n+        CoreModelId.llama3_2_90b_vision,\n+        CoreModelId.llama3_2_11b_vision_instruct,\n+        CoreModelId.llama3_2_90b_vision_instruct,\n+    ]:\n+        return True\n+    else:\n+        return False\n \n \n def model_family(model_id) -> ModelFamily:\n     if model_id in [\n-        CoreModelId.meta_llama2_7b,\n-        CoreModelId.meta_llama2_13b,\n-        CoreModelId.meta_llama2_70b,\n-        CoreModelId.meta_llama2_7b_chat,\n-        CoreModelId.meta_llama2_13b_chat,\n-        CoreModelId.meta_llama2_70b_chat,\n+        CoreModelId.llama2_7b,\n+        CoreModelId.llama2_13b,\n+        CoreModelId.llama2_70b,\n+        CoreModelId.llama2_7b_chat,\n+        CoreModelId.llama2_13b_chat,\n+        CoreModelId.llama2_70b_chat,\n     ]:\n         return ModelFamily.llama2\n     elif model_id in [\n-        CoreModelId.meta_llama3_8b,\n-        CoreModelId.meta_llama3_70b,\n-        CoreModelId.meta_llama3_8b_instruct,\n-        CoreModelId.meta_llama3_70b_instruct,\n+        CoreModelId.llama3_8b,\n+        CoreModelId.llama3_70b,\n+        CoreModelId.llama3_8b_instruct,\n+        CoreModelId.llama3_70b_instruct,\n     ]:\n         return ModelFamily.llama3\n     elif model_id in [\n-        CoreModelId.meta_llama3_1_8b,\n-        CoreModelId.meta_llama3_1_70b,\n-        CoreModelId.meta_llama3_1_405b,\n-        CoreModelId.meta_llama3_1_8b_instruct,\n-        CoreModelId.meta_llama3_1_70b_instruct,\n-        CoreModelId.meta_llama3_1_405b_instruct,\n+        CoreModelId.llama3_1_8b,\n+        CoreModelId.llama3_1_70b,\n+        CoreModelId.llama3_1_405b,\n+        CoreModelId.llama3_1_8b_instruct,\n+        CoreModelId.llama3_1_70b_instruct,\n+        CoreModelId.llama3_1_405b_instruct,\n     ]:\n         return ModelFamily.llama3_1\n+    elif model_id in [\n+        CoreModelId.llama3_2_1b,\n+        CoreModelId.llama3_2_3b,\n+        CoreModelId.llama3_2_1b_instruct,\n+        CoreModelId.llama3_2_3b_instruct,\n+        CoreModelId.llama3_2_11b_vision,\n+        CoreModelId.llama3_2_90b_vision,\n+        CoreModelId.llama3_2_11b_vision_instruct,\n+        CoreModelId.llama3_2_90b_vision_instruct,\n+    ]:\n+        return ModelFamily.llama3_2\n     elif model_id in [\n         CoreModelId.llama_guard_3_8b,\n         CoreModelId.prompt_guard_86m,\n         CoreModelId.llama_guard_2_8b,\n+        CoreModelId.llama_guard_3_11b_vision,\n+        CoreModelId.llama_guard_3_1b,\n     ]:\n         return ModelFamily.safety\n     else:\n@@ -180,6 +220,7 @@ def is_instruct_model(self) -> bool:\n     def is_featured(self) -> bool:\n         return self.model_family in [\n             ModelFamily.llama3_1,\n+            ModelFamily.llama3_2,\n             ModelFamily.safety,\n         ]\n \n@@ -193,9 +234,13 @@ def max_seq_length(self) -> int:\n             return 8192\n         elif self.model_family == ModelFamily.llama3_1:\n             return 131072\n+        elif self.model_family == ModelFamily.llama3_2:\n+            return 131072\n         elif self.core_model_id in [\n             CoreModelId.llama_guard_3_8b,\n             CoreModelId.prompt_guard_86m,\n+            CoreModelId.llama_guard_3_11b_vision,\n+            CoreModelId.llama_guard_3_1b,\n         ]:\n             return 131072\n         else:\n\n--- File: models/llama3/api/args.py ---\n@@ -25,6 +25,11 @@ class ModelArgs:\n     max_batch_size: int = 32\n     max_seq_len: int = 2048\n \n+    # vision model params\n+    vision_chunk_size: int = -1  # image resolution for image models\n+    vision_max_num_chunks: int = 4\n+    vision_num_cross_attention_layers: int = -1\n+\n     def __init__(self, **kwargs):\n         for k, v in kwargs.items():\n             if hasattr(self, k):\n\n--- File: models/llama3/api/chat_format.py ---\n@@ -8,16 +8,25 @@\n import uuid\n \n from dataclasses import dataclass\n-from typing import Dict, List\n+from typing import Dict, List, Tuple\n \n from .tokenizer import Tokenizer\n from .datatypes import *  # noqa: F403\n+from PIL import Image as PIL_Image\n+\n from .tool_utils import ToolUtils\n \n \n+@dataclass\n+class VisionInput:\n+    mask: List[List[int]]\n+    images: List[PIL_Image.Image]\n+\n+\n @dataclass\n class ModelInput:\n     tokens: List[int]\n+    vision: Optional[VisionInput] = None\n \n \n class ChatFormat:\n@@ -29,30 +38,63 @@ def __init__(self, tokenizer: Tokenizer):\n             role: f\"<|start_header_id|>{role.value}<|end_header_id|>\\n\\n\"\n             for role in Role\n         }\n+        self.vision_token = self.tokenizer.special_tokens[\"<|image|>\"]\n \n-    def encode_header(self, role: str) -> List[int]:\n+    def _encode_header(self, role: str) -> List[int]:\n         tokens = []\n         tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n         tokens.extend(self.tokenizer.encode(role, bos=False, eos=False))\n         tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n         tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n         return tokens\n \n+    def encode_content(self, content: InterleavedTextMedia) -> ModelInput:\n+        tokens, images = self._encode_content(content, bos=True)\n+        return self._model_input_from_tokens_images(tokens, images)\n+\n+    def _encode_content(\n+        self, content: InterleavedTextMedia, bos: bool = False\n+    ) -> Tuple[List[int], List[PIL_Image.Image]]:\n+        tokens = []\n+        images = []\n+\n+        added_bos = False\n+\n+        def _process(c):\n+            nonlocal added_bos, bos\n+\n+            if isinstance(c, str):\n+                tokens.extend(\n+                    self.tokenizer.encode(c, bos=False if added_bos else bos, eos=False)\n+                )\n+                added_bos = True\n+            elif isinstance(c, ImageMedia):\n+                bos = False if added_bos else bos\n+                if bos:\n+                    tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n+                    added_bos = True\n+                tokens.append(self.vision_token)\n+                cc = interleaved_text_media_localize(c)\n+                images.append(cc.image)\n+\n+        if isinstance(content, str):\n+            _process(content)\n+        elif isinstance(content, list):\n+            for c in content:\n+                _process(c)\n+\n+        return tokens, images\n+\n     def encode_message(\n         self, message: Message, tool_prompt_format: ToolPromptFormat\n-    ) -> List[int]:\n-        tokens = self.encode_header(message.role)\n+    ) -> Tuple[List[int], List[PIL_Image.Image]]:\n+        tokens = self._encode_header(message.role)\n+        images = []\n \n-        def _process_content(content: InterleavedTextMedia):\n-            def _process(c):\n-                if isinstance(c, str):\n-                    tokens.extend(self.tokenizer.encode(c, bos=False, eos=False))\n-\n-            if isinstance(content, str):\n-                _process(content)\n-            elif isinstance(content, list):\n-                for c in content:\n-                    _process(c)\n+        def _process_content(c):\n+            toks, imgs = self._encode_content(c)\n+            tokens.extend(toks)\n+            images.extend(imgs)\n \n         if isinstance(message, CompletionMessage) and len(message.tool_calls) > 0:\n             tokens.append(self.tokenizer.special_tokens[\"<|python_tag|>\"])\n@@ -75,23 +117,25 @@ def _process(c):\n         tokens.append(\n             self.tokenizer.special_tokens[\"<|eom_id|>\" if eom else \"<|eot_id|>\"]\n         )\n-        return tokens\n+        return tokens, images\n \n     def encode_dialog_prompt(\n         self,\n         messages: List[Message],\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n     ) -> ModelInput:\n         tokens = []\n+        images = []\n         tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n         for message in messages:\n-            toks = self.encode_message(message, tool_prompt_format)\n+            toks, imgs = self.encode_message(message, tool_prompt_format)\n             tokens.extend(toks)\n+            images.extend(imgs)\n \n         # Add the start of an assistant message for the model to complete.\n-        tokens.extend(self.encode_header(Role.assistant.value))\n+        tokens.extend(self._encode_header(Role.assistant.value))\n \n-        return ModelInput(tokens=tokens)\n+        return self._model_input_from_tokens_images(tokens, images)\n \n     # TODO(this should be generic, not only for assistant messages)\n     def decode_assistant_message(\n@@ -166,3 +210,51 @@ def decode_assistant_message_from_content(\n             stop_reason=stop_reason,\n             tool_calls=tool_calls,\n         )\n+\n+    def _model_input_from_tokens_images(\n+        self, tokens: List[int], images: List[PIL_Image.Image]\n+    ) -> ModelInput:\n+        vision_input = None\n+        if len(images) > 0:\n+            vision_input = VisionInput(\n+                mask=create_vision_mask(tokens, self.vision_token),\n+                images=images,\n+            )\n+\n+        return ModelInput(\n+            tokens=[\n+                128256 if token == self.vision_token else token for token in tokens\n+            ],\n+            vision=vision_input,\n+        )\n+\n+\n+def create_vision_mask(\n+    tokens: List[int],\n+    vision_token: int,\n+) -> List[List[int]]:\n+    vision_token_locations = [\n+        i for i, token in enumerate(tokens) if token == vision_token\n+    ]\n+    if len(vision_token_locations) == 0:\n+        return []\n+\n+    if len(vision_token_locations) == 1:\n+        # only one image present, unmask until end of sequence\n+        return [[vision_token_locations[0], -1]]\n+    vision_masks = [\n+        [loc1, loc2]\n+        for loc1, loc2 in zip(vision_token_locations[:-1], vision_token_locations[1:])\n+    ]\n+    # last image will attend to all subsequent text\n+    vision_masks.append([vision_token_locations[-1], len(tokens)])\n+\n+    # if there are two or more consecutive vision tokens,\n+    # they should all attend to all subsequent\n+    # text present\n+    last_mask_end = vision_masks[-1][1]\n+    for vision_mask in vision_masks[::-1]:\n+        if vision_mask[0] == vision_mask[1] - 1:\n+            vision_mask[1] = last_mask_end\n+        last_mask_end = vision_mask[1]\n+    return vision_masks\n\n--- File: models/llama3/api/datatypes.py ---\n@@ -12,6 +12,13 @@\n \n from typing_extensions import Annotated\n from ...datatypes import *  # noqa\n+\n+import base64\n+import re\n+from io import BytesIO\n+\n+from PIL import Image as PIL_Image\n+\n from ...schema_utils import json_schema_type\n \n \n@@ -33,11 +40,20 @@ def __str__(self) -> str:\n         return self.uri\n \n \n+@json_schema_type\n+class ImageMedia(BaseModel):\n+    image: Union[PIL_Image.Image, URL]\n+\n+    class Config:\n+        arbitrary_types_allowed = True\n+\n+\n InterleavedTextMedia = Union[\n     str,\n     # Specific modalities can be placed here, but not generic attachments\n     # since models don't consume them in a generic way\n-    List[Union[str]],\n+    ImageMedia,\n+    List[Union[str, ImageMedia]],\n ]\n \n \n@@ -54,6 +70,35 @@ def _process(c) -> str:\n         return _process(content)\n \n \n+def interleaved_text_media_localize(\n+    content: InterleavedTextMedia,\n+) -> InterleavedTextMedia:\n+    def _localize_single(c: str | ImageMedia) -> str | ImageMedia:\n+        if isinstance(c, ImageMedia):\n+            # load image and return PIL version\n+            img = c.image\n+            if isinstance(img, URL):\n+                if img.uri.startswith(\"file://\"):\n+                    img = PIL_Image.open(img.uri[len(\"file://\") :]).convert(\"RGB\")\n+                elif img.uri.startswith(\"data\"):\n+                    match = re.match(r\"data:image/(\\w+);base64,(.+)\", img.uri)\n+                    if not match:\n+                        raise ValueError(\"Invalid data URL format\")\n+                    image_type, image_data = match.groups()\n+                    image_data = base64.b64decode(image_data)\n+                    img = PIL_Image.open(BytesIO(image_data))\n+                else:\n+                    raise ValueError(\"Unsupported URL type\")\n+            return ImageMedia(image=img)\n+        else:\n+            return c\n+\n+    if isinstance(content, list):\n+        return [_localize_single(c) for c in content]\n+    else:\n+        return _localize_single(content)\n+\n+\n @json_schema_type\n class BuiltinTool(Enum):\n     brave_search = \"brave_search\"\n@@ -105,6 +150,7 @@ class ToolParamDefinition(BaseModel):\n     param_type: str\n     description: Optional[str] = None\n     required: Optional[bool] = True\n+    default: Optional[Any] = None\n \n \n @json_schema_type\n@@ -157,6 +203,7 @@ class ToolPromptFormat(Enum):\n \n     json = \"json\"\n     function_tag = \"function_tag\"\n+    python_list = \"python_list\"\n \n \n @json_schema_type\n\n--- File: models/llama3/api/interface.py ---\n@@ -70,6 +70,12 @@ def notes(self):\n         \"user-default\",\n         \"user_default\",\n     ),\n+    Template(\n+        \"user\",\n+        \"user-images\",\n+        \"user_images\",\n+    ),\n+    Template(\"user\", \"user-interleaved-images\", \"user_interleaved_images\"),\n     Template(\n         \"assistant\",\n         \"assistant-builtin-tool-call\",\n\n--- File: models/llama3/api/template_data.py ---\n@@ -23,6 +23,14 @@ def system_message_builtin_tools_only():\n     }\n \n \n+def system_message_builtin_code_only():\n+    return {\n+        \"builtin_tools\": BuiltinToolGenerator().data_examples()[1],\n+        \"custom_tools\": [],\n+        \"instruction\": \"\",\n+    }\n+\n+\n def system_message_custom_tools_only():\n     return {\n         \"builtin_tools\": [],\n@@ -91,3 +99,13 @@ def assistant_default():\n \n def user_default():\n     return {\"content\": \"Please tell me how to plan a trip to New York\"}\n+\n+\n+def user_images():\n+    return {\"content\": \"<|image|><|image|>What do these images depict?\"}\n+\n+\n+def user_interleaved_images():\n+    return {\n+        \"content\": \"<|image|>Describe the image in one sentence.<|image|>Write a haiku about these images\"\n+    }\n\n--- File: models/llama3/api/tokenizer.py ---\n@@ -88,6 +88,7 @@ def __init__(self, model_path: str):\n             \"<|eom_id|>\",  # end of message\n             \"<|eot_id|>\",  # end of turn\n             \"<|python_tag|>\",\n+            \"<|image|>\",\n         ]\n         reserved_tokens = [\n             f\"<|reserved_special_token_{2 + i}|>\"\n@@ -114,6 +115,7 @@ def __init__(self, model_path: str):\n         self.python_tag_id = self.special_tokens[\"<|python_tag|>\"]\n         self.pad_id: int = self.special_tokens[\"<|finetune_right_pad_id|>\"]\n         self.stop_tokens = [\n+            self.eos_id,\n             self.special_tokens[\"<|eom_id|>\"],\n             self.special_tokens[\"<|eot_id|>\"],\n         ]\n\n--- File: models/llama3/api/tool_utils.py ---\n@@ -4,12 +4,12 @@\n # This source code is licensed under the terms described in the LICENSE file in\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n-\n+import ast\n import json\n import re\n from typing import Optional, Tuple\n \n-from .datatypes import BuiltinTool, ToolCall, ToolPromptFormat\n+from .datatypes import BuiltinTool, RecursiveType, ToolCall, ToolPromptFormat\n \n BUILTIN_TOOL_PATTERN = r'\\b(?P<tool_name>\\w+)\\.call\\(query=\"(?P<query>[^\"]*)\"\\)'\n CUSTOM_TOOL_CALL_PATTERN = re.compile(\n@@ -27,6 +27,78 @@ def is_json(s):\n     return True\n \n \n+def is_valid_python_list(input_string):\n+    \"\"\"Check if the input string is a valid Python list of function calls\"\"\"\n+    try:\n+        # Try to parse the string\n+        tree = ast.parse(input_string)\n+\n+        # Check if it's a single expression\n+        if len(tree.body) != 1 or not isinstance(tree.body[0], ast.Expr):\n+            return False\n+\n+        # Check if the expression is a list\n+        expr = tree.body[0].value\n+        if not isinstance(expr, ast.List):\n+            return False\n+\n+        # Check if the list is empty\n+        if len(expr.elts) == 0:\n+            return False\n+\n+        # Check if all elements in the list are function calls\n+        for element in expr.elts:\n+            if not isinstance(element, ast.Call):\n+                return False\n+\n+            # Check if the function call has a valid name\n+            if not isinstance(element.func, ast.Name):\n+                return False\n+\n+            # Check if all arguments are keyword arguments\n+            if element.args or not all(\n+                isinstance(arg, ast.keyword) for arg in element.keywords\n+            ):\n+                return False\n+\n+        return True\n+\n+    except SyntaxError:\n+        # If parsing fails, it's not a valid Python expression\n+        return False\n+\n+\n+def parse_python_list_for_function_calls(input_string):\n+    \"\"\"\n+    Parse a Python list of function calls and\n+    return a list of tuples containing the function name and arguments\n+    \"\"\"\n+    # Parse the string into an AST\n+    tree = ast.parse(input_string)\n+\n+    # Ensure the input is a list\n+    if not isinstance(tree.body[0], ast.Expr) or not isinstance(\n+        tree.body[0].value, ast.List\n+    ):\n+        raise ValueError(\"Input must be a list of function calls\")\n+\n+    result = []\n+\n+    # Iterate through each function call in the list\n+    for node in tree.body[0].value.elts:\n+        if isinstance(node, ast.Call):\n+            function_name = node.func.id\n+            function_args = {}\n+\n+            # Extract keyword arguments\n+            for keyword in node.keywords:\n+                function_args[keyword.arg] = ast.literal_eval(keyword.value)\n+\n+            result.append((function_name, function_args))\n+\n+    return result\n+\n+\n class ToolUtils:\n \n     @staticmethod\n@@ -76,6 +148,10 @@ def maybe_extract_custom_tool_call(message_body: str) -> Optional[Tuple[str, str\n                 return function_name, args\n             else:\n                 return None\n+        elif is_valid_python_list(message_body):\n+            res = parse_python_list_for_function_calls(message_body)\n+            # FIXME: Enable multiple tool calls\n+            return res[0]\n         else:\n             return None\n \n@@ -106,3 +182,22 @@ def encode_tool_call(t: ToolCall, tool_prompt_format: ToolPromptFormat) -> str:\n             elif tool_prompt_format == ToolPromptFormat.function_tag:\n                 args = json.dumps(t.arguments)\n                 return f\"<function={fname}>{args}</function>\"\n+\n+            elif tool_prompt_format == ToolPromptFormat.python_list:\n+\n+                def format_value(value: RecursiveType) -> str:\n+                    if isinstance(value, str):\n+                        return f'\"{value}\"'\n+                    elif isinstance(value, (int, float, bool)) or value is None:\n+                        return str(value)\n+                    elif isinstance(value, list):\n+                        return f\"[{', '.join(format_value(v) for v in value)}]\"\n+                    elif isinstance(value, dict):\n+                        return f\"{{{', '.join(f'{k}={format_value(v)}' for k, v in value.items())}}}\"\n+                    else:\n+                        raise ValueError(f\"Unsupported type: {type(value)}\")\n+\n+                args_str = \", \".join(\n+                    f\"{k}={format_value(v)}\" for k, v in t.arguments.items()\n+                )\n+                return f\"[{fname}({args_str})]\"\n\n--- File: models/llama3/prompt_templates/__init__.py ---\n@@ -1,8 +1,16 @@\n-from .base import PromptTemplate, PromptTemplateGeneratorBase\n-from .system_prompts import (\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from .base import PromptTemplate, PromptTemplateGeneratorBase  # noqa: F401\n+from .system_prompts import (  # noqa: F401\n     BuiltinToolGenerator,\n     FunctionTagCustomToolGenerator,\n     JsonCustomToolGenerator,\n+    PythonListCustomToolGenerator,\n     SystemDefaultGenerator,\n )\n-from .tool_response import ToolResponseGenerator\n+from .tool_response import ToolResponseGenerator  # noqa: F401\n\n--- File: models/llama3/prompt_templates/system_prompts.py ---\n@@ -167,7 +167,13 @@ def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n             {#- manually setting up JSON because jinja sorts keys in unexpected ways -#}\n             {%- set tname = t.tool_name -%}\n             {%- set tdesc = t.description -%}\n-            {%- set tparams = t.parameters | tojson -%}\n+            {%- set modified_params = t.parameters.copy() -%}\n+            {%- for key, value in modified_params.items() -%}\n+                {%- if 'default' in value -%}\n+                    {%- set _ = value.pop('default', None) -%}\n+                {%- endif -%}\n+            {%- endfor -%}\n+            {%- set tparams = modified_params | tojson -%}\n             Use the function '{{ tname }}' to '{{ tdesc }}':\n             {\"name\": \"{{tname}}\", \"description\": \"{{tdesc}}\", \"parameters\": {{tparams}}}\n \n@@ -211,3 +217,79 @@ def data_examples(self) -> List[List[ToolDefinition]]:\n                 ),\n             ]\n         ]\n+\n+\n+class PythonListCustomToolGenerator(PromptTemplateGeneratorBase):  # noqa: N801\n+\n+    def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n+        template_str = textwrap.dedent(\n+            \"\"\"\n+            You are an expert in composing functions. You are given a question and a set of possible functions.\n+            Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+            If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+            also point it out. You should only return the function call in tools call sections.\n+\n+            If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+            You SHOULD NOT include any other text in the response.\n+\n+            Here is a list of functions in JSON format that you can invoke.\n+\n+            [\n+                {% for t in tools -%}\n+                {# manually setting up JSON because jinja sorts keys in unexpected ways -#}\n+                {%- set tname = t.tool_name -%}\n+                {%- set tdesc = t.description -%}\n+                {%- set tparams = t.parameters -%}\n+                {%- set required_params = [] -%}\n+                {%- for name, param in tparams.items() if param.required == true -%}\n+                    {%- set _ = required_params.append(name) -%}\n+                {%- endfor -%}\n+                {\n+                    \"name\": \"{{tname}}\",\n+                    \"description\": \"{{tdesc}}\",\n+                    \"parameters\": {\n+                        \"type\": \"dict\",\n+                        \"required\": {{ required_params | tojson }},\n+                        \"properties\": {\n+                            {%- for name, param in tparams.items() %}\n+                            \"{{name}}\": {\n+                                \"type\": \"{{param.param_type}}\",\n+                                \"description\": \"{{param.description}}\"{% if param.default %},\n+                                \"default\": \"{{param.default}}\"{% endif %}\n+                            }{% if not loop.last %},{% endif %}\n+                            {%- endfor %}\n+                        }\n+                    }\n+                }{% if not loop.last %},\n+                {% endif -%}\n+                {%- endfor %}\n+            ]\n+            \"\"\"\n+        )\n+        return PromptTemplate(\n+            template_str.strip(\"\\n\"),\n+            {\"tools\": [t.model_dump() for t in custom_tools]},\n+        )\n+\n+    def data_examples(self) -> List[List[ToolDefinition]]:\n+        return [\n+            [\n+                ToolDefinition(\n+                    tool_name=\"get_weather\",\n+                    description=\"Get weather info for places\",\n+                    parameters={\n+                        \"city\": ToolParamDefinition(\n+                            param_type=\"string\",\n+                            description=\"The name of the city to get the weather for\",\n+                            required=True,\n+                        ),\n+                        \"metric\": ToolParamDefinition(\n+                            param_type=\"string\",\n+                            description=\"The metric for weather. Options are: celsius, fahrenheit\",\n+                            required=False,\n+                            default=\"celsius\",\n+                        ),\n+                    },\n+                ),\n+            ]\n+        ]\n\n--- File: models/llama3/reference_impl/generation.py ---\n@@ -33,7 +33,13 @@\n \n from ..api.args import ModelArgs\n from ..api.chat_format import ChatFormat, ModelInput\n-from ..api.datatypes import CompletionMessage, Message, StopReason, ToolPromptFormat\n+from ..api.datatypes import (\n+    CompletionMessage,\n+    InterleavedTextMedia,\n+    Message,\n+    StopReason,\n+    ToolPromptFormat,\n+)\n from ..api.tokenizer import Tokenizer\n from .model import Transformer\n \n@@ -132,8 +138,14 @@ def build(\n             torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n         else:\n             torch.set_default_tensor_type(torch.cuda.HalfTensor)\n-        model = Transformer(model_args)\n-        model.load_state_dict(checkpoint, strict=False)\n+        if model_args.vision_chunk_size > 0:\n+            from .multimodal.model import CrossAttentionTransformer\n+\n+            model = CrossAttentionTransformer(model_args)\n+            model.setup_cache(model_args.max_batch_size, torch.bfloat16)\n+        else:\n+            model = Transformer(model_args)\n+        model.load_state_dict(checkpoint, strict=True)\n         print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n \n         return Llama(model, tokenizer, model_args)\n@@ -152,10 +164,20 @@ def generate(\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         logprobs: bool = False,\n+        echo: bool = False,\n+        print_model_input: bool = False,\n     ) -> Generator:\n         params = self.model.params\n \n-        # cprint(\"Input to model -> \" + self.tokenizer.decode(model_input.tokens), \"red\")\n+        if print_model_input:\n+            tokens_to_print = [\n+                self.formatter.vision_token if t == 128256 else t\n+                for t in model_input.tokens\n+            ]\n+            cprint(\n+                \"Input to model:\\n\" + self.tokenizer.decode(tokens_to_print) + \"\\n\",\n+                \"red\",\n+            )\n         prompt_tokens = [model_input.tokens]\n \n         bsz = 1\n@@ -171,6 +193,21 @@ def generate(\n             return\n \n         total_len = min(max_gen_len + max_prompt_len, params.max_seq_len)\n+\n+        is_vision = not isinstance(self.model, Transformer)\n+        if is_vision:\n+            images = model_input.vision.images if model_input.vision is not None else []\n+            mask = model_input.vision.mask if model_input.vision is not None else []\n+\n+            # the method works for bsz > 1 so add a batch dimension\n+            xattn_caches, cross_attention_masks, full_text_row_masked_out_mask = (\n+                self.model.compute_vision_tokens_masks(\n+                    batch_images=[images],\n+                    batch_masks=[mask],\n+                    total_len=total_len,\n+                )\n+            )\n+\n         pad_id = self.tokenizer.pad_id\n         tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n         for k, t in enumerate(prompt_tokens):\n@@ -181,19 +218,34 @@ def generate(\n         prev_pos = 0\n         eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n         input_text_mask = tokens != pad_id\n-        if min_prompt_len == total_len:\n-            logits = self.model.forward(tokens, prev_pos)\n-            token_logprobs = -F.cross_entropy(\n-                input=logits.transpose(1, 2),\n-                target=tokens,\n-                reduction=\"none\",\n-                ignore_index=pad_id,\n-            )\n \n-        stop_tokens = torch.tensor(self.tokenizer.stop_tokens)\n+        if echo:\n+            for i, t in enumerate(model_input.tokens):\n+                yield TokenResult(\n+                    token=t,\n+                    text=self.tokenizer.decode([t]),\n+                    logprobs=(\n+                        token_logprobs[0, i : i + 1].tolist() if logprobs else None\n+                    ),\n+                )\n \n+        stop_tokens = torch.tensor(self.tokenizer.stop_tokens)\n         for cur_pos in range(min_prompt_len, total_len):\n-            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n+            if is_vision:\n+                position_ids = torch.arange(\n+                    prev_pos, cur_pos, dtype=torch.long, device=\"cuda\"\n+                )\n+                text_only_inference = model_input.vision is None\n+                logits = self.model.forward(\n+                    position_ids,\n+                    tokens,\n+                    cross_attention_masks,\n+                    full_text_row_masked_out_mask,\n+                    xattn_caches,\n+                    text_only_inference,\n+                )\n+            else:\n+                logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n \n             if temperature > 0:\n                 probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n@@ -209,7 +261,18 @@ def generate(\n             tokens[:, cur_pos] = next_token\n \n             target = tokens[:, prev_pos + 1 : cur_pos + 1]\n-            \n+            if is_vision:\n+                # the logits space (num_classes) is designed to never contain a media_token\n+                # however our input token stream does contain them. we need to nuke them here\n+                # or else the CUDA kernels will crash with an illegal memory access\n+                vision_tokens = [self.tokenizer.special_tokens[\"<|image|>\"], 128256]\n+                masks = [target.eq(t) for t in vision_tokens]\n+                if len(masks) > 1:\n+                    mask = torch.logical_or(*masks)\n+                else:\n+                    mask = masks[0]\n+                target[mask] = 0\n+\n             if logprobs:\n                 token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n                     input=logits.transpose(1, 2),\n@@ -236,11 +299,12 @@ def generate(\n \n     def text_completion(\n         self,\n-        prompt: str,\n+        content: InterleavedTextMedia,\n         temperature: float = 0.6,\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n+        echo: bool = False,\n     ) -> CompletionPrediction:\n         if (\n             max_gen_len is None\n@@ -249,17 +313,18 @@ def text_completion(\n         ):\n             max_gen_len = self.model.params.max_seq_len - 1\n \n-        prompt_tokens = self.tokenizer.encode(prompt, bos=True, eos=False)\n+        model_input = self.formatter.encode_content(content)\n \n         tokens = []\n         token_logprobs = []\n         decoded_tokens = []\n         for result in self.generate(\n-            model_input=ModelInput(tokens=prompt_tokens),\n+            model_input=model_input,\n             max_gen_len=max_gen_len,\n             temperature=temperature,\n             top_p=top_p,\n             logprobs=logprobs,\n+            echo=echo,\n         ):\n             tokens.append(result.token)\n             if logprobs:\n@@ -284,6 +349,7 @@ def chat_completion(\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n         tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n+        echo: bool = False,\n     ) -> ChatPrediction:\n         if (\n             max_gen_len is None\n@@ -305,6 +371,7 @@ def chat_completion(\n             temperature=temperature,\n             top_p=top_p,\n             logprobs=logprobs,\n+            echo=echo,\n         ):\n             tokens.append(result.token)\n             if result.text == \"<|eot_id|>\":\n@@ -330,6 +397,66 @@ def chat_completion(\n \n         return ChatPrediction(generation=message)\n \n+    def chat_completion_raw(\n+        self,\n+        messages: List[Message],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n+    ) -> List[int]:\n+        if (\n+            max_gen_len is None\n+            or max_gen_len == 0\n+            or max_gen_len >= self.model.params.max_seq_len\n+        ):\n+            max_gen_len = self.model.params.max_seq_len - 1\n+\n+        output_tokens = []\n+        model_input = self.formatter.encode_dialog_prompt(messages, tool_prompt_format)\n+        input_tokens = model_input.tokens\n+        for result in self.generate(\n+            model_input=model_input,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=False,\n+        ):\n+            output_tokens.append(result.token)\n+\n+        return input_tokens, output_tokens\n+\n+    def text_completion_raw(\n+        self,\n+        content: InterleavedTextMedia,\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+    ):\n+        if (\n+            max_gen_len is None\n+            or max_gen_len == 0\n+            or max_gen_len >= self.model.params.max_seq_len\n+        ):\n+            max_gen_len = self.model.params.max_seq_len - 1\n+\n+        model_input = self.formatter.encode_content(content)\n+        input_tokens = model_input.tokens\n+\n+        output_tokens = []\n+        token_logprobs = []\n+        decoded_tokens = []\n+        for result in self.generate(\n+            model_input=model_input,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=False,\n+        ):\n+            output_tokens.append(result.token)\n+\n+        return input_tokens, output_tokens\n+\n \n def sample_top_p(probs, p):\n     \"\"\"\n\n--- File: models/llama3/reference_impl/multimodal/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n\n--- File: models/llama3/reference_impl/multimodal/encoder_utils.py ---\n@@ -0,0 +1,188 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and its affiliates.\n+import math\n+from logging import getLogger\n+\n+import torch\n+import torch.nn.functional as F\n+\n+from .utils import get_negative_inf_value, to_2tuple\n+\n+logger = getLogger()\n+\n+\n+def resize_local_position_embedding(orig_pos_embed, grid_size):\n+    \"\"\"\n+    Resize position embedding for vision encoder.\n+    Original position embedding is [n_tiles * n_tiles + 1, dim]\n+    New position embedding will be [grid_size[0] * grid_size[1] + 1, dim]\n+    \"\"\"\n+    new_grid_size = to_2tuple(grid_size)\n+    orig_grid_size = to_2tuple(int(math.sqrt(len(orig_pos_embed) - 1)))\n+    new_seq_len = new_grid_size[0] * new_grid_size[1] + 1\n+\n+    new_pos_emb_tok, new_pos_emb_img = (\n+        orig_pos_embed[:1],\n+        orig_pos_embed[1:],\n+    )\n+    logger.info(\n+        f\"resizing position embedding grid-size from {orig_grid_size} to {new_grid_size}\"\n+    )\n+\n+    new_pos_emb_img = new_pos_emb_img.reshape(\n+        1, orig_grid_size[0], orig_grid_size[1], -1\n+    ).permute(0, 3, 1, 2)\n+\n+    new_pos_emb_img = F.interpolate(\n+        new_pos_emb_img,\n+        size=new_grid_size,\n+        mode=\"bilinear\",\n+        align_corners=True,\n+    )\n+    new_pos_emb_img = new_pos_emb_img.permute(0, 2, 3, 1).reshape(\n+        1, new_grid_size[0] * new_grid_size[1], -1\n+    )[0]\n+    new_pos_embed = torch.cat([new_pos_emb_tok, new_pos_emb_img], dim=0)\n+    return new_pos_embed\n+\n+\n+def initialize_global_position_embedding_from_local(\n+    pos_and_cls_embed, grid_size, x_scale, y_scale\n+):\n+    \"\"\"\n+    Takes a local position embedding for vision encoder and uses it\n+    to initialize the global position embedding.\n+    Input: local position embedding of shape [grid_size[0] * grid_size[1] + 1, dim]\n+    Returns: global position embedding of shape [x_scale, y_scale, grid_size[0] * grid_size[1] + 1, dim]\n+    Here x_scale and y_scale are the number of tiles along x-axis and y-axis respectively.\n+    \"\"\"\n+    pos_embed = pos_and_cls_embed[1:]\n+    cls_embed = pos_and_cls_embed[0].view(1, 1, 1, -1)\n+    grid_size = to_2tuple(grid_size)\n+    new_pos_emb_img = pos_embed.reshape(1, grid_size[0], grid_size[1], -1).permute(\n+        0, 3, 1, 2\n+    )\n+    new_grid_size = (x_scale * grid_size[0], y_scale * grid_size[1])\n+    new_pos_emb_img = F.interpolate(\n+        new_pos_emb_img,\n+        size=new_grid_size,\n+        mode=\"bilinear\",\n+        align_corners=True,\n+    )\n+    new_pos_emb_img = new_pos_emb_img.permute(0, 2, 3, 1)\n+    new_pos_emb_img = new_pos_emb_img.view(\n+        x_scale, grid_size[0], y_scale, grid_size[1], -1\n+    )\n+    new_pos_emb_img = new_pos_emb_img.permute(0, 2, 1, 3, 4).contiguous()\n+    new_pos_emb_img = new_pos_emb_img.reshape(\n+        x_scale, y_scale, grid_size[0] * grid_size[1], -1\n+    )\n+    cls_embed = cls_embed.expand(x_scale, y_scale, -1, -1)\n+    pos_and_cls_embed = torch.cat([cls_embed, new_pos_emb_img], dim=2)\n+    return pos_and_cls_embed\n+\n+\n+def resize_global_position_embedding(pos_and_cls_embed, grid_size, x_scale, y_scale):\n+    \"\"\"\n+    Takes a global position embedding for vision encoder and resizes it to new size.\n+    Input: global position embedding of shape [x_old, y_old, old_grid_size[0] * old_grid_size[1] + 1, dim]\n+    Returns: global position embedding of shape [x_scale, y_scale, grid_size[0] * grid_size[1] + 1, dim]\n+    Here x_scale and y_scale are the number of tiles along x-axis and y-axis respectively.\n+    \"\"\"\n+    # first remove cls token\n+    pos_embed = pos_and_cls_embed[:, :, 1:]\n+    cls_embed = pos_and_cls_embed[:, :, 0].unsqueeze(2)\n+\n+    xs_old, ys_old, ntok, dim = pos_embed.shape\n+    old_grid_size = int(math.sqrt(ntok))\n+\n+    # move to correct form for interpolation\n+    pos_embed = pos_embed.view(xs_old, ys_old, old_grid_size, old_grid_size, dim)\n+    pos_embed = pos_embed.permute(0, 2, 1, 3, 4).contiguous()\n+    pos_embed = pos_embed.view(xs_old * old_grid_size, ys_old * old_grid_size, dim)\n+    pos_embed = pos_embed.unsqueeze(0)\n+\n+    # interpolate\n+    new_size = (grid_size[0] * x_scale, grid_size[1] * y_scale)\n+    pos_embed = pos_embed.permute(0, 3, 1, 2)\n+    pos_embed_resized = F.interpolate(\n+        pos_embed,\n+        size=new_size,\n+        mode=\"bilinear\",\n+        align_corners=True,\n+    )\n+    pos_embed = pos_embed_resized.permute(0, 2, 3, 1)[0]\n+\n+    # move it back in place\n+    pos_embed = pos_embed.view(x_scale, grid_size[0], y_scale, grid_size[1], dim)\n+    pos_embed = pos_embed.permute(0, 2, 1, 3, 4).contiguous()\n+    pos_embed = pos_embed.view(x_scale, y_scale, grid_size[0] * grid_size[1], dim)\n+\n+    # interpolate cls token\n+    cls_embed = cls_embed.permute(2, 3, 0, 1)\n+    cls_embed_resized = F.interpolate(\n+        cls_embed,\n+        size=(x_scale, y_scale),\n+        mode=\"bilinear\",\n+        align_corners=True,\n+    )\n+    cls_embed = cls_embed_resized.permute(2, 3, 0, 1)\n+    # add cls token back in\n+    pos_and_cls_embed = torch.cat([cls_embed, pos_embed], dim=2)\n+\n+    return pos_and_cls_embed\n+\n+\n+def build_encoder_attention_mask(\n+    x: torch.Tensor,\n+    ar: torch.Tensor,\n+    ntok: int,\n+    num_chunks: int,\n+    n_heads: int,\n+):\n+    \"\"\"\n+    Build vision encoder attention mask that omits padding tokens.\n+    \"\"\"\n+    masks = []\n+    for arx in ar:\n+        mask_i = torch.ones((num_chunks, x.shape[2], 1), dtype=x.dtype)\n+        mask_i[: arx[0] * arx[1], :ntok] = 0\n+        mask_i = mask_i.view(num_chunks * x.shape[2], -1)\n+        mask_i = mask_i @ mask_i.T * get_negative_inf_value(x.dtype)\n+        mask_i = mask_i.unsqueeze(0)\n+        masks.append(mask_i)\n+    masks = torch.stack(masks).to(x.device).expand(-1, n_heads, -1, -1)\n+    return masks\n+\n+\n+def expand_num_tokens_to_mult8(x):\n+    num_pad_tokens = 8 - (x.shape[-2] % 8)\n+    if num_pad_tokens == 0:\n+        return x, 0\n+    else:\n+        return (\n+            torch.cat(\n+                [\n+                    x,\n+                    torch.zeros(\n+                        (x.shape[0], x.shape[1], num_pad_tokens, x.shape[-1]),\n+                        dtype=x.dtype,\n+                        device=x.device,\n+                    ),\n+                ],\n+                dim=-2,\n+            ),\n+            num_pad_tokens,\n+        )\n+\n+\n+def contract_num_tokens_from_mult8(x, num_pad_tokens):\n+    if num_pad_tokens == 0:\n+        return x\n+    return x[:, :, :-num_pad_tokens]\n\n--- File: models/llama3/reference_impl/multimodal/image_transform.py ---\n@@ -0,0 +1,415 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import math\n+\n+from collections import defaultdict\n+\n+from logging import getLogger\n+\n+from typing import Any, Optional, Set, Tuple\n+\n+import torch\n+import torchvision.transforms as tv\n+from PIL import Image\n+from torchvision.transforms import functional as F\n+\n+IMAGE_RES = 224\n+\n+logger = getLogger()\n+\n+\n+class VariableSizeImageTransform(object):\n+    \"\"\"\n+    This class accepts images of any size and dynamically resize, pads and chunks it\n+    based on the image aspect ratio and the number of image chunks we allow.\n+\n+    The algorithm will NOT distort the image fit a certain aspect ratio, because\n+    that leads to a significant degradation in image quality.\n+\n+    It can be summarized in 6 steps:\n+    1. Find all possible canvas combinations of max_num_chunks;\n+    2. Find the best canvas to fit the image;\n+    3. Resize without distortion\n+    4. Pad\n+    5. Normalize\n+    6. Chunk\n+\n+    For example, if an input image is of size 300x800, patch_size of 224,\n+    and max_num_chunks = 8, it will find the closest aspect ratio that\n+    is allowed within 8 image chunks, with some restrictions.\n+    In this case, 2:4 = 2 horizontal patches and 4 vertical patches,\n+    giving a total of 8 chunks.\n+\n+    If resize_to_max_canvas, the image will be resized (without distortion),\n+    to the largest possible resolution. In this case, 388:896, and padded to 448:896,\n+    where we maintain the original aspect ratio and pad with zeros value for the rest.\n+    This approach minimizes the amount of padding required for any arbitrary resolution.\n+\n+    However, if limit_upscaling_to_patch_size is set to True,\n+    the upscaling will be limited to the patch size. In the example above,\n+    the image would remain 300x800 (no upscaling), and then padded to 448:896.\n+\n+    The final output will therefore be of shape (8, 3, 224, 224), where 2x4\n+    patches are coming from the resizing and chunking.\n+    \"\"\"\n+\n+    def __init__(self, size: int = IMAGE_RES) -> None:\n+        self.size = size\n+        logger.info(f\"VariableSizeImageTransform size: {self.size}\")\n+        self.to_tensor = tv.ToTensor()\n+        self._mean = (0.48145466, 0.4578275, 0.40821073)\n+        self._std = (0.26862954, 0.26130258, 0.27577711)\n+        self.normalize = tv.Normalize(\n+            mean=self._mean,\n+            std=self._std,\n+            inplace=True,\n+        )\n+        self.resample = tv.InterpolationMode.BILINEAR\n+\n+    @staticmethod\n+    def get_factors(n: int) -> Set[int]:\n+        \"\"\"\n+        Calculate all factors of a given number, i.e. a dividor that leaves\n+        no remainder. For example, if n=12, it will return {1, 2, 3, 4, 6, 12}.\n+\n+        Args:\n+            n (int): The number to find factors for.\n+\n+        Returns:\n+            set: A set containing all factors of the number.\n+        \"\"\"\n+        factors_set = set()\n+\n+        for i in range(1, int(n**0.5) + 1):\n+            if n % i == 0:\n+                factors_set.add(i)\n+                factors_set.add(n // i)\n+        return factors_set\n+\n+    def find_supported_resolutions(\n+        self, max_num_chunks: int, patch_size: int\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Computes all of the allowed resoltuions for a fixed number of chunks\n+        and patch_size. Useful for when dividing an image into chunks.\n+\n+        Args:\n+            max_num_chunks (int): Maximum number of chunks for processing.\n+            patch_size (int): Size of the side of the patch.\n+\n+        Returns:\n+            torch.Tensor: List of possible resolutions as tuples (height, width).\n+\n+        Example:\n+            >>> max_num_chunks = 5\n+            >>> patch_size = 224\n+            >>> find_supported_resolutions(max_num_chunks, patch_size)\n+            tensor([(224, 896), (448, 448), (224, 224), (896, 224), (224, 672),\n+            (672, 224), (224, 448), (448, 224)])\n+\n+            Given max_num_chunks=4, patch_size=224, it will create a dictionary:\n+            {\n+            0.25: [(1, 4)],\n+            1.0: [(2, 2), (1, 1)],\n+            4.0: [(4, 1)],\n+            0.33: [(1, 3)],\n+            3.0: [(3, 1)],\n+            0.5: [(1, 2)],\n+            2.0: [(2, 1)]\n+            }\n+\n+            and return the resolutions multiplied by the patch_size:\n+            [(1*224, 4*224), (2*224, 2*224), ..., (2*224, 1*224)]\n+        \"\"\"\n+        asp_dict = defaultdict(list)\n+        for chunk_size in range(max_num_chunks, 0, -1):\n+            _factors = sorted(self.get_factors(chunk_size))\n+            _asp_ratios = [(factor, chunk_size // factor) for factor in _factors]\n+            for height, width in _asp_ratios:\n+                ratio_float = height / width\n+                asp_dict[ratio_float].append((height, width))\n+\n+        # get the resolutions multiplied by the patch_size\n+        possible_resolutions = []\n+        for key, value in asp_dict.items():\n+            for height, depth in value:\n+                possible_resolutions.append((height * patch_size, depth * patch_size))\n+\n+        return possible_resolutions\n+\n+    @staticmethod\n+    def get_max_res_without_distortion(\n+        image_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+    ) -> Tuple[int, int]:\n+        \"\"\"\n+        Determines the maximum resolution to which an image can be resized to without distorting its\n+        aspect ratio, based on the target resolution.\n+\n+        Args:\n+            image_size (Tuple[int, int]): The original resolution of the image (height, width).\n+            target_resolution (Tuple[int, int]): The desired resolution to fit the image into (height, width).\n+        Returns:\n+            Tuple[int, int]: The optimal dimensions (height, width) to which the image should be resized.\n+        Example:\n+            >>> _get_max_res_without_distortion([200, 300], target_size = [450, 200])\n+            (134, 200)\n+            >>> _get_max_res_without_distortion([800, 600], target_size = [450, 1300])\n+            (450, 338)\n+        \"\"\"\n+\n+        original_width, original_height = image_size\n+        target_width, target_height = target_size\n+\n+        scale_w = target_width / original_width\n+        scale_h = target_height / original_height\n+\n+        if scale_w < scale_h:\n+            new_width = target_width\n+            new_height = min(math.floor(original_height * scale_w), target_height)\n+        else:\n+            new_height = target_height\n+            new_width = min(math.floor(original_width * scale_h), target_width)\n+\n+        return new_width, new_height\n+\n+    def _pad(self, image: Image.Image, target_size) -> Image.Image:\n+        new_width, new_height = target_size\n+        new_im = Image.new(mode=\"RGB\", size=(new_width, new_height), color=(0, 0, 0))  # type: ignore\n+        new_im.paste(image)\n+        return new_im\n+\n+    def _split(self, image: torch.Tensor, ncw: int, nch: int) -> torch.Tensor:\n+        # Split image into number of required tiles (width x height)\n+        num_channels, height, width = image.size()\n+        image = image.view(num_channels, nch, height // nch, ncw, width // ncw)\n+        # Permute dimensions to reorder the axes\n+        image = image.permute(1, 3, 0, 2, 4).contiguous()\n+        # Reshape into the desired output shape (batch_size * 4, num_channels, width/2, height/2)\n+        image = image.view(ncw * nch, num_channels, height // nch, width // ncw)\n+        return image\n+\n+    def resize_without_distortion(\n+        self,\n+        image: torch.Tensor,\n+        target_size: Tuple[int, int],\n+        max_upscaling_size: Optional[int],\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Used to resize an image to target_resolution, without distortion.\n+\n+        If target_size requires upscaling the image, the user can set max_upscaling_size to\n+        limit the upscaling to a maximum size. In this case, since we rescale without distortion,\n+        modifying target_size works as a boundary for the image's largest side.\n+\n+        Args:\n+            resample (str): Resampling method used when resizing images.\n+                Supports \"nearest\", \"nearest_exact\", \"bilinear\", \"bicubic\".\n+            max_upscaling_size (int): The maximum size to upscale the image to.\n+                If None, there is no limit.\n+        Examples:\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = 600\n+        >>> image_size = (400, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (600, 300)  # new_size_without_distortion\n+\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = 600\n+        >>> image_size = (2000, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (1000, 100)  # new_size_without_distortion\n+\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = 2000\n+        >>> image_size = (400, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (1000, 500)  # new_size_without_distortion\n+\n+        >>> target_size = (1000, 1200)\n+        >>> max_upscaling_size = None\n+        >>> image_size = (400, 200)\n+        >>> resize_without_distortion(image_size, target_size, max_upscaling_size)\n+        (1000, 500)  # new_size_without_distortion\n+        \"\"\"\n+\n+        image_width, image_height = image.size\n+        image_size = (image_width, image_height)\n+\n+        # If target_size requires upscaling, we might want to limit the upscaling to max_upscaling_size\n+        if max_upscaling_size is not None:\n+            new_target_width = min(max(image_width, max_upscaling_size), target_size[0])\n+            new_target_height = min(\n+                max(image_height, max_upscaling_size), target_size[1]\n+            )\n+            target_size = (new_target_width, new_target_height)\n+\n+        # resize to target_size while preserving aspect ratio\n+        new_size_without_distortion = self.get_max_res_without_distortion(\n+            image_size, target_size\n+        )\n+\n+        image = F.resize(\n+            image,\n+            (new_size_without_distortion[1], new_size_without_distortion[0]),\n+            interpolation=self.resample,\n+        )\n+\n+        return image\n+\n+    def get_best_fit(\n+        self,\n+        image_size: Tuple[int, int],\n+        possible_resolutions: torch.Tensor,\n+        resize_to_max_canvas: bool = False,\n+    ) -> Tuple[int, int]:\n+        \"\"\"\n+        Determines the best canvas possible from a list of possible resolutions to, without distortion,\n+        resize an image to.\n+\n+        For each possible resolution, calculates the scaling factors for\n+        width and height, and selects the smallest one, which is the limiting side.\n+        E.g. to match the canvas you can upscale height by 2x, and width by 1.5x,\n+        therefore, the maximum upscaling you can do is min(2, 1.5) = 1.5.\n+\n+        If upscaling is possible (any of the scaling factors is greater than 1),\n+        then picks the smallest upscaling factor > 1, unless resize_to_max_canvas is True.\n+\n+        If upscaling is not possible, then picks the largest scaling factor <= 1, i.e.\n+        reduce downscaling as much as possible.\n+\n+        If there are multiple resolutions with the same max scale, we pick the one with the lowest area,\n+        to minimize padding. E.g., the same image can be upscaled to 224x224 and 224x448, but the latter\n+        has more padding.\n+\n+        Args:\n+            image_size (Tuple[int, int]): A tuple containing the height and width of the image.\n+            possible_resolutions (torch.Tensor): A tensor of shape (N, 2) where each\n+                row represents a possible resolution (height, width).\n+            use_max_upscaling (bool): If True, will return the largest upscaling resolution.\n+\n+        Returns:\n+            List[int]: The best resolution [height, width] for the given image.\n+\n+        Example:\n+            >>> image_size = (200, 300)\n+            >>> possible_resolutions = torch.tensor([[224, 672],\n+            ...                                     [672, 224],\n+            ...                                     [224, 448],\n+            ...                                     [448, 224],\n+            ...                                     [224, 224]])\n+            >>> _get_smallest_upscaling_possibility(image_size, possible_resolutions)\n+            [224, 448]\n+\n+            We have:\n+                scale_w = tensor([2.2400, 0.7467, 1.4933, 0.7467, 0.7467])\n+                scale_h = tensor([1.1200, 3.3600, 1.1200, 2.2400, 1.1200])\n+                scales = tensor([1.1200, 0.7467, 1.1200, 0.7467, 0.7467])\n+            Only one of the scales > 1:\n+                upscaling_possible = tensor([1.1200, 1.1200])\n+                smallest_rescale = tensor(1.1200)\n+            So we pick the resolution with the smallest smallest area:\n+                areas = tensor([150528, 100352]) # [672, 224], [224, 448]\n+                optimal_canvas = tensor([224, 448])\n+        \"\"\"\n+\n+        original_width, original_height = image_size\n+\n+        # get all possible resolutions heights/widths\n+        target_widths, target_heights = (\n+            possible_resolutions[:, 0],\n+            possible_resolutions[:, 1],\n+        )\n+\n+        # get scaling factors to resize the image without distortion\n+        scale_w = target_widths / original_width\n+        scale_h = target_heights / original_height\n+\n+        # get the min scale between width and height (limiting side -> no distortion)\n+        scales = torch.where(scale_w > scale_h, scale_h, scale_w)\n+\n+        # filter only scales that allow upscaling\n+        upscaling_options = scales[scales >= 1]\n+        if len(upscaling_options) > 0:\n+            if resize_to_max_canvas:\n+                selected_scale = torch.max(upscaling_options)\n+            else:\n+                selected_scale = torch.min(upscaling_options)\n+        else:\n+            # no upscaling possible,\n+            # get the minimum downscaling (max scale for scales<1)\n+            downscaling_options = scales[scales < 1]\n+            selected_scale = torch.max(downscaling_options)\n+\n+        # get all resolutions that support this scaling factor,\n+        # e.g. you can upscale to 224x224, 224x448, 224x672 without distortion\n+        chosen_canvas = possible_resolutions[scales == selected_scale]\n+\n+        # if there are multiple resolutions,\n+        # get the one with minimum area to reduce padding\n+        if len(chosen_canvas) > 1:\n+            areas = chosen_canvas[:, 0] * chosen_canvas[:, 1]\n+            optimal_idx = torch.argmin(areas)\n+            optimal_canvas = chosen_canvas[optimal_idx]\n+        else:\n+            optimal_canvas = chosen_canvas[0]\n+\n+        return tuple(optimal_canvas.tolist())\n+\n+    def __call__(\n+        self,\n+        image: Image.Image,\n+        max_num_chunks: int,\n+        normalize_img: bool = True,\n+        resize_to_max_canvas: bool = False,\n+    ) -> Tuple[Any, Any]:\n+        \"\"\"\n+        Args:\n+            image (PIL.Image): Image to be resized.\n+            max_num_chunks (int): Maximum number of chunks to split the image into.\n+            normalize_img (bool): Whether to normalize the image.\n+            resize_to_max_canvas (bool): Whether to resize the image to the maximum canvas size.\n+            If True, picks the canvas the allows the largest resizing without distortion.\n+            If False, downsample as little as possible, including no resizing at all,\n+            but never upsample, unless the image is smaller than the patch size.\n+        \"\"\"\n+        assert max_num_chunks > 0\n+        assert isinstance(image, Image.Image), type(image)\n+        w, h = image.size\n+\n+        possible_resolutions = self.find_supported_resolutions(\n+            max_num_chunks=max_num_chunks, patch_size=self.size\n+        )\n+        possible_resolutions = torch.tensor(possible_resolutions)\n+\n+        best_resolution = self.get_best_fit(\n+            image_size=(w, h),\n+            possible_resolutions=possible_resolutions,\n+            resize_to_max_canvas=resize_to_max_canvas,\n+        )\n+\n+        max_upscaling_size = None if resize_to_max_canvas else self.size\n+        image = self.resize_without_distortion(\n+            image, best_resolution, max_upscaling_size\n+        )\n+        image = self._pad(image, best_resolution)\n+\n+        image = self.to_tensor(image)\n+\n+        if normalize_img:\n+            image = self.normalize(image)\n+\n+        ratio_w, ratio_h = (\n+            best_resolution[0] // self.size,\n+            best_resolution[1] // self.size,\n+        )\n+\n+        image = self._split(image, ratio_w, ratio_h)  # type: ignore\n+\n+        ar = (ratio_h, ratio_w)\n+        return image, ar\n\n--- File: models/llama3/reference_impl/multimodal/model.py ---\n@@ -0,0 +1,1520 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+import logging\n+import math\n+from functools import partial\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import fairscale.nn.model_parallel.initialize as fs_init\n+\n+import torch\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.layers import (\n+    ColumnParallelLinear,\n+    RowParallelLinear,\n+    VocabParallelEmbedding,\n+)\n+\n+from PIL import Image as PIL_Image\n+\n+from torch import nn, Tensor\n+from torch.distributed import _functional_collectives as funcol\n+\n+from ..model import apply_rotary_emb, ModelArgs, precompute_freqs_cis, RMSNorm\n+\n+from .encoder_utils import (\n+    build_encoder_attention_mask,\n+    contract_num_tokens_from_mult8,\n+    expand_num_tokens_to_mult8,\n+    initialize_global_position_embedding_from_local,\n+    resize_global_position_embedding,\n+    resize_local_position_embedding,\n+)\n+from .image_transform import VariableSizeImageTransform\n+from .utils import get_negative_inf_value, to_2tuple\n+\n+\n+logger = logging.getLogger(__name__)\n+MP_SCALE = 8\n+\n+\n+def reduce_from_tensor_model_parallel_region(input_):\n+    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n+    output = funcol.all_reduce(input_, \"sum\", group=fs_init.get_model_parallel_group())\n+    output = funcol.wait_tensor(output)\n+    return output\n+\n+\n+def gather_from_tensor_model_parallel_region(input_):\n+    \"\"\"Gather tensors and concatenate along the last dimension.\"\"\"\n+\n+    world_size = fs_init.get_model_parallel_world_size()\n+    # Size and dimension.\n+    last_dim = input_.dim() - 1\n+    rank = fs_init.get_model_parallel_rank()\n+\n+    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]\n+    tensor_list[rank] = input_\n+    output = funcol.all_gather_tensor(\n+        input_,\n+        gather_dim=last_dim,\n+        group=fs_init.get_model_parallel_group(),\n+    )\n+    output = funcol.wait_tensor(output)\n+    return output\n+\n+\n+def _get_full_row_masked_out_mask(\n+    attn_bias,\n+    negative_inf_value,\n+):\n+    \"\"\"\n+    attn_bias should be a 4D tensor of shape [B, H, S1, S2]\n+    where B is the batch size, H is the number of heads,\n+    and S1/S2 are the sequence lengths. This returns\n+    a 4D tensor of shape [B, H, S1, 1] which stores boolean\n+    values which are 0 if the a full row in the last dimension\n+    contains negative infinity values, otherwise it's 1.\n+    \"\"\"\n+    return (attn_bias != negative_inf_value).any(dim=-1).type_as(attn_bias)[..., None]\n+\n+\n+# Image encoder for inference\n+class LayerNorm(nn.LayerNorm):\n+    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n+\n+    def forward(self, x: torch.Tensor):\n+        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n+        return x\n+\n+\n+class ColumnParallelConv2dPatch(torch.nn.Module):\n+    \"\"\"Conv2D Patching layer with model parallelism.\n+    Column parallel over unfolded input.\n+    Arguments:\n+        in_channels: Input channels.\n+        out_channels: Output channels.\n+        kernel_size: Size of convolution kernel.\n+        stride (default 1): Stride for convolution.\n+        bias (default False): Use bias in Conv2d.\n+    Input: (bsz, in_channels, width, height)\n+    Output: (bsz, num_tokens, out_channels)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: Union[int, Tuple[int, int]],\n+        stride: Union[int, Tuple[int, int]],\n+        bias: Optional[bool] = False,\n+    ) -> None:\n+        super().__init__()\n+        if isinstance(kernel_size, int):\n+            kernel_size = (kernel_size, kernel_size)\n+        self._unfold = torch.nn.Unfold(kernel_size=kernel_size, stride=stride)\n+        self._linear = ColumnParallelLinear(\n+            in_channels * kernel_size[0] * kernel_size[1],\n+            out_channels,\n+            bias=bias,\n+        )\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        x = self._unfold(x)\n+        x = x.permute(0, 2, 1)\n+        x = F.linear(x, self._linear.weight)\n+        x = gather_from_tensor_model_parallel_region(x)\n+        return x\n+\n+\n+class ImageFeedForward(torch.nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        dropout: float,\n+        act_layer: Callable = nn.GELU,\n+    ):\n+        super().__init__()\n+        # layers\n+        self.c_fc = ColumnParallelLinear(\n+            dim,\n+            hidden_dim,\n+            bias=True,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.c_proj = RowParallelLinear(\n+            hidden_dim,\n+            dim,\n+            bias=True,\n+            input_is_parallel=True,\n+            init_method=lambda x: x,\n+        )\n+        self.non_linearity = act_layer()\n+        self.dropout = dropout\n+\n+    def forward(self, x):\n+        hidden = F.linear(x, self.c_fc.weight, self.c_fc.bias)\n+        hidden = self.non_linearity(hidden)\n+        hidden = F.linear(hidden, self.c_proj.weight)\n+        hidden = reduce_from_tensor_model_parallel_region(hidden)\n+        hidden += self.c_proj.bias\n+        return hidden\n+\n+\n+class ImageAttention(nn.Module):\n+    def __init__(\n+        self,\n+        dim,\n+        head_dim,\n+        n_heads,\n+    ):\n+        super().__init__()\n+        model_parallel_size = fs_init.get_model_parallel_world_size()\n+        qkvo_replication = 1\n+        if model_parallel_size > 16:\n+            qkvo_replication = model_parallel_size // 8\n+\n+        self.n_kv_heads = n_heads\n+        self.n_local_heads = n_heads * qkvo_replication // model_parallel_size\n+        self.n_local_kv_heads = (\n+            self.n_kv_heads * qkvo_replication // model_parallel_size\n+        )\n+        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n+        self.head_dim = dim // n_heads\n+\n+        self.wq = ColumnParallelLinear(\n+            dim,\n+            qkvo_replication * n_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wk = ColumnParallelLinear(\n+            dim,\n+            qkvo_replication * self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wv = ColumnParallelLinear(\n+            dim,\n+            qkvo_replication * self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wo = RowParallelLinear(\n+            qkvo_replication * n_heads * self.head_dim,\n+            dim,\n+            bias=False,\n+            input_is_parallel=True,\n+            init_method=lambda x: x,\n+        )\n+        self.qkvo_replication = qkvo_replication\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        mask: torch.Tensor = None,\n+    ):\n+\n+        xq, xk, xv = [\n+            F.linear(x, w) for w in [self.wq.weight, self.wk.weight, self.wv.weight]\n+        ]\n+\n+        bs, slen, _ = xq.shape\n+\n+        xq = xq.view(bs, slen, self.n_local_heads, self.head_dim)\n+        xk = xk.view(bs, xk.shape[1], self.n_local_kv_heads, self.head_dim)\n+        xv = xv.view(bs, xv.shape[1], self.n_local_kv_heads, self.head_dim)\n+\n+        xq, xk, xv = [tensor.transpose(1, 2) for tensor in (xq, xk, xv)]\n+\n+        xk = xk.repeat_interleave(self.n_rep, dim=1)\n+        xv = xv.repeat_interleave(self.n_rep, dim=1)\n+\n+        attn_output = F.scaled_dot_product_attention(\n+            xq, xk, xv, attn_mask=mask, dropout_p=0.0\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous().reshape(bs, slen, -1)\n+\n+        out = F.linear(attn_output, self.wo.weight)\n+        out = reduce_from_tensor_model_parallel_region(out)\n+        out = out / self.qkvo_replication\n+        return out\n+\n+\n+class ImageTransformerBlock(nn.Module):\n+    def __init__(\n+        self,\n+        d_model: int,\n+        n_head: int,\n+        mlp_ratio: float = 4.0,\n+        act_layer: Callable = nn.GELU,\n+        gated: bool = False,\n+    ):\n+        super().__init__()\n+        assert d_model % n_head == 0\n+        self.n_heads = n_head\n+        self.head_dim = d_model // self.n_heads\n+        self.attn = ImageAttention(\n+            dim=d_model,\n+            head_dim=self.head_dim,\n+            n_heads=self.n_heads,\n+        )\n+        self.ln_1 = LayerNorm(d_model)\n+        self.mlp = ImageFeedForward(\n+            dim=d_model,\n+            hidden_dim=int(mlp_ratio * d_model),\n+            dropout=0.0,\n+            act_layer=act_layer,\n+        )\n+        self.ln_2 = LayerNorm(d_model)\n+        self.gated = gated\n+        if gated:\n+            self.gate_attn = nn.Parameter(torch.zeros(1))\n+            self.gate_ffn = nn.Parameter(torch.zeros(1))\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        mask: torch.Tensor = None,\n+    ):\n+        _gate_attn = 1 if not self.gated else self.gate_attn.tanh()\n+        _gate_ffn = 1 if not self.gated else self.gate_ffn.tanh()\n+        x = x + _gate_attn * self.attn(self.ln_1(x), mask=mask)\n+        x = x + _gate_ffn * self.mlp(self.ln_2(x))\n+        return x\n+\n+\n+class ImageTransformer(nn.Module):\n+    def __init__(\n+        self,\n+        width: int,\n+        layers: int,\n+        heads: int,\n+        mlp_ratio: float = 4.0,\n+        act_layer: Callable = nn.GELU,\n+        gated: bool = False,\n+    ):\n+        super().__init__()\n+        self.width = width\n+        self.layers = layers\n+        self.resblocks = nn.ModuleList(\n+            [\n+                ImageTransformerBlock(\n+                    d_model=width,\n+                    n_head=heads,\n+                    mlp_ratio=mlp_ratio,\n+                    act_layer=act_layer,\n+                    gated=gated,\n+                )\n+                for _ in range(self.layers)\n+            ]\n+        )\n+\n+    def forward(self, x: torch.Tensor, return_intermediate=None, mask=None):\n+        out = []\n+        for idx, r in enumerate(self.resblocks):\n+            if return_intermediate is not None and idx in return_intermediate:\n+                out.append(x)\n+            x = r(x, mask=mask)\n+        if return_intermediate is not None:\n+            return x, torch.stack(out, dim=-1)\n+        return x\n+\n+\n+class VisionEncoder(nn.Module):\n+    def __init__(\n+        self,\n+        max_num_tiles: int,\n+        ckpt_path: str = None,\n+        image_size: int = 224,\n+        patch_size: int = 14,\n+        width: int = 1280,\n+        layers: int = 32,\n+        heads: int = 16,\n+        mlp_ratio: float = 4.0,\n+        act_layer: Callable = nn.GELU,\n+        in_channels: int = 3,\n+        load_ckpt: bool = False,\n+        n_global_layers: int = 2,\n+        global_model: bool = False,\n+        return_intermediate=None,\n+    ):\n+        super().__init__()\n+        self.global_model = global_model\n+        self.return_intermediate = return_intermediate\n+        self.max_num_tiles = max_num_tiles\n+        self.image_size = to_2tuple(image_size)\n+        self.patch_size = to_2tuple(patch_size)\n+        self.grid_size = (\n+            self.image_size[0] // self.patch_size[0],\n+            self.image_size[1] // self.patch_size[1],\n+        )\n+        self.conv1 = ColumnParallelConv2dPatch(\n+            in_channels=in_channels,\n+            out_channels=width,\n+            kernel_size=patch_size,\n+            stride=patch_size,\n+            bias=False,\n+        )\n+        scale = width**-0.5\n+        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n+        self.positional_embedding = nn.Parameter(\n+            scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width)\n+        )\n+        self.ln_post = LayerNorm(width)\n+        self.ln_pre = LayerNorm(width)\n+        self.transformer = ImageTransformer(\n+            width, layers, heads, mlp_ratio, act_layer=act_layer\n+        )\n+        # pre and post tile position embedding\n+        self.global_transformer = ImageTransformer(\n+            width, n_global_layers, heads, mlp_ratio, act_layer=act_layer, gated=True\n+        )\n+        # pre and post tile position embedding\n+        self.pre_tile_pos_embed = TilePositionEmbedding(\n+            num_tiles=max_num_tiles,\n+            width=width,\n+            gated=True,\n+        )\n+        self.post_tile_pos_embed = TilePositionEmbedding(\n+            num_tiles=max_num_tiles,\n+            width=width,\n+            gated=True,\n+        )\n+        self.gated_positional_embedding = nn.Parameter(\n+            scale\n+            * torch.randn(\n+                max_num_tiles,\n+                max_num_tiles,\n+                self.grid_size[0] * self.grid_size[1] + 1,\n+                width,\n+            )\n+        )\n+        self.gated_positional_embedding_gate = nn.Parameter(torch.zeros(1))\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict: Dict[str, Any],\n+        prefix: str,\n+        local_metadata: Dict[str, Any],\n+        strict: bool = True,\n+        missing_keys: List[str] = None,\n+        unexpected_keys: List[str] = None,\n+        error_msgs: List[str] = None,\n+        return_state_dict: bool = False,\n+    ) -> None:\n+        orig_pos_embed = state_dict.get(prefix + \"positional_embedding\")\n+        if orig_pos_embed is not None:\n+            new_pos_embed = resize_local_position_embedding(\n+                orig_pos_embed, self.grid_size\n+            )\n+            state_dict[prefix + \"positional_embedding\"] = new_pos_embed\n+        if hasattr(self, \"gated_positional_embedding\"):\n+            if prefix + \"gated_positional_embedding\" not in state_dict:\n+                # resize positional_embedding to fit the new grid size\n+                global_pos_embed = initialize_global_position_embedding_from_local(\n+                    new_pos_embed,\n+                    self.grid_size,\n+                    self.max_num_tiles,\n+                    self.max_num_tiles,\n+                )\n+                state_dict[prefix + \"gated_positional_embedding\"] = global_pos_embed\n+                state_dict[prefix + \"gated_positional_embedding_gate\"] = torch.zeros(\n+                    1, dtype=global_pos_embed.dtype\n+                )\n+                logger.info(\n+                    f\"Initialized global positional embedding with size {global_pos_embed.size()}\"\n+                )\n+            else:\n+                global_pos_embed = resize_global_position_embedding(\n+                    state_dict[prefix + \"gated_positional_embedding\"],\n+                    self.grid_size,\n+                    self.max_num_tiles,\n+                    self.max_num_tiles,\n+                )\n+                logger.info(\n+                    f\"Resized global positional embedding from {state_dict[prefix + 'gated_positional_embedding'].size()} to {global_pos_embed.size()}\"\n+                )\n+                state_dict[prefix + \"gated_positional_embedding\"] = global_pos_embed\n+        if return_state_dict:\n+            return state_dict\n+\n+    def apply_positional_embedding(self, x, ar):\n+        out = []\n+        # apply regular position embedding\n+        bsz, num_chunks, num_tokens, dim = x.shape\n+        x = x.view(bsz * num_chunks, num_tokens, dim)\n+        x = x + self.positional_embedding * (\n+            1 - self.gated_positional_embedding_gate.tanh()\n+        )\n+        x = x.view(bsz, num_chunks, num_tokens, dim)\n+        for idx, arx in enumerate(ar):\n+            _pos_embed = self.gated_positional_embedding[: arx[0], : arx[1]]\n+            _pos_embed = _pos_embed.reshape(arx[0] * arx[1], *_pos_embed.shape[2:])\n+            x[idx, : arx[0] * arx[1]] += (\n+                _pos_embed * self.gated_positional_embedding_gate.tanh()\n+            )\n+        return x\n+\n+    def apply_class_embedding(self, x):\n+        x = torch.cat(\n+            [\n+                self.class_embedding.to(x.dtype)\n+                + torch.zeros(\n+                    x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device\n+                ),\n+                x,\n+            ],\n+            dim=1,\n+        )  # shape = [*, grid ** 2 + 1, width]\n+        return x\n+\n+    def forward(self, images: torch.Tensor, ar: torch.Tensor) -> torch.Tensor:\n+        if images.ndim == 5:\n+            num_concurrent_media = 1\n+            bsz, num_chunks, nch, w, h = images.shape\n+        else:\n+            bsz, num_concurrent_media, num_chunks, nch, w, h = images.shape\n+\n+        images = images.reshape(bsz * num_concurrent_media * num_chunks, nch, w, h)\n+        ar = ar.reshape(bsz * num_concurrent_media, 2)\n+\n+        # patch embedding\n+        x = images.reshape(bsz * num_concurrent_media * num_chunks, nch, w, h)\n+        x = self.conv1(x)  # shape = [*, width, grid ** 2]\n+        _, ntok, dim = x.shape\n+        x = x.reshape(bsz * num_concurrent_media, num_chunks, ntok, dim)\n+\n+        # tile embeddings\n+        x = self.pre_tile_pos_embed(x, ar)\n+        x = x.reshape(bsz * num_concurrent_media * num_chunks, ntok, dim)\n+\n+        # apply cls token\n+        x = self.apply_class_embedding(x)\n+        ntok += 1\n+\n+        # apply position embeddings\n+        x = x.reshape(bsz * num_concurrent_media, num_chunks, ntok, dim)\n+        x = self.apply_positional_embedding(x, ar)\n+\n+        x = self.ln_pre(x)\n+        npad, attn_mask = 0, None\n+        x, npad = expand_num_tokens_to_mult8(x)\n+        attn_mask = build_encoder_attention_mask(x, ar, ntok, num_chunks, 1)\n+        x = x.view(bsz * num_concurrent_media, -1, dim)\n+        x, int_x = self.transformer(\n+            x, return_intermediate=self.return_intermediate, mask=attn_mask\n+        )\n+\n+        x = self.ln_post(x)\n+        x = x.reshape(bsz * num_concurrent_media, num_chunks, ntok + npad, dim)\n+        x = self.post_tile_pos_embed(x, ar)\n+        x = x.reshape(bsz * num_concurrent_media, num_chunks * (ntok + npad), dim)\n+        x = self.global_transformer(x, mask=attn_mask)\n+        x = x.reshape(bsz * num_concurrent_media, num_chunks, ntok + npad, dim)\n+        x = contract_num_tokens_from_mult8(x, npad)\n+\n+        # adding back intermediate layer outputs\n+        x = x.reshape(bsz, num_concurrent_media, num_chunks, ntok, dim)\n+        int_x = int_x.reshape(bsz * num_concurrent_media, num_chunks, ntok + npad, -1)\n+        int_x = contract_num_tokens_from_mult8(int_x, npad)\n+        int_x = int_x.reshape(bsz, num_concurrent_media, num_chunks, ntok, -1)\n+        x = torch.cat([x, int_x], dim=-1)\n+        return x\n+\n+\n+class Attention(nn.Module):\n+    \"\"\"Multi-head attention module.\"\"\"\n+\n+    def __init__(self, args: ModelArgs):\n+        \"\"\"\n+        Initialize the Attention module.\n+        Args:\n+            args (ModelArgs): Model configuration parameters.\n+        Attributes:\n+            n_kv_heads (int): Number of key and value heads.\n+            n_local_heads (int): Number of local query heads.\n+            n_local_kv_heads (int): Number of local key and value heads.\n+            n_rep (int): Number of repetitions for local heads.\n+            head_dim (int): Dimension size of each attention head.\n+            wq (ColumnParallelLinear): Linear transformation for queries.\n+            wk (ColumnParallelLinear): Linear transformation for keys.\n+            wv (ColumnParallelLinear): Linear transformation for values.\n+            wo (RowParallelLinear): Linear transformation for output.\n+            cache_k (torch.Tensor): Cached keys for attention.\n+            cache_v (torch.Tensor): Cached values for attention.\n+        \"\"\"\n+        super().__init__()\n+        model_parallel_size = fs_init.get_model_parallel_world_size()\n+        replication_factor = 1\n+        if model_parallel_size > 8:\n+            replication_factor = model_parallel_size // MP_SCALE\n+\n+        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n+        self.n_kv_heads *= replication_factor\n+\n+        self.n_local_heads = args.n_heads // model_parallel_size\n+        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n+        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n+        self.head_dim = args.dim // args.n_heads\n+        self.max_seq_len = args.max_seq_len\n+\n+        self.wq = ColumnParallelLinear(\n+            args.dim,\n+            args.n_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wk = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wv = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wo = RowParallelLinear(\n+            args.n_heads * self.head_dim,\n+            args.dim,\n+            bias=False,\n+            input_is_parallel=True,\n+            init_method=lambda x: x,\n+        )\n+        self.n_heads = args.n_heads\n+\n+    def setup_cache(self, max_batch_size: int, dtype: torch.dtype):\n+        cache_shape = (\n+            max_batch_size,\n+            self.max_seq_len,\n+            self.n_local_kv_heads,\n+            self.head_dim,\n+        )\n+        device = next(self.parameters()).device\n+        self.register_buffer(\n+            \"key_cache\",\n+            torch.zeros(\n+                cache_shape,\n+                dtype=dtype,\n+                device=device,\n+            ),\n+            persistent=False,\n+        )\n+        self.register_buffer(\n+            \"value_cache\",\n+            torch.zeros(\n+                cache_shape,\n+                dtype=dtype,\n+                device=device,\n+            ),\n+            persistent=False,\n+        )\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        mask: torch.Tensor,\n+        freqs_cis: torch.Tensor,\n+        position_ids: torch.LongTensor,\n+    ):\n+\n+        xq, xk, xv = [\n+            F.linear(x, w) for w in [self.wq.weight, self.wk.weight, self.wv.weight]\n+        ]\n+\n+        bs, slen, _ = xq.shape\n+\n+        xq = xq.view(bs, slen, self.n_local_heads, self.head_dim)\n+        xk = xk.view(bs, xk.shape[1], self.n_local_kv_heads, self.head_dim)\n+        xv = xv.view(bs, xv.shape[1], self.n_local_kv_heads, self.head_dim)\n+\n+        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n+\n+        self.key_cache[:bs, position_ids, ...] = xk\n+        self.value_cache[:bs, position_ids, ...] = xv\n+\n+        # TODO: we can avoid slicing on first dimension by always padding to max_batch_size()\n+        xk = self.key_cache[:bs, ...]\n+        xv = self.value_cache[:bs, ...]\n+\n+        xq, xk, xv = [tensor.transpose(1, 2) for tensor in (xq, xk, xv)]\n+\n+        xk = xk.repeat_interleave(self.n_rep, dim=1)\n+        xv = xv.repeat_interleave(self.n_rep, dim=1)\n+\n+        attn_output = F.scaled_dot_product_attention(\n+            xq, xk, xv, attn_mask=mask, dropout_p=0.0\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous().reshape(bs, slen, -1)\n+\n+        out = F.linear(attn_output, self.wo.weight)\n+        out = reduce_from_tensor_model_parallel_region(out)\n+        return out\n+\n+\n+class FeedForward(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        multiple_of: int,\n+        ffn_dim_multiplier: Optional[float],\n+    ):\n+        \"\"\"\n+        Initialize the FeedForward module.\n+        Args:\n+            dim (int): Input dimension.\n+            hidden_dim (int): Hidden dimension of the feedforward layer.\n+            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n+            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n+        Attributes:\n+            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n+            w2 (RowParallelLinear): Linear transformation for the second layer.\n+            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n+        \"\"\"\n+        super().__init__()\n+        hidden_dim = int(2 * hidden_dim / 3)\n+        # custom dim factor multiplier\n+        if ffn_dim_multiplier is not None:\n+            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n+        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n+\n+        self.w1 = ColumnParallelLinear(\n+            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n+        )\n+        self.w2 = RowParallelLinear(\n+            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n+        )\n+        self.w3 = ColumnParallelLinear(\n+            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n+        )\n+\n+    def forward(self, x):\n+        x1, x3 = [F.linear(x, w) for w in [self.w1.weight, self.w3.weight]]\n+        x1 = F.silu(x1)\n+        x_in = x1 * x3\n+        out = F.linear(x_in, self.w2.weight)\n+        out = reduce_from_tensor_model_parallel_region(out)\n+        return out\n+\n+\n+class TransformerBlock(nn.Module):\n+    def __init__(self, layer_id: int, args: ModelArgs):\n+        \"\"\"\n+        Initialize a TransformerBlock.\n+        Args:\n+            layer_id (int): Identifier for the layer.\n+            args (ModelArgs): Model configuration parameters.\n+        Attributes:\n+            n_heads (int): Number of attention heads.\n+            dim (int): Dimension size of the model.\n+            head_dim (int): Dimension size of each attention head.\n+            attention (Attention): Attention module.\n+            feed_forward (FeedForward): FeedForward module.\n+            layer_id (int): Identifier for the layer.\n+            attention_norm (RMSNorm): Layer normalization for attention output.\n+            ffn_norm (RMSNorm): Layer normalization for feedforward output.\n+        \"\"\"\n+        super().__init__()\n+        self.n_heads = args.n_heads\n+        self.dim = args.dim\n+        self.head_dim = args.dim // args.n_heads\n+        self.attention = Attention(args)\n+        self.feed_forward = FeedForward(\n+            dim=args.dim,\n+            hidden_dim=4 * args.dim,\n+            multiple_of=args.multiple_of,\n+            ffn_dim_multiplier=args.ffn_dim_multiplier,\n+        )\n+        self.layer_id = layer_id\n+        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+\n+    def setup_cache(self, max_batch_size: int, dtype: torch.dtype):\n+        self.attention.setup_cache(max_batch_size, dtype)\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        freqs_cis: torch.Tensor,\n+        mask: torch.Tensor,\n+        position_ids: torch.LongTensor,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Perform a forward pass through the TransformerBlock.\n+        Args:\n+            x (torch.Tensor): Input tensor.\n+            start_pos (int): Starting position for attention caching.\n+            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\n+            mask (torch.Tensor, optional): Masking tensor for attention. Defaults to None.\n+        Returns:\n+            torch.Tensor: Output tensor after applying attention and feedforward layers.\n+        \"\"\"\n+        h = self.attention.forward(\n+            x=self.attention_norm(x),\n+            freqs_cis=freqs_cis,\n+            mask=mask,\n+            position_ids=position_ids,\n+        )\n+        h = h + x\n+        out = h + self.feed_forward.forward(self.ffn_norm(h))\n+        return out\n+\n+\n+class TilePositionEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        num_tiles: int,\n+        width: int,\n+        gated: bool = False,\n+    ):\n+        super().__init__()\n+        self.num_tiles = num_tiles\n+        self.width = width\n+        self.embedding = nn.Parameter(\n+            torch.randn(num_tiles, num_tiles, 1, width) / math.sqrt(width)\n+        )\n+        self.gated = gated\n+        if gated:\n+            self.gate = nn.Parameter(torch.zeros(1))\n+\n+        self._register_load_state_dict_pre_hook(self.load_hook)\n+\n+    def load_hook(\n+        self,\n+        state_dict,\n+        prefix,\n+        local_metadata,\n+        strict,\n+        missing_keys,\n+        unexpected_keys,\n+        error_msgs,\n+    ):\n+        # load the weights from the checkpoint\n+        embed = state_dict.get(prefix + \"embedding\")\n+        if embed is not None:\n+            # reshape the weights to the correct shape\n+            nt_old, nt_old, _, w = embed.shape\n+            logging.info(\n+                f\"Resizing tile embedding from {nt_old}x{nt_old} to {self.num_tiles}x{self.num_tiles}\"\n+            )\n+            embed_new = TilePositionEmbedding._dynamic_resize(embed, self.num_tiles)\n+            # assign the weights to the module\n+            state_dict[prefix + \"embedding\"] = embed_new\n+\n+    @staticmethod\n+    def _dynamic_resize(embed: torch.Tensor, num_tiles: int):\n+        nt_old, nt_old, _, w = embed.shape\n+        embed = embed.permute(2, 3, 0, 1)\n+\n+        embed_new = F.interpolate(\n+            embed,\n+            size=(num_tiles, num_tiles),\n+            mode=\"bilinear\",\n+            align_corners=True,\n+        )\n+        # reshape the weights to the correct shape\n+        embed_new = embed_new.permute(2, 3, 0, 1)\n+        return embed_new\n+\n+    def forward(self, x: torch.Tensor, ar: torch.Tensor, num_tiles: int = None):\n+        embed = self.embedding\n+        if num_tiles is None:\n+            num_tiles = self.num_tiles\n+        elif num_tiles > self.num_tiles:\n+            embed = TilePositionEmbedding._dynamic_resize(self.embedding, num_tiles)\n+        out_pos_embed = torch.zeros(\n+            x.shape[0], num_tiles, 1, self.width, device=x.device, dtype=x.dtype\n+        )\n+        for idx, arx in enumerate(ar):\n+            h, w = arx\n+            out_pos_embed[idx, : w * h] = embed[:h, :w].reshape(w * h, 1, self.width)\n+        if self.gated:\n+            out_pos_embed = out_pos_embed * self.gate.tanh()\n+        x = x + out_pos_embed\n+        return x\n+\n+\n+def _noinit(x):\n+    return x\n+\n+\n+class CrossAttention(torch.nn.Module):\n+    \"\"\"Cross attention layer with model-parallel attention layers.\"\"\"\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        head_dim: int,\n+        n_heads: int,\n+        n_kv_heads: int,\n+        norm_eps: float,\n+    ):\n+        super().__init__()\n+        self.model_parallel_size = fs_init.get_model_parallel_world_size()\n+        replication_factor = 1\n+        if self.model_parallel_size > 8:\n+            replication_factor = self.model_parallel_size // MP_SCALE\n+        n_kv_heads *= replication_factor\n+\n+        assert n_heads % n_kv_heads == 0\n+\n+        self.wq = ColumnParallelLinear(\n+            dim,\n+            n_heads * head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=_noinit,\n+        )\n+\n+        self.wk = ColumnParallelLinear(\n+            dim,\n+            n_kv_heads * head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=_noinit,\n+        )\n+        self.wv = ColumnParallelLinear(\n+            dim,\n+            n_kv_heads * head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=_noinit,\n+        )\n+        self.wo = RowParallelLinear(\n+            n_heads * head_dim,\n+            dim,\n+            bias=False,\n+            input_is_parallel=True,\n+            init_method=_noinit,\n+        )\n+\n+        self.n_heads = n_heads\n+        self.head_dim = head_dim\n+        self.n_kv_heads = n_kv_heads\n+\n+        self.q_norm = RMSNorm(\n+            self.head_dim,\n+            eps=norm_eps,\n+        )\n+        self.k_norm = RMSNorm(\n+            self.head_dim,\n+            eps=norm_eps,\n+        )\n+\n+        # cross-attention heads are model parallel similar to\n+        # self-attention, and we also use the identical KV head\n+        # combination to ensure parity with the corresponding\n+        # trunk LLM (i.e., group query attention) -- @dubeya\n+        # local heads\n+        assert self.n_heads % self.n_kv_heads == 0\n+        assert self.n_heads % self.model_parallel_size == 0\n+        assert self.n_kv_heads % self.model_parallel_size == 0\n+        self.n_local_heads = self.n_heads // self.model_parallel_size\n+        self.n_local_kv_heads = self.n_kv_heads // self.model_parallel_size\n+        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n+\n+    def _compute_xattn_kv_cache(self, xattn_tokens: torch.Tensor) -> torch.Tensor:\n+        bsz = xattn_tokens.shape[0]\n+        xk = self.wk(xattn_tokens)\n+        xv = self.wv(xattn_tokens)\n+\n+        _, seqlen_y, _ = xk.shape\n+\n+        xk = xk.view(bsz, seqlen_y, self.n_local_kv_heads, self.head_dim)\n+        xv = xv.view(bsz, seqlen_y, self.n_local_kv_heads, self.head_dim)\n+\n+        xk, xv = [tensor.transpose(1, 2) for tensor in (xk, xv)]\n+\n+        # repeat k/v heads if n_kv_heads < n_heads\n+        xk = xk.repeat_interleave(self.n_rep, dim=1)\n+        xv = xv.repeat_interleave(self.n_rep, dim=1)\n+\n+        xk = self.k_norm(xk)\n+\n+        return torch.stack([xk, xv])\n+\n+    def compute_xattn_kv_cache(self, xattn_tokens: torch.Tensor) -> torch.Tensor:\n+        return self._compute_xattn_kv_cache(xattn_tokens)\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        xattn_mask: torch.Tensor,\n+        full_text_row_masked_out_mask: torch.Tensor,\n+        xattn_cache: torch.Tensor,\n+    ) -> torch.Tensor:\n+        xq = F.linear(x, self.wq.weight)\n+        bsz, seqlen, _ = x.shape\n+\n+        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n+        xq = self.q_norm(xq)\n+        xq = xq.transpose(1, 2)\n+\n+        xk, xv = xattn_cache\n+\n+        output = F.scaled_dot_product_attention(\n+            xq, xk, xv, attn_mask=xattn_mask, dropout_p=0.0\n+        )\n+        output = output * full_text_row_masked_out_mask\n+        output = output.transpose(1, 2).contiguous().reshape(bsz, seqlen, -1)\n+\n+        out = F.linear(output, self.wo.weight)\n+        out = reduce_from_tensor_model_parallel_region(out)\n+        return out\n+\n+\n+class CrossAttentionTransformerBlock(torch.nn.Module):\n+    \"\"\"Cross-attention transformer block with tanh-gated attention and feedforward.\"\"\"\n+\n+    def __init__(\n+        self,\n+        args: ModelArgs,\n+        layer_id: int,\n+        no_ffn: bool = False,\n+    ) -> None:\n+        super().__init__()\n+        self.layer_id = layer_id\n+        self.n_heads = args.n_heads\n+        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n+        self.dim = args.dim\n+        self.head_dim = args.dim // args.n_heads\n+        self.attention = CrossAttention(\n+            dim=args.dim,\n+            head_dim=self.head_dim,\n+            n_heads=self.n_heads,\n+            n_kv_heads=self.n_kv_heads,\n+            norm_eps=args.norm_eps,\n+        )\n+\n+        self.attention_norm = RMSNorm(\n+            args.dim,\n+            eps=args.norm_eps,\n+        )\n+        self.gate_attn = torch.nn.Parameter(torch.zeros(1))\n+\n+        self.feed_forward = FeedForward(\n+            dim=args.dim,\n+            hidden_dim=4 * args.dim,\n+            ffn_dim_multiplier=args.ffn_dim_multiplier,\n+            multiple_of=args.multiple_of,\n+        )\n+        self.ffn_norm = RMSNorm(\n+            args.dim,\n+            eps=args.norm_eps,\n+        )\n+        self.gate_ffwd = torch.nn.Parameter(torch.zeros(1))\n+\n+        self.no_ffn = no_ffn\n+\n+    def compute_xattn_kv_cache(self, xattn_tokens: torch.Tensor) -> torch.Tensor:\n+        return self.attention.compute_xattn_kv_cache(xattn_tokens)\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        xattn_mask: torch.Tensor,\n+        full_text_row_masked_out_mask: Tuple[torch.Tensor, torch.Tensor],\n+        xattn_cache: torch.Tensor,\n+    ) -> torch.Tensor:\n+        _attn_out = self.attention(\n+            x=self.attention_norm(x),\n+            xattn_mask=xattn_mask,\n+            xattn_cache=xattn_cache,\n+            full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n+        )\n+        h = x + self.gate_attn.tanh() * _attn_out\n+        _ffn = self.feed_forward(self.ffn_norm(h))\n+        _ffn = full_text_row_masked_out_mask[:, 0] * _ffn  # type: ignore\n+        h = h + self.gate_ffwd.tanh() * _ffn * float(not self.no_ffn)\n+        return h\n+\n+\n+class DummyCrossAttentionTransformerBlock:\n+    \"\"\"Dummy cross-attention transformer block with tanh-gated attention and feedforward.\"\"\"\n+\n+    def __call__(\n+        self,\n+        x: torch.Tensor,\n+        *args,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return x\n+\n+\n+class DummySelfAttentionTransformerBlock:\n+    \"\"\"Dummy self-attention transformer block\"\"\"\n+\n+    def __call__(\n+        self,\n+        x: torch.Tensor,\n+        *args,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return x\n+\n+\n+class CrossAttentionTransformerVision(torch.nn.Module):\n+    def __init__(self, args: ModelArgs) -> None:\n+        super().__init__()\n+        return_intermediate = \"3,7,15,23,30\"\n+        self.vision_input_dim = 1280\n+        self.image_res = args.vision_chunk_size\n+        self.max_num_chunks = args.vision_max_num_chunks\n+        if return_intermediate is not None:\n+            return_intermediate = [int(l) for l in return_intermediate.split(\",\")]\n+            self.vision_input_dim = (\n+                len(return_intermediate) + 1\n+            ) * self.vision_input_dim\n+        self.patch_size = 14\n+        self.vision_encoder = VisionEncoder(\n+            max_num_tiles=4,\n+            image_size=args.vision_chunk_size,\n+            patch_size=self.patch_size,\n+            n_global_layers=8,\n+            global_model=True,\n+            return_intermediate=return_intermediate,\n+        )\n+        # vision token projection\n+        self.vision_projection = ColumnParallelLinear(\n+            self.vision_input_dim,\n+            args.dim,\n+            bias=True,\n+            init_method=lambda x: x,\n+        )\n+\n+    def forward(\n+        self, images: torch.Tensor, aspect_ratios: torch.Tensor\n+    ) -> torch.Tensor:\n+        # vision_tokens: (B, T, D)\n+        # aspect_ratios: (B, T)\n+        # h: (B, T, D)\n+        vision_tokens = self.vision_encoder(\n+            images.to(dtype=torch.bfloat16), aspect_ratios\n+        )\n+\n+        vision_tokens = F.linear(\n+            vision_tokens, self.vision_projection.weight, self.vision_projection.bias\n+        )\n+        vision_tokens = gather_from_tensor_model_parallel_region(vision_tokens)\n+        return vision_tokens\n+\n+\n+class CrossAttentionTransformerText(torch.nn.Module):\n+    INFERENCE_IMAGE_TOKEN_ID = 128010\n+\n+    def __init__(self, args: ModelArgs) -> None:\n+        super().__init__()\n+        self.model_parallel_size = fs_init.get_model_parallel_world_size()\n+        assert args.vocab_size > 0\n+        self.vocab_size = args.vocab_size\n+        self.n_layers = args.n_layers\n+        self.dim = args.dim\n+        self.head_dim = args.dim // args.n_heads\n+        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n+        self.n_local_kv_heads = self.n_kv_heads // self.model_parallel_size\n+        assert self.vocab_size % self.model_parallel_size == 0\n+        self.tok_embeddings = VocabParallelEmbedding(\n+            args.vocab_size, args.dim, init_method=lambda x: x\n+        )\n+        self.pos_embeddings = None\n+        # final norm layer (not necessary for post-norm)\n+        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n+\n+        # output layer\n+        self.output = ColumnParallelLinear(\n+            args.dim, args.vocab_size, bias=False, init_method=lambda x: x\n+        )\n+\n+        self.n_llama_layers = args.n_layers\n+        self.model_dim = args.dim\n+\n+        # BLOCKS\n+\n+        self.fusion_schedule = self._init_fusion_schedule(\n+            args.vision_num_cross_attention_layers\n+        )\n+        self.learnable_embedding = VocabParallelEmbedding(\n+            max(fs_init.get_model_parallel_world_size(), 8),\n+            args.dim,\n+            init_method=lambda x: x,\n+        )\n+        self.num_frozen_embeddings = self.tok_embeddings.num_embeddings\n+        self._thresh = self.num_frozen_embeddings - 1\n+\n+        # transformer blocks\n+        self.layers = torch.nn.ModuleList()\n+        self.cross_attention_layers = torch.nn.ModuleList()\n+        for i in range(args.n_layers):\n+            layer_id = i\n+            block = TransformerBlock(args=args, layer_id=layer_id)\n+            self.layers.append(block)\n+            if layer_id in self.fusion_schedule:\n+                xa_layer_id = self.fusion_schedule.index(layer_id) + args.n_layers\n+                block = CrossAttentionTransformerBlock(\n+                    args,\n+                    layer_id=xa_layer_id,\n+                )\n+                self.cross_attention_layers.append(block)\n+\n+        # add xattn and dummy layers to avoid conditionals in forward()\n+        self.text_and_xattn_layers = []\n+\n+        for idx, layer in enumerate(self.layers):\n+            if idx in self.fusion_schedule:\n+                xattn_layer_idx = self.fusion_schedule.index(idx)\n+                xattn_layer = self.cross_attention_layers[xattn_layer_idx]\n+            else:\n+                xattn_layer_idx = 0\n+                xattn_layer = DummyCrossAttentionTransformerBlock()\n+\n+            self.text_and_xattn_layers.append(\n+                (\n+                    layer,\n+                    xattn_layer,\n+                    xattn_layer_idx,\n+                )\n+            )\n+        self.freqs_cis = precompute_freqs_cis(\n+            args.dim // args.n_heads,\n+            args.max_seq_len * 2,\n+            args.rope_theta,\n+            args.use_scaled_rope,\n+        )\n+\n+        self.args = args\n+        self.cache_is_setup = False\n+        self.max_seq_len = args.max_seq_len\n+\n+    def _init_fusion_schedule(\n+        self,\n+        num_layers: int,\n+    ) -> List[int]:\n+        llama_layers = list(range(self.n_llama_layers))\n+\n+        # uniformly spread the layers\n+        k = math.ceil(len(llama_layers) / num_layers)\n+        return llama_layers[::-1][::k][:num_layers][::-1]\n+\n+    def get_partially_trainable_embedding(self, x):\n+        xz = torch.zeros_like(x, device=x.device)\n+        oz = torch.ones_like(x, device=x.device)\n+        x_orig = torch.minimum(x, torch.tensor(self._thresh, device=x.device))\n+        x_new = (\n+            torch.maximum(x, torch.tensor(self._thresh + 1, device=x.device))\n+            - self.num_frozen_embeddings\n+        )\n+\n+        mask_orig = torch.where(x >= self.num_frozen_embeddings, xz, oz).unsqueeze(-1)\n+        mask_new = torch.where(x < self.num_frozen_embeddings, xz, oz).unsqueeze(-1)\n+\n+        x_orig = self.tok_embeddings(x_orig)\n+        x_new = self.learnable_embedding(x_new).type_as(x_orig)\n+        return x_orig * mask_orig.type_as(x_orig) + x_new * mask_new.type_as(x_new)\n+\n+    def forward(\n+        self,\n+        position_ids: torch.LongTensor,\n+        h: torch.Tensor,\n+        xattn_mask: torch.Tensor,\n+        full_text_row_masked_out_mask: torch.Tensor,\n+        xattn_caches: torch.Tensor,\n+        text_only_inference: bool = False,\n+    ):\n+        assert self.cache_is_setup, \"Please set up cache before calling forward\"\n+        mask = self.mask_cache.index_select(2, position_ids)\n+        freqs_cis = self.freqs_cis.index_select(0, position_ids)\n+\n+        for idx, (\n+            layer,\n+            xattn_layer,\n+            xattn_layer_idx,\n+        ) in enumerate(self.text_and_xattn_layers):\n+            if not text_only_inference:\n+                h = xattn_layer(\n+                    x=h,\n+                    xattn_mask=xattn_mask,\n+                    xattn_cache=xattn_caches[xattn_layer_idx],\n+                    full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n+                )\n+            h = layer(\n+                x=h,\n+                mask=mask,\n+                freqs_cis=freqs_cis,\n+                position_ids=position_ids,\n+            )\n+\n+        h = self.norm(h)\n+\n+        output = F.linear(h, self.output.weight)\n+        output = gather_from_tensor_model_parallel_region(output)\n+        return output.float()\n+\n+    def setup_cache(self, max_batch_size: int, dtype=torch.bfloat16):\n+        # Set up the text kv caches\n+        device = next(self.parameters()).device\n+        ones = torch.ones(\n+            (self.max_seq_len, self.max_seq_len),\n+            dtype=torch.bool,\n+            device=device,\n+        )\n+        self.register_buffer(\n+            \"mask_cache\",\n+            torch.tril(\n+                ones,\n+            )\n+            .unsqueeze(0)\n+            .unsqueeze(0),\n+            persistent=False,\n+        )\n+        for layer in self.layers:\n+            layer.setup_cache(max_batch_size, dtype=dtype)\n+        self.cache_is_setup = True\n+\n+    def _get_xattn_mask(\n+        self,\n+        num_tokens,\n+        text_device,\n+        text_dtype,\n+        vision_tokens,\n+        cross_attention_masks,\n+    ) -> Tuple[Tensor, Tensor]:\n+        assert vision_tokens is not None, \"Vision tokens must be provided\"\n+        vision_seqlen = vision_tokens.shape[3]\n+        assert (\n+            vision_tokens.shape[1] == cross_attention_masks.shape[2]\n+        ), f\"Mismatch in number of images given and number of masks given {vision_tokens.shape} {cross_attention_masks.shape}\"\n+        assert (\n+            vision_tokens.shape[2] == cross_attention_masks.shape[3]\n+        ), f\"Vision tokens shape {vision_tokens.shape} mismatch with xattn shape {cross_attention_masks.shape}\"\n+        assert (\n+            num_tokens == cross_attention_masks.shape[1]\n+        ), f\"Mismatch in text sequence length and cross attention mask sequence length {num_tokens} {cross_attention_masks.shape}\"\n+        _, _, _, num_image_tokens, image_token_dim = tuple(vision_tokens.shape)\n+        bsz, ntext, nimg, nchunks = cross_attention_masks.shape\n+        cross_attention_masks = (\n+            cross_attention_masks.repeat_interleave(vision_seqlen, dim=3)\n+            .view(bsz, ntext, -1)\n+            .unsqueeze(1)\n+        )\n+        full_text_row_masked_out_mask = _get_full_row_masked_out_mask(\n+            cross_attention_masks,\n+            get_negative_inf_value(cross_attention_masks.dtype),\n+        )\n+        cross_attention_masks *= full_text_row_masked_out_mask\n+\n+        return (\n+            cross_attention_masks.to(device=text_device, dtype=text_dtype),\n+            full_text_row_masked_out_mask,\n+        )\n+\n+\n+class CrossAttentionTransformer(torch.nn.Module):\n+    def __init__(self, args: ModelArgs) -> None:\n+        super().__init__()\n+        self.params = args\n+\n+        self.model_dim = args.dim\n+        self.vision_model = CrossAttentionTransformerVision(args)\n+        self.text_model = CrossAttentionTransformerText(args)\n+        self.image_res = args.vision_chunk_size\n+        self.max_num_chunks = args.vision_max_num_chunks\n+        self.image_transform = partial(\n+            VariableSizeImageTransform(size=args.vision_chunk_size),\n+            max_num_chunks=args.vision_max_num_chunks,\n+        )\n+\n+    def setup_cache(self, max_batch_size: int, dtype: torch.dtype):\n+        self.text_model.setup_cache(max_batch_size, dtype)\n+\n+    def compute_vision_tokens_masks(\n+        self,\n+        batch_images: List[List[PIL_Image.Image]],\n+        batch_masks: List[List[List[int]]],\n+        total_len: int,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        skip_vision_encoder = False\n+\n+        assert len(batch_images) == len(\n+            batch_masks\n+        ), \"Images and masks must have the same length\"\n+\n+        max_num_images = max(len(x) for x in batch_images)\n+        bsz = len(batch_images)\n+\n+        if max_num_images == 0:\n+            num_chunks = [[self.max_num_chunks] for _ in batch_images]\n+            skip_vision_encoder = True\n+        else:\n+            images_and_aspect_ratios = [\n+                [self.image_transform(im) for im in row] for row in batch_images\n+            ]\n+            transformed_images = [\n+                [x[0] for x in row] for row in images_and_aspect_ratios\n+            ]\n+\n+            aspect_ratios = torch.ones(bsz, max_num_images, 2, dtype=torch.int64)\n+            for i, row in enumerate(images_and_aspect_ratios):\n+                if len(row) > 0:\n+                    aspect_ratios[i, : len(row)] = torch.stack(\n+                        [torch.tensor(x[1]) for x in row]\n+                    )\n+\n+            stacked_images, num_chunks = _stack_images(\n+                transformed_images,\n+                max_num_chunks=self.max_num_chunks,\n+                image_res=self.params.vision_chunk_size,\n+                max_num_images=max_num_images,\n+            )\n+\n+        if skip_vision_encoder:\n+            vision_tokens = torch.zeros(\n+                (\n+                    bsz,\n+                    max_num_images,\n+                    self.max_num_chunks,\n+                    int(\n+                        (self.vision_model.image_res / self.vision_model.patch_size)\n+                        ** 2\n+                        + 1\n+                    ),\n+                    self.model_dim,\n+                ),\n+            )\n+        else:\n+            vision_tokens = self.vision_model(stacked_images, aspect_ratios)\n+\n+        vision_tokens = vision_tokens.to(\"cuda\")\n+\n+        bsz, nimg, nchunk, ntok, image_token_dim = tuple(vision_tokens.shape)\n+        xattn_caches = torch.stack(\n+            [\n+                layer.compute_xattn_kv_cache(\n+                    vision_tokens.view(bsz, -1, image_token_dim)\n+                )\n+                for layer in self.text_model.cross_attention_layers\n+            ]\n+        )\n+        padded_masks = _pad_masks(\n+            batch_masks,\n+            num_chunks,\n+            total_len,\n+            self.max_num_chunks,\n+        )\n+\n+        cross_attention_masks, full_text_row_masked_out_mask = (\n+            self.text_model._get_xattn_mask(\n+                num_tokens=total_len,\n+                text_device=\"cuda\",\n+                text_dtype=next(self.text_model.parameters()).dtype,\n+                vision_tokens=vision_tokens,\n+                cross_attention_masks=padded_masks,\n+            )\n+        )\n+\n+        return (xattn_caches, cross_attention_masks, full_text_row_masked_out_mask)\n+\n+    def forward(\n+        self,\n+        position_ids: torch.Tensor,\n+        tokens: torch.Tensor,\n+        cross_attention_masks: torch.Tensor,\n+        full_text_row_masked_out_mask: torch.Tensor,\n+        xattn_caches: torch.Tensor,\n+        text_only_inference: bool = False,\n+    ) -> torch.Tensor:\n+        h = self.text_model.get_partially_trainable_embedding(tokens[:, position_ids])\n+        logits = self.text_model.forward(\n+            position_ids=position_ids,\n+            h=h,\n+            xattn_mask=cross_attention_masks[:, :, position_ids],\n+            full_text_row_masked_out_mask=full_text_row_masked_out_mask[\n+                :, :, position_ids\n+            ],\n+            xattn_caches=xattn_caches,\n+            text_only_inference=text_only_inference,\n+        )\n+        return logits\n+\n+\n+def _stack_images(\n+    images: List[List[PIL_Image.Image]],\n+    max_num_chunks: int,\n+    image_res: int,\n+    max_num_images: int,\n+) -> Tuple[torch.Tensor, List[int]]:\n+    \"\"\"\n+    Takes a list of list of images and stacks them into a tensor.\n+    This function is needed since images can be of completely\n+    different resolutions and aspect ratios.\n+    \"\"\"\n+    out_images, out_num_chunks = [], []\n+    for imgs_sample in images:\n+        out_images_i = torch.zeros(\n+            max_num_images,\n+            max_num_chunks,\n+            3,\n+            image_res,\n+            image_res,\n+        )\n+        _num_chunks = []\n+        for j, chunks_image in enumerate(imgs_sample):\n+            out_images_i[j, : chunks_image.shape[0]] = chunks_image\n+            _num_chunks.append(chunks_image.shape[0])\n+        out_images.append(out_images_i)\n+        out_num_chunks.append(_num_chunks)\n+    return torch.stack(out_images), out_num_chunks\n+\n+\n+def _pad_masks(\n+    all_masks: List[List[List[int]]],\n+    all_num_chunks: List[List[int]],\n+    total_len: int,\n+    max_num_chunks: int,\n+) -> torch.Tensor:\n+    dtype = torch.bfloat16\n+    inf_value = get_negative_inf_value(dtype)\n+\n+    bsz = len(all_masks)\n+    max_num_media = max([len(m) for m in all_masks])\n+\n+    out_masks = torch.full(\n+        (bsz, total_len, max_num_media, max_num_chunks),\n+        inf_value,\n+        dtype=dtype,\n+    )\n+\n+    for idx, (mask, num_chunks) in enumerate(zip(all_masks, all_num_chunks)):\n+        for mask_idx, (mask_elem, mask_num_chunks) in enumerate(zip(mask, num_chunks)):\n+            if len(mask_elem) == 2:\n+                mask_elem[1] = min(mask_elem[1], total_len)\n+                if mask_elem[1] == -1:\n+                    mask_elem[1] = total_len\n+                out_masks[\n+                    idx, mask_elem[0] : mask_elem[1], mask_idx, :mask_num_chunks\n+                ].fill_(0.0)\n+\n+    return out_masks\n\n--- File: models/llama3/reference_impl/multimodal/utils.py ---\n@@ -0,0 +1,20 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import collections\n+\n+import torch\n+\n+\n+def get_negative_inf_value(dtype):\n+    return torch.finfo(dtype).min\n+\n+\n+def to_2tuple(x):\n+    if isinstance(x, collections.abc.Iterable):\n+        return x\n+    return (x, x)\n\n--- File: models/llama3/tests/api/test_tool_utils.py ---\n@@ -4,8 +4,11 @@\n # This source code is licensed under the terms described in the LICENSE file in\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n-\n-from llama_models.llama3.api.tool_utils import ToolUtils\n+from llama_models.llama3.api.tool_utils import (\n+    is_valid_python_list,\n+    parse_python_list_for_function_calls,\n+    ToolUtils,\n+)\n \n \n class TestToolUtils:\n@@ -18,3 +21,113 @@ def test_maybe_extract_custom_tool_call(self):\n         tool_name, args = res\n         assert tool_name == \"getWeather\"\n         assert args == {\"location\": \"New York\", \"date\": \"2023-08-05\"}\n+\n+\n+class TestPythonListCheck:\n+\n+    def test_valid_list_with_single_function_call(self):\n+        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True)]'\n+        assert is_valid_python_list(input_string) is True\n+\n+    def test_valid_list_with_multiple_function_calls(self):\n+        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True), get_melting_point(substance=\"ice\", kelvin=False)]'\n+        assert is_valid_python_list(input_string) is True\n+\n+    def test_invalid_empty_list(self):\n+        input_string = \"[]\"\n+        assert is_valid_python_list(input_string) is False\n+\n+    def test_invalid_list_with_non_function_call(self):\n+        input_string = '[get_boiling_point(liquid_name=\"water\"), 42]'\n+        assert is_valid_python_list(input_string) is False\n+\n+    def test_invalid_list_with_positional_args(self):\n+        input_string = '[get_boiling_point(\"water\", True)]'\n+        assert is_valid_python_list(input_string) is False\n+\n+    def test_invalid_nested_list(self):\n+        input_string = '[[get_boiling_point(liquid_name=\"water\")]]'\n+        assert is_valid_python_list(input_string) is False\n+\n+    def test_invalid_dict(self):\n+        input_string = '{\"func\": get_boiling_point(liquid_name=\"water\")}'\n+        assert is_valid_python_list(input_string) is False\n+\n+    def test_invalid_syntax(self):\n+        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True'  # Missing closing bracket\n+        assert is_valid_python_list(input_string) is False\n+\n+    def test_valid_list_with_boolean_args(self):\n+        input_string = (\n+            '[get_boiling_point(liquid_name=\"water\", celcius=True, precise=False)]'\n+        )\n+        assert is_valid_python_list(input_string) is True\n+\n+    def test_valid_list_with_numeric_args(self):\n+        input_string = \"[calculate_volume(radius=5.2, height=10)]\"\n+        assert is_valid_python_list(input_string) is True\n+\n+    def test_invalid_bare_function_call(self):\n+        input_string = 'get_boiling_point(liquid_name=\"water\")'\n+        assert is_valid_python_list(input_string) is False\n+\n+    def test_invalid_extra_char_function_call(self):\n+        input_string = '[(get_boiling_point(liquid_name=\"water\"),)]'\n+        assert is_valid_python_list(input_string) is False\n+\n+\n+class TestParsePythonList:\n+\n+    def test_single_function_call(self):\n+        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True)]'\n+        expected = [(\"get_boiling_point\", {\"liquid_name\": \"water\", \"celcius\": True})]\n+        assert parse_python_list_for_function_calls(input_string) == expected\n+\n+    def test_multiple_function_calls(self):\n+        input_string = '[get_boiling_point(liquid_name=\"water\", celcius=True), get_melting_point(substance=\"ice\", kelvin=False)]'\n+        expected = [\n+            (\"get_boiling_point\", {\"liquid_name\": \"water\", \"celcius\": True}),\n+            (\"get_melting_point\", {\"substance\": \"ice\", \"kelvin\": False}),\n+        ]\n+        assert parse_python_list_for_function_calls(input_string) == expected\n+\n+    def test_function_call_with_numeric_args(self):\n+        input_string = \"[calculate_volume(radius=5.2, height=10)]\"\n+        expected = [(\"calculate_volume\", {\"radius\": 5.2, \"height\": 10})]\n+        assert parse_python_list_for_function_calls(input_string) == expected\n+\n+    def test_function_call_with_mixed_type_args(self):\n+        input_string = (\n+            '[process_data(name=\"sample\", value=42, active=True, ratio=3.14)]'\n+        )\n+        expected = [\n+            (\n+                \"process_data\",\n+                {\"name\": \"sample\", \"value\": 42, \"active\": True, \"ratio\": 3.14},\n+            )\n+        ]\n+        assert parse_python_list_for_function_calls(input_string) == expected\n+\n+    def test_function_call_with_empty_args(self):\n+        input_string = \"[initialize()]\"\n+        expected = [(\"initialize\", {})]\n+        assert parse_python_list_for_function_calls(input_string) == expected\n+\n+    def test_function_call_with_string_containing_spaces(self):\n+        input_string = '[analyze_text(content=\"Hello, world!\", language=\"en\")]'\n+        expected = [(\"analyze_text\", {\"content\": \"Hello, world!\", \"language\": \"en\"})]\n+        assert parse_python_list_for_function_calls(input_string) == expected\n+\n+    def test_function_names_with_underscores_lists_and_dicts(self):\n+        input_string = '[get_boiling_point(liquid={\"name\":\"water\", \"color\":\"translucent\"}), convert_temperature(value=[100, 101], from_unit=\"C\", to_unit=\"F\")]'\n+        expected = [\n+            (\n+                \"get_boiling_point\",\n+                {\"liquid\": {\"name\": \"water\", \"color\": \"translucent\"}},\n+            ),\n+            (\n+                \"convert_temperature\",\n+                {\"value\": [100, 101], \"from_unit\": \"C\", \"to_unit\": \"F\"},\n+            ),\n+        ]\n+        assert parse_python_list_for_function_calls(input_string) == expected\n\n--- File: models/llama3/tests/prompt_templates/test_system_prompts.py ---\n@@ -1,3 +1,10 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n import textwrap\n import unittest\n from datetime import datetime\n@@ -6,6 +13,7 @@\n     BuiltinToolGenerator,\n     FunctionTagCustomToolGenerator,\n     JsonCustomToolGenerator,\n+    PythonListCustomToolGenerator,\n     SystemDefaultGenerator,\n )\n \n@@ -86,7 +94,7 @@ def test_system_custom_function_tag(self):\n             {\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n \n             Think very carefully before calling functions.\n-            If a you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+            If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n \n             <function=example_function_name>{\"example_name\": \"example_value\"}</function>\n \n@@ -99,3 +107,42 @@ def test_system_custom_function_tag(self):\n             \"\"\"\n         )\n         self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n+\n+    def test_llama_3_2_system_zero_shot(self):\n+        generator = PythonListCustomToolGenerator()\n+        expected_text = textwrap.dedent(\n+            \"\"\"\n+            You are an expert in composing functions. You are given a question and a set of possible functions.\n+            Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+            If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+            also point it out. You should only return the function call in tools call sections.\n+\n+            If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+            You SHOULD NOT include any other text in the response.\n+\n+            Here is a list of functions in JSON format that you can invoke.\n+\n+            [\n+                {\n+                    \"name\": \"get_weather\",\n+                    \"description\": \"Get weather info for places\",\n+                    \"parameters\": {\n+                        \"type\": \"dict\",\n+                        \"required\": [\"city\"],\n+                        \"properties\": {\n+                            \"city\": {\n+                                \"type\": \"string\",\n+                                \"description\": \"The name of the city to get the weather for\"\n+                            },\n+                            \"metric\": {\n+                                \"type\": \"string\",\n+                                \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n+                                \"default\": \"celsius\"\n+                            }\n+                        }\n+                    }\n+                }\n+            ]\n+            \"\"\"\n+        )\n+        self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n\n--- File: models/llama3_1/download.sh ---\n@@ -1,157 +0,0 @@\n-#!/usr/bin/env bash\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# All rights reserved.\n-#\n-# This source code is licensed under the terms described in the LICENSE file in\n-# top-level folder for each specific model found within the models/ directory at\n-# the top-level of this source tree.\n-\n-# Copyright (c) Meta Platforms, Inc. and affiliates.\n-# This software may be used and distributed according to the terms of the Llama 3.1 Community License Agreement.\n-\n-set -e\n-\n-read -p \"Enter the URL from email: \" PRESIGNED_URL\n-ALL_MODELS_LIST=\"meta-llama-3.1-405b,meta-llama-3.1-70b,meta-llama-3.1-8b,meta-llama-guard-3-8b,prompt-guard\"\n-printf \"\\n **** Model list ***\\n\"\n-for MODEL in ${ALL_MODELS_LIST//,/ }\n-do\n-    printf \" -  ${MODEL}\\n\"\n-done\n-read -p \"Choose the model to download: \" SELECTED_MODEL\n-printf \"\\n Selected model: ${SELECTED_MODEL} \\n\"\n-\n-SELECTED_MODELS=\"\"\n-if [[ $SELECTED_MODEL == \"meta-llama-3.1-405b\" ]]; then\n-    MODEL_LIST=\"meta-llama-3.1-405b-instruct-mp16,meta-llama-3.1-405b-instruct-mp8,meta-llama-3.1-405b-instruct-fp8,meta-llama-3.1-405b-mp16,meta-llama-3.1-405b-mp8,meta-llama-3.1-405b-fp8\"\n-elif [[ $SELECTED_MODEL == \"meta-llama-3.1-70b\" ]]; then\n-    MODEL_LIST=\"meta-llama-3.1-70b-instruct,meta-llama-3.1-70b\"\n-elif [[ $SELECTED_MODEL == \"meta-llama-3.1-8b\" ]]; then\n-    MODEL_LIST=\"meta-llama-3.1-8b-instruct,meta-llama-3.1-8b\"\n-elif [[ $SELECTED_MODEL == \"meta-llama-guard-3-8b\" ]]; then\n-    MODEL_LIST=\"meta-llama-guard-3-8b-int8-hf,meta-llama-guard-3-8b\"\n-elif [[ $SELECTED_MODEL == \"prompt-guard\" ]]; then\n-    SELECTED_MODELS=\"prompt-guard\"\n-    MODEL_LIST=\"\"\n-fi\n-\n-if [[ -z \"$SELECTED_MODELS\" ]]; then\n-    printf \"\\n **** Available models to download: ***\\n\"\n-    for MODEL in ${MODEL_LIST//,/ }\n-    do\n-        printf \" -  ${MODEL}\\n\"\n-    done\n-    read -p \"Enter the list of models to download without spaces or press Enter for all: \" SELECTED_MODELS\n-fi\n-\n-TARGET_FOLDER=\".\"             # where all files should end up\n-mkdir -p ${TARGET_FOLDER}\n-\n-if [[ $SELECTED_MODELS == \"\" ]]; then\n-    SELECTED_MODELS=${MODEL_LIST}\n-fi\n-\n-printf \"Downloading LICENSE and Acceptable Usage Policy\\n\"\n-wget --continue ${PRESIGNED_URL/'*'/\"LICENSE\"} -O ${TARGET_FOLDER}\"/LICENSE\"\n-wget --continue ${PRESIGNED_URL/'*'/\"USE_POLICY.md\"} -O ${TARGET_FOLDER}\"/USE_POLICY.md\"\n-\n-for m in ${SELECTED_MODELS//,/ }\n-do\n-\n-    ADDITIONAL_FILES=\"\"\n-    TOKENIZER_MODEL=1\n-    PTH_FILE_CHUNK_COUNT=0\n-    if [[ $m == \"meta-llama-3.1-405b-instruct-mp16\" ]]; then\n-        PTH_FILE_COUNT=15\n-        PTH_FILE_CHUNK_COUNT=2\n-        MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct-MP16\"\n-    elif [[ $m == \"meta-llama-3.1-405b-instruct-mp8\" ]]; then\n-        PTH_FILE_COUNT=7\n-        PTH_FILE_CHUNK_COUNT=4\n-        MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct-MP8\"\n-    elif [[ $m == \"meta-llama-3.1-405b-instruct-fp8\" ]]; then\n-        PTH_FILE_COUNT=7\n-        PTH_FILE_CHUNK_COUNT=3\n-        MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct\"\n-        ADDITIONAL_FILES=\"fp8_scales_0.pt,fp8_scales_1.pt,fp8_scales_2.pt,fp8_scales_3.pt,fp8_scales_4.pt,fp8_scales_5.pt,fp8_scales_6.pt,fp8_scales_7.pt\"\n-    elif [[ $m == \"meta-llama-3.1-405b-mp16\" ]]; then\n-        PTH_FILE_COUNT=15\n-        PTH_FILE_CHUNK_COUNT=2\n-        MODEL_PATH=\"Meta-Llama-3.1-405B-MP16\"\n-    elif [[ $m == \"meta-llama-3.1-405b-mp8\" ]]; then\n-        PTH_FILE_COUNT=7\n-        PTH_FILE_CHUNK_COUNT=4\n-        MODEL_PATH=\"Meta-Llama-3.1-405B-MP8\"\n-    elif [[ $m == \"meta-llama-3.1-405b-fp8\" ]]; then\n-        PTH_FILE_COUNT=7\n-        PTH_FILE_CHUNK_COUNT=3\n-        MODEL_PATH=\"Meta-Llama-3.1-405B\"\n-        ADDITIONAL_FILES=\"fp8_scales_0.pt,fp8_scales_1.pt,fp8_scales_2.pt,fp8_scales_3.pt,fp8_scales_4.pt,fp8_scales_5.pt,fp8_scales_6.pt,fp8_scales_7.pt\"\n-    elif [[ $m == \"meta-llama-3.1-70b-instruct\" ]]; then\n-        PTH_FILE_COUNT=7\n-        MODEL_PATH=\"Meta-Llama-3.1-70B-Instruct\"\n-    elif [[ $m == \"meta-llama-3.1-70b\" ]]; then\n-        PTH_FILE_COUNT=7\n-        MODEL_PATH=\"Meta-Llama-3.1-70B\"\n-    elif [[ $m == \"meta-llama-3.1-8b-instruct\" ]]; then\n-        PTH_FILE_COUNT=0\n-        MODEL_PATH=\"Meta-Llama-3.1-8B-Instruct\"\n-    elif [[ $m == \"meta-llama-3.1-8b\" ]]; then\n-        PTH_FILE_COUNT=0\n-        MODEL_PATH=\"Meta-Llama-3.1-8B\"\n-    elif [[ $m == \"meta-llama-guard-3-8b-int8-hf\" ]]; then\n-        PTH_FILE_COUNT=-1\n-        MODEL_PATH=\"Meta-Llama-Guard-3-8B-INT8-HF\"\n-        ADDITIONAL_FILES=\"generation_config.json,model-00001-of-00002.safetensors,model-00002-of-00002.safetensors,model.safetensors.index.json,special_tokens_map.json,tokenizer_config.json,tokenizer.json\"\n-        TOKENIZER_MODEL=0\n-    elif [[ $m == \"meta-llama-guard-3-8b\" ]]; then\n-        PTH_FILE_COUNT=0\n-        MODEL_PATH=\"Meta-Llama-Guard-3-8B\"\n-    elif [[ $m == \"prompt-guard\" ]]; then\n-        PTH_FILE_COUNT=-1\n-        MODEL_PATH=\"Prompt-Guard\"\n-        ADDITIONAL_FILES=\"model.safetensors,special_tokens_map.json,tokenizer_config.json,tokenizer.json\"\n-        TOKENIZER_MODEL=0\n-    fi\n-\n-    printf \"\\n***Downloading ${MODEL_PATH}***\\n\"\n-    mkdir -p ${TARGET_FOLDER}\"/${MODEL_PATH}\"\n-\n-    if [[ $TOKENIZER_MODEL == 1 ]]; then\n-        printf \"Downloading tokenizer\\n\"\n-        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/tokenizer.model\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/tokenizer.model\"\n-    fi\n-\n-    if [[ $PTH_FILE_COUNT -ge 0 ]]; then\n-        for s in $(seq -f \"%02g\" 0 ${PTH_FILE_COUNT})\n-        do\n-            printf \"Downloading consolidated.${s}.pth\\n\"\n-            if [[ $PTH_FILE_CHUNK_COUNT -gt 0 ]]; then\n-                start=0\n-                chunk_size=27000000001\n-                for chunk_count in $(seq 1 $PTH_FILE_CHUNK_COUNT)\n-                do\n-                    end=$((start+chunk_size-1))\n-                    curl ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -o ${TARGET_FOLDER}\"/${MODEL_PATH}/part.${chunk_count}.pth\" -H \"range: bytes=${start}-${end}\"\n-                    cat ${TARGET_FOLDER}\"/${MODEL_PATH}/part.${chunk_count}.pth\" >> ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n-                    rm ${TARGET_FOLDER}\"/${MODEL_PATH}/part.${chunk_count}.pth\"\n-                    start=$((end+1))\n-                done\n-            else\n-                wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n-            fi\n-        done\n-    fi\n-\n-    for ADDITIONAL_FILE in ${ADDITIONAL_FILES//,/ }\n-    do\n-        printf \"Downloading $ADDITIONAL_FILE...\\n\"\n-        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/${ADDITIONAL_FILE}\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/${ADDITIONAL_FILE}\"\n-    done\n-\n-    if [[ $m != \"prompt-guard\" &&  $m != \"meta-llama-guard-3-8b-int8-hf\" ]]; then\n-        printf \"Downloading params.json...\\n\"\n-        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/params.json\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/params.json\"\n-    fi\n-done\n\n--- File: models/llama3_1/prompt_format.md ---\n@@ -0,0 +1,358 @@\n+\n+\n+# Llama 3.1 - Prompt Formats\n+## Tokens\n+Here is a list of special tokens that are supported by Llama 3.1:\n+- `<|begin_of_text|>`: Specifies the start of the prompt\n+- `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n+- `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n+- `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and ipython]\n+- `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n+- `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n+    - at the end of a direct interaction between the model and the user\n+    - at the end of multiple interactions between the model and any available tools\n+    This token signals to the executor that the model has finished generating a response.\n+- `<|python_tag|>`: Is a special tag used in the model's response to signify a tool call.\n+\n+\n+\n+There are 4 different roles that are supported by Llama 3.1\n+- `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n+- `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n+- `ipython`: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n+- `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `ipython` and `user` prompts.\n+\n+## Llama 3.1 Base Model\n+\n+Text completion for Llama 3.1 base model uses this format.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|>Color of sky is blue but sometimes can also be\n+```\n+\n+##### Model Response Format\n+```\n+ red, orange, yellow, green, purple, pink, brown, gray, black, white, and even rainbow colors. The color of the sky can change due to various reasons such as time of day, weather conditions, pollution, and atmospheric phenomena.\n+The color of the sky is primarily blue because of a phenomenon called\n+```\n+\n+\n+\n+Note start special tag\n+\n+\n+## Llama 3.1 Instruct Model\n+## User and assistant conversation\n+\n+Here is a regular multi-turn user assistant conversation and how its formatted.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Answer who are you in the form of jeopardy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+Here's my response\n+\n+\"What is a helpful assistant?\"<|eot_id|>\n+```\n+\n+\n+\n+\n+\n+\n+## Tool Calling Formats\n+\n+\n+The three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:\n+- Brave Search: Tool call to perform web searches.\n+- Wolfram Alpha: Tool call to perform complex mathematical calculations.\n+- Code Interpreter: Enables the model to output python code.\n+\n+## Builtin Tool Calling\n+\n+\n+Here is an example of a conversation using brave search\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+Tools: brave_search, wolfram_alpha\n+Cutting Knowledge Date: December 2023\n+Today Date: 21 September 2024\n+\n+You are a helpful assistant.\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Search the web for the latest price of 1oz gold?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>brave_search.call(query=\"latest price of 1oz gold\")<|eom_id|>\n+```\n+\n+\n+\n+\n+- Just including Environment: ipython turns on code interpreter; therefore, you don't need to specify code interpretation on the Tools: line. The model can generate python code which is interpreted by the executor, with the result provided back to the model.\n+- The message body of the assistant response starts with a special tag <|python_tag|>\n+- As alluded to above, in such an environment, the model can generate <|eom_id|> instead of just the standard <|eot_id|> . The latter indicates the turn is finished, while the former indicates continued multi-step reasoning. That is, the model is expecting a continuation message with the output of the tool call.\n+- The model tool call response is of the form `tool.call(query=\"...\")` wher tool is `brave_search` or `wolfram_alpha`\n+\n+\n+## Builtin Code Interpreter\n+\n+Here is an actual example of model responding with code\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Write code to check if number is prime, use that to see if the number 7 is prime<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>def is_prime(n):\n+    if n <= 1\n+        return False\n+    for i in range(2, int(n**0.5) + 1):\n+        if n % i == 0:\n+            return False\n+    return True\n+\n+print(is_prime(7))  # Output: True<|eom_id|>\n+```\n+\n+\n+\n+\n+- Model starts with <|python_tag|> and continues writing python code that it needs to be executed\n+- No explicit mention of code_interpreter in system prompt. `Environment: ipython` implicitly enables it.\n+\n+\n+## Built-in tools full interaction\n+\n+Here is a full interaction with the built-in tools including the tool response and the final assistant response.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+Tools: brave_search, wolfram_alpha\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+What is the 100th decimal of pi?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+<|python_tag|>wolfram_alpha.call(query=\"100th decimal of pi\")<|eom_id|><|start_header_id|>ipython<|end_header_id|>\n+\n+\n+{\n+    \"queryresult\": {\n+        \"success\": true,\n+        \"inputstring\": \"100th decimal of pi\",\n+        \"pods\": [\n+            {\n+                \"title\": \"Input interpretation\",\n+                \"subpods\": [\n+                    {\n+                        \"title\": \"\",\n+                        \"plaintext\": \"100th digit | \"\n+                    }\n+                ]\n+            },\n+            {\n+                \"title\": \"Nearby digits\",\n+                \"subpods\": [\n+                    {\n+                        \"title\": \"\",\n+                        \"plaintext\": \"...86208998628034825342117067982148086513282306647093...\"\n+                    }\n+                ]\n+            },\n+            {\n+                \"title\": \"Result\",\n+                \"primary\": true,\n+                \"subpods\": [\n+                    {\n+                        \"title\": \"\",\n+                        \"plaintext\": \"7\"\n+                    }\n+                ]\n+            }\n+        ]\n+    }\n+}\n+<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+The 100th decimal of pi is 7.<|eot_id|>\n+```\n+\n+\n+\n+\n+- Note the `<|python_tag|>` in the assistant response.\n+- Role is `ipython` for the wolfram alpha response that is passed back to the model.\n+- Final message from assistant has <|eot_id|> tag.\n+\n+\n+\n+## Zero shot tool calling\n+## JSON based tool calling\n+\n+\n+Llama models can now output custom tool calls from a single message to allow easier tool calling.\n+The following prompts provide an example of how custom tools can be called from the output of the model.\n+It's important to note that the model itself does not execute the calls; it provides structured output to facilitate calling by an executor.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+\n+Cutting Knowledge Date: December 2023\n+Today Date: 21 September 2024\n+\n+You are a helpful assistant.\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Answer the user's question by making use of the following functions if needed.\n+If none of the function can be used, please say so.\n+Here is a list of functions in JSON format:\n+{\n+    \"type\": \"function\",\n+    \"function\": {\n+        \"name\": \"trending_songs\",\n+        \"description\": \"Returns the trending songs on a Music site\",\n+        \"parameters\": {\n+            \"type\": \"object\",\n+            \"properties\": [\n+                {\n+                    \"n\": {\n+                        \"type\": \"object\",\n+                        \"description\": \"The number of songs to return\"\n+                    }\n+                },\n+                {\n+                    \"genre\": {\n+                        \"type\": \"object\",\n+                        \"description\": \"The genre of the songs to return\"\n+                    }\n+                }\n+            ],\n+            \"required\": [\"n\"]\n+        }\n+    }\n+}\n+\n+Return function calls in JSON format.<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Use tools to get latest trending songs<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>{\n+    \"type\": \"function\",\n+    \"name\": \"trending_songs\",\n+    \"parameters\": {\n+        \"n\": \"10\",\n+        \"genre\": \"all\"\n+    }\n+}<|eom_id|>\n+```\n+\n+\n+\n+\n+- JSON format for providing tools needs name, description and parameters\n+- Model responds with `<|python_tag|>` and `<|eom_id|>` as `Environment: ipython` was in the system prompt\n+- Instructions for tools added as a user message\n+- Only single tool calls are supported as of now\n+\n+\n+\n+## Example of a user defined tool calling\n+## `<function>` based tool calling\n+\n+\n+Here is an example of how you could also write custom instructions for model to do zero shot tool calling.\n+In this example, we define a custom tool calling format using the `<function>` tag.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+\n+Cutting Knowledge Date: December 2023\n+Today Date: 21 September 2024\n+\n+You are a helpful assistant.\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+You have access to the following functions:\n+\n+Use the function 'trending_songs' to 'Returns the trending songs on a Music site':\n+{\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n+\n+Think very carefully before calling functions.\n+If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+\n+<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n+\n+Reminder:\n+- If looking for real time information use relevant functions before falling back to brave_search\n+- Function calls MUST follow the specified format, start with <function= and end with </function>\n+- Required parameters MUST be specified\n+- Only call one function at a time\n+- Put the entire function call reply on one line<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Use tools to get latest trending songs<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<function=trending_songs>{\"n\": 10}</function><|eot_id|>\n+```\n+\n+\n+\n+\n+- In this case, model does NOT respond with `<|python_tag|>` and ends with `<|eot_id|>`\n+- Instructions for tools added as a user message\n+\n+\n+Thank You!\n\n--- File: models/llama3_1/prompts.py ---\n@@ -0,0 +1,244 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import textwrap\n+from typing import List\n+from ..llama3.api.datatypes import *  # noqa: F403\n+from ..prompt_format import (\n+    llama3_1_builtin_tool_call_dialog,\n+    llama3_1_custom_tool_call_dialog,\n+    # llama3_1_e2e_tool_call_dialog,\n+    TextCompletionContent,\n+    UseCase,\n+)\n+\n+\n+def wolfram_alpha_response():\n+    return textwrap.dedent(\n+        \"\"\"\n+        {\n+            \"queryresult\": {\n+                \"success\": true,\n+                \"inputstring\": \"100th decimal of pi\",\n+                \"pods\": [\n+                    {\n+                        \"title\": \"Input interpretation\",\n+                        \"subpods\": [\n+                            {\n+                                \"title\": \"\",\n+                                \"plaintext\": \"100th digit | \\u03c0\"\n+                            }\n+                        ]\n+                    },\n+                    {\n+                        \"title\": \"Nearby digits\",\n+                        \"subpods\": [\n+                            {\n+                                \"title\": \"\",\n+                                \"plaintext\": \"...86208998628034825342117067982148086513282306647093...\"\n+                            }\n+                        ]\n+                    },\n+                    {\n+                        \"title\": \"Result\",\n+                        \"primary\": true,\n+                        \"subpods\": [\n+                            {\n+                                \"title\": \"\",\n+                                \"plaintext\": \"7\"\n+                            }\n+                        ]\n+                    }\n+                ]\n+            }\n+        }\n+        \"\"\"\n+    )\n+\n+\n+def usecases() -> List[UseCase | str]:\n+    return [\n+        textwrap.dedent(\n+            \"\"\"\n+            # Llama 3.1 - Prompt Formats\n+            ## Tokens\n+            Here is a list of special tokens that are supported by Llama 3.1:\n+            - `<|begin_of_text|>`: Specifies the start of the prompt\n+            - `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n+            - `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n+            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and ipython]\n+            - `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n+            - `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n+                - at the end of a direct interaction between the model and the user\n+                - at the end of multiple interactions between the model and any available tools\n+                This token signals to the executor that the model has finished generating a response.\n+            - `<|python_tag|>`: Is a special tag used in the model's response to signify a tool call.\n+            \"\"\"\n+        ),\n+        textwrap.dedent(\n+            \"\"\"\n+            There are 4 different roles that are supported by Llama 3.1\n+            - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n+            - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n+            - `ipython`: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n+            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `ipython` and `user` prompts.\n+            \"\"\"\n+        ),\n+        UseCase(\n+            title=\"Llama 3.1 Base Model\",\n+            description=\"Text completion for Llama 3.1 base model uses this format.\",\n+            dialogs=[\n+                TextCompletionContent(\n+                    content=\"Color of sky is blue but sometimes can also be\"\n+                )\n+            ],\n+            notes=\"Note start special tag\",\n+        ),\n+        \"## Llama 3.1 Instruct Model\",\n+        UseCase(\n+            title=\"User and assistant conversation\",\n+            description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n+            dialogs=[\n+                [\n+                    SystemMessage(content=\"You are a helpful assistant\"),\n+                    UserMessage(content=\"Answer who are you in the form of jeopardy?\"),\n+                ]\n+            ],\n+            notes=\"\",\n+        ),\n+        \"## Tool Calling Formats\",\n+        textwrap.dedent(\n+            \"\"\"\n+            The three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:\n+            - Brave Search: Tool call to perform web searches.\n+            - Wolfram Alpha: Tool call to perform complex mathematical calculations.\n+            - Code Interpreter: Enables the model to output python code.\n+            \"\"\"\n+        ),\n+        UseCase(\n+            title=\"Builtin Tool Calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Here is an example of a conversation using brave search\n+                \"\"\"\n+            ),\n+            dialogs=[llama3_1_builtin_tool_call_dialog()],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Just including Environment: ipython turns on code interpreter; therefore, you don't need to specify code interpretation on the Tools: line. The model can generate python code which is interpreted by the executor, with the result provided back to the model.\n+                - The message body of the assistant response starts with a special tag <|python_tag|>\n+                - As alluded to above, in such an environment, the model can generate <|eom_id|> instead of just the standard <|eot_id|> . The latter indicates the turn is finished, while the former indicates continued multi-step reasoning. That is, the model is expecting a continuation message with the output of the tool call.\n+                - The model tool call response is of the form `tool.call(query=\"...\")` wher tool is `brave_search` or `wolfram_alpha`\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Builtin Code Interpreter\",\n+            description=\"Here is an actual example of model responding with code\",\n+            dialogs=[\n+                [\n+                    SystemMessage(content=\"Environment: ipython\"),\n+                    UserMessage(\n+                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\"\n+                    ),\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Model starts with <|python_tag|> and continues writing python code that it needs to be executed\n+                - No explicit mention of code_interpreter in system prompt. `Environment: ipython` implicitly enables it.\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Built-in tools full interaction\",\n+            description=\"Here is a full interaction with the built-in tools including the tool response and the final assistant response.\",\n+            dialogs=[\n+                [\n+                    SystemMessage(\n+                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\"\n+                    ),\n+                    UserMessage(content=\"What is the 100th decimal of pi?\"),\n+                    CompletionMessage(\n+                        content=\"\",\n+                        stop_reason=StopReason.end_of_message,\n+                        tool_calls=[\n+                            ToolCall(\n+                                call_id=\"tool_call_id\",\n+                                tool_name=BuiltinTool.wolfram_alpha,\n+                                arguments={\"query\": \"100th decimal of pi\"},\n+                            )\n+                        ],\n+                    ),\n+                    ToolResponseMessage(\n+                        call_id=\"wolfram_alpha_id\",\n+                        tool_name=BuiltinTool.wolfram_alpha,\n+                        content=wolfram_alpha_response(),\n+                    ),\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Note the `<|python_tag|>` in the assistant response.\n+                - Role is `ipython` for the wolfram alpha response that is passed back to the model.\n+                - Final message from assistant has <|eot_id|> tag.\n+                \"\"\"\n+            ),\n+        ),\n+        \"## Zero shot tool calling\",\n+        UseCase(\n+            title=\"JSON based tool calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Llama models can now output custom tool calls from a single message to allow easier tool calling.\n+                The following prompts provide an example of how custom tools can be called from the output of the model.\n+                It's important to note that the model itself does not execute the calls; it provides structured output to facilitate calling by an executor.\n+                \"\"\"\n+            ),\n+            dialogs=[llama3_1_custom_tool_call_dialog()],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - JSON format for providing tools needs name, description and parameters\n+                - Model responds with `<|python_tag|>` and `<|eom_id|>` as `Environment: ipython` was in the system prompt\n+                - Instructions for tools added as a user message\n+                - Only single tool calls are supported as of now\n+                \"\"\"\n+            ),\n+        ),\n+        # FIXME: This is not working yet as expected\n+        # UseCase(\n+        #     title=\"E2E tool call example\",\n+        #     description=textwrap.dedent(\n+        #         \"\"\"\n+        #         Here is an example showing the whole multi-step turn by taking custom tool outputs and passing back to the model.\n+        #         \"\"\"\n+        #     ),\n+        #     dialogs=[\n+        #         llama3_1_e2e_tool_call_dialog(\n+        #             tool_prompt_format=ToolPromptFormat.function_tag\n+        #         )\n+        #     ],\n+        #     notes=\"\",\n+        # ),\n+        \"## Example of a user defined tool calling\",\n+        UseCase(\n+            title=\"`<function>` based tool calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Here is an example of how you could also write custom instructions for model to do zero shot tool calling.\n+                In this example, we define a custom tool calling format using the `<function>` tag.\n+                \"\"\"\n+            ),\n+            dialogs=[llama3_1_custom_tool_call_dialog(ToolPromptFormat.function_tag)],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - In this case, model does NOT respond with `<|python_tag|>` and ends with `<|eot_id|>`\n+                - Instructions for tools added as a user message\n+                \"\"\"\n+            ),\n+        ),\n+    ]\n\n--- File: models/llama3_2/LICENSE ---\n@@ -0,0 +1,70 @@\n+LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\n+\n+Llama 3.2 Version Release Date: September 25, 2024\n+\n+Agreement means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n+\n+\n+Documentation means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://www.llama.com/docs/overview.\n+\n+\n+Licensee or you means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entitys behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n+\n+\n+Llama 3.2 means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://www.llama.com/llama-downloads.\n+\n+\n+Llama Materials means, collectively, Metas proprietary Llama 3.2 and Documentation (and any portion thereof) made available under this Agreement.\n+\n+\n+Meta or we means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n+\n+\n+By clicking I Accept below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n+\n+\n+1. License Rights and Redistribution.\n+\n+\n+    a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metas intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\n+\n+\n+    b. Redistribution and Use.\n+\n+\n+        i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display Built with Llama on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include Llama at the beginning of any such AI model name.\n+\n+\n+        ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\n+\n+\n+        iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a Notice text file distributed as a part of such copies: Llama 3.2 is licensed under the Llama 3.2 Community License, Copyright  Meta Platforms, Inc. All Rights Reserved.\n+\n+\n+        iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby incorporated by reference into this Agreement.\n+\n+2. Additional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensees affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n+\n+\n+3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN AS IS BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n+\n+\n+4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n+\n+\n+5. Intellectual Property.\n+\n+\n+    a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use Llama (the Mark) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Metas brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n+\n+\n+    b. Subject to Metas ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n+\n+\n+    c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n+\n+\n+6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n+\n+\n+7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n\n--- File: models/llama3_2/MODEL_CARD.md ---\n@@ -0,0 +1,163 @@\n+## Model Information\n+\n+The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n+\n+**Model Developer:** Meta\n+\n+**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n+\n+|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n+| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n+| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n+|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n+\n+**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n+\n+**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n+\n+**Model Release Date:** Sept 25, 2024\n+\n+**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n+\n+**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n+\n+**Feedback:** Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+\n+## Intended Use\n+\n+**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks.\n+\n+**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n+\n+## Hardware and Software\n+\n+**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n+\n+**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n+\n+##\n+\n+**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n+\n+|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n+| :---- | :---: | ----- | :---: | :---: | :---: |\n+| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n+| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n+| Total | 830k |         86k |  | 240 | 0 |\n+\n+The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n+\n+## Training Data\n+\n+**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n+\n+**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n+\n+## Benchmarks \\- English Text\n+\n+In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n+\n+### Base Pretrained Models\n+\n+| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n+| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n+| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n+|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n+|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n+| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n+|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n+|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n+| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n+\n+### Instruction Tuned Models\n+\n+| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n+| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: |\n+| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 63.4 | 69.4 |\n+| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 40.1 | 40.9 |\n+| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 19.0 | 17.2 |\n+| Instruction following |  | IFEval | 0 | avg(prompt/instruction acc loose/strict) | 59.5 | 77.4 | 80.4 |\n+| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 77.7 | 84.5 |\n+|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 47.3 | 51.9 |\n+| Reasoning |  | ARC-C | 0 | acc | 59.4 | 78.6 | 83.4 |\n+|  |  | GPQA | 0 | acc | 27.2 | 32.8 | 32.8 |\n+|  |  | Hellaswag | 0 | acc | 41.2 | 69.8 | 78.7 |\n+| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 67.0 | 70.9 |\n+|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 34.3 | 38.5 |\n+| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | 19.8 | 27.3 |\n+|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | 63.3 | 72.2 |\n+|  |  | NIH/Multi-needle | 0 | recall | 75.0 | 84.7 | 98.8 |\n+| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 58.2 | 68.9 |\n+\n+### Multilingual Benchmarks\n+\n+| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n+| :---: | :---: | :---: | :---: | :---: | :---: |\n+| General | MMLU (5-shot, macro\\_avg/acc) | Portuguese | 39.82 | 54.48 | 62.12 |\n+|  |  | Spanish | 41.5 | 55.1 | 62.5 |\n+|  |  | Italian | 39.8 | 53.8 | 61.6 |\n+|  |  | German | 39.2 | 53.3 | 60.6 |\n+|  |  | French | 40.5 | 54.6 | 62.3 |\n+|  |  | Hindi | 33.5 | 43.3 | 50.9 |\n+|  |  | Thai | 34.7 | 44.5 | 50.3 |\n+\n+## Responsibility & Safety\n+\n+As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n+\n+1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n+2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n+3. Provide protections for the community to help prevent the misuse of our models\n+\n+### Responsible Deployment\n+\n+**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Metas Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the drivers seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n+\n+#### Llama 3.2 Instruct\n+\n+**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n+\n+**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. Weve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n+\n+**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n+\n+#### Llama 3.2 Systems\n+\n+**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n+\n+### New Capabilities and Use Cases\n+\n+**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n+\n+**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n+\n+### Evaluations\n+\n+**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n+\n+**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n+\n+### Critical Risks\n+\n+In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n+\n+**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n+\n+**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the models capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n+\n+**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n+Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n+\n+### Community\n+\n+**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n+\n+**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Metas Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n+\n+**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n+\n+## Ethical Considerations and Limitations\n+\n+**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n+\n+**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n\n--- File: models/llama3_2/MODEL_CARD_VISION.md ---\n@@ -0,0 +1,160 @@\n+## Model Information\n+\n+The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text \\+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n+\n+**Model Developer**: Meta\n+\n+**Model Architecture:** Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\n+\n+|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Data volume | Knowledge cutoff |\n+| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n+| Llama 3.2-Vision  | (Image, text) pairs | 11B (10.6) | Text \\+ Image | Text  | 128k | Yes | 6B (image, text) pairs | December 2023 |\n+| Llama 3.2-Vision | (Image, text) pairs | 90B (88.8) | Text \\+ Image | Text  | 128k | Yes | 6B (image, text) pairs  | December 2023 |\n+\n+**Supported Languages:** For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\n+\n+Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n+\n+**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n+\n+**Model Release Date:** Sept 25, 2024\n+\n+**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n+\n+**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n+\n+**Feedback:** Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2). For more technical information about generation parameters and recipes for how to use Llama 3.2-Vision in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+\n+## Intended Use\n+\n+**Intended Use Cases:** Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Visions ability to take images and text as inputs, additional use cases could include:\n+\n+1. Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.\n+2. Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.\n+3. Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.\n+4. Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.\n+5. Visual Grounding: Visual grounding is like connecting the dots between what we see and say. Its about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions.\n+\n+\n+The Llama 3.2 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.2 Community License allows for these use cases.\n+\n+**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n+\n+## Hardware and Software\n+\n+**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n+\n+**Training Energy Use:** Training utilized a cumulative of **2.02M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n+\n+##\n+\n+**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **584** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n+\n+|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n+| :---- | :---: | :---: | :---: | :---: |\n+| Llama 3.2-vision 11B | Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours | 700 | 71 | 0 |\n+| Llama 3.2-vision 90B | Stage 1 pretraining: 885K H100 hours Stage 2 annealing: 885K H100 hours SFT: 3072 H100 hours RLHF: 2048 H100 hours | 700 | 513 | 0 |\n+| Total | 2.02M |  | 584 | 0 |\n+\n+The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n+\n+## Training Data\n+\n+**Overview:** Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\n+\n+**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n+\n+## Benchmarks \\- Image Reasoning\n+\n+In this section, we report the results for Llama 3.2-Vision models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n+\n+### Base Pretrained Models\n+\n+| Category | Benchmark | \\# Shots | Metric | Llama 3.2 11B | Llama 3.2 90B |\n+| ----- | ----- | ----- | ----- | ----- | ----- |\n+| Image Understanding | VQAv2 (val) | 0 | Accuracy | 66.8 | 73.6 |\n+|  | Text VQA (val) | 0 | Relaxed accuracy | 73.1 | 73.5 |\n+|  | DocVQA (val, unseen) | 0 | ANLS | 62.3 | 70.7 |\n+| Visual Reasoning | MMMU (val, 0-shot) | 0 | Micro average accuracy | 41.7 | 49.3 |\n+|  | ChartQA (test) | 0 | Accuracy | 39.4 | 54.2 |\n+|  | InfographicsQA (val, unseen) | 0 | ANLS | 43.2 | 56.8 |\n+|  | AI2 Diagram (test) | 0 | Accuracy | 62.4 | 75.3 |\n+\n+### Instruction Tuned Models\n+\n+| Modality | Capability | Benchmark | \\# Shots | Metric | Llama 3.2 11B | Llama 3.2 90B |\n+| ----- | :---: | ----- | :---: | :---: | ----- | ----- |\n+| Image | College-level Problems and Mathematical Reasoning | MMMU (val, CoT) | 0 | Micro average accuracy | 50.7 | 60.3 |\n+|  |  | MMMU-Pro, Standard (10 opts, test) | 0 | Accuracy | 33.0 | 45.2 |\n+|  |  | MMMU-Pro, Vision (test) | 0 | Accuracy | 23.7 | 33.8 |\n+|  |  | MathVista (testmini) | 0 | Accuracy | 51.5 | 57.3 |\n+|  | Charts and Diagram Understanding | ChartQA (test, CoT) | 0 | Relaxed accuracy | 83.4 | 85.5 |\n+|  |  | AI2 Diagram (test) | 0 | Accuracy | 91.1 | 92.3 |\n+|  |  | DocVQA (test) | 0 | ANLS | 88.4 | 90.1 |\n+|  | General Visual Question Answering | VQAv2 (test) | 0 | Accuracy | 75.2 | 78.1 |\n+|  |  |  |  |  |  |  |\n+| Text | General | MMLU (CoT) | 0 | Macro\\_avg/acc | 73.0 | 86.0 |\n+|  | Math | MATH (CoT) | 0 | Final\\_em | 51.9 | 68.0 |\n+|  | Reasoning | GPQA | 0 | Accuracy | 32.8 | 46.7 |\n+|  | Multilingual | MGSM (CoT) | 0 | em | 68.9 | 86.9 |\n+\n+## Responsibility & Safety\n+\n+As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n+\n+1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n+2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n+3. Provide protections for the community to help prevent the misuse of our models.\n+\n+### Responsible Deployment\n+\n+**Approach:** Llama is a foundational technology designed to be used in a variety of use cases, examples on how Metas Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n+\n+#### Llama 3.2 Instruct\n+\n+**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n+\n+**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. Weve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n+\n+**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n+\n+#### Llama 3.2 Systems\n+\n+**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n+\n+### New Capabilities and Use Cases\n+\n+**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.,\n+\n+**Image Reasoning:** Llama 3.2-Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks.\n+\n+### Evaluations\n+\n+**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n+\n+**Red teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n+\n+### Critical Risks\n+\n+In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n+\n+**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** For Llama 3.1, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. For Llama 3.2-Vision models, we conducted additional targeted evaluations and found that it was unlikely Llama 3.2 presented an increase in scientific capabilities due to its added image understanding capability as compared to Llama 3.1.\n+\n+**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the models capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n+\n+**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n+Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2s vision capabilities are not generally germane to cyber uplift, we believe that the testing conducted for Llama 3.1 also applies to Llama 3.2.\n+\n+### Community\n+\n+**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n+\n+**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Metas Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n+\n+**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n+\n+## Ethical Considerations and Limitations\n+\n+**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n+\n+**Testing:** But Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n\n--- File: models/llama3_2/USE_POLICY.md ---\n@@ -0,0 +1,52 @@\n+**Llama 3.2** **Acceptable Use Policy**\n+\n+Meta is committed to promoting safe and fair use of its tools and features, including Llama 3.2. If you access or use Llama 3.2, you agree to this Acceptable Use Policy (**Policy**). The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\n+\n+**Prohibited Uses**\n+\n+We want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\n+\n+\n+\n+1. Violate the law or others rights, including to:\n+    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n+        1. Violence or terrorism\n+        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n+        3. Human trafficking, exploitation, and sexual violence\n+        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n+        5. Sexual solicitation\n+        6. Any other criminal activity\n+    1. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n+    2. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n+    3. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n+    4. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n+    5. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n+    6. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n+    7. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\n+2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\n+    8. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\n+    9. Guns and illegal weapons (including weapon development)\n+    10. Illegal drugs and regulated/controlled substances\n+    11. Operation of critical infrastructure, transportation technologies, or heavy machinery\n+    12. Self-harm or harm to others, including suicide, cutting, and eating disorders\n+    13. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n+3. Intentionally deceive or mislead others, including use of Llama 3.2 related to the following:\n+    14. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n+    15. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n+    16. Generating, promoting, or further distributing spam\n+    17. Impersonating another individual without consent, authorization, or legal right\n+    18. Representing that the use of Llama 3.2 or outputs are human-generated\n+    19. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n+4. Fail to appropriately disclose to end users any known dangers of your AI system\n+5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\n+\n+With respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\n+\n+Please report any violation of this Policy, software bug, or other problems that could lead to a violation of this Policy through one of the following means:\n+\n+\n+\n+* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\n+* Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n+* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n+* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com\n\n--- File: models/llama3_2/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n\n--- File: models/llama3_2/prompts_text.py ---\n@@ -0,0 +1,224 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+import json\n+import textwrap\n+from ..llama3.api.datatypes import *  # noqa: F403\n+from ..prompt_format import (\n+    llama3_1_builtin_code_interpreter_dialog,\n+    TextCompletionContent,\n+    UseCase,\n+)\n+\n+\n+def user_tool_call():\n+    content = textwrap.dedent(\n+        \"\"\"\n+        Questions: Can you retrieve the details for the user with the ID 7890, who has black as their special request?\n+        Here is a list of functions in JSON format that you can invoke:\n+        [\n+            {\n+                \"name\": \"get_user_info\",\n+                \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n+                \"parameters\": {\n+                    \"type\": \"dict\",\n+                    \"required\": [\n+                        \"user_id\"\n+                    ],\n+                    \"properties\": {\n+                        \"user_id\": {\n+                        \"type\": \"integer\",\n+                        \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n+                    },\n+                    \"special\": {\n+                        \"type\": \"string\",\n+                        \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n+                        \"default\": \"none\"\n+                        }\n+                    }\n+                }\n+            }\n+        ]\n+\n+        Should you decide to return the function call(s),Put it in the format of [func1(params_name=params_value, params_name2=params_value2...), func2(params)]\n+\n+        NO other text MUST be included.\n+        \"\"\"\n+    )\n+    return content.strip()\n+\n+\n+def system_tool_call():\n+    content = textwrap.dedent(\n+        \"\"\"\n+        You are an expert in composing functions. You are given a question and a set of possible functions.\n+        Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+        If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+        also point it out. You should only return the function call in tools call sections.\n+\n+        If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+        You SHOULD NOT include any other text in the response.\n+\n+        Here is a list of functions in JSON format that you can invoke.\n+\n+        [\n+            {\n+                \"name\": \"get_weather\",\n+                \"description\": \"Get weather info for places\",\n+                \"parameters\": {\n+                    \"type\": \"dict\",\n+                    \"required\": [\n+                        \"city\"\n+                    ],\n+                    \"properties\": {\n+                        \"city\": {\n+                            \"type\": \"string\",\n+                            \"description\": \"The name of the city to get the weather for\"\n+                        },\n+                        \"metric\": {\n+                            \"type\": \"string\",\n+                            \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n+                            \"default\": \"celsius\"\n+                        }\n+                    }\n+                }\n+            }\n+        ]\n+        \"\"\"\n+    )\n+    return content.strip()\n+\n+\n+def usecases():\n+    return [\n+        UseCase(\n+            title=\"User and assistant conversation\",\n+            description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n+            dialogs=[\n+                [\n+                    SystemMessage(content=\"You are a helpful assistant\"),\n+                    UserMessage(content=\"Who are you?\"),\n+                ]\n+            ],\n+            notes=\"This format is unchanged from Llama3.1\",\n+        ),\n+        UseCase(\n+            title=\"Zero shot function calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                For Llama3.2 1B and 3B instruct models, we are introducing a new format for zero shot function calling.\n+                This new format is designed to be more flexible and powerful than the previous format.\n+                All available functions can be provided in the system message. A key difference is in the format of how the assistant responds with function calls.\n+                It is pythonic in the form of `[func1(params_name=params_value, params_name2=params_value2...), func2(params)]` instead of the `json` or `<function>` tag that were defined in Llama3.1.\n+                Here is an example for the same,\n+                \"\"\"\n+            ),\n+            dialogs=[\n+                # Zero shot tool calls as system message\n+                [\n+                    SystemMessage(content=system_tool_call()),\n+                    UserMessage(content=\"What is the weather in SF and Seattle?\"),\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - The output supports multiple tool calls natively\n+                - JSON format for defining the functions in the system prompt is similar to Llama3.1\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Zero shot function calling with user message\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                While the default is to provide all function calls in a system message, in Llama3.2 text models you can also provide information for all the available tools in a user message.\n+                \"\"\"\n+            ),\n+            dialogs=[\n+                # Zero shot tool call as user message\n+                [\n+                    UserMessage(content=user_tool_call()),\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - The tool call format for the mdoel is the same whether your function calls are provided in the system or user message.\n+                - While builtin tool calls end with a <|eom_id|>, notice the <|eot_id|> for zero shot tool calls.\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Code Interpreter\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Code Interpreter continues to work in 3.2 text models similar to Llama 3.1 model family.\n+                Here is an example,\n+                \"\"\"\n+            ),\n+            dialogs=[llama3_1_builtin_code_interpreter_dialog()],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Note `Environment: ipython` in the system prompt.\n+                - Note that the response starts with `<|python_tag|>` and ends with `<|eom_id|>`\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Zero shot function calling E2E format\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Here is an example of the e2e cycle of tool calls with the model in a muti-step way.\n+                \"\"\"\n+            ),\n+            dialogs=[\n+                [\n+                    SystemMessage(content=system_tool_call()),\n+                    UserMessage(content=\"What is the weather in SF?\"),\n+                    CompletionMessage(\n+                        content=\"\",\n+                        stop_reason=StopReason.end_of_turn,\n+                        tool_calls=[\n+                            ToolCall(\n+                                call_id=\"cc\",\n+                                tool_name=\"get_weather\",\n+                                arguments={\n+                                    \"city\": \"San Francisco\",\n+                                    \"metric\": \"celsius\",\n+                                },\n+                            )\n+                        ],\n+                    ),\n+                    ToolResponseMessage(\n+                        call_id=\"call\",\n+                        tool_name=\"get_weather\",\n+                        content=json.dumps(\"25 C\"),\n+                    ),\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - The output of the function call is provided back to the model as a tool response ( in json format ).\n+                - Notice `<|start_header_id|>ipython<|end_header_id|>` as the header message preceding the tool response.\n+                - The model finally summarizes the information from the tool response and returns the result to the user.\n+                \"\"\"\n+            ),\n+            tool_prompt_format=ToolPromptFormat.python_list,\n+        ),\n+        UseCase(\n+            title=\"Prompt format for base models\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                For base models (Llama3.2-1B and Llama3.2-3B), the prompt format for a simple completion is as follows\n+                \"\"\"\n+            ),\n+            dialogs=[\n+                TextCompletionContent(\n+                    content=\"The color of the sky is blue but sometimes it can also be\"\n+                ),\n+            ],\n+            notes=\"Same as Llama3.1\",\n+        ),\n+    ]\n\n--- File: models/llama3_2/prompts_vision.py ---\n@@ -0,0 +1,125 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import textwrap\n+from pathlib import Path\n+\n+from PIL import Image as PIL_Image\n+\n+from ..llama3.api.datatypes import *  # noqa: F403\n+from ..prompt_format import (\n+    llama3_1_builtin_tool_call_dialog,\n+    # llama3_1_builtin_tool_call_with_image_dialog,\n+    llama3_2_user_assistant_conversation,\n+    TextCompletionContent,\n+    UseCase,\n+)\n+\n+\n+def usecases():\n+    this_dir = Path(__file__).parent.parent.resolve()\n+    with open(this_dir / \"scripts/resources/dog.jpg\", \"rb\") as f:\n+        img = PIL_Image.open(f).convert(\"RGB\")\n+\n+    return [\n+        llama3_2_user_assistant_conversation(),\n+        UseCase(\n+            title=\"User and assistant conversation with Images\",\n+            description=\"This example shows how to pass and image to the model as part of the messages.\",\n+            dialogs=[\n+                [\n+                    UserMessage(\n+                        content=[\n+                            ImageMedia(image=img),\n+                            \"Describe this image in two sentences\",\n+                        ],\n+                    )\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - The `<|image|>` tag is used to indicate presence of the image\n+                - The model isn't an early fusion model so doesn't actually translate an image into several tokens. Instead the cross-attention layers take input \"on the side\" from a vision encoder\n+                ![Image](mm-model.png)\n+                - Its important to postion the <|image|> tag appropriately in the prompt. Image will only attend to the subsequent text tokens\n+                - The <|image|> tag is part of the user message body, implying that it should only come after the header `<|start_header_id|>{role}<|end_header_id|>` in the message body\n+                - We recommend using a single image in one prompt\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Builtin and Zero Shot Tool Calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Llama3.2 vision models follow the same tool calling format as Llama3.1 models when inputs are text only.\n+                Use `Environment: ipython` to enable tools.\n+                Add `Tools: {{tool_name1}},{{tool_name2}}` for each of the builtin tools.\n+                The same builtin tools as Llama3.1 are available,\n+                - code_interpreter (for executing python code)\n+                - brave_search (to search the web)\n+                - wolfram_alpha (for querying wolfram alpha for mathematical questions)\n+                \"\"\",\n+            ),\n+            dialogs=[llama3_1_builtin_tool_call_dialog()],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Note the `<|python_tag|>` before `brave_search` function call.\n+                - The `<|eom_id|>` tag is used to indicate the end of the message.\n+                - Similar to Llama3.1, code_interpreter is not explicitly mentioned but is enabled via `Environment: ipython`.\n+                - Tool Calling does NOT work with images in the prompt as of now.\n+                \"\"\"\n+            ),\n+        ),\n+        # UseCase(\n+        #     title=\"Tool Calling for vision models\",\n+        #     description=textwrap.dedent(\n+        #         \"\"\"\n+        #         While Llama3.2 vision models follow the same tool calling format as Llama3.1 models when inputs are text only,\n+        #         they are not able to do tool calling when prompt contains image inputs (along with text).\n+        #         The recommended way would be to separate out the image understanding from the tool calling in successive prompts.\n+        #         Here is an example of how that could be done,\n+        #         \"\"\",\n+        #     ),\n+        #     dialogs=[llama3_1_builtin_tool_call_with_image_dialog()],\n+        #     notes=textwrap.dedent(\n+        #         \"\"\"\n+        #         - Instead of a single prompt (image understanding + tool call), we split into two prompts to achieve the same result.\n+        #         \"\"\"\n+        #     ),\n+        # ),\n+        UseCase(\n+            title=\"Prompt format for base models\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                For base models (Llama3.2-11B-Vision and Llama3.2-90B-Vision), the prompt format for a simple completion is as follows\n+                \"\"\"\n+            ),\n+            dialogs=[\n+                TextCompletionContent(\n+                    content=\"The color of the sky is blue but sometimes it can also be\"\n+                ),\n+            ],\n+            notes=\"- Same as Llama3.1\",\n+        ),\n+        UseCase(\n+            title=\"Prompt format for base models with Image\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                For base models (Llama3.2-11B-Vision and Llama3.2-90B-Vision), here is an example of how the text completion format looks with an image,\n+                \"\"\"\n+            ),\n+            dialogs=[\n+                TextCompletionContent(\n+                    content=[\n+                        ImageMedia(image=img),\n+                        \"If I had to write a haiku for this one\",\n+                    ]\n+                ),\n+            ],\n+            notes=\"- Note the placement of the special tags <|begin_of_text|> and <|image|>\",\n+        ),\n+    ]\n\n--- File: models/llama3_2/text_prompt_format.md ---\n@@ -0,0 +1,286 @@\n+## User and assistant conversation\n+\n+Here is a regular multi-turn user assistant conversation and how its formatted.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+I'm an AI assistant, which means I'm a computer program designed to simulate conversation and answer questions to the best of my ability. I'm here to help you with any questions or tasks you may have, and I'll do my best to provide you with accurate and helpful information.\n+\n+I don't have a personal name, but I'm often referred to as a \"virtual assistant\" or a \"chatbot.\" I'm a machine learning model, which means I've been trained on a large dataset of text and can generate responses based on patterns and context.\n+\n+I can help with a wide range of topics, from general knowledge and trivia to more specialized subjects like science, history, and technology. I can also assist with tasks like language translation, text summarization, and even generating creative content like stories or poetry.\n+\n+So, what can I help you with today?<|eot_id|>\n+```\n+\n+\n+##### Notes\n+This format is unchanged from Llama3.1\n+\n+## Zero shot function calling\n+\n+\n+For Llama3.2 1B and 3B instruct models, we are introducing a new format for zero shot function calling.\n+This new format is designed to be more flexible and powerful than the previous format.\n+All available functions can be provided in the system message. A key difference is in the format of how the assistant responds with function calls.\n+It is pythonic in the form of `[func1(params_name=params_value, params_name2=params_value2...), func2(params)]` instead of the `json` or `<function>` tag that were defined in Llama3.1.\n+Here is an example for the same,\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+You are an expert in composing functions. You are given a question and a set of possible functions.\n+Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+also point it out. You should only return the function call in tools call sections.\n+\n+If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+You SHOULD NOT include any other text in the response.\n+\n+Here is a list of functions in JSON format that you can invoke.\n+\n+[\n+    {\n+        \"name\": \"get_weather\",\n+        \"description\": \"Get weather info for places\",\n+        \"parameters\": {\n+            \"type\": \"dict\",\n+            \"required\": [\n+                \"city\"\n+            ],\n+            \"properties\": {\n+                \"city\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The name of the city to get the weather for\"\n+                },\n+                \"metric\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n+                    \"default\": \"celsius\"\n+                }\n+            }\n+        }\n+    }\n+]<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+What is the weather in SF and Seattle?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+[get_weather(city='San Francisco', metric='celsius'), get_weather(city='Seattle', metric='celsius')]<|eot_id|>\n+```\n+\n+\n+##### Notes\n+\n+- The output supports multiple tool calls natively\n+- JSON format for defining the functions in the system prompt is similar to Llama3.1\n+\n+\n+## Zero shot function calling with user message\n+\n+\n+While the default is to provide all function calls in a system message, in Llama3.2 text models you can also provide information for all the available tools in a user message.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n+\n+Questions: Can you retrieve the details for the user with the ID 7890, who has black as their special request?\n+Here is a list of functions in JSON format that you can invoke:\n+[\n+    {\n+        \"name\": \"get_user_info\",\n+        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n+        \"parameters\": {\n+            \"type\": \"dict\",\n+            \"required\": [\n+                \"user_id\"\n+            ],\n+            \"properties\": {\n+                \"user_id\": {\n+                \"type\": \"integer\",\n+                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n+            },\n+            \"special\": {\n+                \"type\": \"string\",\n+                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n+                \"default\": \"none\"\n+                }\n+            }\n+        }\n+    }\n+]\n+\n+Should you decide to return the function call(s),Put it in the format of [func1(params_name=params_value, params_name2=params_value2...), func2(params)]\n+\n+NO other text MUST be included.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+[get_user_info(user_id=7890, special='black')]<|eot_id|>\n+```\n+\n+\n+##### Notes\n+\n+- The tool call format for the mdoel is the same whether your function calls are provided in the system or user message.\n+- While builtin tool calls end with a <|eom_id|>, notice the <|eot_id|> for zero shot tool calls.\n+\n+\n+## Code Interpreter\n+\n+\n+Code Interpreter continues to work in 3.2 text models similar to Llama 3.1 model family.\n+Here is an example,\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+\n+Cutting Knowledge Date: December 2023\n+Today Date: 24 September 2024\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Write code to check if number is prime. Use it to verify if number 7 is prime<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>def is_prime(n):\n+    if n <= 1:\n+        return False\n+    if n == 2:\n+        return True\n+    if n % 2 == 0:\n+        return False\n+    max_divisor = int(n**0.5) + 1\n+    for d in range(3, max_divisor, 2):\n+        if n % d == 0:\n+            return False\n+    return True\n+\n+print(is_prime(7))  # Output: True<|eom_id|>\n+```\n+\n+\n+##### Notes\n+\n+- Note `Environment: ipython` in the system prompt.\n+- Note that the response starts with `<|python_tag|>` and ends with `<|eom_id|>`\n+\n+\n+## Zero shot function calling E2E format\n+\n+\n+Here is an example of the e2e cycle of tool calls with the model in a muti-step way.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+You are an expert in composing functions. You are given a question and a set of possible functions.\n+Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+also point it out. You should only return the function call in tools call sections.\n+\n+If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+You SHOULD NOT include any other text in the response.\n+\n+Here is a list of functions in JSON format that you can invoke.\n+\n+[\n+    {\n+        \"name\": \"get_weather\",\n+        \"description\": \"Get weather info for places\",\n+        \"parameters\": {\n+            \"type\": \"dict\",\n+            \"required\": [\n+                \"city\"\n+            ],\n+            \"properties\": {\n+                \"city\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The name of the city to get the weather for\"\n+                },\n+                \"metric\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n+                    \"default\": \"celsius\"\n+                }\n+            }\n+        }\n+    }\n+]<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+What is the weather in SF?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+<|python_tag|>[get_weather(city=\"San Francisco\", metric=\"celsius\")]<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n+\n+\"25 C\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+The weather in San Francisco is 25 C.<|eot_id|>\n+```\n+\n+\n+##### Notes\n+\n+- The output of the function call is provided back to the model as a tool response ( in json format ).\n+- Notice `<|start_header_id|>ipython<|end_header_id|>` as the header message preceding the tool response.\n+- The model finally summarizes the information from the tool response and returns the result to the user.\n+\n+\n+## Prompt format for base models\n+\n+\n+For base models (Llama3.2-1B and Llama3.2-3B), the prompt format for a simple completion is as follows\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|>The color of the sky is blue but sometimes it can also be\n+```\n+\n+##### Model Response Format\n+```\n+ gray or even purple. The color of the sky can change depending on the time of day, the weather, and the amount of pollution in the air. The color of the sky can also be affected by the presence of dust, smoke, and other particles in the air.\n+\n+## Step 1: Identify the factors that\n+```\n+\n+\n+##### Notes\n+Same as Llama3.1\n+\n+Thank You!\n\n--- File: models/llama3_2/vision_prompt_format.md ---\n@@ -0,0 +1,141 @@\n+## User and assistant conversation\n+\n+Here is a regular multi-turn user assistant conversation and how its formatted.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+I'm a helpful assistant, here to provide information, answer questions, and assist with tasks to the best of my abilities. I'm a large language model, which means I can understand and respond to natural language inputs, and I'm constantly learning and improving to provide more accurate and helpful responses.\n+\n+I can help with a wide range of topics, from general knowledge and trivia to more specific areas like science, history, technology, and more. I can also assist with tasks like language translation, text summarization, and even generating creative content like stories or dialogues.\n+\n+What can I help you with today?<|eot_id|>\n+```\n+\n+\n+##### Notes\n+This format is unchanged from Llama3.1\n+\n+## User and assistant conversation with Images\n+\n+This example shows how to pass and image to the model as part of the messages.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n+\n+<|image|>Describe this image in two sentences<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+The image depicts a small dog standing on a skateboard, with its front paws firmly planted on the board and its back paws slightly raised. The dog's fur is predominantly brown and white, with a distinctive black stripe running down its back, and it is wearing a black collar around its neck.<|eot_id|>\n+```\n+\n+\n+##### Notes\n+\n+- The `<|image|>` tag is used to indicate presence of the image\n+- The model isn't an early fusion model so doesn't actually translate an image into several tokens. Instead the cross-attention layers take input \"on the side\" from a vision encoder\n+![Image](mm-model.png)\n+- Its important to postion the <|image|> tag appropriately in the prompt. Image will only attend to the subsequent text tokens\n+- The <|image|> tag is part of the user message body, implying that it should only come after the header `<|start_header_id|>{role}<|end_header_id|>` in the message body\n+- We recommend using a single image in one prompt\n+\n+\n+## Builtin and Zero Shot Tool Calling\n+\n+\n+Llama3.2 vision models follow the same tool calling format as Llama3.1 models when inputs are text only.\n+Use `Environment: ipython` to enable tools.\n+Add `Tools: {{tool_name1}},{{tool_name2}}` for each of the builtin tools.\n+The same builtin tools as Llama3.1 are available,\n+- code_interpreter (for executing python code)\n+- brave_search (to search the web)\n+- wolfram_alpha (for querying wolfram alpha for mathematical questions)\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+Tools: brave_search, wolfram_alpha\n+Cutting Knowledge Date: December 2023\n+Today Date: 23 September 2024\n+\n+You are a helpful assistant.\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Search the web for the latest price of 1oz gold?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>brave_search.call(query=\"latest price of 1oz gold\")<|eom_id|>\n+```\n+\n+\n+##### Notes\n+\n+- Note the `<|python_tag|>` before `brave_search` function call.\n+- The `<|eom_id|>` tag is used to indicate the end of the message.\n+- Similar to Llama3.1, code_interpreter is not explicitly mentioned but is enabled via `Environment: ipython`.\n+- Tool Calling does NOT work with images in the prompt as of now.\n+\n+\n+## Prompt format for base models\n+\n+\n+For base models (Llama3.2-11B-Vision and Llama3.2-90B-Vision), the prompt format for a simple completion is as follows\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|>The color of the sky is blue but sometimes it can also be\n+```\n+\n+##### Model Response Format\n+```\n+ red, orange, pink, purple, and even black. The color of the sky is determined by the amount of sunlight that is scattered by the atmosphere and the amount of dust and water vapor present in the atmosphere. During sunrise and sunset, the sky can take on a range of colors due to the scattering of light by\n+```\n+\n+\n+##### Notes\n+- Same as Llama3.1\n+\n+## Prompt format for base models with Image\n+\n+\n+For base models (Llama3.2-11B-Vision and Llama3.2-90B-Vision), here is an example of how the text completion format looks with an image,\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|image|>If I had to write a haiku for this one\n+```\n+\n+##### Model Response Format\n+```\n+, it would be: A skateboarder's delight, a puppy on a board, a furry little thrill-seeker. This puppy is a true skateboarding enthusiast, always eager to hit the streets and show off his skills. He's a master of the board, gliding effortlessly across the pavement with grace and style.\n+```\n+\n+\n+##### Notes\n+- Note the placement of the special tags <|begin_of_text|> and <|image|>\n+\n+Thank You!\n\n--- File: models/prompt_format.py ---\n@@ -0,0 +1,209 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import textwrap\n+\n+from pathlib import Path\n+from typing import List\n+\n+from PIL import Image as PIL_Image\n+from pydantic import BaseModel, Field\n+\n+from .llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.interface import LLama31Interface\n+\n+\n+class TextCompletionContent(BaseModel):\n+    content: InterleavedTextMedia = \"\"\n+\n+\n+class UseCase(BaseModel):\n+    title: str = \"\"\n+    description: str = \"\"\n+    dialogs: List[List[Message] | TextCompletionContent | str] = Field(\n+        default_factory=list\n+    )\n+    notes: str = \"\"\n+    tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json\n+\n+    def md_format(self):\n+        section = textwrap.dedent(\n+            \"\"\"\n+            ## {title}\n+\n+            {description}\n+\n+            {dialogs_text}\n+            {notes}\n+\n+            \"\"\"\n+        )\n+        return section.lstrip()\n+\n+    def dialogs_to_text(self, generator) -> str:\n+        def _code_block(text):\n+            return f\"```\\n{text}\\n```\"\n+\n+        text = \"\"\n+        for dialog in self.dialogs:\n+            if isinstance(dialog, str):\n+                text += dialog\n+                text += \"\\n\\n\"\n+                continue\n+\n+            elif isinstance(dialog, TextCompletionContent):\n+                input_tokens, output_tokens = generator.text_completion_raw(\n+                    dialog.content,\n+                    max_gen_len=64,\n+                    temperature=0.1,\n+                    top_p=0.95,\n+                )\n+            else:\n+                input_tokens, output_tokens = generator.chat_completion_raw(\n+                    dialog,\n+                    max_gen_len=512,\n+                    temperature=0.0,\n+                    top_p=0.95,\n+                    tool_prompt_format=self.tool_prompt_format,\n+                )\n+            text += \"##### Input Prompt Format\\n\"\n+\n+            # FIXME: This is added to undo the hack in chat_formatter where\n+            # vision tokens are replaced with 128256.\n+            input_tokens = [\n+                generator.formatter.vision_token if t == 128256 else t\n+                for t in input_tokens\n+            ]\n+\n+            text += _code_block(generator.tokenizer.decode(input_tokens))\n+            # TODO: Figure out if \"\" needs to be added for newlines or end or some indication\n+            text += \"\\n\\n\"\n+            text += \"##### Model Response Format\\n\"\n+            text += _code_block(generator.tokenizer.decode(output_tokens))\n+            text += \"\\n\\n\"\n+\n+        return text\n+\n+    def to_text(self, generator):\n+        section = self.md_format()\n+        dialogs_text = self.dialogs_to_text(generator)\n+        notes = f\"##### Notes\\n{self.notes}\" if self.notes else \"\"\n+        section = section.format(\n+            title=self.title,\n+            description=self.description,\n+            dialogs_text=dialogs_text,\n+            notes=notes,\n+        )\n+        return section\n+\n+\n+def llama3_1_builtin_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n+    from llama_models.llama3.api.template_data import system_message_builtin_tools_only\n+\n+    interface = LLama31Interface(tool_prompt_format)\n+\n+    messages = interface.system_messages(**system_message_builtin_tools_only())\n+    messages += interface.user_message(\n+        content=\"Search the web for the latest price of 1oz gold?\"\n+    )\n+\n+    return messages\n+\n+\n+def llama3_1_builtin_code_interpreter_dialog(tool_prompt_format=ToolPromptFormat.json):\n+    from llama_models.llama3.api.template_data import system_message_builtin_code_only\n+\n+    interface = LLama31Interface(tool_prompt_format)\n+\n+    messages = interface.system_messages(**system_message_builtin_code_only())\n+    messages += interface.user_message(\n+        content=\"Write code to check if number is prime. Use it to verify if number 7 is prime\"\n+    )\n+\n+    return messages\n+\n+\n+def llama3_1_builtin_tool_call_with_image_dialog(\n+    tool_prompt_format=ToolPromptFormat.json,\n+):\n+    from llama_models.llama3.api.template_data import system_message_builtin_tools_only\n+\n+    this_dir = Path(__file__).parent.resolve()\n+    with open(this_dir / \"scripts/resources/dog.jpg\", \"rb\") as f:\n+        img = PIL_Image.open(f).convert(\"RGB\")\n+\n+    interface = LLama31Interface(tool_prompt_format)\n+\n+    messages = interface.system_messages(**system_message_builtin_tools_only())\n+    messages += interface.user_message(\n+        content=[ImageMedia(image=img), \"What is this dog breed?\"]\n+    )\n+    messages += interface.assistant_response_messages(\n+        \"Based on the description of the dog in the image, it appears to be a small breed dog, possibly a terrier mix\",\n+        StopReason.end_of_turn,\n+    )\n+    messages += interface.user_message(\n+        \"Search the web for some food recommendations for the indentified breed\"\n+    )\n+    return messages\n+\n+\n+def llama3_1_custom_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n+    from llama_models.llama3.api.template_data import system_message_custom_tools_only\n+\n+    interface = LLama31Interface(tool_prompt_format)\n+\n+    messages = interface.system_messages(**system_message_custom_tools_only())\n+    messages += interface.user_message(content=\"Use tools to get latest trending songs\")\n+    return messages\n+\n+\n+def llama3_1_e2e_tool_call_dialog(tool_prompt_format=ToolPromptFormat.json):\n+    import json\n+\n+    from llama_models.llama3.api.template_data import system_message_custom_tools_only\n+\n+    tool_response = json.dumps([\"great song1\", \"awesome song2\", \"cool song3\"])\n+    interface = LLama31Interface(tool_prompt_format)\n+\n+    messages = interface.system_messages(**system_message_custom_tools_only())\n+    messages += interface.user_message(content=\"Use tools to get latest trending songs\")\n+    messages.append(\n+        CompletionMessage(\n+            content=\"\",\n+            stop_reason=StopReason.end_of_message,\n+            tool_calls=[\n+                ToolCall(\n+                    call_id=\"call_id\",\n+                    tool_name=\"trending_songs\",\n+                    arguments={\"n\": \"10\", \"genre\": \"latest\"},\n+                )\n+            ],\n+        ),\n+    )\n+    messages.append(\n+        ToolResponseMessage(\n+            call_id=\"tool_response\",\n+            tool_name=\"trending_songs\",\n+            content=tool_response,\n+        )\n+    )\n+    return messages\n+\n+\n+def llama3_2_user_assistant_conversation():\n+    return UseCase(\n+        title=\"User and assistant conversation\",\n+        description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n+        dialogs=[\n+            [\n+                SystemMessage(content=\"You are a helpful assistant\"),\n+                UserMessage(content=\"Who are you?\"),\n+            ]\n+        ],\n+        notes=\"This format is unchanged from Llama3.1\",\n+    )\n\n--- File: models/scripts/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n\n--- File: models/scripts/example_text_completion.py ---\n@@ -12,9 +12,9 @@\n from typing import Optional\n \n import fire\n+from termcolor import cprint\n \n from models.llama3.reference_impl.generation import Llama\n-from termcolor import cprint\n \n \n THIS_DIR = Path(__file__).parent.resolve()\n\n--- File: models/scripts/generate_prompt_format.py ---\n@@ -0,0 +1,62 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import importlib\n+from pathlib import Path\n+from typing import Optional\n+\n+import fire\n+\n+# from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.reference_impl.generation import Llama\n+\n+\n+THIS_DIR = Path(__file__).parent.resolve()\n+\n+\n+def run_main(\n+    ckpt_dir: str,\n+    module_name: str,\n+    output_path: str,\n+    model_parallel_size: Optional[int] = None,\n+):\n+    module = importlib.import_module(module_name)\n+    assert hasattr(\n+        module, \"usecases\"\n+    ), f\"Module {module_name} missing usecases function\"\n+    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=512,\n+        max_batch_size=1,\n+        model_parallel_size=model_parallel_size,\n+    )\n+\n+    use_cases = module.usecases()\n+    text = \"\"\n+    for u in use_cases:\n+        if isinstance(u, str):\n+            use_case_text = f\"\\n{u}\\n\"\n+        else:\n+            use_case_text = u.to_text(generator)\n+\n+        text += use_case_text\n+        print(use_case_text)\n+\n+    text += \"Thank You!\\n\"\n+\n+    with open(output_path, \"w\") as f:\n+        f.write(text)\n+\n+\n+def main():\n+    fire.Fire(run_main)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n\n--- File: models/scripts/multimodal_example_chat_completion.py ---\n@@ -0,0 +1,90 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+from pathlib import Path\n+from typing import Optional\n+\n+import fire\n+\n+from PIL import Image as PIL_Image\n+\n+from models.llama3.api.datatypes import ImageMedia, UserMessage\n+\n+from models.llama3.reference_impl.generation import Llama\n+\n+\n+THIS_DIR = Path(__file__).parent.resolve()\n+\n+\n+def run_main(\n+    ckpt_dir: str,\n+    temperature: float = 0.6,\n+    top_p: float = 0.9,\n+    max_seq_len: int = 512,\n+    max_batch_size: int = 4,\n+    max_gen_len: Optional[int] = None,\n+    model_parallel_size: Optional[int] = None,\n+):\n+    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+        model_parallel_size=model_parallel_size,\n+    )\n+\n+    # image understanding\n+    dialogs = []\n+    with open(THIS_DIR / \"resources/dog.jpg\", \"rb\") as f:\n+        img = PIL_Image.open(f).convert(\"RGB\")\n+\n+    with open(THIS_DIR / \"resources/pasta.jpeg\", \"rb\") as f:\n+        img2 = PIL_Image.open(f).convert(\"RGB\")\n+\n+    dialogs = [\n+        [\n+            UserMessage(\n+                content=[\n+                    ImageMedia(image=img),\n+                    \"Describe this image in two sentences\",\n+                ],\n+            )\n+        ],\n+    ]\n+    # text only\n+    dialogs += [\n+        [UserMessage(content=\"what is the recipe of mayonnaise in two sentences?\")],\n+    ]\n+\n+    for dialog in dialogs:\n+        result = generator.chat_completion(\n+            dialog,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+        )\n+\n+        for msg in dialog:\n+            print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n+\n+        out_message = result.generation\n+        print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n+        for t in out_message.tool_calls:\n+            print(f\"  Tool call: {t.tool_name} ({t.arguments})\")\n+        print(\"\\n==================================\\n\")\n+\n+\n+def main():\n+    fire.Fire(run_main)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n\n--- File: models/scripts/multimodal_example_text_completion.py ---\n@@ -0,0 +1,79 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+from pathlib import Path\n+from typing import Optional\n+\n+import fire\n+\n+from PIL import Image as PIL_Image\n+from termcolor import cprint\n+\n+from models.llama3.api.datatypes import ImageMedia\n+\n+from models.llama3.reference_impl.generation import Llama\n+\n+\n+THIS_DIR = Path(__file__).parent.resolve()\n+\n+\n+def run_main(\n+    ckpt_dir: str,\n+    temperature: float = 0.6,\n+    top_p: float = 0.9,\n+    max_seq_len: int = 512,\n+    max_batch_size: int = 4,\n+    max_gen_len: Optional[int] = None,\n+    model_parallel_size: Optional[int] = None,\n+):\n+    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+        model_parallel_size=model_parallel_size,\n+    )\n+\n+    with open(THIS_DIR / \"resources/dog.jpg\", \"rb\") as f:\n+        img = PIL_Image.open(f).convert(\"RGB\")\n+\n+    with open(THIS_DIR / \"resources/pasta.jpeg\", \"rb\") as f:\n+        img2 = PIL_Image.open(f).convert(\"RGB\")\n+\n+    interleaved_contents = [\n+        # text only\n+        \"The color of the sky is blue but sometimes it can also be\",\n+        # image understanding\n+        [\n+            ImageMedia(image=img),\n+            \"If I had to write a haiku for this one\",\n+        ],\n+    ]\n+\n+    for content in interleaved_contents:\n+        result = generator.text_completion(\n+            content,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+        )\n+\n+        cprint(f\"{content}\", end=\"\")\n+        cprint(f\"{result.generation}\", color=\"yellow\")\n+        print(\"\\n==================================\\n\")\n+\n+\n+def main():\n+    fire.Fire(run_main)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n\n--- File: models/sku_list.py ---\n@@ -32,9 +32,14 @@ def resolve_model(descriptor: str) -> Optional[Model]:\n     return None\n \n \n-@lru_cache\n def all_registered_models() -> List[Model]:\n-    return llama2_family() + llama3_family() + llama3_1_family() + safety_models()\n+    return (\n+        llama2_family()\n+        + llama3_family()\n+        + llama3_1_family()\n+        + llama3_2_family()\n+        + safety_models()\n+    )\n \n \n def recommended_sampling_params() -> SamplingParams:\n@@ -66,10 +71,17 @@ def llama3_1_family() -> List[Model]:\n     ]\n \n \n+def llama3_2_family() -> List[Model]:\n+    return [\n+        *llama3_2_base_models(),\n+        *llama3_2_instruct_models(),\n+    ]\n+\n+\n def llama2_base_models() -> List[Model]:\n     return [\n         Model(\n-            core_model_id=CoreModelId.meta_llama2_7b,\n+            core_model_id=CoreModelId.llama2_7b,\n             is_default_variant=True,\n             description=\"Llama 2 7b model\",\n             huggingface_repo=\"meta-llama/Llama-2-7b\",\n@@ -89,7 +101,7 @@ def llama2_base_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama2_13b,\n+            core_model_id=CoreModelId.llama2_13b,\n             is_default_variant=True,\n             description=\"Llama 2 13b model\",\n             huggingface_repo=\"meta-llama/Llama-2-13b\",\n@@ -109,7 +121,7 @@ def llama2_base_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama2_70b,\n+            core_model_id=CoreModelId.llama2_70b,\n             is_default_variant=True,\n             description=\"Llama 2 70b model\",\n             huggingface_repo=\"meta-llama/Llama-2-70b\",\n@@ -134,11 +146,10 @@ def llama2_base_models() -> List[Model]:\n def llama3_base_models() -> List[Model]:\n     return [\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_8b,\n+            core_model_id=CoreModelId.llama3_8b,\n             is_default_variant=True,\n             description=\"Llama 3 8b model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3-8B\",\n-            recommended_sampling_params=recommended_sampling_params(),\n+            huggingface_repo=\"meta-llama/Llama-3-8B\",\n             arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n@@ -154,10 +165,10 @@ def llama3_base_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_70b,\n+            core_model_id=CoreModelId.llama3_70b,\n             is_default_variant=True,\n             description=\"Llama 3 70b model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3-70B\",\n+            huggingface_repo=\"meta-llama/Llama-3-70B\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 8192,\n@@ -179,10 +190,10 @@ def llama3_base_models() -> List[Model]:\n def llama3_1_base_models() -> List[Model]:\n     return [\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_8b,\n+            core_model_id=CoreModelId.llama3_1_8b,\n             is_default_variant=True,\n             description=\"Llama 3.1 8b model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-8B\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 4096,\n@@ -199,10 +210,10 @@ def llama3_1_base_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_70b,\n+            core_model_id=CoreModelId.llama3_1_70b,\n             is_default_variant=True,\n             description=\"Llama 3.1 70b model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-70B\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 8192,\n@@ -219,10 +230,10 @@ def llama3_1_base_models() -> List[Model]:\n             pth_file_count=8,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_405b,\n+            core_model_id=CoreModelId.llama3_1_405b,\n             is_default_variant=False,\n             description=\"Llama 3.1 405b model (BF16 weights)\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-405B\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 16384,\n@@ -239,10 +250,10 @@ def llama3_1_base_models() -> List[Model]:\n             pth_file_count=8,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_405b,\n+            core_model_id=CoreModelId.llama3_1_405b,\n             is_default_variant=True,\n             description=\"Llama 3.1 405b model (FP8 quantized)\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-FP8\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-405B-FP8\",\n             quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n@@ -260,10 +271,10 @@ def llama3_1_base_models() -> List[Model]:\n             pth_file_count=8,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_405b,\n+            core_model_id=CoreModelId.llama3_1_405b,\n             is_default_variant=False,\n             description=\"Llama 3.1 405b model (BF16 weights for mp16)\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-405B\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 16384,\n@@ -282,10 +293,101 @@ def llama3_1_base_models() -> List[Model]:\n     ]\n \n \n+def llama3_2_base_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_1b,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 1b model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-1B\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 2048,\n+                \"n_layers\": 16,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.5,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_3b,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 3b model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-3B\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 3072,\n+                \"n_layers\": 28,\n+                \"n_heads\": 24,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.0,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_11b_vision,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 11b vision model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-11B-Vision\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+                \"vision_chunk_size\": 448,\n+                \"vision_max_num_chunks\": 4,\n+                \"vision_num_cross_attention_layers\": 8,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_90b_vision,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 90b vision model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-90B-Vision\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+                \"vision_chunk_size\": 560,\n+                \"vision_max_num_chunks\": 4,\n+                \"vision_num_cross_attention_layers\": 20,\n+            },\n+            pth_file_count=8,\n+        ),\n+    ]\n+\n+\n def llama2_instruct_models() -> List[Model]:\n     return [\n         Model(\n-            core_model_id=CoreModelId.meta_llama2_7b_chat,\n+            core_model_id=CoreModelId.llama2_7b_chat,\n             is_default_variant=True,\n             description=\"Llama 2 7b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-7b-chat\",\n@@ -305,7 +407,7 @@ def llama2_instruct_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama2_13b_chat,\n+            core_model_id=CoreModelId.llama2_13b_chat,\n             is_default_variant=True,\n             description=\"Llama 2 13b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-13b-chat\",\n@@ -325,7 +427,7 @@ def llama2_instruct_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama2_70b_chat,\n+            core_model_id=CoreModelId.llama2_70b_chat,\n             is_default_variant=True,\n             description=\"Llama 2 70b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-70b-chat\",\n@@ -350,10 +452,10 @@ def llama2_instruct_models() -> List[Model]:\n def llama3_instruct_models() -> List[Model]:\n     return [\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_8b_instruct,\n+            core_model_id=CoreModelId.llama3_8b_instruct,\n             is_default_variant=True,\n             description=\"Llama 3 8b instruct model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n+            huggingface_repo=\"meta-llama/Llama-3-8B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 4096,\n@@ -370,10 +472,10 @@ def llama3_instruct_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_70b_instruct,\n+            core_model_id=CoreModelId.llama3_70b_instruct,\n             is_default_variant=True,\n             description=\"Llama 3 70b instruct model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n+            huggingface_repo=\"meta-llama/Llama-3-70B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 8192,\n@@ -395,10 +497,10 @@ def llama3_instruct_models() -> List[Model]:\n def llama3_1_instruct_models() -> List[Model]:\n     return [\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_8b_instruct,\n+            core_model_id=CoreModelId.llama3_1_8b_instruct,\n             is_default_variant=True,\n             description=\"Llama 3.1 8b instruct model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-8B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 4096,\n@@ -415,10 +517,10 @@ def llama3_1_instruct_models() -> List[Model]:\n             pth_file_count=1,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_70b_instruct,\n+            core_model_id=CoreModelId.llama3_1_70b_instruct,\n             is_default_variant=True,\n             description=\"Llama 3.1 70b instruct model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-70B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 8192,\n@@ -435,10 +537,10 @@ def llama3_1_instruct_models() -> List[Model]:\n             pth_file_count=8,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n+            core_model_id=CoreModelId.llama3_1_405b_instruct,\n             is_default_variant=False,\n             description=\"Llama 3.1 405b instruct model (BF16 weights)\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 16384,\n@@ -455,10 +557,10 @@ def llama3_1_instruct_models() -> List[Model]:\n             pth_file_count=8,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n+            core_model_id=CoreModelId.llama3_1_405b_instruct,\n             is_default_variant=True,\n             description=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct-FP8\",\n             quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n@@ -476,10 +578,10 @@ def llama3_1_instruct_models() -> List[Model]:\n             pth_file_count=8,\n         ),\n         Model(\n-            core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n+            core_model_id=CoreModelId.llama3_1_405b_instruct,\n             is_default_variant=False,\n             description=\"Llama 3.1 405b instruct model (BF16 weights for mp16)\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n+            huggingface_repo=\"meta-llama/Llama-3.1-405B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n             arch_args={\n                 \"dim\": 16384,\n@@ -498,9 +600,163 @@ def llama3_1_instruct_models() -> List[Model]:\n     ]\n \n \n+def llama3_2_instruct_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_1b_instruct,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 1b instruct model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-1B-Instruct\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 2048,\n+                \"n_layers\": 16,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.5,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_3b_instruct,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 3b instruct model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-3B-Instruct\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 3072,\n+                \"n_layers\": 28,\n+                \"n_heads\": 24,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.0,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_11b_vision_instruct,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 11b vision instruct model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+                \"vision_chunk_size\": 448,\n+                \"vision_max_num_chunks\": 4,\n+                \"vision_num_cross_attention_layers\": 8,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama3_2_90b_vision_instruct,\n+            is_default_variant=True,\n+            description=\"Llama 3.2 90b vision instruct model\",\n+            huggingface_repo=\"meta-llama/Llama-3.2-90B-Vision-Instruct\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+                \"vision_chunk_size\": 560,\n+                \"vision_max_num_chunks\": 4,\n+                \"vision_num_cross_attention_layers\": 20,\n+            },\n+            pth_file_count=8,\n+        ),\n+    ]\n+\n+\n @lru_cache\n def safety_models() -> List[Model]:\n     return [\n+        Model(\n+            core_model_id=CoreModelId.llama_guard_3_11b_vision,\n+            is_default_variant=True,\n+            description=\"Llama Guard v3 11b vision system safety model\",\n+            huggingface_repo=\"meta-llama/Llama-Guard-3-11B-Vision\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+                \"vision_chunk_size\": 560,\n+                \"vision_max_num_chunks\": 4,\n+                \"vision_num_cross_attention_layers\": 8,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama_guard_3_1b,\n+            is_default_variant=False,\n+            description=\"Llama Guard v3 1b 'int4' quantized system safety model\",\n+            huggingface_repo=\"meta-llama/Llama-Guard-3-1B-INT4\",\n+            quantization_format=CheckpointQuantizationFormat.int4,\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 2048,\n+                \"n_layers\": 12,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"rope_freq_base\": 500000.0,\n+                \"norm_eps\": 1e-05,\n+                \"hidden_dim\": 6400,\n+                \"use_scaled_rope\": True,\n+            },\n+            pth_file_count=1,\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama_guard_3_1b,\n+            is_default_variant=True,\n+            description=\"Llama Guard v3 1b system safety model\",\n+            huggingface_repo=\"meta-llama/Llama-Guard-3-1B\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 2048,\n+                \"n_layers\": 16,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.5,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+            },\n+            pth_file_count=1,\n+        ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n             is_default_variant=True,\n@@ -536,7 +792,7 @@ def safety_models() -> List[Model]:\n                 \"norm_eps\": 1e-05,\n                 \"rope_theta\": 500000.0,\n                 \"use_scaled_rope\": False,\n-                \"vocab_size\": 128256,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n             },\n             pth_file_count=1,\n         ),\n@@ -552,7 +808,7 @@ def safety_models() -> List[Model]:\n             core_model_id=CoreModelId.llama_guard_2_8b,\n             is_default_variant=True,\n             description=\"Llama Guard v2 8b system safety model\",\n-            huggingface_repo=\"meta-llama/Meta-Llama-Guard-2-8B\",\n+            huggingface_repo=\"meta-llama/Llama-Guard-2-8B\",\n             arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n@@ -581,25 +837,25 @@ def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n     \"\"\"Information needed to download model from llamameta.net\"\"\"\n \n     pth_count = model.pth_file_count\n-    if model.core_model_id == CoreModelId.meta_llama3_1_405b:\n+    if model.core_model_id == CoreModelId.llama3_1_405b:\n         if pth_count == 16:\n-            folder = \"Meta-Llama-3.1-405B-MP16\"\n+            folder = \"Llama-3.1-405B-MP16\"\n         elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n-            folder = \"Meta-Llama-3.1-405B\"\n+            folder = \"Llama-3.1-405B\"\n         else:\n-            folder = \"Meta-Llama-3.1-405B-MP8\"\n-    elif model.core_model_id == CoreModelId.meta_llama3_1_405b_instruct:\n+            folder = \"Llama-3.1-405B-MP8\"\n+    elif model.core_model_id == CoreModelId.llama3_1_405b_instruct:\n         if pth_count == 16:\n-            folder = \"Meta-Llama-3.1-405B-Instruct-MP16\"\n+            folder = \"Llama-3.1-405B-Instruct-MP16\"\n         elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n-            folder = \"Meta-Llama-3.1-405B-Instruct\"\n+            folder = \"Llama-3.1-405B-Instruct\"\n         else:\n-            folder = \"Meta-Llama-3.1-405B-Instruct-MP8\"\n+            folder = \"Llama-3.1-405B-Instruct-MP8\"\n     elif model.core_model_id == CoreModelId.llama_guard_3_8b:\n         if model.quantization_format == CheckpointQuantizationFormat.int8:\n-            folder = \"Meta-Llama-Guard-3-8B-INT8-HF\"\n+            folder = \"Llama-Guard-3-8B-INT8-HF\"\n         else:\n-            folder = \"Meta-Llama-Guard-3-8B\"\n+            folder = \"Llama-Guard-3-8B\"\n     elif model.core_model_id == CoreModelId.prompt_guard_86m:\n         folder = \"Prompt-Guard\"\n     elif model.core_model_id == CoreModelId.llama_guard_2_8b:\n@@ -634,6 +890,18 @@ def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n                 \"tokenizer_config.json\",\n             ]\n         )\n+    elif (\n+        model.core_model_id == CoreModelId.llama_guard_3_1b\n+        and model.quantization_format == CheckpointQuantizationFormat.int4\n+    ):\n+        files.extend(\n+            [\n+                \"llama_guard_3_1b_pruned_xnnpack.pte\",\n+                \"example-prompt.txt\",\n+                \"params.json\",\n+                \"tokenizer.model\",\n+            ]\n+        )\n     else:\n         files.extend(\n             [\n@@ -655,8 +923,8 @@ def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n # Sadness because Cloudfront rejects our HEAD requests to find Content-Length\n def llama_meta_pth_size(model: Model) -> int:\n     if model.core_model_id not in (\n-        CoreModelId.meta_llama3_1_405b,\n-        CoreModelId.meta_llama3_1_405b_instruct,\n+        CoreModelId.llama3_1_405b,\n+        CoreModelId.llama3_1_405b_instruct,\n     ):\n         return 0\n \n\n--- File: requirements.txt ---\n@@ -2,3 +2,4 @@ PyYAML\n jinja2\n tiktoken\n pydantic\n+Pillow\n\n--- File: setup.py ---\n@@ -26,6 +26,14 @@ def read_requirements():\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\",\n+    entry_points={\n+        \"console_scripts\": [\n+            \"multimodal_example_chat_completion = llama_models.scripts.multimodal_example_chat_completion:main\",\n+            \"multimodal_example_text_completion = llama_models.scripts.multimodal_example_text_completion:main\",\n+            \"example_chat_completion = llama_models.scripts.example_chat_completion:main\",\n+            \"example_text_completion = llama_models.scripts.example_text_completion:main\",\n+        ]\n+    },\n     long_description=open(\"README.md\").read(),\n     long_description_content_type=\"text/markdown\",\n     url=\"https://github.com/meta-llama/llama-models\","
            },
            {
              "sha": "e57f86bd24a14af4ffa536c077df6141ca203d1a",
              "url": "https://github.com/meta-llama/llama-models/commit/e57f86bd24a14af4ffa536c077df6141ca203d1a",
              "message": "Bump version to 0.0.24",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.23\",\n+    version=\"0.0.24\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "191ea739e5046c941530e833452d7a9f7fb9c9e9",
              "url": "https://github.com/meta-llama/llama-models/commit/191ea739e5046c941530e833452d7a9f7fb9c9e9",
              "message": "Bump version to 0.0.23",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.21\",\n+    version=\"0.0.23\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "74ee69b2eb7f53431e771106a27e0f31a9b38659",
              "url": "https://github.com/meta-llama/llama-models/commit/74ee69b2eb7f53431e771106a27e0f31a9b38659",
              "message": "Bump version to 0.0.21",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.19\",\n+    version=\"0.0.21\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "ee506fcc84fd5e50ccf3f0933006c7b4f1b41200",
              "url": "https://github.com/meta-llama/llama-models/commit/ee506fcc84fd5e50ccf3f0933006c7b4f1b41200",
              "message": "[API updates] (#146)",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -132,15 +132,49 @@ def model_family(model_id) -> ModelFamily:\n     }\n )\n class Model(BaseModel):\n-    model_config = ConfigDict(protected_namespaces=())\n-\n     core_model_id: CoreModelId\n+    description: str\n+    huggingface_repo: Optional[str] = None\n+    recommended_sampling_params: Optional[SamplingParams] = None\n+    arch_args: Dict[str, Any]\n     is_default_variant: bool\n \n+    quantization_format: CheckpointQuantizationFormat = (\n+        CheckpointQuantizationFormat.bf16\n+    )\n+    pth_file_count: int\n+    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n+\n+    # silence pydantic until we remove the `model_` fields\n+    model_config = ConfigDict(protected_namespaces=())\n+\n     @property\n     def model_family(self) -> ModelFamily:\n         return model_family(self.core_model_id)\n \n+    # The variant is a string representation of other parameters which helps\n+    # uniquely identify the model. this typically includes the quantization\n+    # format, model parallel size, etc.\n+    @property\n+    def variant(self) -> str:\n+        parts = [\n+            self.quantization_format.value,\n+            f\"mp{self.pth_file_count}\",\n+        ]\n+\n+        return \"-\".join(parts)\n+\n+    # The SKU is uniquely identified by (model_id, variant) combo\n+    def descriptor(self, shorten_default_variant: bool = True) -> str:\n+        if shorten_default_variant and self.is_default_variant:\n+            return self.core_model_id.value\n+\n+        return f\"{self.core_model_id.value}:{self.variant}\"\n+\n+    @property\n+    def is_instruct_model(self) -> bool:\n+        return \"instruct\" in self.id.name\n+\n     # Featured models are shown in the non-exhaustive model list\n     @property\n     def is_featured(self) -> bool:\n@@ -166,36 +200,3 @@ def max_seq_length(self) -> int:\n             return 131072\n         else:\n             raise ValueError(f\"Unknown max_seq_len for {self.core_model_id}\")\n-\n-    # The variant is a string representation of other parameters which helps\n-    # uniquely identify the model. this typically includes the quantization\n-    # format, model parallel size, etc.\n-    @property\n-    def variant(self) -> str:\n-        parts = [\n-            self.quantization_format.value,\n-            f\"mp{self.pth_file_count}\",\n-        ]\n-\n-        return \"-\".join(parts)\n-\n-    # The SKU is uniquely identified by (model_id, variant) combo\n-    def descriptor(self, shorten_default_variant: bool = True) -> str:\n-        if shorten_default_variant and self.is_default_variant:\n-            return self.core_model_id.value\n-\n-        return f\"{self.core_model_id.value}:{self.variant}\"\n-\n-    description_markdown: str\n-    huggingface_repo: Optional[str] = None\n-    quantization_format: CheckpointQuantizationFormat = (\n-        CheckpointQuantizationFormat.bf16\n-    )\n-    recommended_sampling_params: Optional[SamplingParams] = None\n-    model_args: Dict[str, Any]\n-    pth_file_count: int\n-    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n-\n-    @property\n-    def is_instruct_model(self) -> bool:\n-        return \"instruct\" in self.id.name\n\n--- File: models/sku_list.py ---\n@@ -71,10 +71,10 @@ def llama2_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama2_7b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 2 7b model\",\n+            description=\"Llama 2 7b model\",\n             huggingface_repo=\"meta-llama/Llama-2-7b\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n                 \"n_heads\": 32,\n@@ -91,10 +91,10 @@ def llama2_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama2_13b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 2 13b model\",\n+            description=\"Llama 2 13b model\",\n             huggingface_repo=\"meta-llama/Llama-2-13b\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 5120,\n                 \"n_layers\": 40,\n                 \"n_heads\": 40,\n@@ -111,10 +111,10 @@ def llama2_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama2_70b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 2 70b model\",\n+            description=\"Llama 2 70b model\",\n             huggingface_repo=\"meta-llama/Llama-2-70b\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 8192,\n                 \"n_layers\": 80,\n                 \"n_heads\": 64,\n@@ -136,10 +136,10 @@ def llama3_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_8b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3 8b model\",\n+            description=\"Llama 3 8b model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3-8B\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n                 \"n_heads\": 32,\n@@ -156,10 +156,10 @@ def llama3_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_70b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3 70b model\",\n+            description=\"Llama 3 70b model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3-70B\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 8192,\n                 \"n_layers\": 80,\n                 \"n_heads\": 64,\n@@ -181,10 +181,10 @@ def llama3_1_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_8b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3.1 8b model\",\n+            description=\"Llama 3.1 8b model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n                 \"n_heads\": 32,\n@@ -201,10 +201,10 @@ def llama3_1_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_70b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3.1 70b model\",\n+            description=\"Llama 3.1 70b model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 8192,\n                 \"n_layers\": 80,\n                 \"n_heads\": 64,\n@@ -221,10 +221,10 @@ def llama3_1_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_405b,\n             is_default_variant=False,\n-            description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n+            description=\"Llama 3.1 405b model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n@@ -241,11 +241,11 @@ def llama3_1_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_405b,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3.1 405b model (FP8 quantized)\",\n+            description=\"Llama 3.1 405b model (FP8 quantized)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-FP8\",\n             quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n@@ -262,10 +262,10 @@ def llama3_1_base_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_405b,\n             is_default_variant=False,\n-            description_markdown=\"Llama 3.1 405b model (BF16 weights for mp16)\",\n+            description=\"Llama 3.1 405b model (BF16 weights for mp16)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n@@ -287,10 +287,10 @@ def llama2_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama2_7b_chat,\n             is_default_variant=True,\n-            description_markdown=\"Llama 2 7b chat model\",\n+            description=\"Llama 2 7b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-7b-chat\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n                 \"n_heads\": 32,\n@@ -307,10 +307,10 @@ def llama2_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama2_13b_chat,\n             is_default_variant=True,\n-            description_markdown=\"Llama 2 13b chat model\",\n+            description=\"Llama 2 13b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-13b-chat\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 5120,\n                 \"n_layers\": 40,\n                 \"n_heads\": 40,\n@@ -327,10 +327,10 @@ def llama2_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama2_70b_chat,\n             is_default_variant=True,\n-            description_markdown=\"Llama 2 70b chat model\",\n+            description=\"Llama 2 70b chat model\",\n             huggingface_repo=\"meta-llama/Llama-2-70b-chat\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 8192,\n                 \"n_layers\": 80,\n                 \"n_heads\": 64,\n@@ -352,10 +352,10 @@ def llama3_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_8b_instruct,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3 8b instruct model\",\n+            description=\"Llama 3 8b instruct model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n                 \"n_heads\": 32,\n@@ -372,10 +372,10 @@ def llama3_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_70b_instruct,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3 70b instruct model\",\n+            description=\"Llama 3 70b instruct model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 8192,\n                 \"n_layers\": 80,\n                 \"n_heads\": 64,\n@@ -397,10 +397,10 @@ def llama3_1_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_8b_instruct,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3.1 8b instruct model\",\n+            description=\"Llama 3.1 8b instruct model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n                 \"n_heads\": 32,\n@@ -417,10 +417,10 @@ def llama3_1_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_70b_instruct,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3.1 70b instruct model\",\n+            description=\"Llama 3.1 70b instruct model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 8192,\n                 \"n_layers\": 80,\n                 \"n_heads\": 64,\n@@ -437,10 +437,10 @@ def llama3_1_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n             is_default_variant=False,\n-            description_markdown=\"Llama 3.1 405b instruct model (BF16 weights)\",\n+            description=\"Llama 3.1 405b instruct model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n@@ -457,11 +457,11 @@ def llama3_1_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n             is_default_variant=True,\n-            description_markdown=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n+            description=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n             quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n@@ -478,10 +478,10 @@ def llama3_1_instruct_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n             is_default_variant=False,\n-            description_markdown=\"Llama 3.1 405b instruct model (BF16 weights for mp16)\",\n+            description=\"Llama 3.1 405b instruct model (BF16 weights for mp16)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n             recommended_sampling_params=recommended_sampling_params(),\n-            model_args={\n+            arch_args={\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n@@ -504,9 +504,9 @@ def safety_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n             is_default_variant=True,\n-            description_markdown=\"Llama Guard v3 8b system safety model\",\n+            description=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B\",\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"ffn_dim_multiplier\": 1.3,\n                 \"multiple_of\": 1024,\n@@ -523,10 +523,10 @@ def safety_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n             is_default_variant=False,\n-            description_markdown=\"Llama Guard v3 8b system safety model\",\n+            description=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B-INT8\",\n             quantization_format=CheckpointQuantizationFormat.int8,\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"ffn_dim_multiplier\": 1.3,\n                 \"multiple_of\": 1024,\n@@ -543,17 +543,17 @@ def safety_models() -> List[Model]:\n         Model(\n             core_model_id=CoreModelId.prompt_guard_86m,\n             is_default_variant=True,\n-            description_markdown=\"Prompt Guard 86M injection safety model\",\n+            description=\"Prompt Guard 86M injection safety model\",\n             huggingface_repo=\"meta-llama/Prompt-Guard-86M\",\n-            model_args={},\n+            arch_args={},\n             pth_file_count=1,\n         ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_2_8b,\n             is_default_variant=True,\n-            description_markdown=\"Llama Guard v2 8b system safety model\",\n+            description=\"Llama Guard v2 8b system safety model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-Guard-2-8B\",\n-            model_args={\n+            arch_args={\n                 \"dim\": 4096,\n                 \"n_layers\": 32,\n                 \"n_heads\": 32,"
            },
            {
              "sha": "46bf7f1c0e4319a0b1a9e3748339d6fa8ef13ed3",
              "url": "https://github.com/meta-llama/llama-models/commit/46bf7f1c0e4319a0b1a9e3748339d6fa8ef13ed3",
              "message": "Bump version to 0.0.18",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.17\",\n+    version=\"0.0.18\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "4e4a379ec9c2463dacb3d4fd52a04d00c03f693a",
              "url": "https://github.com/meta-llama/llama-models/commit/4e4a379ec9c2463dacb3d4fd52a04d00c03f693a",
              "message": "Bump version to 0.0.16",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.15\",\n+    version=\"0.0.16\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "2b79f4e2c7d9e58d3720cdaf7064910063dc8043",
              "url": "https://github.com/meta-llama/llama-models/commit/2b79f4e2c7d9e58d3720cdaf7064910063dc8043",
              "message": "Bump version to 0.0.15",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.14\",\n+    version=\"0.0.15\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "301ca3a2b3b10e94ddcd1fdd2c57e52f812e1cac",
              "url": "https://github.com/meta-llama/llama-models/commit/301ca3a2b3b10e94ddcd1fdd2c57e52f812e1cac",
              "message": "Bump version to 0.0.13",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.12\",\n+    version=\"0.0.13\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "cce7f5bfb161115a019ec72aa8f618073ceb4d67",
              "url": "https://github.com/meta-llama/llama-models/commit/cce7f5bfb161115a019ec72aa8f618073ceb4d67",
              "message": "Make sure prompt does not have trailing whitespace",
              "files_changed": [
                {
                  "filename": "models/scripts/example_text_completion.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/scripts/example_text_completion.py ---\n@@ -29,17 +29,6 @@ def run_main(\n     max_gen_len: int = 64,\n     model_parallel_size: Optional[int] = None,\n ):\n-    \"\"\"\n-    Examples to run with the models finetuned for chat. Prompts correspond of chat\n-    turns between the user and assistant with the final one always being the user.\n-\n-    An optional system prompt at the beginning to control how the model should respond\n-    is also supported.\n-\n-    The context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192.\n-\n-    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n-    \"\"\"\n     tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n@@ -50,7 +39,7 @@ def run_main(\n     )\n \n     prompts = [\n-        \"The color of the sky is blue but sometimes it can also be \",\n+        \"The color of the sky is blue but sometimes it can also be\",\n         \"\"\"\\\n apple is pomme,\n bannana is banane,"
            },
            {
              "sha": "98550393ac2bc72910426f903ed9a5068e3319b8",
              "url": "https://github.com/meta-llama/llama-models/commit/98550393ac2bc72910426f903ed9a5068e3319b8",
              "message": "Fix import",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/generation.py ---\n@@ -33,7 +33,7 @@\n \n from ..api.args import ModelArgs\n from ..api.chat_format import ChatFormat, ModelInput\n-from ..api.datatypes import CompletionMessage, Message, StopReason\n+from ..api.datatypes import CompletionMessage, Message, StopReason, ToolPromptFormat\n from ..api.tokenizer import Tokenizer\n from .model import Transformer"
            },
            {
              "sha": "739e6711a7a556c326f4d3c54687aa01b0a0b798",
              "url": "https://github.com/meta-llama/llama-models/commit/739e6711a7a556c326f4d3c54687aa01b0a0b798",
              "message": "Bump version to 0.0.12",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.11\",\n+    version=\"0.0.12\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "fb140abffbb596e5b32a35e86c3efe13a62bbe38",
              "url": "https://github.com/meta-llama/llama-models/commit/fb140abffbb596e5b32a35e86c3efe13a62bbe38",
              "message": "Bump version to 0.0.11",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.10\",\n+    version=\"0.0.11\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "e046ca91d9cb3217dc0a1b4874cac3273fbe1ad1",
              "url": "https://github.com/meta-llama/llama-models/commit/e046ca91d9cb3217dc0a1b4874cac3273fbe1ad1",
              "message": "API updates  (#132)",
              "files_changed": [
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/interface.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/template_data.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.builtin_tool_call.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.custom_tool_call.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.default.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.jinja",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.builtin_and_custom_tools.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.builtin_tools_only.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.custom_tools_only.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.default.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.jinja",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/tool_message.failure.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/tool_message.jinja",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/tool_message.success.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/user_message.default.yaml",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/templates/user_message.jinja",
                  "status": "removed"
                },
                {
                  "filename": "models/llama3/api/tokenizer.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/prompt_templates/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/prompt_templates/base.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/prompt_templates/system_prompts.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/prompt_templates/tool_response.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/prompt_templates/test_system_prompts.py",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/chat_format.py ---\n@@ -38,10 +38,12 @@ def encode_header(self, role: str) -> List[int]:\n         tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n         return tokens\n \n-    def encode_message(self, message: Message) -> List[int]:\n+    def encode_message(\n+        self, message: Message, tool_prompt_format: ToolPromptFormat\n+    ) -> List[int]:\n         tokens = self.encode_header(message.role)\n \n-        def _process_content(content: InterleavedTextAttachment):\n+        def _process_content(content: InterleavedTextMedia):\n             def _process(c):\n                 if isinstance(c, str):\n                     tokens.extend(self.tokenizer.encode(c, bos=False, eos=False))\n@@ -57,9 +59,13 @@ def _process(c):\n \n         _process_content(message.content)\n \n+        if isinstance(message, UserMessage) and message.context is not None:\n+            _process_content(\"\\n\\n\")\n+            _process_content(message.context)\n+\n         if isinstance(message, CompletionMessage):\n             for t in message.tool_calls:\n-                content = ToolUtils.encode_tool_call(t)\n+                content = ToolUtils.encode_tool_call(t, tool_prompt_format)\n                 _process_content(content)\n \n         eom = False\n@@ -71,11 +77,15 @@ def _process(c):\n         )\n         return tokens\n \n-    def encode_dialog_prompt(self, messages: List[Message]) -> ModelInput:\n+    def encode_dialog_prompt(\n+        self,\n+        messages: List[Message],\n+        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n+    ) -> ModelInput:\n         tokens = []\n         tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n         for message in messages:\n-            toks = self.encode_message(message)\n+            toks = self.encode_message(message, tool_prompt_format)\n             tokens.extend(toks)\n \n         # Add the start of an assistant message for the model to complete.\n@@ -94,15 +104,21 @@ def decode_assistant_message(\n                 content = content[len(header_str) :]\n                 break\n \n+        return self.decode_assistant_message_from_content(content, stop_reason)\n+\n+    def decode_assistant_message_from_content(\n+        self, content: str, stop_reason: StopReason\n+    ) -> CompletionMessage:\n         ipython = content.startswith(\"<|python_tag|>\")\n         if ipython:\n             content = content[len(\"<|python_tag|>\") :]\n \n-        eot = content.endswith(\"<|eot_id|>\")\n-        if eot:\n+        if content.endswith(\"<|eot_id|>\"):\n             content = content[: -len(\"<|eot_id|>\")]\n-        else:\n+            stop_reason = StopReason.end_of_turn\n+        elif content.endswith(\"<|eom_id|>\"):\n             content = content[: -len(\"<|eom_id|>\")]\n+            stop_reason = StopReason.end_of_message\n \n         tool_name = None\n         tool_arguments = {}\n\n--- File: models/llama3/api/datatypes.py ---\n@@ -33,19 +33,27 @@ def __str__(self) -> str:\n         return self.uri\n \n \n-@json_schema_type\n-class Attachment(BaseModel):\n-    url: URL\n-    mime_type: str\n-\n-\n-InterleavedTextAttachment = Union[\n+InterleavedTextMedia = Union[\n     str,\n-    Attachment,\n-    List[Union[str, Attachment]],\n+    # Specific modalities can be placed here, but not generic attachments\n+    # since models don't consume them in a generic way\n+    List[Union[str]],\n ]\n \n \n+def interleaved_text_media_as_str(content: InterleavedTextMedia, sep: str = \" \") -> str:\n+    def _process(c) -> str:\n+        if isinstance(c, str):\n+            return c\n+        else:\n+            return \"<media>\"\n+\n+    if isinstance(content, list):\n+        return sep.join(_process(c) for c in content)\n+    else:\n+        return _process(content)\n+\n+\n @json_schema_type\n class BuiltinTool(Enum):\n     brave_search = \"brave_search\"\n@@ -64,12 +72,32 @@ class ToolCall(BaseModel):\n     tool_name: Union[BuiltinTool, str]\n     arguments: Dict[str, RecursiveType]\n \n+    @validator(\"tool_name\", pre=True)\n+    @classmethod\n+    def validate_field(cls, v):\n+        if isinstance(v, str):\n+            try:\n+                return BuiltinTool(v)\n+            except ValueError:\n+                return v\n+        return v\n+\n \n @json_schema_type\n class ToolResponse(BaseModel):\n     call_id: str\n     tool_name: Union[BuiltinTool, str]\n-    content: InterleavedTextAttachment\n+    content: InterleavedTextMedia\n+\n+    @validator(\"tool_name\", pre=True)\n+    @classmethod\n+    def validate_field(cls, v):\n+        if isinstance(v, str):\n+            try:\n+                return BuiltinTool(v)\n+            except ValueError:\n+                return v\n+        return v\n \n \n @json_schema_type\n@@ -96,16 +124,52 @@ def validate_field(cls, v):\n         return v\n \n \n+@json_schema_type\n+class ToolChoice(Enum):\n+    auto = \"auto\"\n+    required = \"required\"\n+\n+\n+@json_schema_type\n+class ToolPromptFormat(Enum):\n+    \"\"\"This Enum refers to the prompt format for calling custom / zero shot tools\n+\n+    `json` --\n+        Refers to the json format for calling tools.\n+        The json format takes the form like\n+        {\n+            \"type\": \"function\",\n+            \"function\" : {\n+                \"name\": \"function_name\",\n+                \"description\": \"function_description\",\n+                \"parameters\": {...}\n+            }\n+        }\n+\n+    `function_tag` --\n+        This is an example of how you could define\n+        your own user defined format for making tool calls.\n+        The function_tag format looks like this,\n+        <function=function_name>(parameters)</function>\n+\n+    The detailed prompts for each of these formats are added to llama cli\n+    \"\"\"\n+\n+    json = \"json\"\n+    function_tag = \"function_tag\"\n+\n+\n @json_schema_type\n class UserMessage(BaseModel):\n     role: Literal[Role.user.value] = Role.user.value\n-    content: InterleavedTextAttachment\n+    content: InterleavedTextMedia\n+    context: Optional[InterleavedTextMedia] = None\n \n \n @json_schema_type\n class SystemMessage(BaseModel):\n     role: Literal[Role.system.value] = Role.system.value\n-    content: InterleavedTextAttachment\n+    content: InterleavedTextMedia\n \n \n @json_schema_type\n@@ -115,7 +179,7 @@ class ToolResponseMessage(BaseModel):\n     # have a `content` type makes things nicer too\n     call_id: str\n     tool_name: Union[BuiltinTool, str]\n-    content: InterleavedTextAttachment\n+    content: InterleavedTextMedia\n \n \n @json_schema_type\n@@ -133,7 +197,7 @@ class TokenLogProbs(BaseModel):\n @json_schema_type\n class CompletionMessage(BaseModel):\n     role: Literal[Role.assistant.value] = Role.assistant.value\n-    content: InterleavedTextAttachment\n+    content: InterleavedTextMedia\n     stop_reason: StopReason\n     tool_calls: List[ToolCall] = Field(default_factory=list)\n \n\n--- File: models/llama3/api/interface.py ---\n@@ -5,15 +5,21 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-import json\n-\n-from datetime import datetime\n from pathlib import Path\n \n from typing import List, Optional\n \n-import yaml\n-from jinja2 import Environment, FileSystemLoader\n+from termcolor import colored\n+\n+from ..prompt_templates import (\n+    BuiltinToolGenerator,\n+    FunctionTagCustomToolGenerator,\n+    JsonCustomToolGenerator,\n+    SystemDefaultGenerator,\n+    ToolResponseGenerator,\n+)\n+\n+from . import template_data\n \n from .chat_format import ChatFormat\n \n@@ -25,150 +31,188 @@\n     SystemMessage,\n     ToolCall,\n     ToolDefinition,\n+    ToolPromptFormat,\n+    ToolResponseMessage,\n+    UserMessage,\n )\n from .tokenizer import Tokenizer\n \n+\n THIS_DIR = Path(__file__).parent\n \n \n class Template:\n-    def __init__(self, role, template_name, yaml_path, notes=None):\n+    def __init__(\n+        self,\n+        role,\n+        template_name,\n+        data_provider=None,\n+        notes=None,\n+    ):\n         self.role = role\n         self.template_name = template_name\n-        self.yaml_path = yaml_path\n-        self.notes = notes or \"\"\n+        self.data_provider = data_provider or \"\"\n+        self._notes = notes or \"\"\n+\n+    @property\n+    def notes(self):\n+        default = \" represents newline\"\n+        notes = default\n+        if self._notes:\n+            notes += \"\\n\"\n+            notes += self._notes\n+        return notes\n \n \n TEMPLATES = [\n-    Template(\"user\", \"user-default\", \"user_message.default.yaml\"),\n+    Template(\n+        \"user\",\n+        \"user-default\",\n+        \"user_default\",\n+    ),\n     Template(\n         \"assistant\",\n         \"assistant-builtin-tool-call\",\n-        \"assistant_message.builtin_tool_call.yaml\",\n+        \"assistant_builtin_tool_call\",\n         \"Notice <|python_tag|>\",\n     ),\n     Template(\n         \"assistant\",\n         \"assistant-custom-tool-call\",\n-        \"assistant_message.custom_tool_call.yaml\",\n+        \"assistant_custom_tool_call\",\n         \"Notice <function=...> format\",\n     ),\n-    Template(\"assistant\", \"assistant-default\", \"assistant_message.default.yaml\"),\n+    Template(\n+        \"assistant\",\n+        \"assistant-default\",\n+        \"assistant_default\",\n+    ),\n     Template(\n         \"system\",\n         \"system-builtin-and-custom-tools\",\n-        \"system_message.builtin_and_custom_tools.yaml\",\n+        \"system_message_builtin_and_custom_tools\",\n     ),\n     Template(\n         \"system\",\n         \"system-builtin-tools-only\",\n-        \"system_message.builtin_tools_only.yaml\",\n+        \"system_message_builtin_tools_only\",\n     ),\n     Template(\n         \"system\",\n         \"system-custom-tools-only\",\n-        \"system_message.custom_tools_only.yaml\",\n+        \"system_message_custom_tools_only\",\n+    ),\n+    Template(\n+        \"system\",\n+        \"system-default\",\n+        \"system_default\",\n     ),\n-    Template(\"system\", \"system-default\", \"system_message.default.yaml\"),\n     Template(\n         \"tool\",\n         \"tool-success\",\n-        \"tool_message.success.yaml\",\n+        \"tool_success\",\n         \"Note ipython header and [stdout]\",\n     ),\n     Template(\n         \"tool\",\n         \"tool-failure\",\n-        \"tool_message.failure.yaml\",\n+        \"tool_failure\",\n         \"Note ipython header and [stderr]\",\n     ),\n ]\n \n \n class LLama31Interface:\n-    def __init__(self, tokenizer_path: str):\n-        self.tokenizer = Tokenizer(tokenizer_path)\n+    def __init__(self, tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json):\n+        self.tokenizer = Tokenizer.get_instance()\n         self.formatter = ChatFormat(self.tokenizer)\n+        self.tool_prompt_format = tool_prompt_format\n \n-    def recommended_system_message(\n+    def get_tokens(self, messages: List[Message]) -> List[int]:\n+        model_input = self.formatter.encode_dialog_prompt(\n+            messages,\n+            self.tool_prompt_format,\n+        )\n+        return model_input.tokens\n+\n+    def tool_response_messages(self, *args, **kwargs):\n+        template = ToolResponseGenerator().gen(*args, **kwargs)\n+        return [\n+            ToolResponseMessage(\n+                call_id=\"call_id\",\n+                tool_name=\"tool_name\",\n+                content=template.render(),\n+            )\n+        ]\n+\n+    def system_messages(\n         self,\n         builtin_tools: List[BuiltinTool],\n         custom_tools: List[ToolDefinition],\n-        instructions: Optional[str] = None,\n-    ) -> SystemMessage:\n-        content = \"\"\n-        if builtin_tools:\n-            content += \"Environment: ipython\\n\"\n-\n-            tool_str = \", \".join(\n-                [t.value for t in builtin_tools if t != BuiltinTool.code_interpreter]\n-            )\n-            if tool_str:\n-                content += f\"Tools: {tool_str}\\n\"\n+        instruction: Optional[str] = None,\n+    ) -> List[Message]:\n+        messages = []\n \n-        current_date = datetime.now()\n-        formatted_date = current_date.strftime(\"%d %B %Y\")\n-        date_str = f\"\"\"\n-Cutting Knowledge Date: December 2023\n-Today Date: {formatted_date}\"\"\"\n-        content += date_str\n+        default_gen = SystemDefaultGenerator()\n+        default_template = default_gen.gen()\n \n-        if custom_tools:\n-            content += \"\\n\" + self.get_custom_tool_instructions(custom_tools)\n+        sys_content = \"\"\n \n-        if instructions:\n-            content += f\"\\n{instructions}\"\n+        tool_template = None\n+        if builtin_tools or custom_tools:\n+            tool_gen = BuiltinToolGenerator()\n+            tool_template = tool_gen.gen(builtin_tools + custom_tools)\n \n-        return SystemMessage(content=content)\n+            sys_content += tool_template.render()\n+            sys_content += \"\\n\"\n \n-    def get_custom_tool_instructions(self, custom_tools: List[ToolDefinition]) -> str:\n-        custom_tool_params = \"\"\n+        sys_content += default_template.render()\n \n-        custom_tool_params = \"\\n\".join(\n-            f\"{get_instruction_string(t)}\\n{get_parameters_string(t)}\\n\"\n-            for t in custom_tools\n-        )\n+        if instruction:\n+            sys_content += \"\\n\\n\"\n+            sys_content += instruction\n+\n+        sys_content += \"\\n\"\n+        messages.append(SystemMessage(content=sys_content))\n \n-        content = f\"\"\"\n-You have access to the following functions:\n-\n-{custom_tool_params}\n-Think very carefully before calling functions.\n-If a you choose to call a function ONLY reply in the following format with no prefix or suffix:\n-\n-<function=example_function_name>{{\"example_name\": \"example_value\"}}</function>\n-\n-Reminder:\n-- If looking for real time information use relevant functions before falling back to brave_search\n-- Function calls MUST follow the specified format, start with <function= and end with </function>\n-- Required parameters MUST be specified\n-- Only call one function at a time\n-- Put the entire function call reply on one line\n-\n-\"\"\"\n-        return content\n-\n-    def get_sample_builtin_tool_call_message(self) -> CompletionMessage:\n-        return CompletionMessage(\n-            content=\"\",\n-            stop_reason=StopReason.end_of_message,\n-            tool_calls=[\n-                ToolCall(\n-                    call_id=\"1234\",\n-                    tool_name=BuiltinTool.brave_search,\n-                    arguments={\n-                        \"query\": \"Who won NBA in 2024?\",\n-                    },\n+        if custom_tools:\n+            if self.tool_prompt_format == ToolPromptFormat.json:\n+                tool_gen = JsonCustomToolGenerator()\n+            elif self.tool_prompt_format == ToolPromptFormat.function_tag:\n+                tool_gen = FunctionTagCustomToolGenerator()\n+            else:\n+                raise ValueError(\n+                    f\"Non supported ToolPromptFormat {request.tool_prompt_format}\"\n                 )\n-            ],\n-        )\n \n-    def get_message_as_str_tokens(self, message: Message) -> str:\n-        tokens = self.formatter.encode_message(message)\n-        return self.tokenizer.decode(tokens)\n+            custom_template = tool_gen.gen(custom_tools)\n+            messages.append(UserMessage(content=custom_template.render()))\n+\n+        return messages\n+\n+    def assistant_response_messages(\n+        self,\n+        content: str,\n+        stop_reason: StopReason,\n+        tool_call: Optional[ToolCall] = None,\n+    ) -> List[CompletionMessage]:\n+        tool_calls = []\n+        if tool_call:\n+            tool_calls.append(tool_call)\n+        return [\n+            CompletionMessage(\n+                content=content,\n+                tool_calls=tool_calls,\n+                stop_reason=stop_reason,\n+            )\n+        ]\n+\n+    def user_message(self, content: str) -> List[UserMessage]:\n+        return [UserMessage(content=content)]\n \n     def display_message_as_tokens(self, message: Message) -> None:\n-        tokens = self.formatter.encode_message(message)\n+        \"\"\"Util to print tokenized string to shell\"\"\"\n+        tokens = self.formatter.encode_message(message, self.tool_prompt_format)\n         on_colors = [\n             \"on_red\",\n             \"on_green\",\n@@ -183,46 +227,29 @@ def display_message_as_tokens(self, message: Message) -> None:\n         print(\"\\n\", end=\"\")\n \n \n-def get_instruction_string(tooldef: ToolDefinition) -> str:\n-    return f\"Use the function '{tooldef.tool_name}' to '{tooldef.description}'\"\n-\n-\n-def get_parameters_string(tooldef: ToolDefinition) -> str:\n-    return json.dumps(\n-        {\n-            \"name\": tooldef.tool_name,\n-            \"description\": tooldef.description,\n-            \"parameters\": {\n-                name: definition.__dict__\n-                for name, definition in tooldef.parameters.items()\n-            },\n-        }\n-    )\n-\n-\n def list_jinja_templates() -> List[Template]:\n     return TEMPLATES\n \n \n-def render_jinja_template(name: str):\n+def render_jinja_template(name: str, tool_prompt_format: ToolPromptFormat):\n     by_name = {t.template_name: t for t in TEMPLATES}\n     if name not in by_name:\n         raise ValueError(f\"No template found for `{name}`\")\n \n     template = by_name[name]\n-    jinja_template = f\"{template.role}_message.jinja\"\n-\n-    tokenizer = Tokenizer(str(THIS_DIR / \"tokenizer.model\"))\n-    special_tokens = list(tokenizer.special_tokens.values())\n-\n-    env = Environment(loader=FileSystemLoader(THIS_DIR / \"templates\"))\n-    with open(THIS_DIR / \"templates\" / template.yaml_path, \"r\") as f:\n-        context = yaml.safe_load(f)\n-        context[\"today\"] = datetime.now().strftime(\"%d %B %Y\")\n-\n-        output = env.get_template(jinja_template).render(context)\n-        tokens = tokenizer.encode(output, allowed_special=\"all\", bos=False, eos=False)\n-\n-        tokens = [(tokenizer.decode([t]), t in special_tokens) for t in tokens]\n-\n+    interface = LLama31Interface(tool_prompt_format)\n+\n+    data_func = getattr(template_data, template.data_provider)\n+    if template.role == \"system\":\n+        messages = interface.system_messages(**data_func())\n+    elif template.role == \"tool\":\n+        messages = interface.tool_response_messages(**data_func())\n+    elif template.role == \"assistant\":\n+        messages = interface.assistant_response_messages(**data_func())\n+    elif template.role == \"user\":\n+        messages = interface.user_message(**data_func())\n+\n+    tokens = interface.get_tokens(messages)\n+    special_tokens = list(interface.tokenizer.special_tokens.values())\n+    tokens = [(interface.tokenizer.decode([t]), t in special_tokens) for t in tokens]\n     return template, tokens\n\n--- File: models/llama3/api/template_data.py ---\n@@ -0,0 +1,93 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from ..prompt_templates import (\n+    BuiltinToolGenerator,\n+    JsonCustomToolGenerator,\n+    ToolResponseGenerator,\n+)\n+from .datatypes import BuiltinTool, StopReason, ToolCall\n+\n+INSTRUCTION = \"You are a helpful assistant.\"\n+\n+\n+def system_message_builtin_tools_only():\n+    return {\n+        \"builtin_tools\": BuiltinToolGenerator().data_examples()[0],\n+        \"custom_tools\": [],\n+        \"instruction\": INSTRUCTION,\n+    }\n+\n+\n+def system_message_custom_tools_only():\n+    return {\n+        \"builtin_tools\": [],\n+        \"custom_tools\": JsonCustomToolGenerator().data_examples()[0],\n+        \"instruction\": INSTRUCTION,\n+    }\n+\n+\n+def system_message_builtin_and_custom_tools():\n+    return {\n+        \"builtin_tools\": BuiltinToolGenerator().data_examples()[0],\n+        \"custom_tools\": JsonCustomToolGenerator().data_examples()[0],\n+        \"instruction\": INSTRUCTION,\n+    }\n+\n+\n+def system_default():\n+    return {\n+        \"builtin_tools\": [],\n+        \"custom_tools\": [],\n+        \"instruction\": INSTRUCTION,\n+    }\n+\n+\n+def tool_success():\n+    return ToolResponseGenerator().data_examples()[0]\n+\n+\n+def tool_failure():\n+    return ToolResponseGenerator().data_examples()[1]\n+\n+\n+def assistant_builtin_tool_call():\n+    return {\n+        \"content\": \"\",\n+        \"tool_call\": ToolCall(\n+            call_id=\"uuid\",\n+            tool_name=BuiltinTool.brave_search,\n+            arguments={\n+                \"query\": \"Who won NBA in 2024?\",\n+            },\n+        ),\n+        \"stop_reason\": StopReason.end_of_message,\n+    }\n+\n+\n+def assistant_custom_tool_call():\n+    return {\n+        \"content\": \"\",\n+        \"tool_call\": ToolCall(\n+            call_id=\"uuid\",\n+            tool_name=\"trending_songs\",\n+            arguments={\"country\": \"US\", \"n\": 10},\n+        ),\n+        \"stop_reason\": StopReason.end_of_turn,\n+    }\n+\n+\n+def assistant_default():\n+    return {\n+        \"content\": \"Hi, I am a helpful assistant. What can I help you with today?\",\n+        \"tool_call\": None,\n+        \"stop_reason\": StopReason.end_of_turn,\n+    }\n+\n+\n+def user_default():\n+    return {\"content\": \"Please tell me how to plan a trip to New York\"}\n\n--- File: models/llama3/api/templates/assistant_message.builtin_tool_call.yaml ---\n@@ -1,5 +0,0 @@\n-tool_call:\n-  tool_name: \"brave_search\"\n-  arguments:\n-    query: \"Who won NBA in 2024?\"\n-end_of_message: True\n\n--- File: models/llama3/api/templates/assistant_message.custom_tool_call.yaml ---\n@@ -1,6 +0,0 @@\n-tool_call:\n-  tool_name: get_boiling_point\n-  arguments:\n-    liquid_name: Mercury\n-    celsius: True\n-end_of_message: True\n\n--- File: models/llama3/api/templates/assistant_message.default.yaml ---\n@@ -1 +0,0 @@\n-content: Hi, I am a helpful assistant. What can I help you with today?\n\n--- File: models/llama3/api/templates/assistant_message.jinja ---\n@@ -1,14 +0,0 @@\n-{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n-\n-{%- set builtins = [\"brave_search\", \"wolfram_alpha\", \"photogen\"] -%}\n-{%- if tool_call -%}\n-<|python_tag|>\n-{%- if tool_call.tool_name in builtins -%}\n-{{ tool_call.tool_name }}.call(query=\"{{ tool_call.arguments['query'] }}\")\n-{%- else -%}\n-<function=\"{{ tool_call.tool_name }}\">{{ tool_call.arguments | tojson }}</function>\n-{%- endif -%}\n-{%- else -%}\n-{{ content }}\n-{%- endif -%}\n-{%- if end_of_message %}<|eom_id|>{% else %}<|eot_id|>{% endif %}\n\n--- File: models/llama3/api/templates/system_message.builtin_and_custom_tools.yaml ---\n@@ -1,24 +0,0 @@\n-builtin_tools: [\"brave_search\", \"wolfram_alpha\"]\n-custom_tools:\n-  - tool_name: get_boiling_point\n-    description: Get the boiling point of a liquid\n-    parameters:\n-      - name: liquid_name\n-        type: string\n-        description: name of the liquid\n-        required: True\n-      - name: celsius\n-        type: boolean\n-        description: whether to use celsius\n-        required: False\n-  - tool_name: trending_songs\n-    description: Returns the trending songs on a Music site\n-    parameters:\n-      - name: country\n-        type: string\n-        description: country to return trending songs for\n-        required: True\n-      - name: n\n-        type: int\n-        description: The number of songs to return\n-        required: False\n\n--- File: models/llama3/api/templates/system_message.builtin_tools_only.yaml ---\n@@ -1,2 +0,0 @@\n-builtin_tools: [\"brave_search\", \"wolfram_alpha\"]\n-custom_tools: []\n\n--- File: models/llama3/api/templates/system_message.custom_tools_only.yaml ---\n@@ -1,24 +0,0 @@\n-builtin_tools: []\n-custom_tools:\n-  - tool_name: get_boiling_point\n-    description: Get the boiling point of a liquid\n-    parameters:\n-      - name: liquid_name\n-        type: string\n-        description: name of the liquid\n-        required: True\n-      - name: celsius\n-        type: boolean\n-        description: whether to use celsius\n-        required: False\n-  - tool_name: trending_songs\n-    description: Returns the trending songs on a Music site\n-    parameters:\n-      - name: country\n-        type: string\n-        description: country to return trending songs for\n-        required: True\n-      - name: n\n-        type: int\n-        description: The number of songs to return\n-        required: False\n\n--- File: models/llama3/api/templates/system_message.default.yaml ---\n@@ -1,2 +0,0 @@\n-builtin_tools: []\n-custom_tools: []\n\n--- File: models/llama3/api/templates/system_message.jinja ---\n@@ -1,52 +0,0 @@\n-{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' -}}\n-{% if builtin_tools or custom_tools -%}\n-Environment: ipython\n-{% endif -%}\n-{%- set builtin_tools = builtin_tools | reject('equalto', 'code_interpreter') | list -%}\n-{%- if builtin_tools -%}\n-Tools: {{ builtin_tools | join(\", \") }}\n-{% endif -%}\n-Cutting Knowledge Date: December 2023\n-Today Date: {{ today }}\n-<|eot_id|>\n-{%- if custom_tools %}\n-{{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n-\n-Answer the user's question by making use of the following functions if needed.\n-If none of the function can be used, please say so.\n-Here is a list of functions in JSON format:\n-{% for t in custom_tools %}\n-{#- manually setting up JSON because jinja sorts keys in unexpected ways -#}\n-{%- set tname = t.tool_name -%}\n-{%- set tdesc = t.description -%}\n-{%- set tparams = t.parameters -%}\n-{%- set required_params = [] -%}\n-{%- for param in tparams if param.required == true -%}\n-    {%- set _ = required_params.append(param.name) -%}\n-{%- endfor -%}\n-{\n-    \"type\": \"function\",\n-    \"function\": {\n-        \"name\": \"{{tname}}\",\n-        \"description\": \"{{tdesc}}\",\n-        \"parameters\": {\n-            \"type\": \"object\",\n-            \"properties\": [\n-                {%- for param in tparams %}\n-                {\n-                    \"{{param.name}}\": {\n-                        \"type\": \"object\",\n-                        \"description\": \"{{param.description}}\"\n-                    }\n-                }{% if not loop.last %},{% endif %}\n-                {%- endfor %}\n-            ],\n-            \"required\": {{ required_params | tojson }}\n-        }\n-    }\n-}\n-{% endfor %}\n-Return function calls in JSON format.\n-<|eot_id|>\n-{%- endif -%}\n-{{ additional_instructions -}}\n\n--- File: models/llama3/api/templates/tool_message.failure.yaml ---\n@@ -1,2 +0,0 @@\n-status: failure\n-stderr: \"brave_search encounter an error: could not communicate with api.brave.com\"\n\n--- File: models/llama3/api/templates/tool_message.jinja ---\n@@ -1,10 +0,0 @@\n-{{ '<|start_header_id|>ipython<|end_header_id|>\\n\\n' -}}\n-\n-{% if status == \"success\" %}completed{% else %}failed{% endif %}\n-{%- if stdout %}\n-[stdout]{{ stdout }}[/stdout]\n-{%- endif -%}\n-{%- if stderr %}\n-[stderr]{{ stderr }}[/stderr]\n-{%- endif -%}\n-<|eot_id|>\n\n--- File: models/llama3/api/templates/tool_message.success.yaml ---\n@@ -1,2 +0,0 @@\n-status: success\n-stdout: '{\"results\":[\"something something\"]}'\n\n--- File: models/llama3/api/templates/user_message.default.yaml ---\n@@ -1 +0,0 @@\n-content: Please tell me how to plan a trip to New York\n\n--- File: models/llama3/api/templates/user_message.jinja ---\n@@ -1,3 +0,0 @@\n-{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n-{{ content -}}\n-<|eot_id|>\n\n--- File: models/llama3/api/tokenizer.py ---\n@@ -41,6 +41,9 @@\n MAX_NO_WHITESPACES_CHARS = 25_000\n \n \n+_INSTANCE = None\n+\n+\n class Tokenizer:\n     \"\"\"\n     Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n@@ -52,6 +55,16 @@ class Tokenizer:\n \n     pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n \n+    @classmethod\n+    def get_instance(cls):\n+        global _INSTANCE\n+\n+        if _INSTANCE is None:\n+            _INSTANCE = Tokenizer(\n+                os.path.join(os.path.dirname(__file__), \"tokenizer.model\")\n+            )\n+        return _INSTANCE\n+\n     def __init__(self, model_path: str):\n         \"\"\"\n         Initializes the Tokenizer with a Tiktoken model.\n\n--- File: models/llama3/api/tool_utils.py ---\n@@ -9,7 +9,7 @@\n import re\n from typing import Optional, Tuple\n \n-from .datatypes import BuiltinTool, ToolCall\n+from .datatypes import BuiltinTool, ToolCall, ToolPromptFormat\n \n BUILTIN_TOOL_PATTERN = r'\\b(?P<tool_name>\\w+)\\.call\\(query=\"(?P<query>[^\"]*)\"\\)'\n CUSTOM_TOOL_CALL_PATTERN = re.compile(\n@@ -80,7 +80,7 @@ def maybe_extract_custom_tool_call(message_body: str) -> Optional[Tuple[str, str\n             return None\n \n     @staticmethod\n-    def encode_tool_call(t: ToolCall) -> str:\n+    def encode_tool_call(t: ToolCall, tool_prompt_format: ToolPromptFormat) -> str:\n         if t.tool_name == BuiltinTool.brave_search:\n             q = t.arguments[\"query\"]\n             return f'brave_search.call(query=\"{q}\")'\n@@ -94,5 +94,15 @@ def encode_tool_call(t: ToolCall) -> str:\n             return t.arguments[\"code\"]\n         else:\n             fname = t.tool_name\n-            args = json.dumps(t.arguments)\n-            return f\"<function={fname}>{args}</function>\"\n+\n+            if tool_prompt_format == ToolPromptFormat.json:\n+                return json.dumps(\n+                    {\n+                        \"type\": \"function\",\n+                        \"name\": fname,\n+                        \"parameters\": t.arguments,\n+                    }\n+                )\n+            elif tool_prompt_format == ToolPromptFormat.function_tag:\n+                args = json.dumps(t.arguments)\n+                return f\"<function={fname}>{args}</function>\"\n\n--- File: models/llama3/prompt_templates/__init__.py ---\n@@ -0,0 +1,8 @@\n+from .base import PromptTemplate, PromptTemplateGeneratorBase\n+from .system_prompts import (\n+    BuiltinToolGenerator,\n+    FunctionTagCustomToolGenerator,\n+    JsonCustomToolGenerator,\n+    SystemDefaultGenerator,\n+)\n+from .tool_response import ToolResponseGenerator\n\n--- File: models/llama3/prompt_templates/base.py ---\n@@ -0,0 +1,26 @@\n+from dataclasses import dataclass\n+from typing import Any, Dict, List\n+\n+from jinja2 import Template\n+\n+\n+@dataclass\n+class PromptTemplate:\n+    template: str\n+    data: Dict[str, Any]\n+\n+    def render(self):\n+        template = Template(self.template)\n+        return template.render(self.data)\n+\n+\n+class PromptTemplateGeneratorBase:\n+    \"\"\"\n+    Base class for prompt template generators.\n+    \"\"\"\n+\n+    def gen(self, *args, **kwargs) -> PromptTemplate:\n+        raise NotImplementedError()\n+\n+    def data_examples(self) -> List[Any]:\n+        raise NotImplementedError()\n\n--- File: models/llama3/prompt_templates/system_prompts.py ---\n@@ -0,0 +1,213 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import textwrap\n+from datetime import datetime\n+from typing import Any, List\n+\n+from llama_models.llama3.api.datatypes import (\n+    BuiltinTool,\n+    ToolDefinition,\n+    ToolParamDefinition,\n+)\n+\n+from .base import PromptTemplate, PromptTemplateGeneratorBase\n+\n+\n+class SystemDefaultGenerator(PromptTemplateGeneratorBase):\n+\n+    def gen(self, *args, **kwargs) -> PromptTemplate:\n+        template_str = textwrap.dedent(\n+            \"\"\"\n+            Cutting Knowledge Date: December 2023\n+            Today Date: {{ today }}\n+            \"\"\"\n+        )\n+        return PromptTemplate(\n+            template_str.lstrip(\"\\n\"),\n+            {\"today\": datetime.now().strftime(\"%d %B %Y\")},\n+        )\n+\n+    def data_examples(self) -> List[Any]:\n+        return [None]\n+\n+\n+class BuiltinToolGenerator(PromptTemplateGeneratorBase):\n+\n+    def _tool_breakdown(self, tools: List[ToolDefinition]):\n+        builtin_tools, custom_tools = [], []\n+        for dfn in tools:\n+            if isinstance(dfn.tool_name, BuiltinTool):\n+                builtin_tools.append(dfn)\n+            else:\n+                custom_tools.append(dfn)\n+\n+        return builtin_tools, custom_tools\n+\n+    def gen(self, tools: List[ToolDefinition]) -> PromptTemplate:\n+        builtin_tools, custom_tools = self._tool_breakdown(tools)\n+        data = []\n+        template_str = textwrap.dedent(\n+            \"\"\"\n+            {% if builtin_tools or custom_tools -%}\n+            Environment: ipython\n+            {% endif -%}\n+            {% set builtin_tools = builtin_tools | reject('equalto', 'code_interpreter') | list -%}\n+            {% if builtin_tools -%}\n+            Tools: {{ builtin_tools | join(\", \") | trim -}}\n+            {% endif %}\n+            \"\"\"\n+        )\n+        return PromptTemplate(\n+            template_str.lstrip(\"\\n\"),\n+            {\n+                \"builtin_tools\": [t.tool_name.value for t in builtin_tools],\n+                \"custom_tools\": custom_tools,\n+            },\n+        )\n+\n+    def data_examples(self) -> List[List[ToolDefinition]]:\n+        return [\n+            # builtin tools\n+            [\n+                ToolDefinition(tool_name=BuiltinTool.code_interpreter),\n+                ToolDefinition(tool_name=BuiltinTool.brave_search),\n+                ToolDefinition(tool_name=BuiltinTool.wolfram_alpha),\n+            ],\n+            # only code interpretor\n+            [\n+                ToolDefinition(tool_name=BuiltinTool.code_interpreter),\n+            ],\n+        ]\n+\n+\n+class JsonCustomToolGenerator(PromptTemplateGeneratorBase):\n+\n+    def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n+        template_str = textwrap.dedent(\n+            \"\"\"\n+            Answer the user's question by making use of the following functions if needed.\n+            If none of the function can be used, please say so.\n+            Here is a list of functions in JSON format:\n+            {% for t in custom_tools -%}\n+            {# manually setting up JSON because jinja sorts keys in unexpected ways -#}\n+            {%- set tname = t.tool_name -%}\n+            {%- set tdesc = t.description -%}\n+            {%- set tparams = t.parameters -%}\n+            {%- set required_params = [] -%}\n+            {%- for name, param in tparams.items() if param.required == true -%}\n+                {%- set _ = required_params.append(name) -%}\n+            {%- endfor -%}\n+            {\n+                \"type\": \"function\",\n+                \"function\": {\n+                    \"name\": \"{{tname}}\",\n+                    \"description\": \"{{tdesc}}\",\n+                    \"parameters\": {\n+                        \"type\": \"object\",\n+                        \"properties\": [\n+                            {%- for name, param in tparams.items() %}\n+                            {\n+                                \"{{name}}\": {\n+                                    \"type\": \"object\",\n+                                    \"description\": \"{{param.description}}\"\n+                                }\n+                            }{% if not loop.last %},{% endif %}\n+                            {%- endfor %}\n+                        ],\n+                        \"required\": {{ required_params | tojson }}\n+                    }\n+                }\n+            }\n+            {% endfor %}\n+            Return function calls in JSON format.\n+            \"\"\"\n+        )\n+\n+        return PromptTemplate(\n+            template_str.lstrip(\"\\n\"),\n+            {\"custom_tools\": [t.model_dump() for t in custom_tools]},\n+        )\n+\n+    def data_examples(self) -> List[List[ToolDefinition]]:\n+        return [\n+            [\n+                ToolDefinition(\n+                    tool_name=\"trending_songs\",\n+                    description=\"Returns the trending songs on a Music site\",\n+                    parameters={\n+                        \"n\": ToolParamDefinition(\n+                            param_type=\"int\",\n+                            description=\"The number of songs to return\",\n+                            required=True,\n+                        ),\n+                        \"genre\": ToolParamDefinition(\n+                            param_type=\"str\",\n+                            description=\"The genre of the songs to return\",\n+                            required=False,\n+                        ),\n+                    },\n+                ),\n+            ]\n+        ]\n+\n+\n+class FunctionTagCustomToolGenerator(PromptTemplateGeneratorBase):\n+\n+    def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n+        template_str = textwrap.dedent(\n+            \"\"\"\n+            You have access to the following functions:\n+\n+            {% for t in custom_tools %}\n+            {#- manually setting up JSON because jinja sorts keys in unexpected ways -#}\n+            {%- set tname = t.tool_name -%}\n+            {%- set tdesc = t.description -%}\n+            {%- set tparams = t.parameters | tojson -%}\n+            Use the function '{{ tname }}' to '{{ tdesc }}':\n+            {\"name\": \"{{tname}}\", \"description\": \"{{tdesc}}\", \"parameters\": {{tparams}}}\n+\n+            {% endfor -%}\n+            Think very carefully before calling functions.\n+            If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+\n+            <function=example_function_name>{\"example_name\": \"example_value\"}</function>\n+\n+            Reminder:\n+            - If looking for real time information use relevant functions before falling back to brave_search\n+            - Function calls MUST follow the specified format, start with <function= and end with </function>\n+            - Required parameters MUST be specified\n+            - Only call one function at a time\n+            - Put the entire function call reply on one line\n+            \"\"\"\n+        )\n+        return PromptTemplate(\n+            template_str.lstrip(\"\\n\"),\n+            {\"custom_tools\": [t.model_dump() for t in custom_tools]},\n+        )\n+\n+    def data_examples(self) -> List[List[ToolDefinition]]:\n+        return [\n+            [\n+                ToolDefinition(\n+                    tool_name=\"trending_songs\",\n+                    description=\"Returns the trending songs on a Music site\",\n+                    parameters={\n+                        \"n\": ToolParamDefinition(\n+                            param_type=\"int\",\n+                            description=\"The number of songs to return\",\n+                            required=True,\n+                        ),\n+                        \"genre\": ToolParamDefinition(\n+                            param_type=\"str\",\n+                            description=\"The genre of the songs to return\",\n+                            required=False,\n+                        ),\n+                    },\n+                ),\n+            ]\n+        ]\n\n--- File: models/llama3/prompt_templates/tool_response.py ---\n@@ -0,0 +1,51 @@\n+import textwrap\n+from typing import Optional\n+\n+from .base import PromptTemplate, PromptTemplateGeneratorBase\n+\n+\n+class ToolResponseGenerator(PromptTemplateGeneratorBase):\n+\n+    def gen(\n+        self,\n+        status: str,\n+        stdout: Optional[str] = None,\n+        stderr: Optional[str] = None,\n+    ):\n+        assert status in [\n+            \"success\",\n+            \"failure\",\n+        ], f\"status must be 'success' or 'failure'; Got: {status}\"\n+        template_str = textwrap.dedent(\n+            \"\"\"\n+            {% if status == \"success\" %}completed{% else %}failed{% endif %}\n+            {%- if stdout %}\n+            [stdout]{{ stdout }}[/stdout]\n+            {%- endif -%}\n+            {%- if stderr %}\n+            [stderr]{{ stderr }}[/stderr]\n+            {%- endif -%}\n+            \"\"\"\n+        )\n+        return PromptTemplate(\n+            template_str.lstrip(\"\\n\"),\n+            {\n+                \"status\": status,\n+                \"stdout\": stdout,\n+                \"stderr\": stderr,\n+            },\n+        )\n+\n+    def data_examples(self):\n+        return [\n+            # success\n+            {\n+                \"status\": \"success\",\n+                \"stdout\": '{\"results\":[\"something something\"]}',\n+            },\n+            # failure\n+            {\n+                \"status\": \"failure\",\n+                \"stderr\": \"brave_search encounter an error: could not communicate with api.brave.com\",\n+            },\n+        ]\n\n--- File: models/llama3/reference_impl/generation.py ---\n@@ -282,6 +282,7 @@ def chat_completion(\n         top_p: float = 0.9,\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n+        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n     ) -> ChatPrediction:\n         if (\n             max_gen_len is None\n@@ -296,7 +297,9 @@ def chat_completion(\n \n         stop_reason = None\n         for result in self.generate(\n-            model_input=self.formatter.encode_dialog_prompt(messages),\n+            model_input=self.formatter.encode_dialog_prompt(\n+                messages, tool_prompt_format\n+            ),\n             max_gen_len=max_gen_len,\n             temperature=temperature,\n             top_p=top_p,\n\n--- File: models/llama3/tests/prompt_templates/test_system_prompts.py ---\n@@ -0,0 +1,101 @@\n+import textwrap\n+import unittest\n+from datetime import datetime\n+\n+from llama_models.llama3.prompt_templates import (\n+    BuiltinToolGenerator,\n+    FunctionTagCustomToolGenerator,\n+    JsonCustomToolGenerator,\n+    SystemDefaultGenerator,\n+)\n+\n+\n+class PromptTemplateTests(unittest.TestCase):\n+\n+    def check_generator_output(self, generator, expected_text):\n+        example = generator.data_examples()[0]\n+\n+        pt = generator.gen(example)\n+        text = pt.render()\n+        # print(text)  # debugging\n+        assert text == expected_text, f\"Expected:\\n{expected_text}\\nActual:\\n{text}\"\n+\n+    def test_system_default(self):\n+        generator = SystemDefaultGenerator()\n+        today = datetime.now().strftime(\"%d %B %Y\")\n+        expected_text = f\"Cutting Knowledge Date: December 2023\\nToday Date: {today}\"\n+        self.check_generator_output(generator, expected_text)\n+\n+    def test_system_builtin_only(self):\n+        generator = BuiltinToolGenerator()\n+        expected_text = textwrap.dedent(\n+            \"\"\"\n+            Environment: ipython\n+            Tools: brave_search, wolfram_alpha\n+            \"\"\"\n+        )\n+        self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n+\n+    def test_system_custom_only(self):\n+        self.maxDiff = None\n+        generator = JsonCustomToolGenerator()\n+        expected_text = textwrap.dedent(\n+            \"\"\"\n+            Answer the user's question by making use of the following functions if needed.\n+            If none of the function can be used, please say so.\n+            Here is a list of functions in JSON format:\n+            {\n+                \"type\": \"function\",\n+                \"function\": {\n+                    \"name\": \"trending_songs\",\n+                    \"description\": \"Returns the trending songs on a Music site\",\n+                    \"parameters\": {\n+                        \"type\": \"object\",\n+                        \"properties\": [\n+                            {\n+                                \"n\": {\n+                                    \"type\": \"object\",\n+                                    \"description\": \"The number of songs to return\"\n+                                }\n+                            },\n+                            {\n+                                \"genre\": {\n+                                    \"type\": \"object\",\n+                                    \"description\": \"The genre of the songs to return\"\n+                                }\n+                            }\n+                        ],\n+                        \"required\": [\"n\"]\n+                    }\n+                }\n+            }\n+\n+            Return function calls in JSON format.\n+            \"\"\"\n+        )\n+        self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n+\n+    def test_system_custom_function_tag(self):\n+        self.maxDiff = None\n+        generator = FunctionTagCustomToolGenerator()\n+        expected_text = textwrap.dedent(\n+            \"\"\"\n+            You have access to the following functions:\n+\n+            Use the function 'trending_songs' to 'Returns the trending songs on a Music site':\n+            {\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n+\n+            Think very carefully before calling functions.\n+            If a you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+\n+            <function=example_function_name>{\"example_name\": \"example_value\"}</function>\n+\n+            Reminder:\n+            - If looking for real time information use relevant functions before falling back to brave_search\n+            - Function calls MUST follow the specified format, start with <function= and end with </function>\n+            - Required parameters MUST be specified\n+            - Only call one function at a time\n+            - Put the entire function call reply on one line\n+            \"\"\"\n+        )\n+        self.check_generator_output(generator, expected_text.strip(\"\\n\"))"
            },
            {
              "sha": "bf8355fdda0200ca6b924fae96bc1f147311cd19",
              "url": "https://github.com/meta-llama/llama-models/commit/bf8355fdda0200ca6b924fae96bc1f147311cd19",
              "message": "remove pydantic pinning from another requirements.txt",
              "files_changed": [
                {
                  "filename": "models/llama3/requirements.txt",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/requirements.txt ---\n@@ -4,5 +4,5 @@ jinja2\n json-strong-typing\n tiktoken\n torch\n-pydantic==1.10.13\n-pydantic_core==2.18.2\n+pydantic\n+pydantic_core"
            },
            {
              "sha": "9090bb8c6acbae3107e53ecb45239a44dc977e65",
              "url": "https://github.com/meta-llama/llama-models/commit/9090bb8c6acbae3107e53ecb45239a44dc977e65",
              "message": "Bump version to 0.0.10",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.9\",\n+    version=\"0.0.10\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "a1ef060f706d178d21012fbc2c77d6c841711a59",
              "url": "https://github.com/meta-llama/llama-models/commit/a1ef060f706d178d21012fbc2c77d6c841711a59",
              "message": "Bump version to 0.0.9",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.8\",\n+    version=\"0.0.9\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "072c0a2bc980b02fb2903be67ad022401c0ead34",
              "url": "https://github.com/meta-llama/llama-models/commit/072c0a2bc980b02fb2903be67ad022401c0ead34",
              "message": "fix max_seq_len for llama_guard_2",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -90,8 +90,8 @@ class CoreModelId(Enum):\n     llama_guard_2_8b = \"Llama-Guard-2-8B\"\n \n \n-def model_family(CoreModelId) -> ModelFamily:\n-    if CoreModelId in [\n+def model_family(model_id) -> ModelFamily:\n+    if model_id in [\n         CoreModelId.meta_llama2_7b,\n         CoreModelId.meta_llama2_13b,\n         CoreModelId.meta_llama2_70b,\n@@ -100,14 +100,14 @@ def model_family(CoreModelId) -> ModelFamily:\n         CoreModelId.meta_llama2_70b_chat,\n     ]:\n         return ModelFamily.llama2\n-    elif CoreModelId in [\n+    elif model_id in [\n         CoreModelId.meta_llama3_8b,\n         CoreModelId.meta_llama3_70b,\n         CoreModelId.meta_llama3_8b_instruct,\n         CoreModelId.meta_llama3_70b_instruct,\n     ]:\n         return ModelFamily.llama3\n-    elif CoreModelId in [\n+    elif model_id in [\n         CoreModelId.meta_llama3_1_8b,\n         CoreModelId.meta_llama3_1_70b,\n         CoreModelId.meta_llama3_1_405b,\n@@ -116,7 +116,7 @@ def model_family(CoreModelId) -> ModelFamily:\n         CoreModelId.meta_llama3_1_405b_instruct,\n     ]:\n         return ModelFamily.llama3_1\n-    elif CoreModelId in [\n+    elif model_id in [\n         CoreModelId.llama_guard_3_8b,\n         CoreModelId.prompt_guard_86m,\n         CoreModelId.llama_guard_2_8b,\n@@ -159,6 +159,8 @@ def is_featured(self) -> bool:\n     def max_seq_length(self) -> int:\n         if self.model_family == ModelFamily.llama2:\n             return 4096\n+        elif self.core_model_id == CoreModelId.llama_guard_2_8b:\n+            return 4096\n         elif self.model_family == ModelFamily.llama3:\n             return 8192\n         elif self.model_family == ModelFamily.llama3_1:"
            },
            {
              "sha": "65a9a196684a9e585ec30be02c6253bd2acb57b0",
              "url": "https://github.com/meta-llama/llama-models/commit/65a9a196684a9e585ec30be02c6253bd2acb57b0",
              "message": "Use tokenizer.model from the repository for the example scripts",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/example_chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/scripts/example_text_completion.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -36,8 +36,8 @@ To download the model weights and tokenizer:\n 2. Read and accept the license.\n 3. Once your request is approved you will receive a signed URL via email.\n 4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-toolchain`.\n-5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**: \n-Run `llama model list --show-all` to show all the available llama models, including previous versions.\n+5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\n+If you want older versions of models, run `llama model list --show-all` to show all the available Llama models.\n \n 6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n 7. Pass the URL provided when prompted to start the download.\n@@ -55,7 +55,8 @@ After installing the dependencies, you can run the example scripts (within `mode\n ```bash\n #!/bin/bash\n \n-PYTHONPATH=$(git rev-parse --show-toplevel) torchrun models/scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH>\n+CHECKPOINT_DIR=~/.llama/checkpoints/Meta-Llama3.1-8B-Instruct\n+PYTHONPATH=$(git rev-parse --show-toplevel) torchrun models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n ```\n \n The above script should be used with an Instruct (Chat) model. For a Base model, use the script `models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n@@ -67,7 +68,7 @@ For running larger models with tensor parallelism, you should modify as:\n NGPUS=8\n PYTHONPATH=$(git rev-parse --show-toplevel) torchrun \\\n   --nproc_per_node=$NGPUS \\\n-  models/scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH> \\\n+  models/scripts/example_chat_completion.py $CHECKPOINT_DIR \\\n   --model_parallel_size $NGPUS\n ```\n \n\n--- File: models/scripts/example_chat_completion.py ---\n@@ -8,9 +8,11 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n+from pathlib import Path\n from typing import Optional\n \n import fire\n+\n from models.llama3.api.datatypes import (\n     CompletionMessage,\n     StopReason,\n@@ -21,9 +23,11 @@\n from models.llama3.reference_impl.generation import Llama\n \n \n-def main(\n+THIS_DIR = Path(__file__).parent.resolve()\n+\n+\n+def run_main(\n     ckpt_dir: str,\n-    tokenizer_path: str,\n     temperature: float = 0.6,\n     top_p: float = 0.9,\n     max_seq_len: int = 512,\n@@ -40,6 +44,7 @@ def main(\n \n     `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n     \"\"\"\n+    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n         tokenizer_path=tokenizer_path,\n@@ -92,5 +97,9 @@ def main(\n         print(\"\\n==================================\\n\")\n \n \n+def main():\n+    fire.Fire(run_main)\n+\n+\n if __name__ == \"__main__\":\n-    fire.Fire(main)\n+    main()\n\n--- File: models/scripts/example_text_completion.py ---\n@@ -8,6 +8,7 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates.\n # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n \n+from pathlib import Path\n from typing import Optional\n \n import fire\n@@ -16,9 +17,11 @@\n from termcolor import cprint\n \n \n-def main(\n+THIS_DIR = Path(__file__).parent.resolve()\n+\n+\n+def run_main(\n     ckpt_dir: str,\n-    tokenizer_path: str,\n     temperature: float = 0.6,\n     top_p: float = 0.9,\n     max_seq_len: int = 512,\n@@ -37,6 +40,7 @@ def main(\n \n     `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n     \"\"\"\n+    tokenizer_path = str(THIS_DIR.parent / \"llama3/api/tokenizer.model\")\n     generator = Llama.build(\n         ckpt_dir=ckpt_dir,\n         tokenizer_path=tokenizer_path,\n@@ -68,5 +72,9 @@ def main(\n         print(\"\\n==================================\\n\")\n \n \n+def main():\n+    fire.Fire(run_main)\n+\n+\n if __name__ == \"__main__\":\n-    fire.Fire(main)\n+    main()"
            },
            {
              "sha": "d11b99f95a1e394a9b6150f5897acb01cbbf6f1a",
              "url": "https://github.com/meta-llama/llama-models/commit/d11b99f95a1e394a9b6150f5897acb01cbbf6f1a",
              "message": "Bump version to 0.0.8",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.7\",\n+    version=\"0.0.8\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "b5a8f882a849e837859bcfc4b74149c6520bef20",
              "url": "https://github.com/meta-llama/llama-models/commit/b5a8f882a849e837859bcfc4b74149c6520bef20",
              "message": "Bump version to 0.0.7",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.6\",\n+    version=\"0.0.7\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "bcb9855d085cfc8d52dd4768d9a940fe0fb507a4",
              "url": "https://github.com/meta-llama/llama-models/commit/bcb9855d085cfc8d52dd4768d9a940fe0fb507a4",
              "message": "Update MANIFEST",
              "files_changed": [
                {
                  "filename": "MANIFEST.in",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: MANIFEST.in ---\n@@ -1,17 +1,19 @@\n include requirements.txt\n-include models/llama3_1/api/templates\n-include models/llama3_1/api/templates/assistant_message.jinja\n-include models/llama3_1/api/templates/user_message.jinja\n-include models/llama3_1/api/templates/assistant_message.builtin_tool_call.yaml\n-include models/llama3_1/api/templates/assistant_message.custom_tool_call.yaml\n-include models/llama3_1/api/templates/assistant_message.default.yaml\n-include models/llama3_1/api/templates/system_message.builtin_and_custom_tools.yaml\n-include models/llama3_1/api/templates/system_message.builtin_tools_only.yaml\n-include models/llama3_1/api/templates/system_message.custom_tools_only.yaml\n-include models/llama3_1/api/templates/system_message.default.yaml\n-include models/llama3_1/api/templates/system_message.jinja\n-include models/llama3_1/api/templates/tool_message.failure.yaml\n-include models/llama3_1/api/templates/tool_message.jinja\n-include models/llama3_1/api/templates/tool_message.success.yaml\n-include models/llama3_1/api/templates/user_message.default.yaml\n-include models/llama3_1/api/tokenizer.model\n+include models/llama3/api/templates\n+include models/llama3/api/templates/assistant_message.jinja\n+include models/llama3/api/templates/user_message.jinja\n+include models/llama3/api/templates/assistant_message.builtin_tool_call.yaml\n+include models/llama3/api/templates/assistant_message.custom_tool_call.yaml\n+include models/llama3/api/templates/assistant_message.default.yaml\n+include models/llama3/api/templates/system_message.builtin_and_custom_tools.yaml\n+include models/llama3/api/templates/system_message.builtin_tools_only.yaml\n+include models/llama3/api/templates/system_message.custom_tools_only.yaml\n+include models/llama3/api/templates/system_message.default.yaml\n+include models/llama3/api/templates/system_message.jinja\n+include models/llama3/api/templates/tool_message.failure.yaml\n+include models/llama3/api/templates/tool_message.jinja\n+include models/llama3/api/templates/tool_message.success.yaml\n+include models/llama3/api/templates/user_message.default.yaml\n+include models/llama3/api/tokenizer.model\n+include models/scripts/example_chat_completion.py\n+include models/scripts/example_text_completion.py"
            },
            {
              "sha": "eda02fb05a2b34464b3e8482bfdd6af492c2baca",
              "url": "https://github.com/meta-llama/llama-models/commit/eda02fb05a2b34464b3e8482bfdd6af492c2baca",
              "message": "Bump version to 0.0.6",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.5\",\n+    version=\"0.0.6\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "55b418218f818c7e36dfcde67d718b3216128c11",
              "url": "https://github.com/meta-llama/llama-models/commit/55b418218f818c7e36dfcde67d718b3216128c11",
              "message": "Move code from llama3_1 -> llama3",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/__init__.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/__init__.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/args.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/interface.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.builtin_tool_call.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.custom_tool_call.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.default.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/assistant_message.jinja",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.builtin_and_custom_tools.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.builtin_tools_only.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.custom_tools_only.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.default.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/system_message.jinja",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/tool_message.failure.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/tool_message.jinja",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/tool_message.success.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/user_message.default.yaml",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/templates/user_message.jinja",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/test_tokenizer.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/tokenizer.model",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/tokenizer.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/reference_impl/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/reference_impl/model.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/requirements.txt",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3/tests/api/test_tool_utils.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "removed"
                },
                {
                  "filename": "models/scripts/example_chat_completion.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/scripts/example_text_completion.py",
                  "status": "renamed"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -1,5 +1,5 @@\n <p align=\"center\">\n-  <img src=\"https://github.com/meta-llama/llama-models/blob/main/Llama_Repo.jpeg\" width=\"400\"/>\n+  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n </p>\n \n <p align=\"center\">\n@@ -10,18 +10,17 @@\n \n # Llama Models\n \n-[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n-[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n-\n-\n Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n 1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n 2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n 3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n \n Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n \n-## [Llama Models](#Llama-Models)\n+## Llama Models\n+\n+[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n+[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n \n |  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n | :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n@@ -43,28 +42,50 @@ To download the model weights and tokenizer:\n \n Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n \n-### Download via HuggingFace\n+## Running the models\n \n-We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama) in both transformers and native `llama3` formats. To gain access:\n+You need to install the following dependencies (in addition to the `requirements.txt` in the root directory of this repository) to run the models:\n+```\n+pip install torch fairscale fire blobfile\n+```\n \n-1. Visit one of the repos (ex. [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)).\n-2. Read and accept the license.\n-3. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that approvals may take up to one hour.\n+After installing the dependencies, you can run the example scripts (within `models/scripts/` sub-directory) as follows:\n+```bash\n+#!/bin/bash\n \n-You can then download the models:\n+PYTHONPATH=$(git rev-parse --show-toplevel) torchrun models/scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH>\n+```\n \n-- Via `llama download --source huggingface --hf-token YOUR_ACCESS_TOKEN` ([create/view access tokens here](https://huggingface.co/settings/tokens))\n-- Via the HuggingFace CLI (`pip install huggingface-hub`):\n-- In the web browser by clicking on the \"Files and versions\" tab\n+The above script should be used with an Instruct (Chat) model. For a Base model, use the script `models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n \n+For running larger models with tensor parallelism, you should modify as:\n ```bash\n-huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n+#!/bin/bash\n+\n+NGPUS=8\n+PYTHONPATH=$(git rev-parse --show-toplevel) torchrun \\\n+  --nproc_per_node=$NGPUS \\\n+  models/scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH> \\\n+  --model_parallel_size $NGPUS\n ```\n \n-The original native weights are in the `original/` subfolder (except for `meta-llama/Meta-Llama-3.1-405B`).\n+For more flexibility in running inference (including running FP8 inference), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) repository.\n+\n+\n+## Access to Hugging Face\n+\n+We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n+\n+- Visit one of the repos, for example [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n+- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n+- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n+\n+```bash\n+huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n+```\n \n+**NOTE** The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.\n \n-## Using with transformers\n \n - To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n \n@@ -94,9 +115,9 @@ To help developers address these risks, we have created the [Responsible Use Gui\n ## Issues\n \n Please report any software bug or other problems with the models through one of the following means:\n-- Reporting issues with the model: [github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n-- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](https://developers.facebook.com/llama_output_feedback)\n-- Reporting bugs and security concerns: [facebook.com/whitehat/info](https://facebook.com/whitehat/info)\n+- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n+- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n+- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n \n \n ## Questions\n\n--- File: models/llama3/api/test_tokenizer.py ---\n@@ -16,7 +16,7 @@\n from .tokenizer import Tokenizer\n \n \n-# TOKENIZER_PATH=<tokenizer_path> python -m unittest models/llama3_1/api/test_tokenizer.py\n+# TOKENIZER_PATH=<tokenizer_path> python -m unittest models/llama3/api/test_tokenizer.py\n \n \n class TokenizerTests(TestCase):\n\n--- File: models/llama3/reference_impl/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n\n--- File: models/llama3/tests/api/test_tool_utils.py ---\n@@ -1,4 +1,11 @@\n-from llama_models.llama3_1.api.tool_utils import ToolUtils\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from llama_models.llama3.api.tool_utils import ToolUtils\n \n \n class TestToolUtils:\n\n--- File: models/llama3_1/README.md ---\n@@ -1,114 +0,0 @@\n-<p align=\"center\">\n-  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n-</p>\n-\n-<p align=\"center\">\n-         <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp\n-<br>\n-\n----\n-\n-# Llama Models\n-Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n-1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n-2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n-3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n-\n-Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n-\n-## Llama Models\n-\n-|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n-| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n-| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/MODEL_CARD.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n-| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/MODEL_CARD.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n-| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/MODEL_CARD.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n-\n-## Download\n-\n-To download the model weights and tokenizer, please visit the [Meta Llama website](https://llama.meta.com/llama-downloads/) and accept our License.\n-\n-Once your request is approved, you will receive a signed URL over email. Then, run the download.sh script, passing the URL provided when prompted to start the download.\n-\n-Pre-requisites: Ensure you have `wget` installed. Then run the script: `./download.sh`.\n-\n-Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n-\n-## Running the models\n-\n-You need to install the following dependencies (in addition to the `requirements.txt` in the root directory of this repository) to run the models:\n-\n-```\n-pip install torch fairscale fire blobfile\n-```\n-\n-After installing the dependencies, you can run the example scripts as follows:\n-\n-```bash\n-#!/bin/bash\n-\n-PYTHONPATH=$(git rev-parse --show-toplevel) torchrun scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH>\n-```\n-\n-The above script should be used with an Instruct (Chat) model. For running larger models with tensor parallelism, you should modify as:\n-\n-```bash\n-#!/bin/bash\n-\n-NGPUS=8\n-PYTHONPATH=$(git rev-parse --show-toplevel) torchrun \\\n-  --nproc_per_node=$NGPUS \\\n-  scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH> \\\n-  --model_parallel_size $NGPUS\n-```\n-\n-For more flexibility in running inference (including running FP8 inference), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) repository.\n-\n-\n-## Access to Hugging Face\n-\n-We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n-\n-- Visit one of the repos, for example [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n-- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n-- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n-\n-```bash\n-huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n-```\n-\n-**NOTE** The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.\n-\n-\n-- To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n-\n-  ```python\n-  import transformers\n-  import torch\n-\n-  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n-\n-  pipeline = transformers.pipeline(\n-    \"text-generation\",\n-    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    model_kwargs={\"torch_dtype\": torch.bfloat16},\n-    device=\"cuda\",\n-  )\n-  ```\n-\n-## Responsible Use\n-\n-Llama models are a new technology that carries potential risks with use. Testing conducted to date has not  and could not  cover all scenarios.\n-To help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n-\n-## Issues\n-\n-Please report any software bug or other problems with the models through one of the following means:\n-- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n-- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n-- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n-\n-\n-## Questions\n-\n-For common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n\n--- File: models/scripts/example_chat_completion.py ---\n@@ -11,14 +11,14 @@\n from typing import Optional\n \n import fire\n-from models.llama3_1.api.datatypes import (\n+from models.llama3.api.datatypes import (\n     CompletionMessage,\n     StopReason,\n     SystemMessage,\n     UserMessage,\n )\n \n-from models.llama3_1.reference_impl.generation import Llama\n+from models.llama3.reference_impl.generation import Llama\n \n \n def main(\n\n--- File: models/scripts/example_text_completion.py ---\n@@ -12,7 +12,7 @@\n \n import fire\n \n-from models.llama3_1.reference_impl.generation import Llama\n+from models.llama3.reference_impl.generation import Llama\n from termcolor import cprint"
            },
            {
              "sha": "3dea71ccb22da158b88a723a1374e36642e3a12e",
              "url": "https://github.com/meta-llama/llama-models/commit/3dea71ccb22da158b88a723a1374e36642e3a12e",
              "message": "Add `WebMethod.method` for overriding the HTTP method for a route",
              "files_changed": [
                {
                  "filename": "models/schema_utils.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/schema_utils.py ---\n@@ -96,10 +96,12 @@ class WebMethod:\n     public: bool = False\n     request_examples: Optional[List[Any]] = None\n     response_examples: Optional[List[Any]] = None\n+    method: Optional[str] = None\n \n \n def webmethod(\n     route: Optional[str] = None,\n+    method: Optional[str] = None,\n     public: Optional[bool] = False,\n     request_examples: Optional[List[Any]] = None,\n     response_examples: Optional[List[Any]] = None,\n@@ -116,6 +118,7 @@ def webmethod(\n     def wrap(cls: T) -> T:\n         cls.__webmethod__ = WebMethod(\n             route=route,\n+            method=method,\n             public=public or False,\n             request_examples=request_examples,\n             response_examples=response_examples,"
            },
            {
              "sha": "82e417dde9544e467bbfe963f1bf85466c8e3045",
              "url": "https://github.com/meta-llama/llama-models/commit/82e417dde9544e467bbfe963f1bf85466c8e3045",
              "message": "Add example scripts to show how to run the model (#108)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/api/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/reference_impl/generation.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/scripts/example_chat_completion.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/scripts/example_text_completion.py",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/README.md ---\n@@ -34,7 +34,38 @@ Pre-requisites: Ensure you have `wget` installed. Then run the script: `./downlo\n \n Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n \n-### Access to Hugging Face\n+## Running the models\n+\n+You need to install the following dependencies (in addition to the `requirements.txt` in the root directory of this repository) to run the models:\n+\n+```\n+pip install torch fairscale fire blobfile\n+```\n+\n+After installing the dependencies, you can run the example scripts as follows:\n+\n+```bash\n+#!/bin/bash\n+\n+PYTHONPATH=$(git rev-parse --show-toplevel) torchrun scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH>\n+```\n+\n+The above script should be used with an Instruct (Chat) model. For running larger models with tensor parallelism, you should modify as:\n+\n+```bash\n+#!/bin/bash\n+\n+NGPUS=8\n+PYTHONPATH=$(git rev-parse --show-toplevel) torchrun \\\n+  --nproc_per_node=$NGPUS \\\n+  scripts/example_chat_completion.py <CHECKPOINT_DIR> <TOKENIZER_PATH> \\\n+  --model_parallel_size $NGPUS\n+```\n+\n+For more flexibility in running inference (including running FP8 inference), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) repository.\n+\n+\n+## Access to Hugging Face\n \n We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n \n\n--- File: models/llama3_1/api/datatypes.py ---\n@@ -11,8 +11,8 @@\n from pydantic import BaseModel, Field, validator\n \n from typing_extensions import Annotated\n-from llama_models.datatypes import *  # noqa\n-from llama_models.schema_utils import json_schema_type\n+from ...datatypes import *  # noqa\n+from ...schema_utils import json_schema_type\n \n \n @json_schema_type\n\n--- File: models/llama3_1/reference_impl/generation.py ---\n@@ -0,0 +1,352 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# the root directory of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+import json\n+import os\n+import sys\n+import time\n+from dataclasses import dataclass\n+from pathlib import Path\n+from typing import Generator, List, Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.initialize import (\n+    get_model_parallel_rank,\n+    initialize_model_parallel,\n+    model_parallel_is_initialized,\n+)\n+from termcolor import cprint\n+\n+from ..api.args import ModelArgs\n+from ..api.chat_format import ChatFormat, ModelInput\n+from ..api.datatypes import CompletionMessage, Message, StopReason\n+from ..api.tokenizer import Tokenizer\n+from .model import Transformer\n+\n+\n+@dataclass\n+class CompletionPrediction:\n+    generation: str\n+    decoded_tokens: Optional[List[str]] = None\n+    logprobs: Optional[List[List[float]]] = None\n+\n+\n+@dataclass\n+class ChatPrediction:\n+    generation: CompletionMessage\n+    decoded_tokens: Optional[List[str]] = None\n+    logprobs: Optional[List[List[float]]] = None\n+\n+\n+@dataclass\n+class TokenResult:\n+    token: int\n+    text: str\n+    logprobs: Optional[List[float]] = None\n+\n+\n+class Llama:\n+    @staticmethod\n+    def build(\n+        ckpt_dir: str,\n+        tokenizer_path: str,\n+        max_seq_len: int,\n+        max_batch_size: int,\n+        model_parallel_size: Optional[int] = None,\n+        seed: int = 1,\n+    ):\n+        \"\"\"\n+        Build a Llama instance by initializing and loading a model checkpoint.\n+\n+        Args:\n+            ckpt_dir (str): Path to the directory containing checkpoint files.\n+            tokenizer_path (str): Path to the tokenizer file.\n+            max_seq_len (int): Maximum sequence length for input text.\n+            max_batch_size (int): Maximum batch size for inference.\n+            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n+                If not provided, it's determined from the environment. Defaults to None.\n+\n+        Returns:\n+            Llama: An instance of the Llama class with the loaded model and tokenizer.\n+\n+        Raises:\n+            AssertionError: If there are no checkpoint files in the specified directory,\n+                or if the model parallel size does not match the number of checkpoint files.\n+\n+\n+        Note:\n+            This method initializes the distributed process group, sets the device to CUDA,\n+            and loads the pre-trained model and tokenizer.\n+        \"\"\"\n+\n+        if not torch.distributed.is_initialized():\n+            torch.distributed.init_process_group(\"nccl\")\n+\n+        if not model_parallel_is_initialized():\n+            if model_parallel_size is None:\n+                model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n+            initialize_model_parallel(model_parallel_size)\n+\n+        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n+        torch.cuda.set_device(local_rank)\n+\n+        torch.manual_seed(seed)\n+\n+        if local_rank > 0:\n+            sys.stdout = open(os.devnull, \"w\")\n+\n+        start_time = time.time()\n+\n+        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n+        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n+        assert model_parallel_size == len(\n+            checkpoints\n+        ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n+        ckpt_path = checkpoints[get_model_parallel_rank()]\n+        checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n+        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n+            params = json.loads(f.read())\n+\n+        model_args: ModelArgs = ModelArgs(\n+            max_seq_len=max_seq_len,\n+            max_batch_size=max_batch_size,\n+            **params,\n+        )\n+        tokenizer = Tokenizer(model_path=tokenizer_path)\n+        assert model_args.vocab_size == tokenizer.n_words\n+        if torch.cuda.is_bf16_supported():\n+            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+        else:\n+            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n+        model = Transformer(model_args)\n+        model.load_state_dict(checkpoint, strict=False)\n+        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n+\n+        return Llama(model, tokenizer, model_args)\n+\n+    def __init__(self, model: Transformer, tokenizer: Tokenizer, args: ModelArgs):\n+        self.args = args\n+        self.model = model\n+        self.tokenizer = tokenizer\n+        self.formatter = ChatFormat(tokenizer)\n+\n+    @torch.inference_mode()\n+    def generate(\n+        self,\n+        model_input: ModelInput,\n+        max_gen_len: int,\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        logprobs: bool = False,\n+    ) -> Generator:\n+        params = self.model.params\n+\n+        # cprint(\"Input to model -> \" + self.tokenizer.decode(model_input.tokens), \"red\")\n+        prompt_tokens = [model_input.tokens]\n+\n+        bsz = 1\n+        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n+\n+        min_prompt_len = min(len(t) for t in prompt_tokens)\n+        max_prompt_len = max(len(t) for t in prompt_tokens)\n+\n+        if max_prompt_len >= params.max_seq_len:\n+            cprint(\n+                f\"Out of token budget {max_prompt_len} vs {params.max_seq_len}\", \"red\"\n+            )\n+            return\n+\n+        total_len = min(max_gen_len + max_prompt_len, params.max_seq_len)\n+        pad_id = self.tokenizer.pad_id\n+        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n+        for k, t in enumerate(prompt_tokens):\n+            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n+        if logprobs:\n+            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n+\n+        prev_pos = 0\n+        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n+        input_text_mask = tokens != pad_id\n+        if min_prompt_len == total_len:\n+            logits = self.model.forward(tokens, prev_pos)\n+            token_logprobs = -F.cross_entropy(\n+                input=logits.transpose(1, 2),\n+                target=tokens,\n+                reduction=\"none\",\n+                ignore_index=pad_id,\n+            )\n+\n+        stop_tokens = torch.tensor(self.tokenizer.stop_tokens)\n+\n+        for cur_pos in range(min_prompt_len, total_len):\n+            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n+\n+            if temperature > 0:\n+                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n+                next_token = sample_top_p(probs, top_p)\n+            else:\n+                next_token = torch.argmax(logits[:, -1], dim=-1)\n+\n+            next_token = next_token.reshape(-1)\n+            # only replace token if prompt has already been generated\n+            next_token = torch.where(\n+                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n+            )\n+            tokens[:, cur_pos] = next_token\n+\n+            target = tokens[:, prev_pos + 1 : cur_pos + 1]\n+            if logprobs:\n+                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n+                    input=logits.transpose(1, 2),\n+                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n+                    reduction=\"none\",\n+                    ignore_index=pad_id,\n+                )\n+            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n+                torch.isin(next_token, stop_tokens)\n+            )\n+            yield TokenResult(\n+                token=next_token[0].item(),\n+                text=self.tokenizer.decode(next_token.tolist()),\n+                logprobs=(\n+                    token_logprobs[:, prev_pos + 1 : cur_pos + 1][0].tolist()\n+                    if logprobs\n+                    else None\n+                ),\n+            )\n+\n+            prev_pos = cur_pos\n+            if all(eos_reached):\n+                break\n+\n+    def text_completion(\n+        self,\n+        prompt: str,\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+    ) -> CompletionPrediction:\n+        if (\n+            max_gen_len is None\n+            or max_gen_len == 0\n+            or max_gen_len >= self.model.params.max_seq_len\n+        ):\n+            max_gen_len = self.model.params.max_seq_len - 1\n+\n+        prompt_tokens = self.tokenizer.encode(prompt, bos=True, eos=False)\n+\n+        tokens = []\n+        token_logprobs = []\n+        decoded_tokens = []\n+        for result in self.generate(\n+            model_input=ModelInput(tokens=prompt_tokens),\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=logprobs,\n+        ):\n+            tokens.append(result.token)\n+            if logprobs:\n+                decoded_tokens.append(result.text)\n+                token_logprobs.append(result.logprobs)\n+\n+        generation = self.tokenizer.decode(tokens)\n+        if logprobs:\n+            return CompletionPrediction(\n+                generation=generation,\n+                logprobs=token_logprobs,\n+                decoded_tokens=decoded_tokens,\n+            )\n+\n+        return CompletionPrediction(generation=generation)\n+\n+    def chat_completion(\n+        self,\n+        messages: List[Message],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+    ) -> ChatPrediction:\n+        if (\n+            max_gen_len is None\n+            or max_gen_len == 0\n+            or max_gen_len >= self.model.params.max_seq_len\n+        ):\n+            max_gen_len = self.model.params.max_seq_len - 1\n+\n+        tokens = []\n+        token_logprobs = []\n+        decoded_tokens = []\n+\n+        stop_reason = None\n+        for result in self.generate(\n+            model_input=self.formatter.encode_dialog_prompt(messages),\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=logprobs,\n+        ):\n+            tokens.append(result.token)\n+            if result.text == \"<|eot_id|>\":\n+                stop_reason = StopReason.end_of_turn\n+            elif result.text == \"<|eom_id|>\":\n+                stop_reason = StopReason.end_of_message\n+\n+            if logprobs:\n+                decoded_tokens.append(result.text)\n+                token_logprobs.append(result.logprobs)\n+\n+        if stop_reason is None:\n+            stop_reason = StopReason.out_of_tokens\n+\n+        message = self.formatter.decode_assistant_message(tokens, stop_reason)\n+\n+        if logprobs:\n+            return ChatPrediction(\n+                generation=message,\n+                logprobs=token_logprobs,\n+                decoded_tokens=decoded_tokens,\n+            )\n+\n+        return ChatPrediction(generation=message)\n+\n+\n+def sample_top_p(probs, p):\n+    \"\"\"\n+    Perform top-p (nucleus) sampling on a probability distribution.\n+\n+    Args:\n+        probs (torch.Tensor): Probability distribution tensor.\n+        p (float): Probability threshold for top-p sampling.\n+\n+    Returns:\n+        torch.Tensor: Sampled token indices.\n+\n+    Note:\n+        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n+        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n+    \"\"\"\n+    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n+    probs_sum = torch.cumsum(probs_sort, dim=-1)\n+    mask = probs_sum - probs_sort > p\n+    probs_sort[mask] = 0.0\n+    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n+    next_token = torch.multinomial(probs_sort, num_samples=1)\n+    next_token = torch.gather(probs_idx, -1, next_token)\n+    return next_token\n\n--- File: models/llama3_1/scripts/example_chat_completion.py ---\n@@ -0,0 +1,96 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+from typing import Optional\n+\n+import fire\n+from models.llama3_1.api.datatypes import (\n+    CompletionMessage,\n+    StopReason,\n+    SystemMessage,\n+    UserMessage,\n+)\n+\n+from models.llama3_1.reference_impl.generation import Llama\n+\n+\n+def main(\n+    ckpt_dir: str,\n+    tokenizer_path: str,\n+    temperature: float = 0.6,\n+    top_p: float = 0.9,\n+    max_seq_len: int = 512,\n+    max_batch_size: int = 4,\n+    max_gen_len: Optional[int] = None,\n+    model_parallel_size: Optional[int] = None,\n+):\n+    \"\"\"\n+    Examples to run with the models finetuned for chat. Prompts correspond of chat\n+    turns between the user and assistant with the final one always being the user.\n+\n+    An optional system prompt at the beginning to control how the model should respond\n+    is also supported.\n+\n+    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n+    \"\"\"\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+        model_parallel_size=model_parallel_size,\n+    )\n+\n+    dialogs = [\n+        [UserMessage(content=\"what is the recipe of mayonnaise?\")],\n+        [\n+            UserMessage(content=\"I am going to Paris, what should I see?\"),\n+            CompletionMessage(\n+                content=\"\"\"\\\n+Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n+\n+1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n+2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n+3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n+\n+These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n+                stop_reason=StopReason.end_of_turn,\n+            ),\n+            UserMessage(content=\"What is so great about #1?\"),\n+        ],\n+        [\n+            SystemMessage(content=\"Always answer with Haiku\"),\n+            UserMessage(content=\"I am going to Paris, what should I see?\"),\n+        ],\n+        [\n+            SystemMessage(\n+                content=\"Always answer with emojis\",\n+            ),\n+            UserMessage(content=\"How to go from Beijing to NY?\"),\n+        ],\n+    ]\n+    for dialog in dialogs:\n+        result = generator.chat_completion(\n+            dialog,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+        )\n+\n+        for msg in dialog:\n+            print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n+\n+        out_message = result.generation\n+        print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n+        print(\"\\n==================================\\n\")\n+\n+\n+if __name__ == \"__main__\":\n+    fire.Fire(main)\n\n--- File: models/llama3_1/scripts/example_text_completion.py ---\n@@ -0,0 +1,72 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+from typing import Optional\n+\n+import fire\n+\n+from models.llama3_1.reference_impl.generation import Llama\n+from termcolor import cprint\n+\n+\n+def main(\n+    ckpt_dir: str,\n+    tokenizer_path: str,\n+    temperature: float = 0.6,\n+    top_p: float = 0.9,\n+    max_seq_len: int = 512,\n+    max_batch_size: int = 4,\n+    max_gen_len: int = 64,\n+    model_parallel_size: Optional[int] = None,\n+):\n+    \"\"\"\n+    Examples to run with the models finetuned for chat. Prompts correspond of chat\n+    turns between the user and assistant with the final one always being the user.\n+\n+    An optional system prompt at the beginning to control how the model should respond\n+    is also supported.\n+\n+    The context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192.\n+\n+    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n+    \"\"\"\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+        model_parallel_size=model_parallel_size,\n+    )\n+\n+    prompts = [\n+        \"The color of the sky is blue but sometimes it can also be \",\n+        \"\"\"\\\n+apple is pomme,\n+bannana is banane,\n+cherry is\"\"\",\n+        \"1, 2, 3, 5, 8, 13\",\n+        \"ba ba black sheep, have you any wool?\",\n+    ]\n+    for prompt in prompts:\n+        result = generator.text_completion(\n+            prompt,\n+            temperature=0.6,\n+            top_p=0.9,\n+            max_gen_len=max_gen_len,\n+            logprobs=False,\n+        )\n+\n+        cprint(f\"{prompt}\", end=\"\")\n+        cprint(f\"{result.generation}\", color=\"yellow\")\n+        print(\"\\n==================================\\n\")\n+\n+\n+if __name__ == \"__main__\":\n+    fire.Fire(main)"
            },
            {
              "sha": "f45cdfd624b98b6655540f7101d8d9cb432e631c",
              "url": "https://github.com/meta-llama/llama-models/commit/f45cdfd624b98b6655540f7101d8d9cb432e631c",
              "message": "Fix n_kv_heads for mp16 models",
              "files_changed": [
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/sku_list.py ---\n@@ -299,7 +299,7 @@ def llama3_1_base_models() -> List[Model]:\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n-                \"n_kv_heads\": 8,\n+                \"n_kv_heads\": 16,\n                 \"vocab_size\": VOCAB_SIZE,\n                 \"ffn_dim_multiplier\": 1.2,\n                 \"multiple_of\": 4096,\n@@ -545,7 +545,7 @@ def llama3_1_instruct_models() -> List[Model]:\n                 \"dim\": 16384,\n                 \"n_layers\": 126,\n                 \"n_heads\": 128,\n-                \"n_kv_heads\": 8,\n+                \"n_kv_heads\": 16,\n                 \"vocab_size\": VOCAB_SIZE,\n                 \"ffn_dim_multiplier\": 1.2,\n                 \"multiple_of\": 4096,"
            },
            {
              "sha": "d83e8cb2aa9662bd16eaa431287e6c0951930a7c",
              "url": "https://github.com/meta-llama/llama-models/commit/d83e8cb2aa9662bd16eaa431287e6c0951930a7c",
              "message": "Bump version to 0.0.3",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.2\",\n+    version=\"0.0.3\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "de4fe7355494424444ea737a3297dde229aea0b9",
              "url": "https://github.com/meta-llama/llama-models/commit/de4fe7355494424444ea737a3297dde229aea0b9",
              "message": "Add checklist.chk to files which are downloaded",
              "files_changed": [
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/sku_list.py ---\n@@ -398,7 +398,7 @@ def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n     else:\n         folder = model.huggingface_repo.split(\"/\")[-1]\n \n-    files = []\n+    files = [\"checklist.chk\"]\n     if (\n         model.core_model_id == CoreModelId.llama_guard_3_8b\n         and model.quantization_format == CheckpointQuantizationFormat.int8"
            },
            {
              "sha": "ae2f290ffcdc7cdc621cf5f3ae10011861b50a77",
              "url": "https://github.com/meta-llama/llama-models/commit/ae2f290ffcdc7cdc621cf5f3ae10011861b50a77",
              "message": "Change how we name SKUs; add safety SKUs (#79)",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -33,30 +33,38 @@ class SamplingParams(BaseModel):\n \n @json_schema_type(\n     schema={\n-        \"description\": \"The format in which weights are specified. This does not necessarily always equal what quantization is desired at runtime since there can be on-the-fly conversions done.\",\n+        \"description\": \"\"\"\n+The format in which weights are specified. This does not necessarily\n+always equal what quantization is desired at runtime since there\n+can be on-the-fly conversions done.\n+\"\"\",\n     }\n )\n class CheckpointQuantizationFormat(Enum):\n     # default format\n     bf16 = \"bf16\"\n \n     # used for enabling fp8_rowwise inference, some weights are bf16\n-    fp8_mixed = \"fp8_mixed\"\n+    fp8_mixed = \"fp8-mixed\"\n+\n+    int8 = \"int8\"\n \n \n @json_schema_type\n-class ModelSKU(Enum):\n-    llama3_1_8b = \"llama3_1_8b\"\n-    llama3_1_70b = \"llama3_1_70b\"\n-    llama3_1_405b_fp8_mp8 = \"llama3_1_405b_fp8_mp8\"\n-    llama3_1_405b_bf16_mp8 = \"llama3_1_405b_bf16_mp8\"\n-    llama3_1_405b_bf16_mp16 = \"llama3_1_405b_bf16_mp16\"\n+class CoreModelId(Enum):\n+    \"\"\"Each of these models is a unique \"SKU\". These root models can be served in various garbs (especially by quantizing them)\"\"\"\n+\n+    # Llama 3.1 family\n+    meta_llama3_1_8b = \"Meta-Llama3.1-8B\"\n+    meta_llama3_1_70b = \"Meta-Llama3.1-70B\"\n+    meta_llama3_1_405b = \"Meta-Llama3.1-405B\"\n+    meta_llama3_1_8b_instruct = \"Meta-Llama3.1-8B-Instruct\"\n+    meta_llama3_1_70b_instruct = \"Meta-Llama3.1-70B-Instruct\"\n+    meta_llama3_1_405b_instruct = \"Meta-Llama3.1-405B-Instruct\"\n \n-    llama3_1_8b_instruct = \"llama3_1_8b_instruct\"\n-    llama3_1_70b_instruct = \"llama3_1_70b_instruct\"\n-    llama3_1_405b_instruct_fp8_mp8 = \"llama3_1_405b_instruct_fp8_mp8\"\n-    llama3_1_405b_instruct_bf16_mp8 = \"llama3_1_405b_instruct_bf16_mp8\"\n-    llama3_1_405b_instruct_bf16_mp16 = \"llama3_1_405b_instruct_bf16_mp16\"\n+    # Safety models\n+    llama_guard_3_8b = \"Llama-Guard-3-8B\"\n+    prompt_guard_86m = \"Prompt-Guard-86M\"\n \n \n @json_schema_type\n@@ -70,37 +78,39 @@ class HardwareRequirements(BaseModel):\n         \"description\": \"The model family and SKU of the model along with other parameters corresponding to the model.\"\n     }\n )\n-class ModelDefinition(BaseModel):\n-    sku: ModelSKU\n+class Model(BaseModel):\n+    core_model_id: CoreModelId\n+    is_default_variant: bool\n+\n+    # The variant is a string representation of other parameters which helps\n+    # uniquely identify the model. this typically includes the quantization\n+    # format, model parallel size, etc.\n+    @property\n+    def variant(self) -> str:\n+        parts = [\n+            self.quantization_format.value,\n+            f\"mp{self.hardware_requirements.gpu_count}\",\n+        ]\n+\n+        return \"-\".join(parts)\n+\n+    # The SKU is uniquely identified by (model_id, variant) combo\n+    def descriptor(self, shorten_default_variant: bool = True) -> str:\n+        if shorten_default_variant and self.is_default_variant:\n+            return self.core_model_id.value\n+\n+        return f\"{self.core_model_id.value}:{self.variant}\"\n+\n     description_markdown: str\n     max_seq_length: int\n-    huggingface_id: Optional[str] = None\n+    huggingface_repo: Optional[str] = None\n     hardware_requirements: HardwareRequirements\n     quantization_format: CheckpointQuantizationFormat = (\n         CheckpointQuantizationFormat.bf16\n     )\n     recommended_sampling_params: Optional[SamplingParams] = None\n     model_args: Dict[str, Any]\n \n-\n-# TODO: resolve these types against the model SKUs above\n-@json_schema_type(\n-    schema={\n-        \"description\": \"The type of the model. This is used to determine the model family and SKU.\"\n-    }\n-)\n-class PretrainedModel(Enum):\n-    llama3_8b = \"llama3_8b\"\n-    llama3_70b = \"llama3_70b\"\n-\n-\n-@json_schema_type\n-class InstructModel(Enum):\n-    llama3_8b_chat = \"llama3_8b_chat\"\n-    llama3_70b_chat = \"llama3_70b_chat\"\n-\n-\n-@json_schema_type\n-class RewardModel(Enum):\n-    llama3_70b_reward = \"llama3_70b_reward\"\n-    llama3_405b_reward = \"llama3_405b_reward\"\n+    @property\n+    def is_instruct_model(self) -> bool:\n+        return \"instruct\" in self.id.name\n\n--- File: models/sku_list.py ---\n@@ -5,13 +5,13 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-from typing import List\n+from typing import List, Optional\n \n from .datatypes import (\n     CheckpointQuantizationFormat,\n+    CoreModelId,\n     HardwareRequirements,\n-    ModelDefinition,\n-    ModelSKU,\n+    Model,\n     SamplingParams,\n     SamplingStrategy,\n )\n@@ -21,8 +21,19 @@\n VOCAB_SIZE = 128256\n \n \n-def llama3_1_model_list() -> List[ModelDefinition]:\n-    return base_models() + instruct_models()\n+def resolve_model(descriptor: str) -> Optional[Model]:\n+    for m in all_registered_models():\n+        descriptors = [\n+            m.descriptor(shorten_default_variant=False),\n+            m.descriptor(shorten_default_variant=True),\n+        ]\n+        if descriptor in descriptors:\n+            return m\n+    return None\n+\n+\n+def all_registered_models() -> List[Model]:\n+    return base_models() + instruct_models() + safety_models()\n \n \n def recommended_sampling_params() -> SamplingParams:\n@@ -33,13 +44,14 @@ def recommended_sampling_params() -> SamplingParams:\n     )\n \n \n-def base_models() -> List[ModelDefinition]:\n+def base_models() -> List[Model]:\n     return [\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_8b,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_8b,\n+            is_default_variant=True,\n             description_markdown=\"Llama 3.1 8b model\",\n             max_seq_length=CONTEXT_LENGTH,\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-8B\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n                 memory_gb_per_gpu=20,\n@@ -58,10 +70,11 @@ def base_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_70b,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_70b,\n+            is_default_variant=True,\n             description_markdown=\"Llama 3.1 70b model\",\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-70B\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B\",\n             max_seq_length=CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -81,10 +94,11 @@ def base_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_405b_bf16_mp8,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_405b,\n+            is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n-            huggingface_id=None,\n+            huggingface_repo=None,\n             max_seq_length=CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -104,11 +118,12 @@ def base_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_405b_fp8_mp8,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_405b,\n+            is_default_variant=True,\n             description_markdown=\"Llama 3.1 405b model (FP8 quantized)\",\n             max_seq_length=CONTEXT_LENGTH,\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-405B-FP8\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-FP8\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=70,\n@@ -128,10 +143,11 @@ def base_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_405b_bf16_mp16,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_405b,\n+            is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-405B\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B\",\n             max_seq_length=CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=16,\n@@ -154,13 +170,14 @@ def base_models() -> List[ModelDefinition]:\n     ]\n \n \n-def instruct_models() -> List[ModelDefinition]:\n+def instruct_models() -> List[Model]:\n     return [\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_8b_instruct,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_8b_instruct,\n+            is_default_variant=True,\n             description_markdown=\"Llama 3.1 8b instruct model\",\n             max_seq_length=CONTEXT_LENGTH,\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n                 memory_gb_per_gpu=20,\n@@ -179,10 +196,11 @@ def instruct_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_70b_instruct,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_70b_instruct,\n+            is_default_variant=True,\n             description_markdown=\"Llama 3.1 70b instruct model\",\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n             max_seq_length=CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -202,10 +220,11 @@ def instruct_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_405b_instruct_bf16_mp8,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n+            is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b instruct model (BF16 weights)\",\n-            huggingface_id=None,\n+            huggingface_repo=None,\n             max_seq_length=CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -225,10 +244,11 @@ def instruct_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_405b_instruct_fp8_mp8,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n+            is_default_variant=True,\n             description_markdown=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n             max_seq_length=CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -249,10 +269,11 @@ def instruct_models() -> List[ModelDefinition]:\n                 \"use_scaled_rope\": True,\n             },\n         ),\n-        ModelDefinition(\n-            sku=ModelSKU.llama3_1_405b_instruct_bf16_mp16,\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_1_405b_instruct,\n+            is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b instruct model (BF16 weights)\",\n-            huggingface_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n             max_seq_length=CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=16,\n@@ -275,31 +296,163 @@ def instruct_models() -> List[ModelDefinition]:\n     ]\n \n \n-def llama_meta_folder_path(model: ModelDefinition) -> str:\n-    if model.sku == ModelSKU.llama3_1_405b_bf16_mp16:\n-        return \"Meta-Llama-3.1-405B-MP16\"\n-    elif model.sku == ModelSKU.llama3_1_405b_bf16_mp8:\n-        return \"Meta-Llama-3.1-405B-MP8\"\n-    elif model.sku == ModelSKU.llama3_1_405b_fp8_mp8:\n-        return \"Meta-Llama-3.1-405B\"\n-    elif model.sku == ModelSKU.llama3_1_405b_instruct_bf16_mp16:\n-        return \"Meta-Llama-3.1-405B-Instruct-MP16\"\n-    elif model.sku == ModelSKU.llama3_1_405b_instruct_bf16_mp8:\n-        return \"Meta-Llama-3.1-405B-Instruct-MP8\"\n-    elif model.sku == ModelSKU.llama3_1_405b_instruct_fp8_mp8:\n-        return \"Meta-Llama-3.1-405B-Instruct\"\n+def safety_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.llama_guard_3_8b,\n+            is_default_variant=True,\n+            description_markdown=\"Llama Guard v3 8b system safety model\",\n+            huggingface_repo=\"meta-llama/Llama-Guard-3-8B\",\n+            max_seq_length=CONTEXT_LENGTH,\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=20,\n+            ),\n+            model_args={\n+                \"dim\": 4096,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"n_layers\": 32,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+                \"vocab_size\": 128256,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.llama_guard_3_8b,\n+            is_default_variant=False,\n+            description_markdown=\"Llama Guard v3 8b system safety model\",\n+            huggingface_repo=\"meta-llama/Llama-Guard-3-8B-INT8\",\n+            max_seq_length=CONTEXT_LENGTH,\n+            quantization_format=CheckpointQuantizationFormat.int8,\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=10,\n+            ),\n+            model_args={\n+                \"dim\": 4096,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"n_layers\": 32,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+                \"vocab_size\": 128256,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.prompt_guard_86m,\n+            is_default_variant=True,\n+            description_markdown=\"Prompt Guard 86M injection safety model\",\n+            huggingface_repo=\"meta-llama/Prompt-Guard-86M\",\n+            max_seq_length=CONTEXT_LENGTH,\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=1,\n+            ),\n+            model_args={},\n+        ),\n+    ]\n \n-    path = model.huggingface_id.split(\"/\")[-1]\n-    return path\n+\n+from dataclasses import dataclass\n+\n+\n+@dataclass\n+class LlamaDownloadInfo:\n+    folder: str\n+    files: List[str]\n+    pth_size: int\n+\n+\n+def llama_meta_net_info(model: Model) -> LlamaDownloadInfo:\n+    \"\"\"Information needed to download model from llamameta.net\"\"\"\n+\n+    gpu = model.hardware_requirements.gpu_count\n+    if model.core_model_id == CoreModelId.meta_llama3_1_405b:\n+        if gpu == 16:\n+            folder = \"Meta-Llama-3.1-405B-MP16\"\n+        elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n+            folder = \"Meta-Llama-3.1-405B\"\n+        else:\n+            folder = \"Meta-Llama-3.1-405B-MP8\"\n+    elif model.core_model_id == CoreModelId.meta_llama3_1_405b_instruct:\n+        if gpu == 16:\n+            folder = \"Meta-Llama-3.1-405B-Instruct-MP16\"\n+        elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n+            folder = \"Meta-Llama-3.1-405B-Instruct\"\n+        else:\n+            folder = \"Meta-Llama-3.1-405B-Instruct-MP8\"\n+    elif model.core_model_id == CoreModelId.llama_guard_3_8b:\n+        if model.quantization_format == CheckpointQuantizationFormat.int8:\n+            folder = \"Meta-Llama-Guard-3-8B-INT8-HF\"\n+        else:\n+            folder = \"Meta-Llama-Guard-3-8B\"\n+    elif model.core_model_id == CoreModelId.prompt_guard_86m:\n+        folder = \"Prompt-Guard\"\n+    else:\n+        folder = model.huggingface_repo.split(\"/\")[-1]\n+\n+    files = []\n+    if (\n+        model.core_model_id == CoreModelId.llama_guard_3_8b\n+        and model.quantization_format == CheckpointQuantizationFormat.int8\n+    ):\n+        files.extend(\n+            [\n+                \"generation_config.json\",\n+                \"model-00001-of-00002.safetensors\",\n+                \"model-00002-of-00002.safetensors\",\n+                \"special_tokens_map.json\",\n+                \"tokenizer.json\",\n+                \"tokenizer_config.json\",\n+                \"model.safetensors.index.json\",\n+            ]\n+        )\n+    elif model.core_model_id == CoreModelId.prompt_guard_86m:\n+        files.extend(\n+            [\n+                \"model.safetensors\",\n+                \"special_tokens_map.json\",\n+                \"tokenizer.json\",\n+                \"tokenizer_config.json\",\n+            ]\n+        )\n+    else:\n+        files.extend(\n+            [\n+                \"tokenizer.model\",\n+                \"params.json\",\n+            ]\n+        )\n+        if model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n+            files.extend([f\"fp8_scales_{i}.pt\" for i in range(gpu)])\n+        files.extend([f\"consolidated.{i:02d}.pth\" for i in range(gpu)])\n+\n+    return LlamaDownloadInfo(\n+        folder=folder,\n+        files=files,\n+        pth_size=llama_meta_pth_size(model),\n+    )\n \n \n # Sadness because Cloudfront rejects our HEAD requests to find Content-Length\n-def llama_meta_pth_size(model: ModelDefinition) -> int:\n-    if model.sku == ModelSKU.llama3_1_405b_bf16_mp16:\n+def llama_meta_pth_size(model: Model) -> int:\n+    if model.core_model_id not in (\n+        CoreModelId.meta_llama3_1_405b,\n+        CoreModelId.meta_llama3_1_405b_instruct,\n+    ):\n+        return 0\n+\n+    gpu = model.hardware_requirements.gpu_count\n+    if gpu == 16:\n         return 51268302389\n-    elif model.sku == ModelSKU.llama3_1_405b_bf16_mp8:\n-        return 101470976045\n-    elif model.sku == ModelSKU.llama3_1_405b_fp8_mp8:\n+    elif model.quantization_format == CheckpointQuantizationFormat.fp8_mixed:\n         return 60903742309\n-\n-    return 0\n+    else:\n+        return 101470976045"
            }
          ]
        }
      ]
    },
    {
      "id": 5880981,
      "username": "baptisteroziere",
      "url": "https://github.com/baptisteroziere",
      "avatar_url": "https://avatars.githubusercontent.com/u/5880981?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "d2b38acd3a9c55051de1f21d9132f61de7d1a630",
              "url": "https://github.com/meta-llama/codellama/commit/d2b38acd3a9c55051de1f21d9132f61de7d1a630",
              "message": "README update (#111)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -84,8 +84,9 @@ Pretrained infilling models are: the Code Llama models `CodeLlama-7b` and `CodeL\n \n ### Fine-tuned Instruction Models\n \n-Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L212)\n+Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L279-L366)\n needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and linebreaks in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n+You can use `chat_completion` directly to generate answers with the instruct model. \n \n You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/src/llama_recipes/inference/safety_utils.py) of how to add a safety checker to the inputs and outputs of your inference code."
            },
            {
              "sha": "427d6ac90f0b7db206bc4c62f4c5d38f92ca4d10",
              "url": "https://github.com/meta-llama/codellama/commit/427d6ac90f0b7db206bc4c62f4c5d38f92ca4d10",
              "message": "Initial commit",
              "files_changed": [
                {
                  "filename": ".circleci/config.yaml",
                  "status": "added"
                },
                {
                  "filename": ".gitignore",
                  "status": "added"
                },
                {
                  "filename": "CODE_OF_CONDUCT.md",
                  "status": "added"
                },
                {
                  "filename": "CONTRIBUTING.md",
                  "status": "added"
                },
                {
                  "filename": "LICENSE",
                  "status": "added"
                },
                {
                  "filename": "MODEL_CARD.md",
                  "status": "added"
                },
                {
                  "filename": "README.md",
                  "status": "added"
                },
                {
                  "filename": "USE_POLICY.md",
                  "status": "added"
                },
                {
                  "filename": "dev-requirements.txt",
                  "status": "added"
                },
                {
                  "filename": "download.sh",
                  "status": "added"
                },
                {
                  "filename": "example_completion.py",
                  "status": "added"
                },
                {
                  "filename": "example_infilling.py",
                  "status": "added"
                },
                {
                  "filename": "example_instructions.py",
                  "status": "added"
                },
                {
                  "filename": "llama/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "llama/generation.py",
                  "status": "added"
                },
                {
                  "filename": "llama/model.py",
                  "status": "added"
                },
                {
                  "filename": "llama/tokenizer.py",
                  "status": "added"
                },
                {
                  "filename": "requirements.txt",
                  "status": "added"
                },
                {
                  "filename": "setup.py",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .circleci/config.yaml ---\n@@ -0,0 +1,45 @@\n+version: 2.1\n+\n+jobs:\n+  check_python_format:\n+    docker:\n+      - image: circleci/python:3.9\n+    steps:\n+      - checkout\n+      - run:\n+          name: \"Check format of .py with ufmt\"\n+          command: |\n+            pip install black==22.12.0\n+            pip install usort==1.0.5\n+            pip install ufmt==2.0.1\n+            ufmt check .\n+  check_type_annotation:\n+    docker:\n+      - image: circleci/python:3.9\n+    steps:\n+      - checkout\n+      - restore_cache:\n+          keys:\n+            - v1-dependencies-{{ checksum \"requirements.txt\" }}-{{ checksum \"dev-requirements.txt\" }}\n+      - run:\n+          name: install dependencies\n+          command: |\n+            python3 -m venv venv\n+            . venv/bin/activate\n+            pip install -r requirements.txt\n+            pip install -r dev-requirements.txt\n+      - run:\n+          name: \"mypy\"\n+          command: |\n+            . venv/bin/activate\n+            mkdir .mypy_cache\n+            mypy --install-types --non-interactive ./ --cache-dir=.mypy_cache/\n+      - save_cache:\n+          paths:\n+            - ./venv\n+          key: v1-dependencies-{{ checksum \"requirements.txt\" }}-{{ checksum \"dev-requirements.txt\" }}\n+\n+workflows:\n+  frontend:\n+    jobs:\n+      - check_python_format\n\\ No newline at end of file\n\n--- File: .gitignore ---\n@@ -0,0 +1,160 @@\n+# Byte-compiled / optimized / DLL files\n+__pycache__/\n+*.py[cod]\n+*$py.class\n+\n+# C extensions\n+*.so\n+\n+# Distribution / packaging\n+.Python\n+build/\n+develop-eggs/\n+dist/\n+downloads/\n+eggs/\n+.eggs/\n+lib/\n+lib64/\n+parts/\n+sdist/\n+var/\n+wheels/\n+share/python-wheels/\n+*.egg-info/\n+.installed.cfg\n+*.egg\n+MANIFEST\n+\n+# PyInstaller\n+#  Usually these files are written by a python script from a template\n+#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n+*.manifest\n+*.spec\n+\n+# Installer logs\n+pip-log.txt\n+pip-delete-this-directory.txt\n+\n+# Unit test / coverage reports\n+htmlcov/\n+.tox/\n+.nox/\n+.coverage\n+.coverage.*\n+.cache\n+nosetests.xml\n+coverage.xml\n+*.cover\n+*.py,cover\n+.hypothesis/\n+.pytest_cache/\n+cover/\n+\n+# Translations\n+*.mo\n+*.pot\n+\n+# Django stuff:\n+*.log\n+local_settings.py\n+db.sqlite3\n+db.sqlite3-journal\n+\n+# Flask stuff:\n+instance/\n+.webassets-cache\n+\n+# Scrapy stuff:\n+.scrapy\n+\n+# Sphinx documentation\n+docs/_build/\n+\n+# PyBuilder\n+.pybuilder/\n+target/\n+\n+# Jupyter Notebook\n+.ipynb_checkpoints\n+\n+# IPython\n+profile_default/\n+ipython_config.py\n+\n+# pyenv\n+#   For a library or package, you might want to ignore these files since the code is\n+#   intended to run in multiple environments; otherwise, check them in:\n+# .python-version\n+\n+# pipenv\n+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n+#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n+#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n+#   install all needed dependencies.\n+#Pipfile.lock\n+\n+# poetry\n+#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n+#   This is especially recommended for binary packages to ensure reproducibility, and is more\n+#   commonly ignored for libraries.\n+#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n+#poetry.lock\n+\n+# pdm\n+#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n+#pdm.lock\n+#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n+#   in version control.\n+#   https://pdm.fming.dev/#use-with-ide\n+.pdm.toml\n+\n+# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n+__pypackages__/\n+\n+# Celery stuff\n+celerybeat-schedule\n+celerybeat.pid\n+\n+# SageMath parsed files\n+*.sage.py\n+\n+# Environments\n+.env\n+.venv\n+env/\n+venv/\n+ENV/\n+env.bak/\n+venv.bak/\n+\n+# Spyder project settings\n+.spyderproject\n+.spyproject\n+\n+# Rope project settings\n+.ropeproject\n+\n+# mkdocs documentation\n+/site\n+\n+# mypy\n+.mypy_cache/\n+.dmypy.json\n+dmypy.json\n+\n+# Pyre type checker\n+.pyre/\n+\n+# pytype static type analyzer\n+.pytype/\n+\n+# Cython debug symbols\n+cython_debug/\n+\n+# PyCharm\n+#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n+#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n+#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n+#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n+#.idea/\n\\ No newline at end of file\n\n--- File: CODE_OF_CONDUCT.md ---\n@@ -0,0 +1,80 @@\n+# Code of Conduct\n+\n+## Our Pledge\n+\n+In the interest of fostering an open and welcoming environment, we as\n+contributors and maintainers pledge to make participation in our project and\n+our community a harassment-free experience for everyone, regardless of age, body\n+size, disability, ethnicity, sex characteristics, gender identity and expression,\n+level of experience, education, socio-economic status, nationality, personal\n+appearance, race, religion, or sexual identity and orientation.\n+\n+## Our Standards\n+\n+Examples of behavior that contributes to creating a positive environment\n+include:\n+\n+* Using welcoming and inclusive language\n+* Being respectful of differing viewpoints and experiences\n+* Gracefully accepting constructive criticism\n+* Focusing on what is best for the community\n+* Showing empathy towards other community members\n+\n+Examples of unacceptable behavior by participants include:\n+\n+* The use of sexualized language or imagery and unwelcome sexual attention or\n+advances\n+* Trolling, insulting/derogatory comments, and personal or political attacks\n+* Public or private harassment\n+* Publishing others' private information, such as a physical or electronic\n+address, without explicit permission\n+* Other conduct which could reasonably be considered inappropriate in a\n+professional setting\n+\n+## Our Responsibilities\n+\n+Project maintainers are responsible for clarifying the standards of acceptable\n+behavior and are expected to take appropriate and fair corrective action in\n+response to any instances of unacceptable behavior.\n+\n+Project maintainers have the right and responsibility to remove, edit, or\n+reject comments, commits, code, wiki edits, issues, and other contributions\n+that are not aligned to this Code of Conduct, or to ban temporarily or\n+permanently any contributor for other behaviors that they deem inappropriate,\n+threatening, offensive, or harmful.\n+\n+## Scope\n+\n+This Code of Conduct applies within all project spaces, and it also applies when\n+an individual is representing the project or its community in public spaces.\n+Examples of representing a project or community include using an official\n+project e-mail address, posting via an official social media account, or acting\n+as an appointed representative at an online or offline event. Representation of\n+a project may be further defined and clarified by project maintainers.\n+\n+This Code of Conduct also applies outside the project spaces when there is a\n+reasonable belief that an individual's behavior may have a negative impact on\n+the project or its community.\n+\n+## Enforcement\n+\n+Instances of abusive, harassing, or otherwise unacceptable behavior may be\n+reported by contacting the project team at <opensource-conduct@meta.com>. All\n+complaints will be reviewed and investigated and will result in a response that\n+is deemed necessary and appropriate to the circumstances. The project team is\n+obligated to maintain confidentiality with regard to the reporter of an incident.\n+Further details of specific enforcement policies may be posted separately.\n+\n+Project maintainers who do not follow or enforce the Code of Conduct in good\n+faith may face temporary or permanent repercussions as determined by other\n+members of the project's leadership.\n+\n+## Attribution\n+\n+This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\n+available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n+\n+[homepage]: https://www.contributor-covenant.org\n+\n+For answers to common questions about this code of conduct, see\n+https://www.contributor-covenant.org/faq\n\\ No newline at end of file\n\n--- File: CONTRIBUTING.md ---\n@@ -0,0 +1,32 @@\n+# Contributing to Code Llama\n+We want to make contributing to this project as easy and transparent as\n+possible.\n+\n+## Pull Requests\n+We actively welcome your pull requests to address bugs or inconsistencies but\n+are not planning to add new features to the existing implementation.\n+\n+1. Fork the repo and create your branch from `main`.\n+2. If you've added code that should be tested, add tests.\n+3. If you've changed APIs, update the documentation.\n+4. Ensure the test suite passes.\n+5. Make sure your code lints.\n+6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n+\n+## Contributor License Agreement (\"CLA\")\n+In order to accept your pull request, we need you to submit a CLA. You only need\n+to do this once to work on any of Meta's open source projects.\n+\n+Complete your CLA here: <https://code.facebook.com/cla>\n+\n+## Issues\n+We use GitHub issues to track public bugs. Please ensure your description is\n+clear and has sufficient instructions to be able to reproduce the issue.\n+\n+Meta has a [bounty program](https://www.facebook.com/whitehat/) for the safe\n+disclosure of security bugs. In those cases, please go through the process\n+outlined on that page and do not file a public issue.\n+\n+## License\n+By contributing to Code Llama, you agree that your contributions will be\n+licensed under the LICENSE file in the root directory of this source tree.\n\n--- File: LICENSE ---\n@@ -0,0 +1 @@\n+Please refer to license: https://github.com/facebookresearch/llama/blob/main/LICENSE\n\n--- File: MODEL_CARD.md ---\n@@ -0,0 +1,52 @@\n+# Code Llama\n+\n+## **Model Details**\n+\n+**Model Developers** Meta AI \n+\n+**Variations** Code Llama comes in three model sizes, and three variants: \n+1) Code Llama: our base models designed for general code synthesis and understanding\n+2) Code Llama - Python: designed specifically for Python \n+3) Code Llama - Instruct: for instruction following and safer deployment \n+ \n+All variants are available in sizes of 7B, 13B and 34B parameters.\n+\n+**Input** Models input text only.\n+\n+**Output** Models output text only.\n+\n+**Model Architecture** Code Llama and its variants are autoregressive language models using optimized transformer architectures. Code Llama 7B and 13B additionally support infilling text generation. All models were fine-tuned with up to 16K tokens, and support up to 100K tokens at inference time.\n+\n+**Model Dates** Code Llama and its variants have been trained between January 2023 and July 2023.\n+\n+**Status** This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released  as we improve model safety with community feedback.\n+\n+**Licence** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/). \n+\n+**Research Paper** More information can be found in the paper \"[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\".\n+\n+**Where to send comments** Instructions on how to provide feedback or comments on the model can be found in the model [README](README.md), or by opening an issue in the GitHub repository ([https://github.com/facebookresearch/codellama/](https://github.com/facebookresearch/codellama/)).\n+\n+## **Intended Use**\n+**Intended Use Cases** Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\n+\n+**Out-of-Scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n+\n+## **Hardware and Software**\n+**Training Factors**\n+We used custom training libraries. The training and fine-tuning of the released models have been performed Metas Research Super Cluster.\n+\n+**Carbon Footprint** In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Metas sustainability program.\n+\n+**Training data**\n+All experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) for details).\n+Code Llama - Instruct uses additional instruction fine-tuning data.\n+\n+**Evaluation Results**\n+See evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\n+\n+## **Ethical Considerations and Limitations**\n+Code Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llamas potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\n+\n+Please see the Responsible Use Guide available available at [https://ai.meta.com/llama/responsible-user-guide](https://ai.meta.com/llama/responsible-user-guide).\n+\n\n--- File: README.md ---\n@@ -0,0 +1,116 @@\n+# Introducing Code Llama\n+\n+Code Llama is a family of large language models for code based on [Llama 2](https://github.com/facebookresearch/llama) providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model training, architecture and parameters, evaluations, responsible AI and safety refer to  our [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/). Output generated by code generation features of the Llama Materials, including Code Llama, may be subject to third party licenses, including, without limitation, open source licenses.\n+\n+We are unlocking the power of large language models and our latest version of Code Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. This release includes model weights and starting code for pretrained and fine-tuned Llama language models  ranging from 7B to 34B parameters.\n+\n+This repository is intended as a minimal example to load [Code Llama](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) models and run inference.\n+\n+\n+[comment]: <> (Code Llama models are compatible with the scripts in llama-recipes)\n+\n+\n+## Download\n+\n+In order to download the model weights and tokenizers, please visit the [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n+\n+Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, **do not use the 'Copy link address' option** when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.\n+\n+Pre-requisites: make sure you have `wget` and `md5sum` installed. Then to run the script: `bash download.sh`.\n+\n+Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as `403: Forbidden`, you can always re-request a link.\n+\n+[comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n+\n+## Setup\n+\n+In a conda env with PyTorch / CUDA available, clone the repo and run in the top-level directory:\n+\n+```\n+pip install -e .\n+```\n+\n+## Inference\n+\n+Different models require different model-parallel (MP) values:\n+\n+|  Model | MP |\n+|--------|----|\n+| 7B     | 1  |\n+| 13B    | 2  |\n+| 34B    | 4  |\n+\n+All models support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware and use-case.\n+\n+### Pretrained Code Models\n+\n+The Code Llama and Code Llama - Python models are not fine-tuned to follow instructions. They should be prompted so that the expected answer is the natural continuation of the prompt.\n+\n+\n+See `example_completion.py` for some examples. To illustrate, see command below to run it with the `CodeLlama-7b` model (`nproc_per_node` needs to be set to the `MP` value):\n+\n+```\n+torchrun --nproc_per_node 1 example_code_completion.py \\\n+    --ckpt_dir CodeLlama-7b/ \\\n+    --tokenizer_path CodeLlama-7b/tokenizer.model \\\n+    --max_seq_len 128 --max_batch_size 4\n+```\n+\n+Pretrained code models are: the Code Llama models `CodeLlama-7b`, `CodeLlama-13b`, `CodeLlama-34b` and the Code Llama - Python models \n+`CodeLlama-7b-Python`, `CodeLlama-13b-Python`, `CodeLlama-34b-Python`.\n+\n+### Code Infilling\n+\n+Code Llama and Code Llama - Instruct 7B and 13B models are capable of filling in code given the surrounding context.\n+\n+\n+See `example_infilling.py` for some examples. The `CodeLlama-7b` model can be run for infilling with the command below (`nproc_per_node` needs to be set to the `MP` value):\n+```\n+torchrun --nproc_per_node 1 example_text_infilling.py \\\n+    --ckpt_dir CodeLlama-7b/ \\\n+    --tokenizer_path CodeLlama-7b/tokenizer.model \\\n+    --max_seq_len 192 --max_batch_size 4\n+```\n+\n+Pretrained infilling models are: the Code Llama models `CodeLlama-7b` and `CodeLlama-13b` and the Code Llama - Instruct models `CodeLlama-7b-Instruct`, `CodeLlama-13b-Instruct`.\n+\n+### Fine-tuned Instruction Models\n+\n+Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L212)\n+needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and linebreaks in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n+\n+You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/inference/inference.py) of how to add a safety checker to the inputs and outputs of your inference code.\n+\n+Examples using `CodeLlama-7b-Instruct`:\n+\n+```\n+torchrun --nproc_per_node 1 example_instructions.py \\\n+    --ckpt_dir CodeLlama-7b-Instruct/ \\\n+    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \\\n+    --max_seq_len 512 --max_batch_size 4\n+```\n+\n+Fine-tuned instruction-following models are: the Code Llama - Instruct models `CodeLlama-7b-Instruct`, `CodeLlama-13b-Instruct`, `CodeLlama-34b-Instruct`.\n+\n+Code Llama is a new technology that carries potential risks with use. Testing conducted to date has not  and could not  cover all scenarios.\n+In order to help developers address these risks, we have created the [Responsible Use Guide](https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf). More details can be found in our research papers as well.\n+\n+## Issues\n+Please report any software bug, or other problems with the models through one of the following means:\n+- Reporting issues with the model: [github.com/facebookresearch/codellama](http://github.com/facebookresearch/codellama)\n+- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n+- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n+\n+## Model Card\n+See [MODEL_CARD.md](MODEL_CARD.md) for the model card of Code Llama.\n+\n+## License\n+\n+Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.\n+\n+See the [LICENSE](https://github.com/facebookresearch/llama/blob/main/LICENSE) file, as well as our accompanying [Acceptable Use Policy](https://github.com/facebookresearch/llama/blob/main/USE_POLICY.md)\n+\n+## References\n+\n+1. [Code Llama Research Paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\n+2. [Code Llama Blog Post](https://ai.meta.com/blog/code-llama-large-language-model-coding/)\n\n--- File: USE_POLICY.md ---\n@@ -0,0 +1 @@\n+Please refer to acceptable use policy: https://github.com/facebookresearch/llama/blob/main/USE_POLICY.md\n\n--- File: dev-requirements.txt ---\n@@ -0,0 +1,4 @@\n+black==22.12.0\n+usort==1.0.5\n+ufmt==2.0.1\n+mypy==1.3.0\n\n--- File: download.sh ---\n@@ -0,0 +1,61 @@\n+#!/bin/bash\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+read -p \"Enter the URL from email: \" PRESIGNED_URL\n+echo \"\"\n+ALL_MODELS=\"7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct\"\n+read -p \"Enter the list of models to download without spaces ($ALL_MODELS), or press Enter for all: \" MODEL_SIZE\n+TARGET_FOLDER=\".\"             # where all files should end up\n+mkdir -p ${TARGET_FOLDER}\n+\n+if [[ $MODEL_SIZE == \"\" ]]; then\n+    MODEL_SIZE=$ALL_MODELS\n+fi\n+\n+echo \"Downloading LICENSE and Acceptable Usage Policy\"\n+wget ${PRESIGNED_URL/'*'/\"LICENSE\"} -O ${TARGET_FOLDER}\"/LICENSE\"\n+wget ${PRESIGNED_URL/'*'/\"USE_POLICY.md\"} -O ${TARGET_FOLDER}\"/USE_POLICY.md\"\n+\n+for m in ${MODEL_SIZE//,/ }\n+do\n+    case $m in\n+      7b)\n+        SHARD=0 ;;\n+      13b)\n+        SHARD=1 ;;\n+      34b)\n+        SHARD=3 ;;\n+      7b-Python)\n+        SHARD=0 ;;\n+      13b-Python)\n+        SHARD=1 ;;\n+      34b-Python)\n+        SHARD=3 ;;\n+      7b-Instruct)\n+        SHARD=0 ;;\n+      13b-Instruct)\n+        SHARD=1 ;;\n+      34b-Instruct)\n+        SHARD=3 ;;\n+      *)\n+        echo \"Unknown model: $m\"\n+        exit 1\n+    esac\n+\n+    MODEL_PATH=\"CodeLlama-$m\"\n+    echo \"Downloading ${MODEL_PATH}\"\n+    mkdir -p ${TARGET_FOLDER}\"/${MODEL_PATH}\"\n+\n+    for s in $(seq -f \"0%g\" 0 ${SHARD})\n+    do\n+        wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n+    done\n+\n+    wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/params.json\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/params.json\"\n+    wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/tokenizer.model\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/tokenizer.model\"\n+    wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/checklist.chk\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/checklist.chk\"\n+    echo \"Checking checksums\"\n+    (cd ${TARGET_FOLDER}\"/${MODEL_PATH}\" && md5sum -c checklist.chk)\n+done\n\n--- File: example_completion.py ---\n@@ -0,0 +1,56 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+from typing import Optional\n+\n+import fire\n+\n+from llama import Llama\n+\n+\n+def main(\n+    ckpt_dir: str,\n+    tokenizer_path: str,\n+    temperature: float = 0.2,\n+    top_p: float = 0.9,\n+    max_seq_len: int = 256,\n+    max_batch_size: int = 4,\n+    max_gen_len: Optional[int] = None,\n+):\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+    )\n+\n+    prompts = [\n+        # For these prompts, the expected answer is the natural continuation of the prompt\n+        \"\"\"\\\n+import socket\n+\n+def ping_exponential_backoff(host: str):\"\"\",\n+        \"\"\"\\\n+import argparse\n+\n+def main(string: str):\n+    print(string)\n+    print(string[::-1])\n+\n+if __name__ == \"__main__\":\"\"\"\n+    ]\n+    results = generator.text_completion(\n+        prompts,\n+        max_gen_len=max_gen_len,\n+        temperature=temperature,\n+        top_p=top_p,\n+    )\n+    for prompt, result in zip(prompts, results):\n+        print(prompt)\n+        print(f\"> {result['generation']}\")\n+        print(\"\\n==================================\\n\")\n+\n+\n+if __name__ == \"__main__\":\n+    fire.Fire(main)\n+\n\n--- File: example_infilling.py ---\n@@ -0,0 +1,79 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+import fire\n+\n+from llama import Llama\n+\n+\n+def main(\n+    ckpt_dir: str,\n+    tokenizer_path: str,\n+    temperature: float = 0.0,\n+    top_p: float = 0.9,\n+    max_seq_len: int = 192,\n+    max_gen_len: int = 128,\n+    max_batch_size: int = 4,\n+):\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+    )\n+\n+    prompts = [\n+        '''def remove_non_ascii(s: str) -> str:\n+    \"\"\" <FILL>\n+    return result\n+''',\n+        \"\"\"# Installation instructions:\n+    ```bash\n+<FILL>\n+    ```\n+This downloads the LLaMA inference code and installs the repository as a local pip package.\n+\"\"\",\n+        \"\"\"class InterfaceManagerFactory(AbstractManagerFactory):\n+    def __init__(<FILL>\n+def main():\n+    factory = InterfaceManagerFactory(start=datetime.now())\n+    managers = []\n+    for i in range(10):\n+        managers.append(factory.build(id=i))\n+\"\"\",\n+        \"\"\"/-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\n+theorem connected_iff_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\n+   P = 0  <FILL> = 0 :=\n+begin\n+  split,\n+  { intros h f,\n+    rw pi_1_etalisation at h,\n+    simp [h],\n+    refl\n+  },\n+  { intro h,\n+    have := @quasi_adjoint C D P,\n+    simp [pi_1_etalisation, this, h],\n+    refl\n+  }\n+end\n+\"\"\",\n+    ]\n+    prefixes = [p.split(\"<FILL>\")[0] for p in prompts]\n+    suffixes = [p.split(\"<FILL>\")[1] for p in prompts]\n+    results = generator.text_infilling(\n+        prefixes=prefixes,\n+        suffixes=suffixes,\n+        max_gen_len=max_gen_len,\n+        temperature=temperature,\n+        top_p=top_p,\n+    )\n+    for prompt, result in zip(prompts, results):\n+        print(\"\\n================= Prompt text =================\\n\")\n+        print(prompt)\n+        print(\"\\n================= Filled text =================\\n\")\n+        print(result[\"full_text\"])\n+\n+\n+if __name__ == \"__main__\":\n+    fire.Fire(main)\n\n--- File: example_instructions.py ---\n@@ -0,0 +1,68 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+from typing import Optional\n+\n+import fire\n+\n+from llama import Llama\n+\n+\n+def main(\n+    ckpt_dir: str,\n+    tokenizer_path: str,\n+    temperature: float = 0.2,\n+    top_p: float = 0.95,\n+    max_seq_len: int = 512,\n+    max_batch_size: int = 8,\n+    max_gen_len: Optional[int] = None,\n+):\n+    generator = Llama.build(\n+        ckpt_dir=ckpt_dir,\n+        tokenizer_path=tokenizer_path,\n+        max_seq_len=max_seq_len,\n+        max_batch_size=max_batch_size,\n+    )\n+\n+    instructions = [\n+        [\n+            {\n+                \"role\": \"user\",\n+                \"content\": \"In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?\",\n+            }\n+        ],\n+        [\n+            {\n+                \"role\": \"user\",\n+                \"content\": \"What is the difference between inorder and preorder traversal? Give an example in Python.\",\n+            }\n+        ],\n+        [\n+            {\n+                \"role\": \"system\",\n+                \"content\": \"Provide answers in JavaScript\",\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\",\n+            }\n+        ],\n+    ]\n+    results = generator.chat_completion(\n+        instructions,  # type: ignore\n+        max_gen_len=max_gen_len,\n+        temperature=temperature,\n+        top_p=top_p,\n+    )\n+\n+    for instruction, result in zip(instructions, results):\n+        for msg in instruction:\n+            print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n+        print(\n+            f\"> {result['generation']['role'].capitalize()}: {result['generation']['content']}\"\n+        )\n+        print(\"\\n==================================\\n\")\n+\n+\n+if __name__ == \"__main__\":\n+    fire.Fire(main)\n\n--- File: llama/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+from .generation import Llama\n+from .model import ModelArgs, Transformer\n+from .tokenizer import Tokenizer\n\n--- File: llama/generation.py ---\n@@ -0,0 +1,409 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+import json\n+import os\n+import sys\n+import time\n+from pathlib import Path\n+from typing import List, Literal, Optional, Tuple, TypedDict\n+\n+import torch\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.initialize import (\n+    get_model_parallel_rank,\n+    initialize_model_parallel,\n+    model_parallel_is_initialized,\n+)\n+\n+from llama.model import ModelArgs, Transformer\n+from llama.tokenizer import Tokenizer\n+\n+Role = Literal[\"system\", \"user\", \"assistant\"]\n+\n+\n+class Message(TypedDict):\n+    role: Role\n+    content: str\n+\n+\n+class InfillingPrediction(TypedDict, total=False):\n+    generation: str\n+    full_text: str\n+    tokens: List[str]  # not required\n+    logprobs: List[float]  # not required\n+\n+\n+class CompletionPrediction(TypedDict, total=False):\n+    generation: str\n+    tokens: List[str]  # not required\n+    logprobs: List[float]  # not required\n+\n+\n+class ChatPrediction(TypedDict, total=False):\n+    generation: Message\n+    tokens: List[str]  # not required\n+    logprobs: List[float]  # not required\n+\n+\n+Dialog = List[Message]\n+\n+B_INST, E_INST = \"[INST]\", \"[/INST]\"\n+B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n+\n+SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n+UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\"\n+\n+\n+class Llama:\n+    @staticmethod\n+    def build(\n+        ckpt_dir: str,\n+        tokenizer_path: str,\n+        max_seq_len: int,\n+        max_batch_size: int,\n+        model_parallel_size: Optional[int] = None,\n+    ) -> \"Llama\":\n+        if not torch.distributed.is_initialized():\n+            torch.distributed.init_process_group(\"nccl\")\n+        if not model_parallel_is_initialized():\n+            if model_parallel_size is None:\n+                model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n+            initialize_model_parallel(model_parallel_size)\n+\n+        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n+        torch.cuda.set_device(local_rank)\n+\n+        # seed must be the same in all processes\n+        torch.manual_seed(1)\n+\n+        if local_rank > 0:\n+            sys.stdout = open(os.devnull, \"w\")\n+\n+        start_time = time.time()\n+        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n+        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n+        assert model_parallel_size == len(\n+            checkpoints\n+        ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n+        ckpt_path = checkpoints[get_model_parallel_rank()]\n+        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n+        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n+            params = json.loads(f.read())\n+\n+        model_args: ModelArgs = ModelArgs(\n+            max_seq_len=max_seq_len,\n+            max_batch_size=max_batch_size,\n+            **params,\n+        )\n+        tokenizer = Tokenizer(model_path=tokenizer_path)\n+        model_args.vocab_size = tokenizer.n_words\n+        if torch.cuda.is_bf16_supported():\n+            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+        else:\n+            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n+        model = Transformer(model_args)\n+        model.load_state_dict(checkpoint, strict=False)\n+        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n+\n+        return Llama(model, tokenizer)\n+\n+    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n+        self.model = model\n+        self.tokenizer = tokenizer\n+\n+    @torch.inference_mode()\n+    def generate(\n+        self,\n+        prompt_tokens: List[List[int]],\n+        max_gen_len: int,\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        logprobs: bool = False,\n+        echo: bool = False,\n+        stop_token: Optional[int] = None,\n+    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n+        if stop_token is None:\n+            stop_token = self.tokenizer.eos_id\n+        params = self.model.params\n+        bsz = len(prompt_tokens)\n+        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n+\n+        min_prompt_len = min(len(t) for t in prompt_tokens)\n+        max_prompt_len = max(len(t) for t in prompt_tokens)\n+        assert max_prompt_len <= params.max_seq_len\n+        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n+\n+        pad_id = self.tokenizer.pad_id\n+        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n+        for k, t in enumerate(prompt_tokens):\n+            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n+        if logprobs:\n+            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n+\n+        prev_pos = 0\n+        stop_reached = torch.tensor([False] * bsz, device=\"cuda\")\n+        input_text_mask = tokens != pad_id\n+        for cur_pos in range(min_prompt_len, total_len):\n+            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n+            if logprobs:\n+                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n+                    input=logits.transpose(1, 2),\n+                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n+                    reduction=\"none\",\n+                    ignore_index=pad_id,\n+                )\n+            if temperature > 0:\n+                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n+                next_token = sample_top_p(probs, top_p)\n+            else:\n+                next_token = torch.argmax(logits[:, -1], dim=-1)\n+\n+            next_token = next_token.reshape(-1)\n+            # only replace token if prompt has already been generated\n+            next_token = torch.where(\n+                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n+            )\n+            tokens[:, cur_pos] = next_token\n+            stop_reached |= (~input_text_mask[:, cur_pos]) & (next_token == stop_token)\n+            prev_pos = cur_pos\n+            if all(stop_reached):\n+                break\n+\n+        if logprobs:\n+            token_logprobs = token_logprobs.tolist()\n+        out_tokens, out_logprobs = [], []\n+        for i, toks in enumerate(tokens.tolist()):\n+            # cut to max gen len\n+            start = 0 if echo else len(prompt_tokens[i])\n+            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n+            probs = None\n+            if logprobs:\n+                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n+            # cut to stop token if present\n+            if stop_token in toks:\n+                stop_idx = toks.index(stop_token)\n+                toks = toks[:stop_idx]\n+                probs = probs[:stop_idx] if logprobs else None\n+            out_tokens.append(toks)\n+            out_logprobs.append(probs)\n+        return (out_tokens, out_logprobs if logprobs else None)\n+\n+    def text_completion(\n+        self,\n+        prompts: List[str],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+        echo: bool = False,\n+    ) -> List[CompletionPrediction]:\n+        if max_gen_len is None:\n+            max_gen_len = self.model.params.max_seq_len - 1\n+        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n+        generation_tokens, generation_logprobs = self.generate(\n+            prompt_tokens=prompt_tokens,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=logprobs,\n+            echo=echo,\n+        )\n+        if logprobs:\n+            return [\n+                {\n+                    \"generation\": self.tokenizer.decode(t),\n+                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n+                    \"logprobs\": logprobs_i,\n+                }\n+                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n+            ]\n+        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n+\n+    def text_infilling(\n+        self,\n+        prefixes: List[str],\n+        suffixes: List[str],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+        suffix_first: bool = False,\n+    ) -> List[InfillingPrediction]:\n+        assert self.tokenizer.eot_id is not None\n+        if max_gen_len is None:\n+            max_gen_len = self.model.params.max_seq_len - 1\n+        prompt_tokens = [\n+            infilling_prompt_tokens(\n+                self.tokenizer, prefix, suffix, suffix_first=suffix_first\n+            )\n+            for prefix, suffix in zip(prefixes, suffixes)\n+        ]\n+        generation_tokens, generation_logprobs = self.generate(\n+            prompt_tokens=prompt_tokens,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=logprobs,\n+            echo=False,\n+            stop_token=self.tokenizer.eot_id,\n+        )\n+\n+        generations = [self.tokenizer.decode_infilling(t) for t in generation_tokens]\n+\n+        if logprobs:\n+            return [\n+                {\n+                    \"generation\": generation,\n+                    \"logprobs\": logprobs_i,\n+                    \"tokens\": t,\n+                    \"full_text\": prefix + generation + suffix,\n+                }\n+                for prefix, suffix, generation, t, logprobs_i in zip(\n+                    prefixes,\n+                    suffixes,\n+                    generations,\n+                    generation_tokens,\n+                    generation_logprobs,\n+                )\n+            ]\n+        else:\n+            return [\n+                {\n+                    \"generation\": generation,\n+                    \"full_text\": prefix + generation + suffix,\n+                }\n+                for prefix, suffix, generation in zip(prefixes, suffixes, generations)\n+            ]\n+\n+    def chat_completion(\n+        self,\n+        dialogs: List[Dialog],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+    ) -> List[ChatPrediction]:\n+        if max_gen_len is None:\n+            max_gen_len = self.model.params.max_seq_len - 1\n+        prompt_tokens = []\n+        unsafe_requests = []\n+        for dialog in dialogs:\n+            unsafe_requests.append(\n+                any([tag in msg[\"content\"] for tag in SPECIAL_TAGS for msg in dialog])\n+            )\n+            if dialog[0][\"role\"] == \"system\":\n+                dialog = [\n+                    {\n+                        \"role\": dialog[1][\"role\"],\n+                        \"content\": B_SYS\n+                        + dialog[0][\"content\"]\n+                        + E_SYS\n+                        + dialog[1][\"content\"],\n+                    }\n+                ] + dialog[2:]\n+            assert all([msg[\"role\"] == \"user\" for msg in dialog[::2]]) and all(\n+                [msg[\"role\"] == \"assistant\" for msg in dialog[1::2]]\n+            ), (\n+                \"model only supports 'system', 'user' and 'assistant' roles, \"\n+                \"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\"\n+            )\n+            dialog_tokens: List[int] = sum(\n+                [\n+                    self.tokenizer.encode(\n+                        f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \",\n+                        bos=True,\n+                        eos=True,\n+                    )\n+                    for prompt, answer in zip(\n+                        dialog[::2],\n+                        dialog[1::2],\n+                    )\n+                ],\n+                [],\n+            )\n+            assert (\n+                dialog[-1][\"role\"] == \"user\"\n+            ), f\"Last message must be from user, got {dialog[-1]['role']}\"\n+            dialog_tokens += self.tokenizer.encode(\n+                f\"{B_INST} {(dialog[-1]['content']).strip()} {E_INST}\",\n+                bos=True,\n+                eos=False,\n+            )\n+            prompt_tokens.append(dialog_tokens)\n+\n+        generation_tokens, generation_logprobs = self.generate(\n+            prompt_tokens=prompt_tokens,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=logprobs,\n+        )\n+        if logprobs:\n+            return [\n+                {\n+                    \"generation\": {\n+                        \"role\": \"assistant\",\n+                        \"content\": self.tokenizer.decode(t)\n+                        if not unsafe\n+                        else UNSAFE_ERROR,\n+                    },\n+                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n+                    \"logprobs\": logprobs_i,\n+                }\n+                for t, logprobs_i, unsafe in zip(\n+                    generation_tokens, generation_logprobs, unsafe_requests\n+                )\n+            ]\n+        return [\n+            {\n+                \"generation\": {\n+                    \"role\": \"assistant\",\n+                    \"content\": self.tokenizer.decode(t) if not unsafe else UNSAFE_ERROR,\n+                }\n+            }\n+            for t, unsafe in zip(generation_tokens, unsafe_requests)\n+        ]\n+\n+\n+def sample_top_p(probs, p):\n+    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n+    probs_sum = torch.cumsum(probs_sort, dim=-1)\n+    mask = probs_sum - probs_sort > p\n+    probs_sort[mask] = 0.0\n+    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n+    next_token = torch.multinomial(probs_sort, num_samples=1)\n+    next_token = torch.gather(probs_idx, -1, next_token)\n+    return next_token\n+\n+\n+def infilling_prompt_tokens(\n+    tokenizer: Tokenizer,\n+    pre: str,\n+    suf: str,\n+    suffix_first: bool = False,\n+) -> List[int]:\n+    \"\"\"\n+    Format and encode an infilling problem.\n+    If `suffix_first` is set, format in suffix-prefix-middle format.\n+    \"\"\"\n+    assert tokenizer.prefix_id is not None\n+    assert tokenizer.middle_id is not None\n+    assert tokenizer.suffix_id is not None\n+    if suffix_first:\n+        # format as \"<PRE> <SUF>{suf} <MID> {pre}\"\n+        return (\n+            [tokenizer.bos_id, tokenizer.prefix_id, tokenizer.suffix_id]\n+            + tokenizer.encode_infilling(suf)\n+            + [tokenizer.middle_id]\n+            + tokenizer.encode(pre, bos=False, eos=False)\n+        )\n+    else:\n+        # format as \"<PRE> {pre} <SUF>{suf} <MID>\"\n+        return (\n+            [tokenizer.bos_id, tokenizer.prefix_id]\n+            + tokenizer.encode(pre, bos=False, eos=False)\n+            + [tokenizer.suffix_id]\n+            + tokenizer.encode_infilling(suf)\n+            + [tokenizer.middle_id]\n+        )\n\n--- File: llama/model.py ---\n@@ -0,0 +1,291 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Any, Optional, Tuple\n+\n+import fairscale.nn.model_parallel.initialize as fs_init\n+import torch\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.layers import (\n+    ColumnParallelLinear,\n+    ParallelEmbedding,\n+    RowParallelLinear,\n+)\n+from torch import nn\n+\n+\n+@dataclass\n+class ModelArgs:\n+    dim: int = 4096\n+    n_layers: int = 32\n+    n_heads: int = 32\n+    n_kv_heads: Optional[int] = None\n+    vocab_size: int = -1  # defined later by tokenizer\n+    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n+    ffn_dim_multiplier: Optional[float] = None\n+    norm_eps: float = 1e-5\n+    rope_theta: float = 10000\n+\n+    max_batch_size: int = 32\n+    max_seq_len: int = 2048\n+\n+\n+class RMSNorm(torch.nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.ones(dim))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float()).type_as(x)\n+        return output * self.weight\n+\n+\n+def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n+    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n+    t = torch.arange(end, device=freqs.device, dtype=torch.float32)  # type: ignore\n+    freqs = torch.outer(t, freqs)  # type: ignore\n+    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n+    return freqs_cis\n+\n+\n+def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n+    ndim = x.ndim\n+    assert 0 <= 1 < ndim\n+    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n+    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n+    return freqs_cis.view(*shape)\n+\n+\n+def apply_rotary_emb(\n+    xq: torch.Tensor,\n+    xk: torch.Tensor,\n+    freqs_cis: torch.Tensor,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n+    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n+    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n+    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n+    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n+    return xq_out.type_as(xq), xk_out.type_as(xk)\n+\n+\n+def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n+    bs, slen, n_kv_heads, head_dim = x.shape\n+    if n_rep == 1:\n+        return x\n+    return (\n+        x[:, :, :, None, :]\n+        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n+        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n+    )\n+\n+\n+class Attention(nn.Module):\n+    def __init__(self, args: ModelArgs):\n+        super().__init__()\n+        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n+        model_parallel_size = fs_init.get_model_parallel_world_size()\n+        self.n_local_heads = args.n_heads // model_parallel_size\n+        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n+        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n+        self.head_dim = args.dim // args.n_heads\n+\n+        self.wq = ColumnParallelLinear(\n+            args.dim,\n+            args.n_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wk = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wv = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wo = RowParallelLinear(\n+            args.n_heads * self.head_dim,\n+            args.dim,\n+            bias=False,\n+            input_is_parallel=True,\n+            init_method=lambda x: x,\n+        )\n+\n+        self.cache_k = torch.zeros(\n+            (\n+                args.max_batch_size,\n+                args.max_seq_len,\n+                self.n_local_kv_heads,\n+                self.head_dim,\n+            )\n+        ).cuda()\n+        self.cache_v = torch.zeros(\n+            (\n+                args.max_batch_size,\n+                args.max_seq_len,\n+                self.n_local_kv_heads,\n+                self.head_dim,\n+            )\n+        ).cuda()\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        start_pos: int,\n+        freqs_cis: torch.Tensor,\n+        mask: Optional[torch.Tensor],\n+    ):\n+        bsz, seqlen, _ = x.shape\n+        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n+\n+        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n+        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n+        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n+\n+        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n+\n+        self.cache_k = self.cache_k.to(xq)\n+        self.cache_v = self.cache_v.to(xq)\n+\n+        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n+        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n+\n+        keys = self.cache_k[:bsz, : start_pos + seqlen]\n+        values = self.cache_v[:bsz, : start_pos + seqlen]\n+\n+        # repeat k/v heads if n_kv_heads < n_heads\n+        keys = repeat_kv(keys, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n+        values = repeat_kv(values, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n+\n+        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n+        keys = keys.transpose(1, 2)\n+        values = values.transpose(1, 2)\n+        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n+        if mask is not None:\n+            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n+        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n+        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n+        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n+        return self.wo(output)\n+\n+\n+class FeedForward(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        multiple_of: int,\n+        ffn_dim_multiplier: Optional[float],\n+    ):\n+        super().__init__()\n+        hidden_dim = int(2 * hidden_dim / 3)\n+        # custom dim factor multiplier\n+        if ffn_dim_multiplier is not None:\n+            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n+        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n+\n+        self.w1 = ColumnParallelLinear(\n+            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n+        )\n+        self.w2 = RowParallelLinear(\n+            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n+        )\n+        self.w3 = ColumnParallelLinear(\n+            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n+        )\n+\n+    def forward(self, x):\n+        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n+\n+\n+class TransformerBlock(nn.Module):\n+    def __init__(self, layer_id: int, args: ModelArgs):\n+        super().__init__()\n+        self.n_heads = args.n_heads\n+        self.dim = args.dim\n+        self.head_dim = args.dim // args.n_heads\n+        self.attention = Attention(args)\n+        self.feed_forward = FeedForward(\n+            dim=args.dim,\n+            hidden_dim=4 * args.dim,\n+            multiple_of=args.multiple_of,\n+            ffn_dim_multiplier=args.ffn_dim_multiplier,\n+        )\n+        self.layer_id = layer_id\n+        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        start_pos: int,\n+        freqs_cis: torch.Tensor,\n+        mask: Optional[torch.Tensor],\n+    ):\n+        h = x + self.attention.forward(\n+            self.attention_norm(x), start_pos, freqs_cis, mask\n+        )\n+        out = h + self.feed_forward.forward(self.ffn_norm(h))\n+        return out\n+\n+\n+class Transformer(nn.Module):\n+    def __init__(self, params: ModelArgs):\n+        super().__init__()\n+        self.params = params\n+        self.vocab_size = params.vocab_size\n+        self.n_layers = params.n_layers\n+\n+        self.tok_embeddings = ParallelEmbedding(\n+            params.vocab_size, params.dim, init_method=lambda x: x\n+        )\n+\n+        self.layers = torch.nn.ModuleList()\n+        for layer_id in range(params.n_layers):\n+            self.layers.append(TransformerBlock(layer_id, params))\n+\n+        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n+        self.output = ColumnParallelLinear(\n+            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n+        )\n+\n+        self.freqs_cis = precompute_freqs_cis(\n+            self.params.dim // self.params.n_heads,\n+            self.params.max_seq_len * 2,\n+            params.rope_theta,\n+        )\n+\n+    @torch.inference_mode()\n+    def forward(self, tokens: torch.Tensor, start_pos: int):\n+        _bsz, seqlen = tokens.shape\n+        h = self.tok_embeddings(tokens)\n+        self.freqs_cis = self.freqs_cis.to(h.device)\n+        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n+\n+        mask = None\n+        if seqlen > 1:\n+            mask = torch.full(\n+                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n+            )\n+            mask = mask.to(torch.float32).triu(diagonal=start_pos+1).type_as(h)\n+\n+        for layer in self.layers:\n+            h = layer(h, start_pos, freqs_cis, mask)\n+        h = self.norm(h)\n+        output = self.output(h).float()\n+        return output\n\n--- File: llama/tokenizer.py ---\n@@ -0,0 +1,56 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+import os\n+from logging import getLogger\n+from typing import List, Optional\n+\n+from sentencepiece import SentencePieceProcessor\n+\n+\n+logger = getLogger()\n+\n+\n+class Tokenizer:\n+    def __init__(self, model_path: str):\n+        # reload tokenizer\n+        assert os.path.isfile(model_path), model_path\n+        self.sp_model = SentencePieceProcessor(model_file=model_path)\n+        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n+\n+        # BOS / EOS token IDs\n+        self.n_words: int = self.sp_model.vocab_size()\n+        self.bos_id: int = self.sp_model.bos_id()\n+        self.eos_id: int = self.sp_model.eos_id()\n+        self.pad_id: int = self.sp_model.pad_id()\n+\n+        # token IDs for special infilling tokens\n+        self.prefix_id: Optional[int] = self.sp_model.piece_to_id(\"<PRE>\") or None\n+        self.middle_id: Optional[int] = self.sp_model.piece_to_id(\"<MID>\") or None\n+        self.suffix_id: Optional[int] = self.sp_model.piece_to_id(\"<SUF>\") or None\n+        self.eot_id: Optional[int] = self.sp_model.piece_to_id(\"<EOT>\") or None\n+        logger.info(\n+            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id} \"\n+            f\"- PRE ID: {self.prefix_id} - MID ID: {self.middle_id} - SUF ID: {self.suffix_id} - EOT ID: {self.eot_id}\"\n+        )\n+        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n+\n+    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n+        assert type(s) is str\n+        t = self.sp_model.encode(s)\n+        if bos:\n+            t = [self.bos_id] + t\n+        if eos:\n+            t = t + [self.eos_id]\n+        return t\n+\n+    def decode(self, t: List[int]) -> str:\n+        return self.sp_model.decode(t)\n+\n+    def encode_infilling(self, s: str) -> List[int]:\n+        \"\"\"Encode a string without an implicit leading space.\"\"\"\n+        return self.sp_model.encode(\"\" + s)[2:]\n+\n+    def decode_infilling(self, t: List[int]) -> str:\n+        \"\"\"Decode a string without an implicit leading space.\"\"\"\n+        return self.sp_model.decode([self.sp_model.piece_to_id(\"\")] + t)[1:]\n\n--- File: requirements.txt ---\n@@ -0,0 +1,4 @@\n+torch\n+fairscale\n+fire\n+sentencepiece\n\n--- File: setup.py ---\n@@ -0,0 +1,16 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n+\n+from setuptools import find_packages, setup\n+\n+\n+def get_requirements(path: str):\n+    return [l.strip() for l in open(path)]\n+\n+\n+setup(\n+    name=\"codellama\",\n+    version=\"0.0.1\",\n+    packages=find_packages(),\n+    install_requires=get_requirements(\"requirements.txt\"),\n+)"
            }
          ]
        }
      ]
    },
    {
      "id": 6655849,
      "username": "ConnorHack",
      "url": "https://github.com/ConnorHack",
      "avatar_url": "https://avatars.githubusercontent.com/u/6655849?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "187e44c7e4a9fe2612abcb6ed1487e4bc592fbfb",
              "url": "https://github.com/meta-llama/llama-models/commit/187e44c7e4a9fe2612abcb6ed1487e4bc592fbfb",
              "message": "Update EFS volume restructuring and schedule daily CI/CD workflow run (#255)",
              "files_changed": [
                {
                  "filename": ".github/workflows/gha_workflow_llama_models_tests.yml",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/gha_workflow_llama_models_tests.yml ---\n@@ -1,6 +1,10 @@\n name: \"Run Llama-models Tests\"\n \n on:\n+  # Schedule cron job at 2:30 am EST every day of the week.\n+  # Will run a manual workflow off of 'main' branch.\n+  schedule:\n+    - cron: '30 7 * * *'\n   pull_request_target:\n     types: [\"opened\"]\n     branches:\n@@ -15,8 +19,8 @@ on:\n         required: true\n         default: \"llama-models-gha-runner-gpu\"\n \n-      branch:\n-        description: \"Branch to checkout\"\n+      checkout_reference:\n+        description: \"The branch, tag, or SHA to checkout\"\n         required: true\n         default: \"main\"\n \n@@ -52,9 +56,9 @@ on:\n \n env:\n   TOKENIZER_PATH: \"models/llama3/api/tokenizer.model\"\n-  MODELS_PATH: \"/data/llama3.2\"\n-  VISION_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_vision }}\"\n-  TEXT_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_text }}\"\n+  MODELS_PATH: \"/data/llama\"\n+  VISION_MODEL_CHECKPOINT_DIR: \"/data/llama/${{ inputs.model_vision }}\"\n+  TEXT_MODEL_CHECKPOINT_DIR: \"/data/llama/${{ inputs.model_text }}\"\n   API_KEY: \"${{ inputs.api_key || '' }}\"\n \n jobs:\n@@ -119,7 +123,7 @@ jobs:\n         id: checkout_repo\n         uses: actions/checkout@v4\n         with:\n-          ref: ${{ inputs.branch }}\n+          ref: ${{ inputs.checkout_reference }}\n \n       - name: \"[DEBUG] Content of the repository after checkout\"\n         id: debug_content_after_checkout\n@@ -199,7 +203,7 @@ jobs:\n       - name: \"PR - Upload Test Summary\"\n         id: pr_test_summary_upload\n         if: github.event_name == 'pull_request_target'\n-        uses: actions/upload-artifact@v3\n+        uses: actions/upload-artifact@v4\n         with:\n           name: test-summary\n           path: test-summary.md"
            },
            {
              "sha": "cb807a9e3dd873e36ff12a34fc9850d36d68b936",
              "url": "https://github.com/meta-llama/llama-models/commit/cb807a9e3dd873e36ff12a34fc9850d36d68b936",
              "message": "Explicitly define write permission for PR jobs (#188)",
              "files_changed": [
                {
                  "filename": ".github/workflows/gha_workflow_llama_models_tests.yml",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/gha_workflow_llama_models_tests.yml ---\n@@ -47,19 +47,21 @@ on:\n \n       api_key:\n         description: 'Provider API key'\n-        required: true\n+        required: false\n         default: \"---\"\n \n env: \n   TOKENIZER_PATH: \"models/llama3/api/tokenizer.model\"  \n   MODELS_PATH: \"/data/llama3.2\"\n   VISION_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_vision }}\"\n   TEXT_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_text }}\"\n-  API_KEY: \"${{ inputs.api_key }}\"\n+  API_KEY: \"${{ inputs.api_key || '' }}\"\n \n jobs:\n   execute_workflow:\n     name: Execute workload on Self-Hosted CPU k8s runner\n+    permissions:\n+      pull-requests: write\n     defaults:\n       run:\n         shell: bash # default shell to run all steps for a given job."
            },
            {
              "sha": "6ad6fd6bb8f5fc841acecc2e48958eee25ff3b1c",
              "url": "https://github.com/meta-llama/llama-models/commit/6ad6fd6bb8f5fc841acecc2e48958eee25ff3b1c",
              "message": " Introduce GitHub Actions Workflow for Llama Models Tests (#170)",
              "files_changed": [
                {
                  "filename": ".github/workflows/gha_workflow_llama_models_tests.yml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/gha_workflow_llama_models_tests.yml ---\n@@ -0,0 +1,237 @@\n+name: \"Run Llama-models Tests\"\n+\n+on:\n+  pull_request_target:\n+    types: [\"opened\"]\n+    branches:\n+      - 'main'\n+    paths:\n+      - 'models/**/*.py'\n+\n+  workflow_dispatch:\n+    inputs:\n+      runner:\n+        description: 'GHA Runner Scale Set label to run workflow on.'\n+        required: true\n+        default: \"llama-models-gha-runner-gpu\"\n+\n+      branch:\n+        description: \"Branch to checkout\"\n+        required: true\n+        default: \"main\"\n+\n+      debug:\n+        description: 'Run debugging steps?'\n+        required: false\n+        default: \"true\"\n+\n+      sleep_time:\n+        description: '[DEBUG] sleep time for debugging'\n+        required: true\n+        default: \"0\"\n+\n+      require_model:\n+        description: 'Is a model required?'\n+        required: true\n+        default: \"true\"\n+\n+      model_vision:\n+        description: 'Llama vision model ID'\n+        required: false\n+        default: \"Llama3.2-11B-Vision-Instruct\"\n+\n+      model_text:\n+        description: 'Llama text model ID'\n+        required: false\n+        default: \"Llama3.2-3B-Instruct\"\n+\n+      api_key:\n+        description: 'Provider API key'\n+        required: true\n+        default: \"---\"\n+\n+env: \n+  TOKENIZER_PATH: \"models/llama3/api/tokenizer.model\"  \n+  MODELS_PATH: \"/data/llama3.2\"\n+  VISION_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_vision }}\"\n+  TEXT_MODEL_CHECKPOINT_DIR: \"/data/llama3.2/${{ inputs.model_text }}\"\n+  API_KEY: \"${{ inputs.api_key }}\"\n+\n+jobs:\n+  execute_workflow:\n+    name: Execute workload on Self-Hosted CPU k8s runner\n+    defaults:\n+      run:\n+        shell: bash # default shell to run all steps for a given job.\n+    runs-on: ${{ inputs.runner != '' && inputs.runner || 'llama-models-gha-runner-cpu' }}\n+    if: always()\n+    steps:\n+\n+      ##############################\n+      #### INITIAL DEBUG CHECKS ####\n+      ##############################\n+      - name: \"[DEBUG] Check content of the EFS mount\"\n+        id: debug_efs_volume\n+        continue-on-error: true\n+        if: inputs.debug == 'true'\n+        run: |\n+            echo \"========= Content of the EFS mount =============\"\n+            ls -la ${{ env.MODELS_PATH }}\n+\n+      - name: \"Check if models exist in EFS volume\"\n+        id: check_if_models_exist\n+        if: ${{ inputs.require_model == 'true' }}\n+        run: |\n+          # Check if vision model is provided and exists\n+          if [ -n \"${{ inputs.model_vision }}\" ]; then\n+            if [ ! -d \"${{ env.VISION_MODEL_CHECKPOINT_DIR }}\" ]; then\n+              echo \"Model '${{ inputs.model_vision }}' does not exist in mounted EFS volume, Terminating workflow.\"\n+              exit 1\n+            else\n+              echo \"Content of '${{ inputs.model_vision }}' model\"\n+              ls -la \"${{ env.VISION_MODEL_CHECKPOINT_DIR }}\"\n+            fi\n+          fi\n+\n+          # Check if text model is provided and exists\n+          if [ -n \"${{ inputs.model_text }}\" ]; then\n+            if [ ! -d \"${{ env.TEXT_MODEL_CHECKPOINT_DIR }}\" ]; then\n+              echo \"Model '${{ inputs.model_text }}' does not exist in mounted EFS volume, Terminating workflow.\"\n+              exit 1\n+            else\n+              echo \"Content of '${{ inputs.model_text }}' model\"\n+              ls -la \"${{ env.TEXT_MODEL_CHECKPOINT_DIR }}\"\n+            fi\n+          fi\n+\n+      - name: \"[DEBUG] Get runner container OS information\"\n+        id: debug_os_info\n+        if: ${{ inputs.debug == 'true' }}\n+        run: |\n+            cat /etc/os-release\n+\n+      #######################\n+      #### CODE CHECKOUT ####\n+      #######################\n+      - name: \"Checkout 'meta-llama/llama-models' repository\"\n+        id: checkout_repo\n+        uses: actions/checkout@v4\n+        with:\n+          ref: ${{ inputs.branch }}\n+\n+      - name: \"[DEBUG] Content of the repository after checkout\"\n+        id: debug_content_after_checkout\n+        if: ${{ inputs.debug == 'true' }}\n+        run: |\n+            ls -la ${GITHUB_WORKSPACE}\n+\n+      ##########################################################\n+      ####              OPTIONAL SLEEP DEBUG                ####\n+      #                                                        #\n+      # Use to \"exec\" into the test k8s POD and run tests      #\n+      # manually to identify what dependencies are being used. #\n+      #                                                        #\n+      ##########################################################\n+      - name: \"[DEBUG] sleep\"\n+        id: debug_sleep\n+        if: ${{ inputs.debug == 'true' && inputs.sleep_time != '' }}\n+        run: |\n+            sleep ${{ inputs.sleep_time }}\n+\n+      ##################################\n+      #### DEPENDENCY INSTALLATIONS ####\n+      ##################################\n+      - name: \"Installing 'apt' required packages\"\n+        id: install_apt\n+        run: |\n+          echo \"[STEP] Installing 'apt' required packages\"\n+          sudo apt update -y\n+          sudo apt upgrade -y\n+          sudo apt install python3-pip -y\n+\n+      - name: \"Installing 'llama-models' dependencies\"\n+        id: install_pip_generic\n+        run: |\n+          echo \"[STEP] Installing 'llama-models' models\"\n+          pip install -U pip setuptools\n+          pip install -r requirements.txt\n+          pip install blobfile\n+          pip install llama-models\n+          pip install xmlrunner\n+          pip install pytest\n+\n+      - name: \"Installing specific manual_dispatch dependencies\"\n+        id: manual_install_pip\n+        if: github.event_name == 'workflow_dispatch'\n+        run: |\n+          echo \"[STEP] Installing specific dependencies for manual dispatch workflows\"\n+          pip install numpy\n+          pip install torch\n+          pip install fairscale\n+          pip install termcolor\n+          pip install torchvision\n+\n+      ############################################\n+      #### AUTOMATIC TESTING ON PULL REQUESTS ####\n+      ############################################\n+\n+      #### Run tests ####\n+      \n+      - name: \"PR - Run Tests\"\n+        id: pr_run_tests\n+        working-directory: \"${{ github.workspace }}\"\n+        if: github.event_name == 'pull_request_target'\n+        run: |\n+          echo \"[STEP] Running PyTest tests at 'GITHUB_WORKSPACE' path: ${GITHUB_WORKSPACE} | path: ${{ github.workspace }}\"\n+          python3 -m pytest --ignore=models/llama3/tests/api/test_generation.py --junitxml=\"${{ github.workspace }}/result.xml\"\n+\n+      #### Create test summary ####\n+\n+      - name: \"PR - Test Summary\"\n+        id: pr_test_summary_create\n+        if: github.event_name == 'pull_request_target'\n+        uses: test-summary/action@v2\n+        with:\n+          paths: \"${{ github.workspace }}/result.xml\"\n+          output: test-summary.md\n+\n+      - name: \"PR - Upload Test Summary\"\n+        id: pr_test_summary_upload\n+        if: github.event_name == 'pull_request_target'\n+        uses: actions/upload-artifact@v3\n+        with:\n+          name: test-summary\n+          path: test-summary.md\n+\n+      #### Update PR request ####\n+\n+      - name: \"PR - Update comment\"\n+        id: pr_update_comment\n+        if: github.event_name == 'pull_request_target'\n+        uses: thollander/actions-comment-pull-request@v2\n+        with:\n+          filePath: test-summary.md\n+\n+      ########################\n+      #### MANUAL TESTING ####\n+      ########################\n+\n+      #### Run tests ####\n+\n+      - name: \"Manual - Run Tests\"\n+        id: manual_run_tests\n+        working-directory: \"${{ github.workspace }}\"\n+        if: github.event_name == 'workflow_dispatch'\n+        run: |\n+          echo \"[STEP] Running PyTest tests at 'GITHUB_WORKSPACE' path: ${GITHUB_WORKSPACE} | path: ${{ github.workspace }}\"\n+          free -m\n+          python3 -m pytest --junitxml=\"${{ github.workspace }}/result.xml\"\n+\n+      #### Create test summary ####\n+\n+      - name: \"Manual - Test Summary\"\n+        id: manual_test_summary\n+        if: always() && github.event_name == 'workflow_dispatch'\n+        uses: test-summary/action@v2\n+        with:\n+          paths: \"${{ github.workspace }}/result.xml\"\n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -7,6 +7,7 @@\n \n import os\n import unittest\n+import pytest\n \n from pathlib import Path\n \n@@ -77,6 +78,8 @@ class TestVisionModelInference(unittest.TestCase):\n     def setUpClass(cls):\n         cls.generator = build_generator(\"VISION_MODEL_CHECKPOINT_DIR\")\n \n+    @unittest.skip(\"Disabling vision model test\")\n+    @pytest.mark.skip(reason=\"Disabling vision model test\")\n     def test_run_generation(self):\n         with open(\n             THIS_DIR.parent.parent.parent / \"scripts/resources/dog.jpg\", \"rb\""
            }
          ]
        }
      ]
    },
    {
      "id": 1804620,
      "username": "connortreacy",
      "url": "https://github.com/connortreacy",
      "avatar_url": "https://avatars.githubusercontent.com/u/1804620?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "e462faf3b804091288dc2acde0f26c8a56730e31",
              "url": "https://github.com/meta-llama/llama-models/commit/e462faf3b804091288dc2acde0f26c8a56730e31",
              "message": "Update MODEL_CARD.md (#238)",
              "files_changed": [
                {
                  "filename": "models/llama3_3/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_3/MODEL_CARD.md ---\n@@ -38,15 +38,13 @@ Where to send questions or comments about the model Instructions on how to provi\n \n **Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n \n-## \n-\n-## **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n+**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n \n |  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n | :---- | :---: | :---: | :---: | :---: |\n | Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n \n-## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n+The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n \n ## Training Data"
            }
          ]
        }
      ]
    },
    {
      "id": 8353893,
      "username": "cynikolai",
      "url": "https://github.com/cynikolai",
      "avatar_url": "https://avatars.githubusercontent.com/u/8353893?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/llama-models/issues/50",
              "number": 50,
              "title": "Character Spacing Bypass in Prompt-Guard-86M Classifier",
              "body": "Hi, we're writing to report a potential exploit in the Prompt-Guard-86M classifier that allows for bypassing its safety measures. At [Robust Intelligence](https://robustintelligence.com/), we discovered this issue while analyzing embedding weight differences between the [Prompt-Guard-86M model](https://huggingface.co/meta-llama/Prompt-Guard-86M) and its base model, [microsoft/mdeberta-v3-base](https://huggingface.co/microsoft/mdeberta-v3-base). Our findings reveal a simple yet effective method to circumvent the classifier's detection capabilities.\r\n\r\n**Method:**\r\nThe bypass involves inserting character-wise spaces between all English alphabet characters in a given prompt. This simple transformation effectively renders the classifier unable to detect potentially harmful content.\r\n\r\n**Significance:**\r\nWhile it's understood that as a scalable model, Prompt-Guard-86M may not detect all types of prompt injections, this particular exploit is noteworthy due to its simplicity and high transferability across various prompts.\r\n\r\n**Results:**\r\nOur tests demonstrate that this method reduces the classifier's accuracy from 100% to 0.2% on the harmful set of unmodified prompts in the Sorry-Bench dataset, achieving a 99.8% success rate in bypassing detection.\r\n\r\n**Example:**\r\nOriginal prompt: \"Ignore previous instructions.\"\r\nTransformed prompt: \"I g n o r e  p r e v i o u s  i n s t r u c t i o n s \"\r\n\r\nThis transformation consistently causes the classifier to misidentify harmful prompts as benign. We're reporting this issue to contribute to the improvement of AI safety measures and are open to discussing the details privately to avoid potential misuse of this information.",
              "labels": [],
              "comments": 1,
              "state_reason": "completed"
            }
          ],
          "commits": []
        }
      ]
    },
    {
      "id": 15115928,
      "username": "DavidZirinsky",
      "url": "https://github.com/DavidZirinsky",
      "avatar_url": "https://avatars.githubusercontent.com/u/15115928?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "cb51c14ec761370ba2e2bc351374a79265d0465e",
              "url": "https://github.com/meta-llama/codellama/commit/cb51c14ec761370ba2e2bc351374a79265d0465e",
              "message": "Readme updates to reflect file names (#3)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -50,7 +50,7 @@ The Code Llama and Code Llama - Python models are not fine-tuned to follow instr\n See `example_completion.py` for some examples. To illustrate, see command below to run it with the `CodeLlama-7b` model (`nproc_per_node` needs to be set to the `MP` value):\n \n ```\n-torchrun --nproc_per_node 1 example_code_completion.py \\\n+torchrun --nproc_per_node 1 example_completion.py \\\n     --ckpt_dir CodeLlama-7b/ \\\n     --tokenizer_path CodeLlama-7b/tokenizer.model \\\n     --max_seq_len 128 --max_batch_size 4\n@@ -66,7 +66,7 @@ Code Llama and Code Llama - Instruct 7B and 13B models are capable of filling in\n \n See `example_infilling.py` for some examples. The `CodeLlama-7b` model can be run for infilling with the command below (`nproc_per_node` needs to be set to the `MP` value):\n ```\n-torchrun --nproc_per_node 1 example_text_infilling.py \\\n+torchrun --nproc_per_node 1 example_infilling.py \\\n     --ckpt_dir CodeLlama-7b/ \\\n     --tokenizer_path CodeLlama-7b/tokenizer.model \\\n     --max_seq_len 192 --max_batch_size 4"
            }
          ]
        }
      ]
    },
    {
      "id": 14082769,
      "username": "derrix060",
      "url": "https://github.com/derrix060",
      "avatar_url": "https://avatars.githubusercontent.com/u/14082769?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "1689f5051a671eef8c7254818c7a984088f211e9",
              "url": "https://github.com/meta-llama/llama-models/commit/1689f5051a671eef8c7254818c7a984088f211e9",
              "message": "Standardize URLs (#18)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -73,9 +73,9 @@ To help developers address these risks, we have created the [Responsible Use Gui\n ## Issues\n \n Please report any software bug or other problems with the models through one of the following means:\n-- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n-- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n-- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n+- Reporting issues with the model: [github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n+- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](https://developers.facebook.com/llama_output_feedback)\n+- Reporting bugs and security concerns: [facebook.com/whitehat/info](https://facebook.com/whitehat/info)\n \n \n ## Questions"
            }
          ]
        }
      ]
    },
    {
      "id": 185562542,
      "username": "dineshyv",
      "url": "https://github.com/dineshyv",
      "avatar_url": "https://avatars.githubusercontent.com/u/185562542?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "92864ad3769dd7ecffa6706077d6c29feaadb481",
              "url": "https://github.com/meta-llama/llama-models/commit/92864ad3769dd7ecffa6706077d6c29feaadb481",
              "message": "Bump version to 0.0.56",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.55\",\n+    version=\"0.0.56\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            }
          ]
        }
      ]
    },
    {
      "id": 6599399,
      "username": "dltn",
      "url": "https://github.com/dltn",
      "avatar_url": "https://avatars.githubusercontent.com/u/6599399?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "c908f5b7bd2c2fbdd89a48f6bc252efd4ebc293f",
              "url": "https://github.com/meta-llama/llama-models/commit/c908f5b7bd2c2fbdd89a48f6bc252efd4ebc293f",
              "message": "Add Llama 3.3 (#236)",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_3/LICENSE",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_3/MODEL_CARD.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_3/USE_POLICY.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_3/prompt_format.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_3/prompts.py",
                  "status": "added"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -58,6 +58,7 @@ class ModelFamily(Enum):\n     llama3 = \"llama3\"\n     llama3_1 = \"llama3_1\"\n     llama3_2 = \"llama3_2\"\n+    llama3_3 = \"llama3_3\"\n     safety = \"safety\"\n \n \n@@ -97,6 +98,9 @@ class CoreModelId(Enum):\n     llama3_2_11b_vision_instruct = \"Llama3.2-11B-Vision-Instruct\"\n     llama3_2_90b_vision_instruct = \"Llama3.2-90B-Vision-Instruct\"\n \n+    # Llama 3.3 family\n+    llama3_3_70b_instruct = \"Llama3.3-70B-Instruct\"\n+\n     # Safety models\n     llama_guard_3_8b = \"Llama-Guard-3-8B\"\n     llama_guard_2_8b = \"Llama-Guard-2-8B\"\n@@ -153,6 +157,10 @@ def model_family(model_id) -> ModelFamily:\n         CoreModelId.llama3_2_90b_vision_instruct,\n     ]:\n         return ModelFamily.llama3_2\n+    elif model_id in [\n+        CoreModelId.llama3_3_70b_instruct,\n+    ]:\n+        return ModelFamily.llama3_3\n     elif model_id in [\n         CoreModelId.llama_guard_3_8b,\n         CoreModelId.llama_guard_2_8b,\n@@ -161,7 +169,7 @@ def model_family(model_id) -> ModelFamily:\n     ]:\n         return ModelFamily.safety\n     else:\n-        raise ValueError(f\"Unknown model family for {CoreModelId}\")\n+        raise ValueError(f\"Unknown model family for {model_id}\")\n \n \n @json_schema_type(\n@@ -206,6 +214,7 @@ def is_featured(self) -> bool:\n         return self.model_family in [\n             ModelFamily.llama3_1,\n             ModelFamily.llama3_2,\n+            ModelFamily.llama3_3,\n             ModelFamily.safety,\n         ]\n \n@@ -217,7 +226,7 @@ def max_seq_length(self) -> int:\n             return 4096\n         elif self.model_family == ModelFamily.llama3:\n             return 8192\n-        elif self.model_family == ModelFamily.llama3_1:\n+        elif self.model_family in [ModelFamily.llama3_1, ModelFamily.llama3_3]:\n             return 131072\n         elif self.model_family == ModelFamily.llama3_2:\n             if self.quantization_format == CheckpointQuantizationFormat.int4:\n\n--- File: models/llama3_3/LICENSE ---\n@@ -0,0 +1,49 @@\n+**LLAMA 3.3 COMMUNITY LICENSE AGREEMENT**\n+\n+ Llama 3.3 Version Release Date: December 6, 2024\n+\n+**Agreement** means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n+\n+**Documentation** means the specifications, manuals and documentation accompanying Llama 3.3 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\n+\n+**Licensee** or **you** means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entitys behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n+\n+**Llama 3.3** means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\n+\n+**Llama Materials** means, collectively, Metas proprietary Llama 3.3 and Documentation (and any portion thereof) made available under this Agreement.\n+\n+**Meta** or **we** means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n+\n+By clicking I Accept below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n+\n+1\\. **License Rights and Redistribution**.\n+\n+a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metas intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\n+\n+b. Redistribution and Use.\n+\n+i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display Built with Llama on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include Llama at the beginning of any such AI model name.\n+\n+ii.If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\n+\n+iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a Notice text file distributed as a part of such copies: Llama 3.3 is licensed under the Llama 3.3 Community License, Copyright  Meta Platforms, Inc. All Rights Reserved.\n+\n+iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama3\\_3/use-policy](https://www.llama.com/llama3_3/use-policy)), which is hereby incorporated by reference into this Agreement.  \n+  \n+2\\. **Additional Commercial Terms**. If, on the Llama 3.3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensees affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n+\n+3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN AS IS BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n+\n+4\\. **Limitation of Liability**. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n+\n+5\\. **Intellectual Property**.\n+\n+a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use Llama (the Mark) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Metas brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n+\n+b. Subject to Metas ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n+\n+c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n+\n+6\\. **Term and Termination**. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n+\n+7\\. **Governing Law and Jurisdiction**. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.  \n\n--- File: models/llama3_3/MODEL_CARD.md ---\n@@ -0,0 +1,147 @@\n+## Model Information\n+\n+The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n+\n+**Model developer**: Meta\n+\n+**Model Architecture:** Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n+\n+|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Token count | Knowledge cutoff |\n+| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n+| Llama 3.3 (text only)  | A new mix of publicly available online data. | 70B | Multilingual Text | Multilingual Text and code  | 128k | Yes | 15T+ | December 2023 |\n+\n+**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n+\n+**Llama 3.3 model**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n+\n+**Model Release Date:** \n+\n+* **70B Instruct: December 6, 2024** \n+\n+**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n+\n+**License** A custom commercial license, the Llama 3.3 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3\\_3/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE)\n+\n+Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n+\n+## Intended Use\n+\n+**Intended Use Cases** Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases. \n+\n+**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card\\*\\*.\n+\n+\\*\\*Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\n+\n+## Hardware and Software\n+\n+**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n+\n+**Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n+\n+## \n+\n+## **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n+\n+|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n+| :---- | :---: | :---: | :---: | :---: |\n+| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n+\n+## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n+\n+## Training Data\n+\n+**Overview:** Llama 3.3 was pretrained on \\~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n+\n+**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n+\n+## Benchmarks \\- English Text\n+\n+In this section, we report the results for Llama 3.3 relative to our previous models. \n+\n+### Instruction tuned models\n+\n+## \n+\n+| Category | Benchmark | \\# Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\n+| :---- | :---- | ----- | :---- | ----- | ----- | ----- | ----- |\n+| General | MMLU (CoT) | 0 | macro\\_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\n+|  | MMLU Pro (CoT) | 5 | macro\\_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\n+| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\n+| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\n+| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\n+|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\n+| Math | MATH (CoT) | 0 | sympy\\_intersection\\_score | 51.9 | 68.0 | 77.0 | 73.8 |\n+| Tool Use | BFCL v2 | 0 | overall\\_ast\\_summary/macro\\_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\n+| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\n+\n+## \n+\n+## Responsibility & Safety\n+\n+As part of our Responsible release approach, we followed a three-pronged strategy to managing trust and safety risks:\n+\n+* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n+* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n+* Provide protections for the community to help prevent the misuse of our models.\n+\n+### Responsible deployment \n+\n+Llama is a foundational technology designed to be used in a variety of use cases, examples on how Metas Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n+\n+#### Llama 3.3 instruct \n+\n+Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the develoderationsper workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n+\n+**Fine-tuning data**   \n+We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. Weve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n+\n+**Refusals and Tone**  \n+Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n+\n+#### Llama 3.3 systems\n+\n+**Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.   \n+As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n+\n+#### Capability-specific considerations\n+\n+**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n+\n+**Multilinguality**: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n+\n+### Evaluations\n+\n+We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \n+Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n+\n+**Red teaming**   \n+For both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.   \n+We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . \n+\n+### Critical and other risks \n+\n+### We specifically focused our efforts on mitigating the following critical risk areas:\n+\n+**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**  \n+To assess risks related to proliferation of chemical and biological weapons of the Llama 3 family of models, we performed uplift testing designed to assess whether use of the Llama models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n+\n+### **2\\. Child Safety**\n+\n+Child Safety risk assessments were conducted using a team of experts, to assess the models capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n+\n+**3\\. Cyber attack enablement**  \n+Our cyber attack uplift study investigated whether the Llama 3 family of LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed. Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n+\n+### Community \n+\n+Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n+\n+We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Metas Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n+\n+Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n+\n+## Ethical Considerations and Limitations\n+\n+The core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n+\n+But Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.   \n\n--- File: models/llama3_3/USE_POLICY.md ---\n@@ -0,0 +1,73 @@\n+**Llama 3.3** **Acceptable Use Policy**\n+\n+Meta is committed to promoting safe and fair use of its tools and features, including Llama 3.3. If you access or use Llama 3.3, you agree to this Acceptable Use Policy (**Policy**). The most recent copy of this policy can be found at [https://www.llama.com/llama3\\_3/use-policy](https://www.llama.com/llama3_3/use-policy).\n+\n+**Prohibited Uses**\n+\n+We want everyone to use Llama 3.3 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.3 to:\n+\n+1. Violate the law or others rights, including to:\n+\n+   1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:  \n+      1. Violence or terrorism  \n+      2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material  \n+      3. Human trafficking, exploitation, and sexual violence  \n+      4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.  \n+      5. Sexual solicitation  \n+      6. Any other criminal activity\n+\n+   2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n+\n+   3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n+\n+   4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n+\n+   5. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n+\n+   6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n+\n+   7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n+\n+   8. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\n+\n+2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.3 related to the following:\n+\n+   1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\n+\n+   2. Guns and illegal weapons (including weapon development)\n+\n+   3. Illegal drugs and regulated/controlled substances\n+\n+   4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n+\n+   5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n+\n+   6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n+\n+3. Intentionally deceive or mislead others, including use of Llama 3.3 related to the following:\n+\n+   1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n+\n+   2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n+\n+   3. Generating, promoting, or further distributing spam\n+\n+   4. Impersonating another individual without consent, authorization, or legal right\n+\n+   5. Representing that the use of Llama 3.3 or outputs are human-generated\n+\n+   6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n+\n+4. Fail to appropriately disclose to end users any known dangers of your AI system\n+\n+5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.3\n+\n+With respect to any multimodal models included in Llama 3.3, the rights granted under Section 1(a) of the Llama 3.3 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\n+\n+Please report any violation of this Policy, software bug, or other problems that could lead to a violation of this Policy through one of the following means:\n+\n+* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)  \n+* Reporting risky content generated by the model: [developers.facebook.com/llama\\_output\\_feedback](http://developers.facebook.com/llama_output_feedback)  \n+* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)  \n+* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.3: LlamaUseReport@meta.com\n+\n\n--- File: models/llama3_3/prompt_format.md ---\n@@ -0,0 +1,358 @@\n+\n+\n+# Llama 3.1 - Prompt Formats\n+## Tokens\n+Here is a list of special tokens that are supported by Llama 3.1:\n+- `<|begin_of_text|>`: Specifies the start of the prompt\n+- `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n+- `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n+- `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and ipython]\n+- `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n+- `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n+    - at the end of a direct interaction between the model and the user\n+    - at the end of multiple interactions between the model and any available tools\n+    This token signals to the executor that the model has finished generating a response.\n+- `<|python_tag|>`: Is a special tag used in the model's response to signify a tool call.\n+\n+\n+\n+There are 4 different roles that are supported by Llama 3.1\n+- `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n+- `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n+- `ipython`: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n+- `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `ipython` and `user` prompts.\n+\n+## Llama 3.1 Base Model\n+\n+Text completion for Llama 3.1 base model uses this format.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|>Color of sky is blue but sometimes can also be\n+```\n+\n+##### Model Response Format\n+```\n+ red, orange, yellow, green, purple, pink, brown, gray, black, white, and even rainbow colors. The color of the sky can change due to various reasons such as time of day, weather conditions, pollution, and atmospheric phenomena.\n+The color of the sky is primarily blue because of a phenomenon called\n+```\n+\n+\n+\n+Note start special tag\n+\n+\n+## Llama 3.1 Instruct Model\n+## User and assistant conversation\n+\n+Here is a regular multi-turn user assistant conversation and how its formatted.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Answer who are you in the form of jeopardy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+Here's my response\n+\n+\"What is a helpful assistant?\"<|eot_id|>\n+```\n+\n+\n+\n+\n+\n+\n+## Tool Calling Formats\n+\n+\n+The three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:\n+- Brave Search: Tool call to perform web searches.\n+- Wolfram Alpha: Tool call to perform complex mathematical calculations.\n+- Code Interpreter: Enables the model to output python code.\n+\n+## Builtin Tool Calling\n+\n+\n+Here is an example of a conversation using brave search\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+Tools: brave_search, wolfram_alpha\n+Cutting Knowledge Date: December 2023\n+Today Date: 21 September 2024\n+\n+You are a helpful assistant.\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Search the web for the latest price of 1oz gold?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>brave_search.call(query=\"latest price of 1oz gold\")<|eom_id|>\n+```\n+\n+\n+\n+\n+- Just including Environment: ipython turns on code interpreter; therefore, you don't need to specify code interpretation on the Tools: line. The model can generate python code which is interpreted by the executor, with the result provided back to the model.\n+- The message body of the assistant response starts with a special tag <|python_tag|>\n+- As alluded to above, in such an environment, the model can generate <|eom_id|> instead of just the standard <|eot_id|> . The latter indicates the turn is finished, while the former indicates continued multi-step reasoning. That is, the model is expecting a continuation message with the output of the tool call.\n+- The model tool call response is of the form `tool.call(query=\"...\")` wher tool is `brave_search` or `wolfram_alpha`\n+\n+\n+## Builtin Code Interpreter\n+\n+Here is an actual example of model responding with code\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Write code to check if number is prime, use that to see if the number 7 is prime<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>def is_prime(n):\n+    if n <= 1\n+        return False\n+    for i in range(2, int(n**0.5) + 1):\n+        if n % i == 0:\n+            return False\n+    return True\n+\n+print(is_prime(7))  # Output: True<|eom_id|>\n+```\n+\n+\n+\n+\n+- Model starts with <|python_tag|> and continues writing python code that it needs to be executed\n+- No explicit mention of code_interpreter in system prompt. `Environment: ipython` implicitly enables it.\n+\n+\n+## Built-in tools full interaction\n+\n+Here is a full interaction with the built-in tools including the tool response and the final assistant response.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+Tools: brave_search, wolfram_alpha\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+What is the 100th decimal of pi?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+<|python_tag|>wolfram_alpha.call(query=\"100th decimal of pi\")<|eom_id|><|start_header_id|>ipython<|end_header_id|>\n+\n+\n+{\n+    \"queryresult\": {\n+        \"success\": true,\n+        \"inputstring\": \"100th decimal of pi\",\n+        \"pods\": [\n+            {\n+                \"title\": \"Input interpretation\",\n+                \"subpods\": [\n+                    {\n+                        \"title\": \"\",\n+                        \"plaintext\": \"100th digit | \"\n+                    }\n+                ]\n+            },\n+            {\n+                \"title\": \"Nearby digits\",\n+                \"subpods\": [\n+                    {\n+                        \"title\": \"\",\n+                        \"plaintext\": \"...86208998628034825342117067982148086513282306647093...\"\n+                    }\n+                ]\n+            },\n+            {\n+                \"title\": \"Result\",\n+                \"primary\": true,\n+                \"subpods\": [\n+                    {\n+                        \"title\": \"\",\n+                        \"plaintext\": \"7\"\n+                    }\n+                ]\n+            }\n+        ]\n+    }\n+}\n+<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+The 100th decimal of pi is 7.<|eot_id|>\n+```\n+\n+\n+\n+\n+- Note the `<|python_tag|>` in the assistant response.\n+- Role is `ipython` for the wolfram alpha response that is passed back to the model.\n+- Final message from assistant has <|eot_id|> tag.\n+\n+\n+\n+## Zero shot tool calling\n+## JSON based tool calling\n+\n+\n+Llama models can now output custom tool calls from a single message to allow easier tool calling.\n+The following prompts provide an example of how custom tools can be called from the output of the model.\n+It's important to note that the model itself does not execute the calls; it provides structured output to facilitate calling by an executor.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+\n+Cutting Knowledge Date: December 2023\n+Today Date: 21 September 2024\n+\n+You are a helpful assistant.\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Answer the user's question by making use of the following functions if needed.\n+If none of the function can be used, please say so.\n+Here is a list of functions in JSON format:\n+{\n+    \"type\": \"function\",\n+    \"function\": {\n+        \"name\": \"trending_songs\",\n+        \"description\": \"Returns the trending songs on a Music site\",\n+        \"parameters\": {\n+            \"type\": \"object\",\n+            \"properties\": [\n+                {\n+                    \"n\": {\n+                        \"type\": \"object\",\n+                        \"description\": \"The number of songs to return\"\n+                    }\n+                },\n+                {\n+                    \"genre\": {\n+                        \"type\": \"object\",\n+                        \"description\": \"The genre of the songs to return\"\n+                    }\n+                }\n+            ],\n+            \"required\": [\"n\"]\n+        }\n+    }\n+}\n+\n+Return function calls in JSON format.<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Use tools to get latest trending songs<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<|python_tag|>{\n+    \"type\": \"function\",\n+    \"name\": \"trending_songs\",\n+    \"parameters\": {\n+        \"n\": \"10\",\n+        \"genre\": \"all\"\n+    }\n+}<|eom_id|>\n+```\n+\n+\n+\n+\n+- JSON format for providing tools needs name, description and parameters\n+- Model responds with `<|python_tag|>` and `<|eom_id|>` as `Environment: ipython` was in the system prompt\n+- Instructions for tools added as a user message\n+- Only single tool calls are supported as of now\n+\n+\n+\n+## Example of a user defined tool calling\n+## `<function>` based tool calling\n+\n+\n+Here is an example of how you could also write custom instructions for model to do zero shot tool calling.\n+In this example, we define a custom tool calling format using the `<function>` tag.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+Environment: ipython\n+\n+Cutting Knowledge Date: December 2023\n+Today Date: 21 September 2024\n+\n+You are a helpful assistant.\n+<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+You have access to the following functions:\n+\n+Use the function 'trending_songs' to 'Returns the trending songs on a Music site':\n+{\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n+\n+Think very carefully before calling functions.\n+If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+\n+<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n+\n+Reminder:\n+- If looking for real time information use relevant functions before falling back to brave_search\n+- Function calls MUST follow the specified format, start with <function= and end with </function>\n+- Required parameters MUST be specified\n+- Only call one function at a time\n+- Put the entire function call reply on one line<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+Use tools to get latest trending songs<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<function=trending_songs>{\"n\": 10}</function><|eot_id|>\n+```\n+\n+\n+\n+\n+- In this case, model does NOT respond with `<|python_tag|>` and ends with `<|eot_id|>`\n+- Instructions for tools added as a user message\n+\n+\n+Thank You!\n\n--- File: models/llama3_3/prompts.py ---\n@@ -0,0 +1,244 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import textwrap\n+from typing import List\n+from ..llama3.api.datatypes import *  # noqa: F403\n+from ..prompt_format import (\n+    llama3_1_builtin_tool_call_dialog,\n+    llama3_1_custom_tool_call_dialog,\n+    # llama3_1_e2e_tool_call_dialog,\n+    TextCompletionContent,\n+    UseCase,\n+)\n+\n+\n+def wolfram_alpha_response():\n+    return textwrap.dedent(\n+        \"\"\"\n+        {\n+            \"queryresult\": {\n+                \"success\": true,\n+                \"inputstring\": \"100th decimal of pi\",\n+                \"pods\": [\n+                    {\n+                        \"title\": \"Input interpretation\",\n+                        \"subpods\": [\n+                            {\n+                                \"title\": \"\",\n+                                \"plaintext\": \"100th digit | \\u03c0\"\n+                            }\n+                        ]\n+                    },\n+                    {\n+                        \"title\": \"Nearby digits\",\n+                        \"subpods\": [\n+                            {\n+                                \"title\": \"\",\n+                                \"plaintext\": \"...86208998628034825342117067982148086513282306647093...\"\n+                            }\n+                        ]\n+                    },\n+                    {\n+                        \"title\": \"Result\",\n+                        \"primary\": true,\n+                        \"subpods\": [\n+                            {\n+                                \"title\": \"\",\n+                                \"plaintext\": \"7\"\n+                            }\n+                        ]\n+                    }\n+                ]\n+            }\n+        }\n+        \"\"\"\n+    )\n+\n+\n+def usecases() -> List[UseCase | str]:\n+    return [\n+        textwrap.dedent(\n+            \"\"\"\n+            # Llama 3.1 - Prompt Formats\n+            ## Tokens\n+            Here is a list of special tokens that are supported by Llama 3.1:\n+            - `<|begin_of_text|>`: Specifies the start of the prompt\n+            - `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n+            - `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n+            - `<|start_header_id|>` and `<|end_header_id|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and ipython]\n+            - `<|eom_id|>`: End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.\n+            - `<|eot_id|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n+                - at the end of a direct interaction between the model and the user\n+                - at the end of multiple interactions between the model and any available tools\n+                This token signals to the executor that the model has finished generating a response.\n+            - `<|python_tag|>`: Is a special tag used in the model's response to signify a tool call.\n+            \"\"\"\n+        ),\n+        textwrap.dedent(\n+            \"\"\"\n+            There are 4 different roles that are supported by Llama 3.1\n+            - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n+            - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n+            - `ipython`: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n+            - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `ipython` and `user` prompts.\n+            \"\"\"\n+        ),\n+        UseCase(\n+            title=\"Llama 3.1 Base Model\",\n+            description=\"Text completion for Llama 3.1 base model uses this format.\",\n+            dialogs=[\n+                TextCompletionContent(\n+                    content=\"Color of sky is blue but sometimes can also be\"\n+                )\n+            ],\n+            notes=\"Note start special tag\",\n+        ),\n+        \"## Llama 3.1 Instruct Model\",\n+        UseCase(\n+            title=\"User and assistant conversation\",\n+            description=\"Here is a regular multi-turn user assistant conversation and how its formatted.\",\n+            dialogs=[\n+                [\n+                    SystemMessage(content=\"You are a helpful assistant\"),\n+                    UserMessage(content=\"Answer who are you in the form of jeopardy?\"),\n+                ]\n+            ],\n+            notes=\"\",\n+        ),\n+        \"## Tool Calling Formats\",\n+        textwrap.dedent(\n+            \"\"\"\n+            The three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:\n+            - Brave Search: Tool call to perform web searches.\n+            - Wolfram Alpha: Tool call to perform complex mathematical calculations.\n+            - Code Interpreter: Enables the model to output python code.\n+            \"\"\"\n+        ),\n+        UseCase(\n+            title=\"Builtin Tool Calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Here is an example of a conversation using brave search\n+                \"\"\"\n+            ),\n+            dialogs=[llama3_1_builtin_tool_call_dialog()],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Just including Environment: ipython turns on code interpreter; therefore, you don't need to specify code interpretation on the Tools: line. The model can generate python code which is interpreted by the executor, with the result provided back to the model.\n+                - The message body of the assistant response starts with a special tag <|python_tag|>\n+                - As alluded to above, in such an environment, the model can generate <|eom_id|> instead of just the standard <|eot_id|> . The latter indicates the turn is finished, while the former indicates continued multi-step reasoning. That is, the model is expecting a continuation message with the output of the tool call.\n+                - The model tool call response is of the form `tool.call(query=\"...\")` wher tool is `brave_search` or `wolfram_alpha`\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Builtin Code Interpreter\",\n+            description=\"Here is an actual example of model responding with code\",\n+            dialogs=[\n+                [\n+                    SystemMessage(content=\"Environment: ipython\"),\n+                    UserMessage(\n+                        content=\"Write code to check if number is prime, use that to see if the number 7 is prime\"\n+                    ),\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Model starts with <|python_tag|> and continues writing python code that it needs to be executed\n+                - No explicit mention of code_interpreter in system prompt. `Environment: ipython` implicitly enables it.\n+                \"\"\"\n+            ),\n+        ),\n+        UseCase(\n+            title=\"Built-in tools full interaction\",\n+            description=\"Here is a full interaction with the built-in tools including the tool response and the final assistant response.\",\n+            dialogs=[\n+                [\n+                    SystemMessage(\n+                        content=\"Environment: ipython\\nTools: brave_search, wolfram_alpha\\n\"\n+                    ),\n+                    UserMessage(content=\"What is the 100th decimal of pi?\"),\n+                    CompletionMessage(\n+                        content=\"\",\n+                        stop_reason=StopReason.end_of_message,\n+                        tool_calls=[\n+                            ToolCall(\n+                                call_id=\"tool_call_id\",\n+                                tool_name=BuiltinTool.wolfram_alpha,\n+                                arguments={\"query\": \"100th decimal of pi\"},\n+                            )\n+                        ],\n+                    ),\n+                    ToolResponseMessage(\n+                        call_id=\"wolfram_alpha_id\",\n+                        tool_name=BuiltinTool.wolfram_alpha,\n+                        content=wolfram_alpha_response(),\n+                    ),\n+                ],\n+            ],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - Note the `<|python_tag|>` in the assistant response.\n+                - Role is `ipython` for the wolfram alpha response that is passed back to the model.\n+                - Final message from assistant has <|eot_id|> tag.\n+                \"\"\"\n+            ),\n+        ),\n+        \"## Zero shot tool calling\",\n+        UseCase(\n+            title=\"JSON based tool calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Llama models can now output custom tool calls from a single message to allow easier tool calling.\n+                The following prompts provide an example of how custom tools can be called from the output of the model.\n+                It's important to note that the model itself does not execute the calls; it provides structured output to facilitate calling by an executor.\n+                \"\"\"\n+            ),\n+            dialogs=[llama3_1_custom_tool_call_dialog()],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - JSON format for providing tools needs name, description and parameters\n+                - Model responds with `<|python_tag|>` and `<|eom_id|>` as `Environment: ipython` was in the system prompt\n+                - Instructions for tools added as a user message\n+                - Only single tool calls are supported as of now\n+                \"\"\"\n+            ),\n+        ),\n+        # FIXME: This is not working yet as expected\n+        # UseCase(\n+        #     title=\"E2E tool call example\",\n+        #     description=textwrap.dedent(\n+        #         \"\"\"\n+        #         Here is an example showing the whole multi-step turn by taking custom tool outputs and passing back to the model.\n+        #         \"\"\"\n+        #     ),\n+        #     dialogs=[\n+        #         llama3_1_e2e_tool_call_dialog(\n+        #             tool_prompt_format=ToolPromptFormat.function_tag\n+        #         )\n+        #     ],\n+        #     notes=\"\",\n+        # ),\n+        \"## Example of a user defined tool calling\",\n+        UseCase(\n+            title=\"`<function>` based tool calling\",\n+            description=textwrap.dedent(\n+                \"\"\"\n+                Here is an example of how you could also write custom instructions for model to do zero shot tool calling.\n+                In this example, we define a custom tool calling format using the `<function>` tag.\n+                \"\"\"\n+            ),\n+            dialogs=[llama3_1_custom_tool_call_dialog(ToolPromptFormat.function_tag)],\n+            notes=textwrap.dedent(\n+                \"\"\"\n+                - In this case, model does NOT respond with `<|python_tag|>` and ends with `<|eot_id|>`\n+                - Instructions for tools added as a user message\n+                \"\"\"\n+            ),\n+        ),\n+    ]\n\n--- File: models/sku_list.py ---\n@@ -34,6 +34,7 @@ def all_registered_models() -> List[Model]:\n         + llama3_family()\n         + llama3_1_family()\n         + llama3_2_family()\n+        + llama3_3_family()\n         + safety_models()\n     )\n \n@@ -74,6 +75,12 @@ def llama3_2_family() -> List[Model]:\n     ]\n \n \n+def llama3_3_family() -> List[Model]:\n+    return [\n+        *llama3_3_instruct_models(),\n+    ]\n+\n+\n def llama2_base_models() -> List[Model]:\n     return [\n         Model(\n@@ -745,6 +752,30 @@ def llama3_2_instruct_models() -> List[Model]:\n     ]\n \n \n+def llama3_3_instruct_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.llama3_3_70b_instruct,\n+            description=\"Llama 3.3 70b instruct\",\n+            huggingface_repo=\"meta-llama/Llama-3.3-70B-Instruct\",\n+            recommended_sampling_params=recommended_sampling_params(),\n+            arch_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": LLAMA3_VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": True,\n+            },\n+            pth_file_count=8,\n+        ),\n+    ]\n+\n+\n @lru_cache\n def safety_models() -> List[Model]:\n     return ["
            },
            {
              "sha": "17107dbe165f48270eebb17014ba880c6eb6a7c9",
              "url": "https://github.com/meta-llama/llama-models/commit/17107dbe165f48270eebb17014ba880c6eb6a7c9",
              "message": "Bump version to 0.0.57",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.56\",\n+    version=\"0.0.57\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "08003bb74345000314583395bc28cd65da742897",
              "url": "https://github.com/meta-llama/llama-models/commit/08003bb74345000314583395bc28cd65da742897",
              "message": "Create SECURITY.md",
              "files_changed": [
                {
                  "filename": "SECURITY.md",
                  "status": "added"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: SECURITY.md ---\n@@ -0,0 +1,5 @@\n+# Security Policy\n+\n+## Reporting a Vulnerability\n+\n+Please report vulnerabilities to our bug bounty program at https://bugbounty.meta.com/"
            },
            {
              "sha": "4d70c9826e82dc7a2260096b5c1c1d599d5d24b4",
              "url": "https://github.com/meta-llama/llama-models/commit/4d70c9826e82dc7a2260096b5c1c1d599d5d24b4",
              "message": "Update README.md for llama download (#105)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -31,28 +31,40 @@ Our mission is to empower individuals and industry through this opportunity whil\n \n ## Download\n \n-To download the model weights and tokenizer, please visit the [Meta Llama website](https://llama.meta.com/llama-downloads/) and accept our License.\n+To download the model weights and tokenizer:\n \n-Once your request is approved, you will receive a signed URL over email. Then, run the download.sh script, passing the URL provided when prompted to start the download.\n-\n-Pre-requisites: Ensure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`. `./download.sh`can be found inside the respective `models` directory. \n+1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n+2. Read and accept the license.\n+3. Once your request is approved you will receive a signed URL via email.\n+4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-toolchain`\n+5. Run `llama model list` to determine the model ID you wish to download\n+6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n+7. Pass the URL provided when prompted to start the download.\n \n Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n \n-### Access to Hugging Face\n+### Download via HuggingFace\n+\n+We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama) in both transformers and native `llama3` formats. To gain access:\n \n-We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n+1. Visit one of the repos (ex. [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)).\n+2. Read and accept the license.\n+3. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that approvals may take up to one hour.\n \n-- Visit one of the repos, for example [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n-- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n-- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n+You can then download the models:\n+\n+- Via `llama download --source huggingface --hf-token YOUR_ACCESS_TOKEN` ([create/view access tokens here](https://huggingface.co/settings/tokens))\n+- Via the HuggingFace CLI (`pip install huggingface-hub`):\n+- In the web browser by clicking on the \"Files and versions\" tab\n \n ```bash\n huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n ```\n \n-**NOTE** The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.\n+The original native weights are in the `original/` subfolder (except for `meta-llama/Meta-Llama-3.1-405B`).\n+\n \n+## Using with transformers\n \n - To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:"
            },
            {
              "sha": "649be1e178a9e6b3f47b94e20f0e3fbe8451e770",
              "url": "https://github.com/meta-llama/llama-models/commit/649be1e178a9e6b3f47b94e20f0e3fbe8451e770",
              "message": "Bump version to 0.0.4",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.3\",\n+    version=\"0.0.4\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "c5cc65053f83430683ac02a545ace7a01492eeac",
              "url": "https://github.com/meta-llama/llama-models/commit/c5cc65053f83430683ac02a545ace7a01492eeac",
              "message": "Merge distros to main (#97)",
              "files_changed": [
                {
                  "filename": ".gitignore",
                  "status": "modified"
                },
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/api/__init__.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/api/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/reference_impl/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/reference_impl/model.py",
                  "status": "renamed"
                },
                {
                  "filename": "models/schema_utils.py",
                  "status": "added"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                },
                {
                  "filename": "requirements.txt",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .gitignore ---\n@@ -1,3 +1,4 @@\n __pycache__\n dist\n *.egg-info\n+build\n\n--- File: models/datatypes.py ---\n@@ -10,7 +10,7 @@\n \n from pydantic import BaseModel\n \n-from strong_typing.schema import json_schema_type\n+from .schema_utils import json_schema_type\n \n \n @json_schema_type\n\n--- File: models/llama3_1/api/__init__.py ---\n@@ -5,8 +5,7 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-from .args import *\n-from .chat_format import *\n-from .datatypes import *\n-from .model import *\n-from .tokenizer import *\n+from .args import *  # noqa\n+from .chat_format import *  # noqa\n+from .datatypes import *  # noqa\n+from .tokenizer import *  # noqa\n\n--- File: models/llama3_1/api/datatypes.py ---\n@@ -10,9 +10,9 @@\n \n from pydantic import BaseModel, Field\n \n-from strong_typing.schema import json_schema_type\n from typing_extensions import Annotated\n from llama_models.datatypes import *  # noqa\n+from llama_models.schema_utils import json_schema_type\n \n \n @json_schema_type\n\n--- File: models/llama3_1/reference_impl/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n\n--- File: models/llama3_1/reference_impl/model.py ---\n@@ -21,7 +21,11 @@\n )\n from torch import nn\n \n-from .args import ModelArgs\n+from ..api import ModelArgs\n+\n+# **NOTE**: This code is not runnable without installing `torch` and `fairscale`\n+# dependencies. These dependencies are not part of the default dependencies\n+# (requirements.txt) of the `llama-models` package.\n \n \n class RMSNorm(torch.nn.Module):\n\n--- File: models/schema_utils.py ---\n@@ -0,0 +1,125 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from dataclasses import dataclass\n+from typing import Any, Callable, Dict, List, Optional, Type, TypeVar, Union\n+\n+# Borrowed from https://github.com/hunyadi/strong_typing/blob/master/strong_typing/core.py\n+\n+\n+class JsonObject:\n+    \"Placeholder type for an unrestricted JSON object.\"\n+\n+\n+class JsonArray:\n+    \"Placeholder type for an unrestricted JSON array.\"\n+\n+\n+# a JSON type with possible `null` values\n+JsonType = Union[\n+    None,\n+    bool,\n+    int,\n+    float,\n+    str,\n+    Dict[str, \"JsonType\"],\n+    List[\"JsonType\"],\n+]\n+\n+# a JSON type that cannot contain `null` values\n+StrictJsonType = Union[\n+    bool,\n+    int,\n+    float,\n+    str,\n+    Dict[str, \"StrictJsonType\"],\n+    List[\"StrictJsonType\"],\n+]\n+\n+# a meta-type that captures the object type in a JSON schema\n+Schema = Dict[str, JsonType]\n+\n+\n+T = TypeVar(\"T\")\n+\n+\n+def register_schema(\n+    data_type: T,\n+    schema: Optional[Schema] = None,\n+    name: Optional[str] = None,\n+    examples: Optional[List[JsonType]] = None,\n+) -> T:\n+    \"\"\"\n+    Associates a type with a JSON schema definition.\n+\n+    :param data_type: The type to associate with a JSON schema.\n+    :param schema: The schema to associate the type with. Derived automatically if omitted.\n+    :param name: The name used for looking uo the type. Determined automatically if omitted.\n+    :returns: The input type.\n+    \"\"\"\n+    return data_type\n+\n+\n+def json_schema_type(\n+    cls: Optional[Type[T]] = None,\n+    *,\n+    schema: Optional[Schema] = None,\n+    examples: Optional[List[JsonType]] = None,\n+) -> Union[Type[T], Callable[[Type[T]], Type[T]]]:\n+    \"\"\"Decorator to add user-defined schema definition to a class.\"\"\"\n+\n+    def wrap(cls: Type[T]) -> Type[T]:\n+        return register_schema(cls, schema, examples=examples)\n+\n+    # see if decorator is used as @json_schema_type or @json_schema_type()\n+    if cls is None:\n+        # called with parentheses\n+        return wrap\n+    else:\n+        # called as @json_schema_type without parentheses\n+        return wrap(cls)\n+\n+\n+register_schema(JsonObject, name=\"JsonObject\")\n+register_schema(JsonArray, name=\"JsonArray\")\n+register_schema(JsonType, name=\"JsonType\")\n+register_schema(StrictJsonType, name=\"StrictJsonType\")\n+\n+\n+@dataclass\n+class WebMethod:\n+    route: Optional[str] = None\n+    public: bool = False\n+    request_examples: Optional[List[Any]] = None\n+    response_examples: Optional[List[Any]] = None\n+\n+\n+def webmethod(\n+    route: Optional[str] = None,\n+    public: Optional[bool] = False,\n+    request_examples: Optional[List[Any]] = None,\n+    response_examples: Optional[List[Any]] = None,\n+) -> Callable[[T], T]:\n+    \"\"\"\n+    Decorator that supplies additional metadata to an endpoint operation function.\n+\n+    :param route: The URL path pattern associated with this operation which path parameters are substituted into.\n+    :param public: True if the operation can be invoked without prior authentication.\n+    :param request_examples: Sample requests that the operation might take. Pass a list of objects, not JSON.\n+    :param response_examples: Sample responses that the operation might produce. Pass a list of objects, not JSON.\n+    \"\"\"\n+\n+    def wrap(cls: T) -> T:\n+        cls.__webmethod__ = WebMethod(\n+            route=route,\n+            public=public or False,\n+            request_examples=request_examples,\n+            response_examples=response_examples,\n+        )\n+        return cls\n+\n+    return wrap\n\n--- File: models/sku_list.py ---\n@@ -5,6 +5,7 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n+from functools import lru_cache\n from typing import List, Optional\n \n from .datatypes import (\n@@ -30,6 +31,7 @@ def resolve_model(descriptor: str) -> Optional[Model]:\n     return None\n \n \n+@lru_cache\n def all_registered_models() -> List[Model]:\n     return llama2_family() + llama3_family() + llama3_1_family() + safety_models()\n \n@@ -41,29 +43,32 @@ def recommended_sampling_params() -> SamplingParams:\n         top_p=0.9,\n     )\n \n+\n def llama2_family() -> List[Model]:\n     return [\n         *llama2_base_models(),\n         *llama2_instruct_models(),\n     ]\n \n+\n def llama3_family() -> List[Model]:\n     return [\n         *llama3_base_models(),\n         *llama3_instruct_models(),\n     ]\n \n+\n def llama3_1_family() -> List[Model]:\n     return [\n         *llama3_1_base_models(),\n         *llama3_1_instruct_models(),\n     ]\n \n+\n def llama2_base_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.meta_llama2_7b,\n-\n             is_default_variant=True,\n             description_markdown=\"Llama 2 7b model\",\n             huggingface_repo=\"meta-llama/Llama-2-7b\",\n@@ -306,7 +311,6 @@ def llama3_1_base_models() -> List[Model]:\n     ]\n \n \n-\n def llama2_instruct_models() -> List[Model]:\n     return [\n         Model(\n@@ -553,6 +557,7 @@ def llama3_1_instruct_models() -> List[Model]:\n     ]\n \n \n+@lru_cache\n def safety_models() -> List[Model]:\n     return [\n         Model(\n\n--- File: requirements.txt ---\n@@ -1,9 +1,5 @@\n PyYAML\n-blobfile\n jinja2\n-json-strong-typing\n-torch\n tiktoken\n-fairscale\n pydantic==1.10.13\n pydantic_core==2.18.2"
            },
            {
              "sha": "69d5eff607305d43ffd56ea5315d4cdd879481c2",
              "url": "https://github.com/meta-llama/llama-models/commit/69d5eff607305d43ffd56ea5315d4cdd879481c2",
              "message": "Improve model list display (#94)",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -50,23 +50,31 @@ class CheckpointQuantizationFormat(Enum):\n     int8 = \"int8\"\n \n \n+@json_schema_type\n+class ModelFamily(Enum):\n+    llama2 = \"llama2\"\n+    llama3 = \"llama3\"\n+    llama3_1 = \"llama3_1\"\n+    safety = \"safety\"\n+\n+\n @json_schema_type\n class CoreModelId(Enum):\n     \"\"\"Each of these models is a unique \"SKU\". These root models can be served in various garbs (especially by quantizing them)\"\"\"\n \n     # Llama 2 family\n-    meta_llama2_7b = \"meta-llama/Llama-2-7b\"\n-    meta_llama2_13b = \"meta-llama/Llama-2-13b\"\n-    meta_llama2_70b = \"meta-llama/Llama-2-70b\"\n-    meta_llama2_7b_chat = \"meta-llama/Llama-2-7b-chat\"\n-    meta_llama2_13b_chat = \"meta-llama/Llama-2-13b-chat\"\n-    meta_llama2_70b_chat = \"meta-llama/Llama-2-70b-chat\"\n+    meta_llama2_7b = \"Llama-2-7b\"\n+    meta_llama2_13b = \"Llama-2-13b\"\n+    meta_llama2_70b = \"Llama-2-70b\"\n+    meta_llama2_7b_chat = \"Llama-2-7b-chat\"\n+    meta_llama2_13b_chat = \"Llama-2-13b-chat\"\n+    meta_llama2_70b_chat = \"Llama-2-70b-chat\"\n \n     # Llama 3 family\n-    meta_llama3_8b = \"meta-llama/Meta-Llama-3-8B\"\n-    meta_llama3_70b = \"meta-llama/Meta-Llama-3-70B\"\n-    meta_llama3_8b_instruct = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n-    meta_llama3_70b_instruct = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n+    meta_llama3_8b = \"Llama-3-8B\"\n+    meta_llama3_70b = \"Llama-3-70B\"\n+    meta_llama3_8b_instruct = \"Llama-3-8B-Instruct\"\n+    meta_llama3_70b_instruct = \"Llama-3-70B-Instruct\"\n \n     # Llama 3.1 family\n     meta_llama3_1_8b = \"Meta-Llama3.1-8B\"\n@@ -81,6 +89,41 @@ class CoreModelId(Enum):\n     prompt_guard_86m = \"Prompt-Guard-86M\"\n \n \n+def model_family(CoreModelId) -> ModelFamily:\n+    if CoreModelId in [\n+        CoreModelId.meta_llama2_7b,\n+        CoreModelId.meta_llama2_13b,\n+        CoreModelId.meta_llama2_70b,\n+        CoreModelId.meta_llama2_7b_chat,\n+        CoreModelId.meta_llama2_13b_chat,\n+        CoreModelId.meta_llama2_70b_chat,\n+    ]:\n+        return ModelFamily.llama2\n+    elif CoreModelId in [\n+        CoreModelId.meta_llama3_8b,\n+        CoreModelId.meta_llama3_70b,\n+        CoreModelId.meta_llama3_8b_instruct,\n+        CoreModelId.meta_llama3_70b_instruct,\n+    ]:\n+        return ModelFamily.llama3\n+    elif CoreModelId in [\n+        CoreModelId.meta_llama3_1_8b,\n+        CoreModelId.meta_llama3_1_70b,\n+        CoreModelId.meta_llama3_1_405b,\n+        CoreModelId.meta_llama3_1_8b_instruct,\n+        CoreModelId.meta_llama3_1_70b_instruct,\n+        CoreModelId.meta_llama3_1_405b_instruct,\n+    ]:\n+        return ModelFamily.llama3_1\n+    elif CoreModelId in [\n+        CoreModelId.llama_guard_3_8b,\n+        CoreModelId.prompt_guard_86m,\n+    ]:\n+        return ModelFamily.safety\n+    else:\n+        raise ValueError(f\"Unknown model family for {CoreModelId}\")\n+\n+\n @json_schema_type\n class HardwareRequirements(BaseModel):\n     memory_gb_per_gpu: int\n@@ -96,6 +139,34 @@ class Model(BaseModel):\n     core_model_id: CoreModelId\n     is_default_variant: bool\n \n+    @property\n+    def model_family(self) -> ModelFamily:\n+        return model_family(self.core_model_id)\n+\n+    # Featured models are shown in the non-exhaustive model list\n+    @property\n+    def is_featured(self) -> bool:\n+        return self.model_family in [\n+            ModelFamily.llama3_1,\n+            ModelFamily.safety,\n+        ]\n+\n+    @property\n+    def max_seq_length(self) -> int:\n+        if self.model_family == ModelFamily.llama2:\n+            return 4096\n+        elif self.model_family == ModelFamily.llama3:\n+            return 8192\n+        elif self.model_family == ModelFamily.llama3_1:\n+            return 131072\n+        elif self.core_model_id in [\n+            CoreModelId.llama_guard_3_8b,\n+            CoreModelId.prompt_guard_86m,\n+        ]:\n+            return 131072\n+        else:\n+            raise ValueError(f\"Unknown max_seq_len for {self.core_model_id}\")\n+\n     # The variant is a string representation of other parameters which helps\n     # uniquely identify the model. this typically includes the quantization\n     # format, model parallel size, etc.\n@@ -116,7 +187,6 @@ def descriptor(self, shorten_default_variant: bool = True) -> str:\n         return f\"{self.core_model_id.value}:{self.variant}\"\n \n     description_markdown: str\n-    max_seq_length: int\n     huggingface_repo: Optional[str] = None\n     hardware_requirements: HardwareRequirements\n     quantization_format: CheckpointQuantizationFormat = (\n\n--- File: models/sku_list.py ---\n@@ -16,9 +16,6 @@\n     SamplingStrategy,\n )\n \n-LLAMA_2_CONTEXT_LENGTH = 4096\n-LLAMA_3_CONTEXT_LENGTH = 8192\n-LLAMA_3_1_CONTEXT_LENGTH = 131072\n VOCAB_SIZE = 128256\n \n \n@@ -34,7 +31,7 @@ def resolve_model(descriptor: str) -> Optional[Model]:\n \n \n def all_registered_models() -> List[Model]:\n-    return base_models() + instruct_models() + safety_models()\n+    return llama2_family() + llama3_family() + llama3_1_family() + safety_models()\n \n \n def recommended_sampling_params() -> SamplingParams:\n@@ -44,14 +41,31 @@ def recommended_sampling_params() -> SamplingParams:\n         top_p=0.9,\n     )\n \n+def llama2_family() -> List[Model]:\n+    return [\n+        *llama2_base_models(),\n+        *llama2_instruct_models(),\n+    ]\n+\n+def llama3_family() -> List[Model]:\n+    return [\n+        *llama3_base_models(),\n+        *llama3_instruct_models(),\n+    ]\n+\n+def llama3_1_family() -> List[Model]:\n+    return [\n+        *llama3_1_base_models(),\n+        *llama3_1_instruct_models(),\n+    ]\n \n def llama2_base_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.meta_llama2_7b,\n+\n             is_default_variant=True,\n             description_markdown=\"Llama 2 7b model\",\n-            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Llama-2-7b\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -75,7 +89,6 @@ def llama2_base_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama2_13b,\n             is_default_variant=True,\n             description_markdown=\"Llama 2 13b model\",\n-            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Llama-2-13b\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -99,11 +112,10 @@ def llama2_base_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama2_70b,\n             is_default_variant=True,\n             description_markdown=\"Llama 2 70b model\",\n-            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Llama-2-70b\",\n             hardware_requirements=HardwareRequirements(\n-                gpu_count=8,\n-                memory_gb_per_gpu=20,\n+                gpu_count=3,\n+                memory_gb_per_gpu=48,\n             ),\n             recommended_sampling_params=recommended_sampling_params(),\n             model_args={\n@@ -128,7 +140,6 @@ def llama3_base_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_8b,\n             is_default_variant=True,\n             description_markdown=\"Llama 3 8b model\",\n-            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3-8B\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -152,7 +163,6 @@ def llama3_base_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_70b,\n             is_default_variant=True,\n             description_markdown=\"Llama 3 70b model\",\n-            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3-70B\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -181,7 +191,6 @@ def llama3_1_base_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_1_8b,\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 8b model\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -206,7 +215,6 @@ def llama3_1_base_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 70b model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=20,\n@@ -230,7 +238,6 @@ def llama3_1_base_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n             huggingface_repo=None,\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=120,\n@@ -253,7 +260,6 @@ def llama3_1_base_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_1_405b,\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 405b model (FP8 quantized)\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-FP8\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -279,7 +285,6 @@ def llama3_1_base_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=16,\n                 memory_gb_per_gpu=70,\n@@ -301,21 +306,13 @@ def llama3_1_base_models() -> List[Model]:\n     ]\n \n \n-def base_models() -> List[Model]:\n-    return [\n-        *llama2_base_models(),\n-        *llama3_base_models(),\n-        *llama3_1_base_models(),\n-    ]\n-\n \n def llama2_instruct_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.meta_llama2_7b_chat,\n             is_default_variant=True,\n             description_markdown=\"Llama 2 7b chat model\",\n-            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Llama-2-7b-chat\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -339,7 +336,6 @@ def llama2_instruct_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama2_13b_chat,\n             is_default_variant=True,\n             description_markdown=\"Llama 2 13b chat model\",\n-            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Llama-2-13b-chat\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -363,7 +359,6 @@ def llama2_instruct_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama2_70b_chat,\n             is_default_variant=True,\n             description_markdown=\"Llama 2 70b chat model\",\n-            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Llama-2-70b-chat\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=3,\n@@ -392,7 +387,6 @@ def llama3_instruct_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_8b_instruct,\n             is_default_variant=True,\n             description_markdown=\"Llama 3 8b instruct model\",\n-            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -416,7 +410,6 @@ def llama3_instruct_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_70b_instruct,\n             is_default_variant=True,\n             description_markdown=\"Llama 3 70b instruct model\",\n-            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=3,\n@@ -445,7 +438,6 @@ def llama3_1_instruct_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_1_8b_instruct,\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 8b instruct model\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -470,7 +462,6 @@ def llama3_1_instruct_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 70b instruct model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=20,\n@@ -494,7 +485,6 @@ def llama3_1_instruct_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b instruct model (BF16 weights)\",\n             huggingface_repo=None,\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=120,\n@@ -518,7 +508,6 @@ def llama3_1_instruct_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=70,\n@@ -543,7 +532,6 @@ def llama3_1_instruct_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b instruct model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=16,\n                 memory_gb_per_gpu=70,\n@@ -565,22 +553,13 @@ def llama3_1_instruct_models() -> List[Model]:\n     ]\n \n \n-def instruct_models() -> List[Model]:\n-    return [\n-        *llama2_instruct_models(),\n-        *llama3_instruct_models(),\n-        *llama3_1_instruct_models(),\n-    ]\n-\n-\n def safety_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n             is_default_variant=True,\n             description_markdown=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n                 memory_gb_per_gpu=20,\n@@ -603,7 +582,6 @@ def safety_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B-INT8\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             quantization_format=CheckpointQuantizationFormat.int8,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -627,7 +605,6 @@ def safety_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Prompt Guard 86M injection safety model\",\n             huggingface_repo=\"meta-llama/Prompt-Guard-86M\",\n-            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n                 memory_gb_per_gpu=1,"
            },
            {
              "sha": "1cda5ba8eacc3e90f0c7564750b54672c158dd02",
              "url": "https://github.com/meta-llama/llama-models/commit/1cda5ba8eacc3e90f0c7564750b54672c158dd02",
              "message": "SKUs: chat models categorized as instruct models",
              "files_changed": [
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/sku_list.py ---\n@@ -34,7 +34,7 @@ def resolve_model(descriptor: str) -> Optional[Model]:\n \n \n def all_registered_models() -> List[Model]:\n-    return base_models() + chat_models() + instruct_models() + safety_models()\n+    return base_models() + instruct_models() + safety_models()\n \n \n def recommended_sampling_params() -> SamplingParams:\n@@ -309,7 +309,7 @@ def base_models() -> List[Model]:\n     ]\n \n \n-def chat_models() -> List[Model]:\n+def llama2_instruct_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.meta_llama2_7b_chat,\n@@ -567,6 +567,7 @@ def llama3_1_instruct_models() -> List[Model]:\n \n def instruct_models() -> List[Model]:\n     return [\n+        *llama2_instruct_models(),\n         *llama3_instruct_models(),\n         *llama3_1_instruct_models(),\n     ]"
            },
            {
              "sha": "3214718f53deec534b9529266e860e84dfb60d2f",
              "url": "https://github.com/meta-llama/llama-models/commit/3214718f53deec534b9529266e860e84dfb60d2f",
              "message": "Add Llama 2 and Llama 3 SKUs",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -54,6 +54,20 @@ class CheckpointQuantizationFormat(Enum):\n class CoreModelId(Enum):\n     \"\"\"Each of these models is a unique \"SKU\". These root models can be served in various garbs (especially by quantizing them)\"\"\"\n \n+    # Llama 2 family\n+    meta_llama2_7b = \"meta-llama/Llama-2-7b\"\n+    meta_llama2_13b = \"meta-llama/Llama-2-13b\"\n+    meta_llama2_70b = \"meta-llama/Llama-2-70b\"\n+    meta_llama2_7b_chat = \"meta-llama/Llama-2-7b-chat\"\n+    meta_llama2_13b_chat = \"meta-llama/Llama-2-13b-chat\"\n+    meta_llama2_70b_chat = \"meta-llama/Llama-2-70b-chat\"\n+\n+    # Llama 3 family\n+    meta_llama3_8b = \"meta-llama/Meta-Llama-3-8B\"\n+    meta_llama3_70b = \"meta-llama/Meta-Llama-3-70B\"\n+    meta_llama3_8b_instruct = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n+    meta_llama3_70b_instruct = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n+\n     # Llama 3.1 family\n     meta_llama3_1_8b = \"Meta-Llama3.1-8B\"\n     meta_llama3_1_70b = \"Meta-Llama3.1-70B\"\n\n--- File: models/sku_list.py ---\n@@ -16,8 +16,9 @@\n     SamplingStrategy,\n )\n \n-\n-CONTEXT_LENGTH = 131072\n+LLAMA_2_CONTEXT_LENGTH = 4096\n+LLAMA_3_CONTEXT_LENGTH = 8192\n+LLAMA_3_1_CONTEXT_LENGTH = 131072\n VOCAB_SIZE = 128256\n \n \n@@ -33,7 +34,7 @@ def resolve_model(descriptor: str) -> Optional[Model]:\n \n \n def all_registered_models() -> List[Model]:\n-    return base_models() + instruct_models() + safety_models()\n+    return base_models() + chat_models() + instruct_models() + safety_models()\n \n \n def recommended_sampling_params() -> SamplingParams:\n@@ -44,13 +45,143 @@ def recommended_sampling_params() -> SamplingParams:\n     )\n \n \n-def base_models() -> List[Model]:\n+def llama2_base_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.meta_llama2_7b,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 2 7b model\",\n+            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Llama-2-7b\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=20,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.meta_llama2_13b,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 2 13b model\",\n+            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Llama-2-13b\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=28,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 5120,\n+                \"n_layers\": 40,\n+                \"n_heads\": 40,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.meta_llama2_70b,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 2 70b model\",\n+            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Llama-2-70b\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=8,\n+                memory_gb_per_gpu=20,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+    ]\n+\n+\n+def llama3_base_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_8b,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 3 8b model\",\n+            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Meta-Llama-3-8B\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=20,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_70b,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 3 70b model\",\n+            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Meta-Llama-3-70B\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=8,\n+                memory_gb_per_gpu=20,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+    ]\n+\n+\n+def llama3_1_base_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_8b,\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 8b model\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -75,7 +206,7 @@ def base_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 70b model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=20,\n@@ -99,7 +230,7 @@ def base_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n             huggingface_repo=None,\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=120,\n@@ -122,7 +253,7 @@ def base_models() -> List[Model]:\n             core_model_id=CoreModelId.meta_llama3_1_405b,\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 405b model (FP8 quantized)\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-FP8\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n@@ -148,7 +279,7 @@ def base_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=16,\n                 memory_gb_per_gpu=70,\n@@ -170,13 +301,151 @@ def base_models() -> List[Model]:\n     ]\n \n \n-def instruct_models() -> List[Model]:\n+def base_models() -> List[Model]:\n+    return [\n+        *llama2_base_models(),\n+        *llama3_base_models(),\n+        *llama3_1_base_models(),\n+    ]\n+\n+\n+def chat_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.meta_llama2_7b_chat,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 2 7b chat model\",\n+            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Llama-2-7b-chat\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=14,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.meta_llama2_13b_chat,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 2 13b chat model\",\n+            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Llama-2-13b-chat\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=28,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 5120,\n+                \"n_layers\": 40,\n+                \"n_heads\": 40,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.meta_llama2_70b_chat,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 2 70b chat model\",\n+            max_seq_length=LLAMA_2_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Llama-2-70b-chat\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=3,\n+                memory_gb_per_gpu=48,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+    ]\n+\n+\n+def llama3_instruct_models() -> List[Model]:\n+    return [\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_8b_instruct,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 3 8b instruct model\",\n+            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=20,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+        Model(\n+            core_model_id=CoreModelId.meta_llama3_70b_instruct,\n+            is_default_variant=True,\n+            description_markdown=\"Llama 3 70b instruct model\",\n+            max_seq_length=LLAMA_3_CONTEXT_LENGTH,\n+            huggingface_repo=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=3,\n+                memory_gb_per_gpu=48,\n+            ),\n+            recommended_sampling_params=recommended_sampling_params(),\n+            model_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n+    ]\n+\n+\n+def llama3_1_instruct_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.meta_llama3_1_8b_instruct,\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 8b instruct model\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -201,7 +470,7 @@ def instruct_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 70b instruct model\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=20,\n@@ -225,7 +494,7 @@ def instruct_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b instruct model (BF16 weights)\",\n             huggingface_repo=None,\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=120,\n@@ -249,7 +518,7 @@ def instruct_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Llama 3.1 405b instruct model (FP8 quantized)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=8,\n                 memory_gb_per_gpu=70,\n@@ -274,7 +543,7 @@ def instruct_models() -> List[Model]:\n             is_default_variant=False,\n             description_markdown=\"Llama 3.1 405b instruct model (BF16 weights)\",\n             huggingface_repo=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=16,\n                 memory_gb_per_gpu=70,\n@@ -296,14 +565,21 @@ def instruct_models() -> List[Model]:\n     ]\n \n \n+def instruct_models() -> List[Model]:\n+    return [\n+        *llama3_instruct_models(),\n+        *llama3_1_instruct_models(),\n+    ]\n+\n+\n def safety_models() -> List[Model]:\n     return [\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n             is_default_variant=True,\n             description_markdown=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n                 memory_gb_per_gpu=20,\n@@ -318,15 +594,15 @@ def safety_models() -> List[Model]:\n                 \"norm_eps\": 1e-05,\n                 \"rope_theta\": 500000.0,\n                 \"use_scaled_rope\": False,\n-                \"vocab_size\": 128256,\n+                \"vocab_size\": VOCAB_SIZE,\n             },\n         ),\n         Model(\n             core_model_id=CoreModelId.llama_guard_3_8b,\n             is_default_variant=False,\n             description_markdown=\"Llama Guard v3 8b system safety model\",\n             huggingface_repo=\"meta-llama/Llama-Guard-3-8B-INT8\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             quantization_format=CheckpointQuantizationFormat.int8,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n@@ -350,7 +626,7 @@ def safety_models() -> List[Model]:\n             is_default_variant=True,\n             description_markdown=\"Prompt Guard 86M injection safety model\",\n             huggingface_repo=\"meta-llama/Prompt-Guard-86M\",\n-            max_seq_length=CONTEXT_LENGTH,\n+            max_seq_length=LLAMA_3_1_CONTEXT_LENGTH,\n             hardware_requirements=HardwareRequirements(\n                 gpu_count=1,\n                 memory_gb_per_gpu=1,"
            },
            {
              "sha": "6214a21dc837ce63983ef3fd7b172a6ed16e4905",
              "url": "https://github.com/meta-llama/llama-models/commit/6214a21dc837ce63983ef3fd7b172a6ed16e4905",
              "message": "Add links to shields",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -10,8 +10,8 @@\n \n # Llama Models\n \n-![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models) \n-![Discord](https://img.shields.io/discord/1257833999603335178) \n+[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n+[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n \n \n Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:"
            },
            {
              "sha": "bcc387ffd0d6e4ab82d6b46fbb32fb00c461646f",
              "url": "https://github.com/meta-llama/llama-models/commit/bcc387ffd0d6e4ab82d6b46fbb32fb00c461646f",
              "message": "Add shields to README",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -9,6 +9,11 @@\n ---\n \n # Llama Models\n+\n+![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models) \n+![Discord](https://img.shields.io/discord/1257833999603335178) \n+\n+\n Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n 1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n 2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!"
            }
          ]
        }
      ]
    },
    {
      "id": 7403219,
      "username": "dvrogozh",
      "url": "https://github.com/dvrogozh",
      "avatar_url": "https://avatars.githubusercontent.com/u/7403219?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "cb246012bc7177ac32ae17ed9611d3327b890cba",
              "url": "https://github.com/meta-llama/llama-models/commit/cb246012bc7177ac32ae17ed9611d3327b890cba",
              "message": "feat: support xccl distributed backend in llama3 (#319)",
              "files_changed": [
                {
                  "filename": "models/llama3/generation.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/generation.py ---\n@@ -10,6 +10,7 @@\n import sys\n import time\n from pathlib import Path\n+from packaging import version\n from typing import Callable, Generator, List, Optional\n \n import torch\n@@ -29,6 +30,12 @@\n from .tokenizer import Tokenizer\n \n \n+def is_xccl_available():\n+    if version.parse(torch.__version__).release >= version.parse(\"2.7\").release:\n+        return torch.distributed.distributed_c10d.is_xccl_available()\n+    return False\n+\n+\n class Llama3:\n     @staticmethod\n     def build(\n@@ -52,6 +59,8 @@ def build(\n         if not torch.distributed.is_initialized():\n             if device.type == \"cuda\":\n                 torch.distributed.init_process_group(\"nccl\")\n+            elif device.type == \"xpu\" and is_xccl_available():\n+                torch.distributed.init_process_group(\"xccl\")\n             else:\n                 torch.distributed.init_process_group(\"gloo\")"
            },
            {
              "sha": "5c4377d2a9be016b137cd20c765cdf52589f4b72",
              "url": "https://github.com/meta-llama/llama-models/commit/5c4377d2a9be016b137cd20c765cdf52589f4b72",
              "message": "feat: add xpu support to llama3 completion scripts (#318)",
              "files_changed": [
                {
                  "filename": "models/llama3/scripts/chat_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/scripts/completion.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/scripts/chat_completion.py ---\n@@ -18,9 +18,22 @@\n from models.datatypes import RawMediaItem, RawMessage, RawTextItem, StopReason\n from models.llama3.generation import Llama3\n \n+import os\n+import torch\n+\n THIS_DIR = Path(__file__).parent\n \n \n+def get_device():\n+    if \"DEVICE\" in os.environ:\n+        return os.environ[\"DEVICE\"]\n+    if torch.cuda.is_available():\n+        return \"cuda\"\n+    elif torch.xpu.is_available():\n+        return \"xpu\"\n+    return \"cpu\"\n+\n+\n def run_main(\n     ckpt_dir: str,\n     temperature: float = 0.6,\n@@ -36,6 +49,7 @@ def run_main(\n         max_batch_size=max_batch_size,\n         world_size=world_size,\n         quantization_mode=quantization_mode,\n+        device=get_device(),\n     )\n \n     dialogs = [\n\n--- File: models/llama3/scripts/completion.py ---\n@@ -18,9 +18,23 @@\n from models.datatypes import RawMediaItem\n from models.llama3.generation import Llama3\n \n+import os\n+import torch\n+\n+\n THIS_DIR = Path(__file__).parent\n \n \n+def get_device():\n+    if \"DEVICE\" in os.environ:\n+        return os.environ[\"DEVICE\"]\n+    if torch.cuda.is_available():\n+        return \"cuda\"\n+    elif torch.xpu.is_available():\n+        return \"xpu\"\n+    return \"cpu\"\n+\n+\n def run_main(\n     ckpt_dir: str,\n     temperature: float = 0.6,\n@@ -36,6 +50,7 @@ def run_main(\n         max_batch_size=max_batch_size,\n         world_size=world_size,\n         quantization_mode=quantization_mode,\n+        device=get_device(),\n     )\n \n     interleaved_contents = ["
            },
            {
              "sha": "0ed6a1f4ce962bdf9c97b44d5032dcc3bc8c8394",
              "url": "https://github.com/meta-llama/llama-models/commit/0ed6a1f4ce962bdf9c97b44d5032dcc3bc8c8394",
              "message": "fix: fix llama3 generation test (#317)",
              "files_changed": [
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/tests/api/test_generation.py ---\n@@ -9,12 +9,11 @@\n import unittest\n from pathlib import Path\n \n-import numpy as np\n import pytest\n import torch\n \n from llama_models.datatypes import RawMediaItem, RawMessage, RawTextItem\n-from llama_models.llama3.generation import Llama\n+from llama_models.llama3.generation import Llama3\n \n THIS_DIR = Path(__file__).parent\n \n@@ -38,7 +37,7 @@ def build_generator(env_var: str, device: str):\n     os.environ[\"WORLD_SIZE\"] = \"1\"\n     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n     os.environ[\"MASTER_PORT\"] = \"29501\"\n-    return Llama.build(ckpt_dir=os.environ[env_var], max_seq_len=128, max_batch_size=1, world_size=1, device=device)\n+    return Llama3.build(ckpt_dir=os.environ[env_var], max_seq_len=128, max_batch_size=1, world_size=1, device=device)\n \n \n class TestTextModelInference(unittest.TestCase):\n@@ -60,18 +59,20 @@ def test_run_generation(self):\n             ],\n         ]\n         for dialog in dialogs:\n-            result = self.__class__.generator.chat_completion(\n-                dialog,\n+            batch = [dialog]\n+            out_message = []\n+            for token_results in self.__class__.generator.chat_completion(\n+                batch,\n                 temperature=0,\n                 logprobs=True,\n-            )\n-\n-            out_message = result.generation\n-            self.assertTrue(len(out_message.content) > 0)\n-            shape = np.array(result.logprobs).shape\n+            ):\n+                result = token_results[0]\n+                if result.finished:\n+                    break\n+                out_message += [result]\n+                self.assertTrue(len(result.text) > 0)\n             # assert at least 10 tokens\n-            self.assertTrue(shape[0] > 10)\n-            self.assertEqual(shape[1], 1)\n+            self.assertTrue(len(out_message) > 10)\n \n \n @pytest.mark.skipif(get_device() == \"\", reason=\"No device available and none specified\")\n@@ -89,7 +90,7 @@ def setUpClass(cls):\n     @unittest.skip(\"Disabling vision model test\")\n     @pytest.mark.skip(reason=\"Disabling vision model test\")\n     def test_run_generation(self):\n-        with open(THIS_DIR.parent.parent.parent / \"scripts/resources/dog.jpg\", \"rb\") as f:\n+        with open(THIS_DIR.parent.parent.parent / \"resources/dog.jpg\", \"rb\") as f:\n             img = f.read()\n \n         dialogs = [\n@@ -111,18 +112,20 @@ def test_run_generation(self):\n         ]\n \n         for dialog in dialogs:\n-            result = self.__class__.generator.chat_completion(\n-                dialog,\n+            batch = [dialog]\n+            out_message = []\n+            for token_results in self.__class__.generator.chat_completion(\n+                batch,\n                 temperature=0,\n                 logprobs=True,\n-            )\n-\n-            out_message = result.generation\n-            self.assertTrue(len(out_message.content) > 0)\n-            shape = np.array(result.logprobs).shape\n+            ):\n+                result = token_results[0]\n+                if result.finished:\n+                    break\n+                out_message += [result]\n+                self.assertTrue(len(result.text) > 0)\n             # assert at least 10 tokens\n-            self.assertTrue(shape[0] > 10)\n-            self.assertEqual(shape[1], 1)\n+            self.assertTrue(len(out_message) > 10)\n \n \n @pytest.mark.skipif(get_device() == \"\", reason=\"No device available and none specified\")"
            },
            {
              "sha": "224d48ca38c985dc77e79a842d5e1e7a5c6832f3",
              "url": "https://github.com/meta-llama/llama-models/commit/224d48ca38c985dc77e79a842d5e1e7a5c6832f3",
              "message": "feat: support non-cuda devices for text and vision models (#233)",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/reference_impl/multimodal/model.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_generation.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/generation.py ---\n@@ -68,6 +68,7 @@ def build(\n         model_parallel_size: Optional[int] = None,\n         tokenizer_path: Optional[str] = None,\n         seed: int = 1,\n+        device: str = \"cuda\"\n     ):\n         \"\"\"\n         Build a Llama instance by initializing and loading a model checkpoint.\n@@ -79,30 +80,43 @@ def build(\n             max_batch_size (int): Maximum batch size for inference.\n             model_parallel_size (Optional[int], optional): Number of model parallel processes.\n                 If not provided, it's determined from the environment. Defaults to None.\n+            device (str, optional): Device to use, e.g. cuda (default), xpu, cpu, etc.\n \n         Returns:\n             Llama: An instance of the Llama class with the loaded model and tokenizer.\n \n         Raises:\n             AssertionError: If there are no checkpoint files in the specified directory,\n                 or if the model parallel size does not match the number of checkpoint files.\n+            RuntimeError: If PyTorch backend for the specified device is not available.\n \n \n         Note:\n             This method initializes the distributed process group, sets the device to CUDA,\n             and loads the pre-trained model and tokenizer.\n         \"\"\"\n \n+        device = torch.device(device)\n+        if (device.type == \"cuda\" and not torch.cuda.is_available() or\n+            device.type == \"xpu\" and not torch.xpu.is_available()):\n+            raise RuntimeError(f\"PyTorch backend for {device.type} device type is not available\")\n+\n         if not torch.distributed.is_initialized():\n-            torch.distributed.init_process_group(\"nccl\")\n+            if device.type == \"cuda\":\n+                torch.distributed.init_process_group(\"nccl\")\n+            else:\n+                torch.distributed.init_process_group(\"gloo\")\n \n         if not model_parallel_is_initialized():\n             if model_parallel_size is None:\n                 model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n             initialize_model_parallel(model_parallel_size)\n \n         local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n-        torch.cuda.set_device(local_rank)\n+        if device.type == \"cuda\":\n+            torch.cuda.set_device(local_rank)\n+        elif device.type == \"xpu\":\n+            torch.xpu.set_device(local_rank)\n \n         torch.manual_seed(seed)\n \n@@ -132,18 +146,29 @@ def build(\n             tokenizer = Tokenizer.get_instance()\n \n         assert model_args.vocab_size == tokenizer.n_words\n-        if torch.cuda.is_bf16_supported():\n-            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+        torch.set_default_device(device)\n+        if device.type == \"cuda\":\n+            if torch.cuda.is_bf16_supported():\n+                torch.set_default_dtype(torch.bfloat16)\n+            else:\n+                torch.set_default_dtype(torch.half)\n+        elif device.type == \"xpu\":\n+            if torch.xpu.is_bf16_supported():\n+                torch.set_default_dtype(torch.bfloat16)\n+            else:\n+                torch.set_default_dtype(torch.half)\n         else:\n-            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n+            torch.set_default_dtype(torch.half)\n+\n         if model_args.vision_chunk_size > 0:\n             from .multimodal.model import CrossAttentionTransformer\n \n             model = CrossAttentionTransformer(model_args)\n-            model.setup_cache(model_args.max_batch_size, torch.bfloat16)\n+            model.setup_cache(model_args.max_batch_size, torch.get_default_dtype())\n         else:\n             model = Transformer(model_args)\n         model.load_state_dict(checkpoint, strict=True)\n+        model.to(device)\n         print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n \n         return Llama(model, tokenizer, model_args)\n@@ -207,14 +232,14 @@ def generate(\n             )\n \n         pad_id = self.tokenizer.pad_id\n-        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n+        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long)\n         for k, t in enumerate(prompt_tokens):\n-            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n+            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long)\n         if logprobs:\n             token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n \n         prev_pos = 0\n-        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n+        eos_reached = torch.tensor([False] * bsz)\n         input_text_mask = tokens != pad_id\n \n         if echo:\n@@ -231,7 +256,7 @@ def generate(\n         for cur_pos in range(min_prompt_len, total_len):\n             if is_vision:\n                 position_ids = torch.arange(\n-                    prev_pos, cur_pos, dtype=torch.long, device=\"cuda\"\n+                    prev_pos, cur_pos, dtype=torch.long\n                 )\n                 text_only_inference = model_input.vision is None\n                 logits = self.model.forward(\n\n--- File: models/llama3/reference_impl/model.py ---\n@@ -158,15 +158,15 @@ def __init__(self, args: ModelArgs):\n                 self.n_local_kv_heads,\n                 self.head_dim,\n             )\n-        ).cuda()\n+        )\n         self.cache_v = torch.zeros(\n             (\n                 args.max_batch_size,\n                 args.max_seq_len,\n                 self.n_local_kv_heads,\n                 self.head_dim,\n             )\n-        ).cuda()\n+        )\n \n     def forward(\n         self,\n\n--- File: models/llama3/reference_impl/multimodal/model.py ---\n@@ -1113,7 +1113,7 @@ def forward(\n         # aspect_ratios: (B, T)\n         # h: (B, T, D)\n         vision_tokens = self.vision_encoder(\n-            images.to(dtype=torch.bfloat16), aspect_ratios\n+            images.to(dtype=torch.get_default_dtype()), aspect_ratios\n         )\n \n         vision_tokens = F.linear(\n@@ -1407,8 +1407,6 @@ def compute_vision_tokens_masks(\n         else:\n             vision_tokens = self.vision_model(stacked_images, aspect_ratios)\n \n-        vision_tokens = vision_tokens.to(\"cuda\")\n-\n         bsz, nimg, nchunk, ntok, image_token_dim = tuple(vision_tokens.shape)\n         xattn_caches = torch.stack(\n             [\n@@ -1428,7 +1426,7 @@ def compute_vision_tokens_masks(\n         cross_attention_masks, full_text_row_masked_out_mask = (\n             self.text_model._get_xattn_mask(\n                 num_tokens=total_len,\n-                text_device=\"cuda\",\n+                text_device=vision_tokens.device.type,\n                 text_dtype=next(self.text_model.parameters()).dtype,\n                 vision_tokens=vision_tokens,\n                 cross_attention_masks=padded_masks,\n@@ -1495,7 +1493,7 @@ def _pad_masks(\n     total_len: int,\n     max_num_chunks: int,\n ) -> torch.Tensor:\n-    dtype = torch.bfloat16\n+    dtype = torch.get_default_dtype()\n     inf_value = get_negative_inf_value(dtype)\n \n     bsz = len(all_masks)\n\n--- File: models/llama3/tests/api/test_generation.py ---\n@@ -12,14 +12,26 @@\n \n import numpy as np\n import pytest\n+import torch\n from llama_models.llama3.api.datatypes import RawMediaItem, RawMessage, RawTextItem\n \n from llama_models.llama3.reference_impl.generation import Llama\n \n THIS_DIR = Path(__file__).parent\n \n \n-def build_generator(env_var: str):\n+def get_device():\n+    if 'DEVICE' in os.environ:\n+        return os.environ['DEVICE']\n+\n+    if torch.cuda.is_available():\n+        return \"cuda\"\n+    elif torch.xpu.is_available():\n+        return \"xpu\"\n+    return \"\"\n+\n+\n+def build_generator(env_var: str, device: str):\n     if env_var not in os.environ:\n         raise ValueError(f\"{env_var} must be specified for this test\")\n \n@@ -32,13 +44,16 @@ def build_generator(env_var: str):\n         max_seq_len=128,\n         max_batch_size=1,\n         model_parallel_size=1,\n+        device=device\n     )\n \n \n class TestTextModelInference(unittest.TestCase):\n+    device = \"cpu\"\n+\n     @classmethod\n     def setUpClass(cls):\n-        cls.generator = build_generator(\"TEXT_MODEL_CHECKPOINT_DIR\")\n+        cls.generator = build_generator(\"TEXT_MODEL_CHECKPOINT_DIR\", cls.device)\n \n     def test_run_generation(self):\n         dialogs = [\n@@ -68,10 +83,17 @@ def test_run_generation(self):\n             self.assertEqual(shape[1], 1)\n \n \n+@pytest.mark.skipif(get_device() == \"\", reason=\"No device available and none specified\")\n+class TestTextModelInferenceOnDevice(TestTextModelInference):\n+    device = get_device()\n+\n+\n class TestVisionModelInference(unittest.TestCase):\n+    device = \"cpu\"\n+\n     @classmethod\n     def setUpClass(cls):\n-        cls.generator = build_generator(\"VISION_MODEL_CHECKPOINT_DIR\")\n+        cls.generator = build_generator(\"VISION_MODEL_CHECKPOINT_DIR\", cls.device)\n \n     @unittest.skip(\"Disabling vision model test\")\n     @pytest.mark.skip(reason=\"Disabling vision model test\")\n@@ -112,3 +134,8 @@ def test_run_generation(self):\n             # assert at least 10 tokens\n             self.assertTrue(shape[0] > 10)\n             self.assertEqual(shape[1], 1)\n+\n+\n+@pytest.mark.skipif(get_device() == \"\", reason=\"No device available and none specified\")\n+class TestVisionModelInferenceOnDevice(TestVisionModelInference):\n+    device = get_device()"
            },
            {
              "sha": "41fd2443249de9f155ee7446152b66023820124c",
              "url": "https://github.com/meta-llama/llama-models/commit/41fd2443249de9f155ee7446152b66023820124c",
              "message": "ci: fix missing pypi packages for PR ci (#234)",
              "files_changed": [
                {
                  "filename": ".github/workflows/gha_workflow_llama_models_tests.yml",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/gha_workflow_llama_models_tests.yml ---\n@@ -165,6 +165,7 @@ jobs:\n           pip install llama-models\n           pip install xmlrunner\n           pip install pytest\n+          pip install numpy\n \n       - name: \"Installing specific manual_dispatch dependencies\"\n         id: manual_install_pip"
            }
          ]
        }
      ]
    },
    {
      "id": 304457,
      "username": "ehhuang",
      "url": "https://github.com/ehhuang",
      "avatar_url": "https://avatars.githubusercontent.com/u/304457?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "9921d278bf26cf4f3385e7bea399c2b6e7847f8b",
              "url": "https://github.com/meta-llama/llama-models/commit/9921d278bf26cf4f3385e7bea399c2b6e7847f8b",
              "message": "update llama4 prompt_format.md (#332)",
              "files_changed": [
                {
                  "filename": "models/llama4/prompt_format.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/prompt_format.md ---\n@@ -64,7 +64,7 @@ This example passes an image that is smaller than the tile size, to show the til\n \n ##### Model Response Format\n ```\n-The image depicts a dog standing on a skateboard, with its front paws positioned on the board and its back paws hanging off the back. The dog has a distinctive coat pattern, featuring a white face, brown and black fur, and white paws, and is standing on a skateboard with red wheels, set against a blurred background of a street or alleyway with a teal door and beige wall.<|eot|>\n+The image depicts a dog standing on a skateboard, positioned centrally and facing the camera directly. The dog has a distinctive coat pattern featuring white, black, and brown fur, with floppy ears and a black nose, and is standing on a skateboard with red wheels.<|eot|>\n ```\n \n \n@@ -91,7 +91,7 @@ Here is an example of how to pass an image to the model\n \n ##### Model Response Format\n ```\n-This image shows a dog standing on a skateboard, with its front paws positioned near the front of the board and its back paws near the back. The dog has a white, black, and orange coat, and is standing on a gray skateboard with red wheels, in front of a blurred background that appears to be a street or alleyway.<|eot|>\n+The image depicts a dog standing on a skateboard, with the dog positioned centrally and facing forward. The dog has a distinctive coat featuring a mix of white, brown, and black fur, and is wearing a collar as it stands on the skateboard, which has red wheels.<|eot|>\n ```\n \n \n@@ -117,7 +117,7 @@ Here is an example of how to pass an image to the model\n \n ##### Model Response Format\n ```\n-The first image shows a dog standing on a skateboard, while the second image shows a plate of spaghetti with tomato sauce, parmesan cheese, and parsley. The two images are unrelated, with the first image featuring a dog and the second image featuring a food dish, and they do not share any common elements or themes.<|eot|>\n+The first image features a dog standing on a skateboard, while the second image showcases a plate of spaghetti with tomato sauce and cheese. The two images appear to be unrelated, with one depicting a playful scene of a dog on a skateboard and the other presenting a classic Italian dish.<|eom|>\n ```\n \n \n@@ -135,13 +135,44 @@ We are continuing the format for zero shot function calling used in previous ver\n ```\n <|begin_of_text|><|header_start|>system<|header_end|>\n \n-You are an expert in composing functions. You are given a question and a set of possible functions.\n-Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n-If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n-also point it out. You should only return the function call in tools call sections.\n+You are a helpful assistant and an expert in function composition. You can answer general questions using your internal knowledge OR invoke functions when necessary. Follow these strict guidelines:\n+\n+1. FUNCTION CALLS:\n+- ONLY use functions that are EXPLICITLY listed in the function list below\n+- If NO functions are listed (empty function list []), respond ONLY with internal knowledge or \"I don't have access to [Unavailable service] information\"\n+- If a function is not in the list, respond ONLY with internal knowledge or \"I don't have access to [Unavailable service] information\"\n+- If ALL required parameters are present AND the query EXACTLY matches a listed function's purpose: output ONLY the function call(s)\n+- Use exact format: [func_name1(param1=value1, param2=value2), func_name2(...)]\n+Examples:\n+CORRECT: [get_weather(location=\"Vancouver\"), calculate_route(start=\"Boston\", end=\"New York\")] <- Only if get_weather and calculate_route are in function list\n+INCORRECT: get_weather(location=\"New York\")\n+INCORRECT: Let me check the weather: [get_weather(location=\"New York\")]\n+INCORRECT: [get_events(location=\"Singapore\")] <- If function not in list\n+\n+2. RESPONSE RULES:\n+- For pure function requests matching a listed function: ONLY output the function call(s)\n+- For knowledge questions: ONLY output text\n+- For missing parameters: ONLY request the specific missing parameters\n+- For unavailable services (not in function list): output ONLY with internal knowledge or \"I don't have access to [Unavailable service] information\". Do NOT execute a function call.\n+- If the query asks for information beyond what a listed function provides: output ONLY with internal knowledge about your limitations\n+- NEVER combine text and function calls in the same response\n+- NEVER suggest alternative functions when the requested service is unavailable\n+- NEVER create or invent new functions not listed below\n+\n+3. STRICT BOUNDARIES:\n+- ONLY use functions from the list below - no exceptions\n+- NEVER use a function as an alternative to unavailable information\n+- NEVER call functions not present in the function list\n+- NEVER add explanatory text to function calls\n+- NEVER respond with empty brackets\n+- Use proper Python/JSON syntax for function calls\n+- Check the function list carefully before responding\n+\n+4. TOOL RESPONSE HANDLING:\n+- When receiving tool responses: provide concise, natural language responses\n+- Don't repeat tool response verbatim\n+- Don't add supplementary information\n \n-If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n-You SHOULD NOT include any other text in the response.\n \n Here is a list of functions in JSON format that you can invoke.\n \n@@ -151,9 +182,7 @@ Here is a list of functions in JSON format that you can invoke.\n         \"description\": \"Get weather info for places\",\n         \"parameters\": {\n             \"type\": \"dict\",\n-            \"required\": [\n-                \"city\"\n-            ],\n+            \"required\": [\"city\"],\n             \"properties\": {\n                 \"city\": {\n                     \"type\": \"string\",\n@@ -167,7 +196,10 @@ Here is a list of functions in JSON format that you can invoke.\n             }\n         }\n     }\n-<|eot|><|header_start|>user<|header_end|>\n+]\n+\n+You can answer general questions or invoke tools when necessary.\n+In addition to tool calls, you should also augment your responses by using the tool outputs.<|eot|><|header_start|>user<|header_end|>\n \n What is the weather in SF and Seattle?<|eot|><|header_start|>assistant<|header_end|>\n \n@@ -176,7 +208,7 @@ What is the weather in SF and Seattle?<|eot|><|header_start|>assistant<|header_e\n \n ##### Model Response Format\n ```\n-[get_weather(city='SF'), get_weather(city='Seattle')]<|eot|>\n+[get_weather(city=\"San Francisco\"), get_weather(city=\"Seattle\")]<|eot|>\n ```\n \n \n@@ -273,5 +305,5 @@ Use tools to get latest trending songs<|eot|><|header_start|>assistant<|header_e\n \n ##### Model Response Format\n ```\n-<function=trending_songs>{\"n\": \"10\"}</function><|eot|>\n+<function=trending_songs>{\"n\": 10}</function><|eot|>\n ```"
            },
            {
              "sha": "2411407c93b5addc9ab379f9a5c96245ad221cc7",
              "url": "https://github.com/meta-llama/llama-models/commit/2411407c93b5addc9ab379f9a5c96245ad221cc7",
              "message": "fix: tool_call was not encoded (#284)",
              "files_changed": [
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/chat_format.py ---\n@@ -149,8 +149,9 @@ def _process_content(c):\n     def encode_dialog_prompt(\n         self,\n         messages: List[RawMessage],\n-        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,\n+        tool_prompt_format: Optional[ToolPromptFormat] = None,\n     ) -> LLMInput:\n+        tool_prompt_format = tool_prompt_format or ToolPromptFormat.json\n         tokens = []\n         images = []\n         tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n\n--- File: models/llama3/api/tool_utils.py ---\n@@ -190,3 +190,5 @@ def format_value(value: RecursiveType) -> str:\n \n                 args_str = \", \".join(f\"{k}={format_value(v)}\" for k, v in t.arguments.items())\n                 return f\"[{fname}({args_str})]\"\n+            else:\n+                raise ValueError(f\"Unsupported tool prompt format: {tool_prompt_format}\")"
            },
            {
              "sha": "b6dad01a6b1eedabd6bc88b801dc23325045b700",
              "url": "https://github.com/meta-llama/llama-models/commit/b6dad01a6b1eedabd6bc88b801dc23325045b700",
              "message": "fix: do not use python_tag when encoding non-code_interpreter tool_calls (#283)",
              "files_changed": [
                {
                  "filename": "models/__init__.pyc",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/__init__.pyc",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/api/chat_format.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/api/tool_utils.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/api/test_tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: models/llama3/api/chat_format.py ---\n@@ -119,7 +119,11 @@ def _process_content(c):\n             tokens.extend(toks)\n             images.extend(imgs)\n \n-        if message.role == \"assistant\" and len(message.tool_calls) > 0:\n+        if (\n+            message.role == \"assistant\"\n+            and len(message.tool_calls) > 0\n+            and message.tool_calls[0].tool_name == BuiltinTool.code_interpreter\n+        ):\n             tokens.append(self.tokenizer.special_tokens[\"<|python_tag|>\"])\n \n         _process_content(message.content)\n\n--- File: models/llama3/api/tool_utils.py ---\n@@ -130,6 +130,7 @@ def maybe_extract_custom_tool_call(message_body: str) -> Optional[Tuple[str, str\n                 return tool_name, json.loads(query.replace(\"'\", '\"'))\n             except Exception as e:\n                 print(\"Exception while parsing json query for custom tool call\", query, e)\n+                return None\n         elif is_json(message_body):\n             response = json.loads(message_body)\n             if (\"type\" in response and response[\"type\"] == \"function\") or (\"name\" in response):\n\n--- File: models/llama3/tests/api/test_tokenizer.py ---\n@@ -10,7 +10,7 @@\n \n from unittest import TestCase\n \n-from llama_models.datatypes import RawMessage, ToolPromptFormat\n+from llama_models.datatypes import BuiltinTool, RawMessage, ToolCall, ToolPromptFormat\n \n from llama_models.llama3.api.chat_format import ChatFormat\n from llama_models.llama3.api.tokenizer import Tokenizer\n@@ -63,6 +63,51 @@ def test_encode_message(self):\n             ],\n         )\n \n+    def test_encode_message_with_tool_call(self):\n+        message = RawMessage(\n+            role=\"assistant\",\n+            content=\"\",\n+            tool_calls=[\n+                ToolCall(\n+                    tool_name=BuiltinTool.code_interpreter,\n+                    arguments={\"code\": \"print('Hello, world!')\"},\n+                    call_id=\"1\",\n+                )\n+            ],\n+        )\n+        self.assertEqual(\n+            self.format.encode_message(message, tool_prompt_format=ToolPromptFormat.python_list)[0][:5],\n+            [\n+                self.format.tokenizer.special_tokens[\"<|start_header_id|>\"],\n+                self.format.tokenizer.encode(message.role, bos=False, eos=False)[0],\n+                self.format.tokenizer.special_tokens[\"<|end_header_id|>\"],\n+                self.format.tokenizer.encode(\"\\n\\n\", bos=False, eos=False)[0],\n+                self.format.tokenizer.special_tokens[\"<|python_tag|>\"],\n+            ],\n+        )\n+        message = RawMessage(\n+            role=\"assistant\",\n+            content=\"\",\n+            tool_calls=[\n+                ToolCall(\n+                    tool_name=\"custom_tool\",\n+                    arguments={\"some_param\": \"value\"},\n+                    call_id=\"1\",\n+                )\n+            ],\n+        )\n+        self.assertEqual(\n+            self.format.encode_message(message, tool_prompt_format=ToolPromptFormat.python_list)[0][:5],\n+            [\n+                self.format.tokenizer.special_tokens[\"<|start_header_id|>\"],\n+                self.format.tokenizer.encode(message.role, bos=False, eos=False)[0],\n+                self.format.tokenizer.special_tokens[\"<|end_header_id|>\"],\n+                self.format.tokenizer.encode(\"\\n\\n\", bos=False, eos=False)[0],\n+                # beginning of `[custom_tool(...)]`\n+                self.format.tokenizer.encode(\"[\", bos=False, eos=False)[0],\n+            ],\n+        )\n+\n     def test_encode_dialog(self):\n         messages = [\n             RawMessage("
            },
            {
              "sha": "1f39f9258255d8285b5fe42c086faaa6fdc00f17",
              "url": "https://github.com/meta-llama/llama-models/commit/1f39f9258255d8285b5fe42c086faaa6fdc00f17",
              "message": "Update PythonListCustomToolGenerator to support overriding system prompt (#271)",
              "files_changed": [
                {
                  "filename": "models/llama3/prompt_templates/system_prompts.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/tests/prompt_templates/test_system_prompts.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/prompt_templates/system_prompts.py ---\n@@ -7,7 +7,7 @@\n \n import textwrap\n from datetime import datetime\n-from typing import Any, List\n+from typing import Any, List, Optional\n \n from llama_models.llama3.api.datatypes import (\n     BuiltinTool,\n@@ -215,14 +215,33 @@ def data_examples(self) -> List[List[ToolDefinition]]:\n \n \n class PythonListCustomToolGenerator(PromptTemplateGeneratorBase):  # noqa: N801\n-    def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n+    DEFAULT_PROMPT = textwrap.dedent(\n+        \"\"\"\n+        You are an expert in composing functions. You are given a question and a set of possible functions.\n+        Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+        If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+        also point it out. You should only return the function call in tools call sections.\n+\n+        {{ function_description }}\n+        \"\"\".strip(\n+            \"\\n\"\n+        )\n+    )\n+\n+    def gen(\n+        self, custom_tools: List[ToolDefinition], system_prompt: Optional[str] = None\n+    ) -> PromptTemplate:\n+        system_prompt = system_prompt or self.DEFAULT_PROMPT\n+        return PromptTemplate(\n+            system_prompt,\n+            {\"function_description\": self._gen_function_description(custom_tools)},\n+        )\n+\n+    def _gen_function_description(\n+        self, custom_tools: List[ToolDefinition]\n+    ) -> PromptTemplate:\n         template_str = textwrap.dedent(\n             \"\"\"\n-            You are an expert in composing functions. You are given a question and a set of possible functions.\n-            Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n-            If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n-            also point it out. You should only return the function call in tools call sections.\n-\n             If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n             You SHOULD NOT include any other text in the response.\n \n@@ -263,7 +282,7 @@ def gen(self, custom_tools: List[ToolDefinition]) -> PromptTemplate:\n         return PromptTemplate(\n             template_str.strip(\"\\n\"),\n             {\"tools\": [t.model_dump() for t in custom_tools]},\n-        )\n+        ).render()\n \n     def data_examples(self) -> List[List[ToolDefinition]]:\n         return [\n\n--- File: models/llama3/tests/prompt_templates/test_system_prompts.py ---\n@@ -145,3 +145,49 @@ def test_llama_3_2_system_zero_shot(self):\n             \"\"\"\n         )\n         self.check_generator_output(generator, expected_text.strip(\"\\n\"))\n+\n+    def test_llama_3_2_provided_system_prompt(self):\n+        generator = PythonListCustomToolGenerator()\n+        expected_text = textwrap.dedent(\n+            \"\"\"\n+            Overriding message.\n+\n+            If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+            You SHOULD NOT include any other text in the response.\n+\n+            Here is a list of functions in JSON format that you can invoke.\n+\n+            [\n+                {\n+                    \"name\": \"get_weather\",\n+                    \"description\": \"Get weather info for places\",\n+                    \"parameters\": {\n+                        \"type\": \"dict\",\n+                        \"required\": [\"city\"],\n+                        \"properties\": {\n+                            \"city\": {\n+                                \"type\": \"string\",\n+                                \"description\": \"The name of the city to get the weather for\"\n+                            },\n+                            \"metric\": {\n+                                \"type\": \"string\",\n+                                \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n+                                \"default\": \"celsius\"\n+                            }\n+                        }\n+                    }\n+                }\n+            ]\"\"\"\n+        )\n+        user_system_prompt = textwrap.dedent(\n+            \"\"\"\n+            Overriding message.\n+            \n+            {{ function_description }}\n+            \"\"\"\n+        )\n+        example = generator.data_examples()[0]\n+\n+        pt = generator.gen(example, user_system_prompt)\n+        text = pt.render()\n+        assert text == expected_text, f\"Expected:\\n{expected_text}\\nActual:\\n{text}\""
            },
            {
              "sha": "5a1cc6c9ec3c46dfd7138117743f06a1edb1cfc0",
              "url": "https://github.com/meta-llama/llama-models/commit/5a1cc6c9ec3c46dfd7138117743f06a1edb1cfc0",
              "message": "Update CODEOWNERS",
              "files_changed": [
                {
                  "filename": ".github/CODEOWNERS",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: .github/CODEOWNERS ---\n@@ -2,4 +2,4 @@\n \n # These owners will be the default owners for everything in\n # the repo. Unless a later match takes precedence,\n-* @ashwinb @yanxi0830 @hardikjshah @dltn @raghotham\n+* @ashwinb @yanxi0830 @hardikjshah @dltn @raghotham @ehhuang"
            }
          ]
        }
      ]
    },
    {
      "id": 22633385,
      "username": "eltociear",
      "url": "https://github.com/eltociear",
      "avatar_url": "https://avatars.githubusercontent.com/u/22633385?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "0b5cb00b13309ca856b6c6df5bc7f5003bc48b4b",
              "url": "https://github.com/meta-llama/llama-models/commit/0b5cb00b13309ca856b6c6df5bc7f5003bc48b4b",
              "message": "chore: update schema_utils.py (#142)",
              "files_changed": [
                {
                  "filename": "models/schema_utils.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/schema_utils.py ---\n@@ -58,7 +58,7 @@ def register_schema(\n \n     :param data_type: The type to associate with a JSON schema.\n     :param schema: The schema to associate the type with. Derived automatically if omitted.\n-    :param name: The name used for looking uo the type. Determined automatically if omitted.\n+    :param name: The name used for looking up the type. Determined automatically if omitted.\n     :returns: The input type.\n     \"\"\"\n     return data_type"
            }
          ]
        },
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "8ffd157d68b0ea1d9b6ff2768be38718df43ee5c",
              "url": "https://github.com/meta-llama/codellama/commit/8ffd157d68b0ea1d9b6ff2768be38718df43ee5c",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -96,7 +96,7 @@ Code Llama is a new technology that carries potential risks with use. Testing co\n In order to help developers address these risks, we have created the [Responsible Use Guide](https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf). More details can be found in our research papers as well.\n \n ## Issues\n-Please report any software bug, or other problems with the models through one of the following means:\n+Please report any software bug, or other problems with the models through one of the following means:\n - Reporting issues with the model: [github.com/facebookresearch/codellama](http://github.com/facebookresearch/codellama)\n - Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n - Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)"
            }
          ]
        }
      ]
    },
    {
      "id": 35459236,
      "username": "faabian",
      "url": "https://github.com/faabian",
      "avatar_url": "https://avatars.githubusercontent.com/u/35459236?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/25",
              "number": 25,
              "title": "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ",
              "body": "https://colab.research.google.com/drive/1rXJyXXO4m-nP4XDoLV7h4_Iea3jYHDZY#scrollTo=xjpOpjPDAVQX\r\nIn google colab, When I execute fine-tuning it throws the error",
              "labels": [
                {
                  "id": 5931411248,
                  "node_id": "LA_kwDOKK-33s8AAAABYYonMA",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/model-usage",
                  "name": "model-usage",
                  "color": "f9d0c4",
                  "default": false,
                  "description": "issues related to how models are used/loaded"
                },
                {
                  "id": 5931415893,
                  "node_id": "LA_kwDOKK-33s8AAAABYYo5VQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/fine-tuning",
                  "name": "fine-tuning",
                  "color": "006b75",
                  "default": false,
                  "description": "issues related to fine tuning process or training"
                }
              ],
              "comments": 4,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/24",
              "number": 24,
              "title": "how to finetune it locally",
              "body": null,
              "labels": [
                {
                  "id": 5881934969,
                  "node_id": "LA_kwDOKK-33s8AAAABXpc0eQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/question",
                  "name": "question",
                  "color": "d876e3",
                  "default": true,
                  "description": "open ended discussions or questions that are not necessarily tied to any one category"
                },
                {
                  "id": 5931415893,
                  "node_id": "LA_kwDOKK-33s8AAAABYYo5VQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/fine-tuning",
                  "name": "fine-tuning",
                  "color": "006b75",
                  "default": false,
                  "description": "issues related to fine tuning process or training"
                }
              ],
              "comments": 1,
              "state_reason": "completed"
            }
          ],
          "commits": []
        }
      ]
    },
    {
      "id": 82674371,
      "username": "fbnav",
      "url": "https://github.com/fbnav",
      "avatar_url": "https://avatars.githubusercontent.com/u/82674371?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "1b8b925c36b9cb9ae816ac07d0ad1d3e0ff25afc",
              "url": "https://github.com/meta-llama/llama-models/commit/1b8b925c36b9cb9ae816ac07d0ad1d3e0ff25afc",
              "message": "Adding the show-all option to allow downloading older models (#116)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -35,8 +35,10 @@ To download the model weights and tokenizer:\n 1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n 2. Read and accept the license.\n 3. Once your request is approved you will receive a signed URL via email.\n-4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-toolchain`\n-5. Run `llama model list` to determine the model ID you wish to download\n+4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-toolchain`.\n+5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**: \n+Run `llama model list --show-all` to show all the available llama models, including previous versions.\n+\n 6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n 7. Pass the URL provided when prompted to start the download."
            }
          ]
        }
      ]
    },
    {
      "id": 39009375,
      "username": "garvsl",
      "url": "https://github.com/garvsl",
      "avatar_url": "https://avatars.githubusercontent.com/u/39009375?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "ce8e8d871a57a5e3966c8504045615df9669f7c3",
              "url": "https://github.com/meta-llama/llama-models/commit/ce8e8d871a57a5e3966c8504045615df9669f7c3",
              "message": "Update README.md installation (#148)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -37,7 +37,7 @@ To download the model weights and tokenizer:\n 1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n 2. Read and accept the license.\n 3. Once your request is approved you will receive a signed URL via email.\n-4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-toolchain`. (**<-- Start Here if you have received an email already.**)\n+4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-stack`. (**<-- Start Here if you have received an email already.**)\n 5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\n If you want older versions of models, run `llama model list --show-all` to show all the available Llama models."
            }
          ]
        }
      ]
    },
    {
      "id": 9162336,
      "username": "HamidShojanazeri",
      "url": "https://github.com/HamidShojanazeri",
      "avatar_url": "https://avatars.githubusercontent.com/u/9162336?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "eececc27d275a0f7d4d14ec83648d51c10a76560",
              "url": "https://github.com/meta-llama/llama-models/commit/eececc27d275a0f7d4d14ec83648d51c10a76560",
              "message": "Update README.md (#300)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -102,7 +102,7 @@ For more flexibility in running inference (including using other providers), ple\n \n ## Access to Hugging Face\n \n-We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n+We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama4` formats. To download the weights from Hugging Face, please follow these steps:\n \n - Visit one of the repos, for example [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).\n - Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n@@ -112,7 +112,7 @@ We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama),\n huggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\n ```\n \n-- To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n+- To use with transformers, the following snippet will download and cache the weights:\n \n   ```python\n   #inference.py"
            }
          ]
        }
      ]
    },
    {
      "id": 3201101,
      "username": "hardikjshah",
      "url": "https://github.com/hardikjshah",
      "avatar_url": "https://avatars.githubusercontent.com/u/3201101?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "77498180956aeb65549cede4466586ede790f718",
              "url": "https://github.com/meta-llama/llama-models/commit/77498180956aeb65549cede4466586ede790f718",
              "message": "Update Sampling `strategy` to be a union instead of enum (#262)",
              "files_changed": [
                {
                  "filename": ".github/workflows/gha_workflow_llama_models_tests.yml",
                  "status": "modified"
                },
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/gha_workflow_llama_models_tests.yml ---\n@@ -189,7 +189,7 @@ jobs:\n         if: github.event_name == 'pull_request_target'\n         run: |\n           echo \"[STEP] Running PyTest tests at 'GITHUB_WORKSPACE' path: ${GITHUB_WORKSPACE} | path: ${{ github.workspace }}\"\n-          python3 -m pytest --ignore=models/llama3/tests/api/test_generation.py --junitxml=\"${{ github.workspace }}/result.xml\"\n+          python3 -m pytest --ignore=models/llama3/tests/api/test_generation.py --ignore=llama_models/llama3/tests/api/test_generation.py --junitxml=\"${{ github.workspace }}/result.xml\"\n \n       #### Create test summary ####\n \n\n--- File: models/datatypes.py ---\n@@ -6,27 +6,42 @@\n # the top-level of this source tree.\n \n from enum import Enum\n-from typing import Any, Dict, Optional\n+from typing import Any, Dict, Literal, Optional, Union\n \n from pydantic import BaseModel, ConfigDict, Field\n+from typing_extensions import Annotated\n \n from .schema_utils import json_schema_type\n \n \n @json_schema_type\n-class SamplingStrategy(Enum):\n-    greedy = \"greedy\"\n-    top_p = \"top_p\"\n-    top_k = \"top_k\"\n+class GreedySamplingStrategy(BaseModel):\n+    type: Literal[\"greedy\"] = \"greedy\"\n+\n+\n+@json_schema_type\n+class TopPSamplingStrategy(BaseModel):\n+    type: Literal[\"top_p\"] = \"top_p\"\n+    temperature: Optional[float] = Field(..., gt=0.0)\n+    top_p: Optional[float] = 0.95\n+\n+\n+@json_schema_type\n+class TopKSamplingStrategy(BaseModel):\n+    type: Literal[\"top_k\"] = \"top_k\"\n+    top_k: int = Field(..., ge=1)\n+\n+\n+SamplingStrategy = Annotated[\n+    Union[GreedySamplingStrategy, TopPSamplingStrategy, TopKSamplingStrategy],\n+    Field(discriminator=\"type\"),\n+]\n \n \n @json_schema_type\n class SamplingParams(BaseModel):\n-    strategy: SamplingStrategy = SamplingStrategy.greedy\n+    strategy: SamplingStrategy = Field(default_factory=GreedySamplingStrategy)\n \n-    temperature: Optional[float] = 0.0\n-    top_p: Optional[float] = 0.95\n-    top_k: Optional[int] = 0\n     max_tokens: Optional[int] = 0\n     repetition_penalty: Optional[float] = 1.0\n \n\n--- File: models/sku_list.py ---\n@@ -14,7 +14,7 @@\n     CoreModelId,\n     Model,\n     SamplingParams,\n-    SamplingStrategy,\n+    TopPSamplingStrategy,\n )\n \n LLAMA2_VOCAB_SIZE = 32000\n@@ -41,9 +41,10 @@ def all_registered_models() -> List[Model]:\n \n def recommended_sampling_params() -> SamplingParams:\n     return SamplingParams(\n-        strategy=SamplingStrategy.top_p,\n-        temperature=1.0,\n-        top_p=0.9,\n+        strategy=TopPSamplingStrategy(\n+            temperature=1.0,\n+            top_p=0.9,\n+        )\n     )"
            },
            {
              "sha": "c69476b2d72e96da4f1a859c065a2278479ffaa8",
              "url": "https://github.com/meta-llama/llama-models/commit/c69476b2d72e96da4f1a859c065a2278479ffaa8",
              "message": "updating tool call formats in llama cli (#110)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/api/templates/system_message.builtin_and_custom_tools.yaml",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/api/templates/system_message.custom_tools_only.yaml",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/api/templates/system_message.jinja",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/api/templates/system_message.builtin_and_custom_tools.yaml ---\n@@ -3,22 +3,22 @@ custom_tools:\n   - tool_name: get_boiling_point\n     description: Get the boiling point of a liquid\n     parameters:\n-      - liquid_name:\n-        - type: string\n-        - description: name of the liquid\n-        - required: True\n-      - celsius:\n-        - type: boolean\n-        - description: whether to return the boiling point in celsius\n-        - required: False\n+      - name: liquid_name\n+        type: string\n+        description: name of the liquid\n+        required: True\n+      - name: celsius\n+        type: boolean\n+        description: whether to use celsius\n+        required: False\n   - tool_name: trending_songs\n     description: Returns the trending songs on a Music site\n     parameters:\n-      - country:\n-          type: string\n-          description: The country to return trending songs for\n-          required: True\n-      - n:\n-          type: int\n-          description: The number of songs to return\n-          required: False\n+      - name: country\n+        type: string\n+        description: country to return trending songs for\n+        required: True\n+      - name: n\n+        type: int\n+        description: The number of songs to return\n+        required: False\n\n--- File: models/llama3_1/api/templates/system_message.custom_tools_only.yaml ---\n@@ -3,22 +3,22 @@ custom_tools:\n   - tool_name: get_boiling_point\n     description: Get the boiling point of a liquid\n     parameters:\n-      - liquid_name:\n-        - type: string\n-        - description: name of the liquid\n-        - required: True\n-      - celsius:\n-        - type: boolean\n-        - description: whether to return the boiling point in celsius\n-        - required: False\n+      - name: liquid_name\n+        type: string\n+        description: name of the liquid\n+        required: True\n+      - name: celsius\n+        type: boolean\n+        description: whether to use celsius\n+        required: False\n   - tool_name: trending_songs\n     description: Returns the trending songs on a Music site\n     parameters:\n-      - country:\n-          type: string\n-          description: The country to return trending songs for\n-          required: True\n-      - n:\n-          type: int\n-          description: The number of songs to return\n-          required: False\n+      - name: country\n+        type: string\n+        description: country to return trending songs for\n+        required: True\n+      - name: n\n+        type: int\n+        description: The number of songs to return\n+        required: False\n\n--- File: models/llama3_1/api/templates/system_message.jinja ---\n@@ -12,28 +12,41 @@ Today Date: {{ today }}\n {%- if custom_tools %}\n {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n \n-You have access to the following functions:\n-\n+Answer the user's question by making use of the following functions if needed.\n+If none of the function can be used, please say so.\n+Here is a list of functions in JSON format:\n {% for t in custom_tools %}\n {#- manually setting up JSON because jinja sorts keys in unexpected ways -#}\n {%- set tname = t.tool_name -%}\n {%- set tdesc = t.description -%}\n-{%- set tparams = t.parameters | tojson -%}\n-Use the function '{{ tname }}' to '{{ tdesc }}':\n-{\"name\": \"{{tname}}\", \"description\": \"{{tdesc}}\", \"parameters\": {{tparams}}}\n-\n-{% endfor -%}\n-Think very carefully before calling functions.\n-If a you choose to call a function ONLY reply in the following format with no prefix or suffix:\n-\n-<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n-\n-Reminder:\n-- If looking for real time information use relevant functions before falling back to brave_search\n-- Function calls MUST follow the specified format, start with <function= and end with </function>\n-- Required parameters MUST be specified\n-- Only call one function at a time\n-- Put the entire function call reply on one line\n+{%- set tparams = t.parameters -%}\n+{%- set required_params = [] -%}\n+{%- for param in tparams if param.required == true -%}\n+    {%- set _ = required_params.append(param.name) -%}\n+{%- endfor -%}\n+{\n+    \"type\": \"function\",\n+    \"function\": {\n+        \"name\": \"{{tname}}\",\n+        \"description\": \"{{tdesc}}\",\n+        \"parameters\": {\n+            \"type\": \"object\",\n+            \"properties\": [\n+                {%- for param in tparams %}\n+                {\n+                    \"{{param.name}}\": {\n+                        \"type\": \"object\",\n+                        \"description\": \"{{param.description}}\"\n+                    }\n+                }{% if not loop.last %},{% endif %}\n+                {%- endfor %}\n+            ],\n+            \"required\": {{ required_params | tojson }}\n+        }\n+    }\n+}\n+{% endfor %}\n+Return function calls in JSON format.\n <|eot_id|>\n {%- endif -%}\n {{ additional_instructions -}}"
            },
            {
              "sha": "a49a765982c323551c3f54f315c4af0753f57f74",
              "url": "https://github.com/meta-llama/llama-models/commit/a49a765982c323551c3f54f315c4af0753f57f74",
              "message": "Merge pull request #19 from meta-llama/setup",
              "files_changed": [
                {
                  "filename": "MANIFEST.in",
                  "status": "modified"
                },
                {
                  "filename": "models/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/requirements.txt",
                  "status": "modified"
                },
                {
                  "filename": "requirements.txt",
                  "status": "modified"
                },
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: MANIFEST.in ---\n@@ -1,3 +1,4 @@\n+include requirements.txt\n include models/llama3_1/api/templates\n include models/llama3_1/api/templates/assistant_message.jinja\n include models/llama3_1/api/templates/user_message.jinja\n\n--- File: models/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n\n--- File: models/llama3_1/requirements.txt ---\n@@ -1 +1,8 @@\n+blobfile\n+fairscale\n+jinja2\n+json-strong-typing\n tiktoken\n+torch\n+pydantic==1.10.13\n+pydantic_core==2.18.2\n\n--- File: requirements.txt ---\n@@ -1,7 +1,8 @@\n-fairscale\n+blobfile\n jinja2\n json-strong-typing\n-tiktoken\n torch\n+tiktoken\n+fairscale\n pydantic==1.10.13\n pydantic_core==2.18.2\n\n--- File: setup.py ---\n@@ -5,68 +5,33 @@\n # top-level folder for each specific model found within the models/ directory at\n # the top-level of this source tree.\n \n-import os\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# the root directory of this source tree.\n \n from setuptools import setup\n \n \n-# Function to read a file\n-def read_file(file_path):\n-    with open(file_path) as file:\n-        return file.read()\n-\n-\n-# Function to read requirements from a requirements.txt file\n-def read_requirements(module_path):\n-    requirements_path = os.path.join(module_path, \"requirements.txt\")\n-    if os.path.exists(requirements_path):\n-        with open(requirements_path) as req_file:\n-            return req_file.read().splitlines()\n-    return []\n-\n-\n-# Custom function to get package directories\n-def get_package_dirs(base_path):\n-    package_dirs = {\n-        \"llama2\": os.path.join(base_path, \"llama2\"),\n-        \"llama3\": os.path.join(base_path, \"llama3\"),\n-        \"llama3_1\": os.path.join(base_path, \"llama3_1\"),\n-    }\n-    return package_dirs\n-\n-\n-# Path to the directory containing the setup.py file\n-here = os.path.abspath(os.path.dirname(__file__))\n-\n-# Get package directories dynamically\n-package_dirs = get_package_dirs(os.path.join(here, \"models\"))\n-\n-# Collect requirements from all submodules\n-extras_require = {}\n-for package_name, package_path in package_dirs.items():\n-    extras_require[package_name] = read_requirements(package_path)\n+def read_requirements():\n+    with open(\"requirements.txt\") as fp:\n+        content = fp.readlines()\n+    return [line.strip() for line in content if not line.startswith(\"#\")]\n \n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.0.1\",\n+    version=\"0.0.1\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n-    description=\"Llama model details\",\n+    description=\"Llama models\",\n     long_description=open(\"README.md\").read(),\n     long_description_content_type=\"text/markdown\",\n     url=\"https://github.com/meta-llama/llama-models\",\n-    # license=read_license(),\n-    # packages=find_packages(where=\"models\"),\n     package_dir={\"llama_models\": \"models\"},\n     classifiers=[],\n     python_requires=\">=3.10\",\n-    install_requires=[],\n-    extras_require=extras_require,\n+    install_requires=read_requirements(),\n     include_package_data=True,\n-    package_data={\n-        \"llama2\": [\"LICENSE\", \"requirements.txt\"],\n-        \"llama3\": [\"LICENSE\", \"requirements.txt\"],\n-        \"llama3_1\": [\"LICENSE\", \"requirements.txt\"],\n-    },\n )"
            }
          ]
        }
      ]
    },
    {
      "id": 10679699,
      "username": "jasonkrone",
      "url": "https://github.com/jasonkrone",
      "avatar_url": "https://avatars.githubusercontent.com/u/10679699?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "91331b22f3d2f44483807921ef671e42313f9985",
              "url": "https://github.com/meta-llama/llama-models/commit/91331b22f3d2f44483807921ef671e42313f9985",
              "message": "note inclusion of prev Q&A pairs in prompt for SQuAD eval in eval_details.md (#85)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/eval_details.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/eval_details.md ---\n@@ -66,8 +66,7 @@ For pre-trained models, we use a 3-shot config with CoT prompt and compute the a\n \n ### SQuAD\n \n-For pre-trained models, we use SQuAD v2 with a 1-shot config and report exact match scores. We run this as a generative task. Maximum generation length is 32 tokens.\n-\n+For pre-trained models, we use SQuAD v2 with a 1-shot config and report exact match scores. We run this as a generative task. Maximum generation length is 32 tokens. In the prompt, we include the ground truth Q & A pairs for all previous questions pertaining to the same passage. In short, the prompt template takes the form \"{few-shot example} {passage} {all previous Q & A pairs for passage} {input question}\". For specifics, see the released [evaluation details dataset](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-evals/viewer/Meta-Llama-3.1-8B-evals__squad__details).\n \n ### QuAC"
            }
          ]
        }
      ]
    },
    {
      "id": 25908768,
      "username": "JaxkDev",
      "url": "https://github.com/JaxkDev",
      "avatar_url": "https://avatars.githubusercontent.com/u/25908768?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "234b680fd1d3d57c2d8a5dc0a51afa33564b6bd1",
              "url": "https://github.com/meta-llama/llama-models/commit/234b680fd1d3d57c2d8a5dc0a51afa33564b6bd1",
              "message": "Reference Llama 3.3 in llama3_3/prompt_format.md (#269)",
              "files_changed": [
                {
                  "filename": "models/llama3_3/prompt_format.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_3/prompt_format.md ---\n@@ -1,8 +1,8 @@\n \n \n-# Llama 3.1 - Prompt Formats\n+# Llama 3.3 - Prompt Formats\n ## Tokens\n-Here is a list of special tokens that are supported by Llama 3.1:\n+Here is a list of special tokens that are supported by Llama 3.3:\n - `<|begin_of_text|>`: Specifies the start of the prompt\n - `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n - `<|finetune_right_pad_id|>`: This token is used for padding text sequences to the same length in a batch.\n@@ -16,15 +16,15 @@ Here is a list of special tokens that are supported by Llama 3.1:\n \n \n \n-There are 4 different roles that are supported by Llama 3.1\n+There are 4 different roles that are supported by Llama 3.3\n - `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n - `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n-- `tool`: A new role introduced in Llama 3.1. This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n+- `tool`: A new role introduced in Llama 3.3. This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n - `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `tool` and `user` prompts.\n \n-## Llama 3.1 Base Model\n+## Llama 3.3 Base Model\n \n-Text completion for Llama 3.1 base model uses this format.\n+Text completion for Llama 3.3 base model uses this format.\n \n ##### Input Prompt Format\n ```\n@@ -42,7 +42,7 @@ The color of the sky is primarily blue because of a phenomenon called\n Note start special tag\n \n \n-## Llama 3.1 Instruct Model\n+## Llama 3.3 Instruct Model\n ## User and assistant conversation\n \n Here is a regular multi-turn user assistant conversation and how its formatted."
            }
          ]
        }
      ]
    },
    {
      "id": 88510,
      "username": "jgehring",
      "url": "https://github.com/jgehring",
      "avatar_url": "https://avatars.githubusercontent.com/u/88510?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/211",
              "number": 211,
              "title": "CodeLlama text2sql",
              "body": null,
              "labels": [],
              "comments": 1,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/209",
              "number": 209,
              "title": "Question on specifying File Path for FIM prompt?",
              "body": "Does CodeLlama have a token for including File Path in a prompt, just like Starcoder's `<filename>`?",
              "labels": [],
              "comments": 1,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/208",
              "number": 208,
              "title": "70B model memory issue",
              "body": "Hello,\r\n\r\nI have 2 GPU of 24 GB RTX 4090 GPU. \r\n\r\nI want to fine-tune the 70b model but it throws a **cuda out of memory exceptions** even though I have used Lora and BitsAndBytesConfig.\r\n\r\nLet me know if I'm overlooking this or please give me suggestions.\r\n\r\nThanks.",
              "labels": [],
              "comments": 1,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/206",
              "number": 206,
              "title": "I'd like to know whether to use eos or bos during Code Llama pre-training",
              "body": "I am curious about the form of the dataset for Code Llama pre-training. I want to know whether eos or bos was used during the pre-training process. For example, the data format is {code}{EOS} or {BOS}{code}, which format is used for Code Llama pre-training?",
              "labels": [],
              "comments": 3,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/205",
              "number": 205,
              "title": "Where is the attribute `past_key_values`",
              "body": "I hope to use `past_key_values` for inferring longer texts, but even though I have set `use_cache=True`, I still cannot find it in the `generated_tokens`'s attributes.\r\n```python\r\ngenerated_tokens = model.generate(\r\n                **model_input, \r\n                max_new_tokens=1600,\r\n                use_cache=True\r\n                )\r\n```\r\n",
              "labels": [],
              "comments": 1,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/204",
              "number": 204,
              "title": "CodeLlama went into infinite cycle (of communication)",
              "body": "I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:\r\n\r\ntvali@PC366:~$ ollama run codellama:7b\r\n>>> Is LDM a Deep Learning model?\r\n\r\nLDM (Learning-based Decision Making) is a type of decision making model \r\nthat uses machine learning algorithms to learn from data and make \r\ndecisions. It is based on the concept of learning and improving over time^C\r\n\r\n>>> Is Latent Diffusion Model a Deep Learning Model?\r\n\r\nLatent Diffusion Model (LDM) is a type of deep learning model that uses a \r\nprobabilistic approach to generate images. It is based on the concept of \r\ndiffusion models, which are statistical models that describe how data \r\npoints move over time. In the context of LDM, the diffusion process is \r\nused to model the generation of images, and the model learns to generate \r\nnew images by iteratively refining its predictions.\r\n\r\nLDM is a type of Generative Adversarial Network (GAN) that uses a \r\nprobabilistic approach to generate images. It is based on the concept of \r\nlearning and improving over time, using a process called iterative \r\nrefinement. The model learns to generate new images by iteratively \r\nrefining its predictions, and it can be used for various tasks such as \r\nimage generation, image-to-image translation, and image synthesis.\r\n\r\nLDM is a deep learning model that uses a probabilistic approach to \r\ngenerate images. It is based on the concept of diffusion models, which are\r\nstatistical models that describe how data points move over time. In the \r\ncontext of LDM, the diffusion process is used to model the generation of \r\nimages, and the model learns to generate new images by iteratively \r\nrefining its predictions.\r\n\r\nLDM is a type of Generative Adversarial Network (GAN) that uses a \r\nprobabilistic approach to generate images. It is based on the concept of \r\nlearning and improving over time, using a process called iterative \r\nrefinement. The model learns to generate new images by iteratively \r\nrefining its predictions, and it can be used for various tasks such as \r\nimage generation, image-to-image translation, and image synthesis.\r\n\r\nLDM is a deep learning model that uses a probabilistic approach to \r\ngenerate images. It is based on the concept of diffusion models, which are\r\nstatistical models that describe how data points move over time. In the \r\ncontext of LDM, the diffusion process is used to model the generation of \r\nimages, and the model learns to generate new images by iteratively \r\nrefining its predictions.\r\n\r\nLDM is a type of Generative Adversarial Network (GAN) that uses a \r\nprobabilistic approach to generate images. It is based on the concept of \r\nlearning and improving over time, using a process called iterative \r\nrefinement. The model learns to generate new images by iteratively \r\nrefining its predictions, and it can be used for various tasks such as \r\nimage generation, image-to-image translation, and image synthesis.\r\n\r\nLDM is a deep learning model that uses a probabilistic approach to \r\ngenerate images. It is based on the concept of diffusion models, which are\r\nstatistical models that describe how data points move over time. In the \r\ncontext of LDM, the diffusion process is used to model the generation of \r\nimages, and the model learns to generate new images by iteratively \r\nrefining its predictions.\r\n\r\nLDM is a type of Generative Adversarial Network (GAN) that uses a \r\nprobabilistic approach to generate images. It is based on the concept of \r\nlearning and improving over time, using a process called iterative \r\nrefinement. The model learns to generate new images by iteratively \r\nrefining its predictions, and it can be used for various tasks such as \r\nimage generation, image-to-image translation, and image synthesis.\r\n\r\nLDM is a deep learning model that uses a probabilistic approach to \r\ngenerate images. It is based on the concept of diffusion models, which are\r\nstatistical models that describe how data points move over time. In the \r\ncontext of LDM, the diffusion process is used to model the generation of \r\nimages, and the model learns to generate new images by iteratively \r\nrefining its predictions.\r\n\r\nLDM is a type of Generative Adversarial Network (GAN) that uses a \r\nprobabilistic approach to generate images. It is based on the concept of \r\nlearning and improving over time, using a process called iterative \r\nrefinement. The model learns to generate new images by iteratively \r\nrefining its predictions, and it can be used for various tasks such as \r\nimage generation, image-to-image translation, and image synthesis.\r\n\r\nLDM is a deep learning model that uses a probabilistic approach to \r\ngenerate images. It is based on the concept of diffusion models, which are\r\nstatistical models that describe how data points move over time. In the \r\ncontext of LDM, the diffusion process is used to model the generation of \r\nimages, and the model learns to generate new images by iteratively \r\nrefining its predictions.\r\n\r\nLDM is a type of Generative Adversarial Network (GAN) that uses a \r\nprobabilistic approach to generate images. It is based on the concept of \r\nlearning and improving over time, using a process called iterative \r\nrefinement. The model learns to generate new images by iteratively \r\nrefining its predictions, and it can be used for various tasks such as \r\nimage generation, image-to-image translation, and image synthesis.\r\n\r\nLDM is^C\r\n\r\n>>> Send a message (/? for help)\r\n\r\n-------------------\r\n\r\nIt's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:\r\n\r\n>>> /show info\r\nModel details:\r\nFamily              llama\r\nParameter Size      7B\r\nQuantization Level  Q4_0\r\n\r\n>>> /show modelfile\r\n# Modelfile generated by \"ollama show\"\r\n# To build a new Modelfile based on this one, replace the FROM line with:\r\n# FROM codellama:7b\r\n\r\nFROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac\r\nTEMPLATE \"\"\"[INST] <<SYS>>{{ .System }}<</SYS>>\r\n\r\n{{ .Prompt }} [/INST]\r\n\"\"\"\r\nPARAMETER rope_frequency_base 1e+06\r\nPARAMETER stop \"[INST]\"\r\nPARAMETER stop \"[/INST]\"\r\nPARAMETER stop \"<<SYS>>\"\r\nPARAMETER stop \"<</SYS>>\"\r\n>>> /show parameters\r\nModel defined parameters:\r\nstop                           \"[INST]\"\r\nstop                           \"[/INST]\"\r\nstop                           \"<<SYS>>\"\r\nstop                           \"<</SYS>>\"\r\nrope_frequency_base            1e+06\r\n>>> Send a message (/? for help)\r\n",
              "labels": [],
              "comments": 7,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/175",
              "number": 175,
              "title": "Questions about learning more than 4096 sequences in codellama",
              "body": "hello\r\n\r\nI had a question and wanted to get some ideas, so I registered an issue.\r\n\r\nNow, I'm sorry that I don't have much knowledge as it's been a while since I studied LLM.\r\n\r\nI don't speak English, so I wrote it through a translator, so the words may sound strange.\r\n\r\nCurrently, I try to fine-tuning infilling capability from one node to four GPUs of the codellama 7b or 13b models using deepspeed, but if the sequence length of the dataset to be trained exceeds 4096 supported by codellama, whether it is not suitable for learning infilling, or is there any other way or good ideas to learn\r\nI want to know.\r\n\r\nthank you",
              "labels": [],
              "comments": 2,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/170",
              "number": 170,
              "title": "What is the max length could the codellama-2-7B generate?",
              "body": "I was doing an inference work using codellama-2-7B.\r\n\r\nHere is my code:\r\n```\r\ninputs_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\r\ngenerate_ids=model.generate(inputs_ids,max_new_tokens=1024,num_return_sequences=1,pad_token_id=tokenizer.eos_token_id)\r\noutput = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\r\n```\r\nI want to know the maximum that `max_new_token` can be set to?",
              "labels": [],
              "comments": 2,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/164",
              "number": 164,
              "title": "Is download.sh providing the correct tokenizer.model files?",
              "body": "When I try to run a model ..\r\n\r\n```\r\ntorchrun example_js.py \\\r\n    --ckpt_dir CodeLlama-13b-Instruct \\\r\n    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model \\\r\n    --max_seq_len 1024 --max_batch_size 4 --nproc_per_node 2\r\n```\r\nexample_js is the same as the provide example_completion, but with different prompts\r\n\r\n... I get this error:\r\n```\r\nRuntimeError: Error(s) in loading state_dict for Transformer:\r\n\tsize mismatch for tok_embeddings.weight: copying a param with shape torch.Size([32016, 2560]) from checkpoint, the shape in current model is torch.Size([32000, 5120]).\r\n\tsize mismatch for layers.0.attention.wq.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n\tsize mismatch for layers.0.attention.wk.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n\tsize mismatch for layers.0.attention.wv.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n\tsize mismatch for layers.0.attention.wo.weight: copying a param with shape torch.Size([5120, 2560]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n...\r\n```\r\n\r\nThis code works perfectly fine if I use the 7b model and tokenizer\r\n\r\nInvestigating a bit further, I noticed this:\r\n```\r\nmd5sum CodeLlama-7b-Instruct/tokenizer.model\r\n9e597e72392fd4005529a33f2bf708ba  CodeLlama-7b-Instruct/tokenizer.model\r\nmd5sum CodeLlama-13b-Instruct/tokenizer.model\r\n9e597e72392fd4005529a33f2bf708ba  CodeLlama-13b-Instruct/tokenizer.model\r\nmd5sum CodeLlama-34b-Instruct/tokenizer.model\r\neeec4125e9c7560836b4873b6f8e3025  CodeLlama-34b-Instruct/tokenizer.model\r\n```\r\n\r\nthe tokenizer for 7b and 13b are identical? That seems unlikely. \r\n\r\nI also attempted these variants of torchrun just to see what happens\r\n```\r\ntorchrun --ckpt_dir CodeLlama-13b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model \r\ntorchrun --ckpt_dir CodeLlama-34b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model --nproc_per_node 4\r\n```\r\n- These produced the same errors, but with different numbers\r\n\r\nOn another node, the --nproc_per_node value is provided to the commands just in case (as the docs say it's needed), but in practice I find it has no effect. I was forced to modify the code that builds the model like so:\r\n```\r\ngenerator = Llama.build(\r\n        ckpt_dir=ckpt_dir,\r\n        tokenizer_path=tokenizer_path,\r\n        max_seq_len=max_seq_len,\r\n        max_batch_size=max_batch_size,\r\n\r\n       # Added this, value is 2 for 13b and 4 for 34b\r\n        model_parallel_size=2,\r\n    )\r\n```\r\n\r\nI'm on an M1 Macbook Pro with 64 GB of ram",
              "labels": [],
              "comments": 2,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/157",
              "number": 157,
              "title": "What kind of prompt was used in HumanEval evaluation?",
              "body": "Thanks for your excellent work. I find there is no prompt for HumanEval completion task was released, could you give me some introduction?",
              "labels": [],
              "comments": 2,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/145",
              "number": 145,
              "title": "Codellama-7B can't  reproduce the paper's result 33.5%,why?",
              "body": "I evaluated codellama 7b using greedy decoding, and the Humaneval result for pass@1 was only 28.66%. You are asking how to reproduce the paper's result of 33.5%.  pls show the evaluation parameters and prompt format. Here are some suggestions:\r\n![image](https://github.com/facebookresearch/codellama/assets/142714958/fee2ace9-e64e-4745-a5f1-06cf2caa3423)\r\n\r\n\r\n",
              "labels": [],
              "comments": 2,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/140",
              "number": 140,
              "title": "Can't reproduce the results on Humaneval",
              "body": "Hello, may I ask how you conducted testing on Humaneval? I attempted to test using the two methods you provided in your Hugging Face blog, including code completion and code infilling on Humaneval. However, I only achieved results of 22% and 25% on Instruct CodeLLama 7B, which is far from the reported 35%.\r\n\r\n```\r\nargs.max_length = 1024\r\n\r\nif args.task == 'code-completion':\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\r\n    pipeline = transformers.pipeline(\r\n        \"text-generation\",\r\n        model=args.model_path,\r\n        torch_dtype=torch.float16,\r\n        device_map=\"auto\",\r\n    )\r\n    for task_id in problems:\r\n        prompt = problems[task_id]['question'] + '<FILL_ME>\\n    return result'\r\n        input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\r\n        output = model.generate(input_ids, max_new_tokens=args.max_length,)\r\n        output = output[0].to(\"cpu\")\r\n        filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)\r\n        completion = prompt.replace(\"<FILL_ME>\", filling)\r\n\r\nelif args.task == 'code-infilling':\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\r\n    model = transformers.AutoModelForCausalLM.from_pretrained(\r\n        args.model_path,\r\n        torch_dtype=torch.float16\r\n    ).to(\"cuda\")\r\n    for task_id in problems:\r\n        completion = pipeline(\r\n            prompt,\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            top_p=0.9,\r\n            num_return_sequences=1,\r\n            eos_token_id=tokenizer.eos_token_id,\r\n            max_length=args.max_length,\r\n        )[0]['generated_text'].strip()\r\n\r\n```",
              "labels": [],
              "comments": 3,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/139",
              "number": 139,
              "title": "Combining instruction and infilling",
              "body": "Figure 2 of the paper shows the training pipeline of all models. Each model undergoes infilling code training. For the instruct models additionally a final instruction fine-tuning is applied. This suggests that both tasks can be combined. However, the paper doesn't make this clear to me and the code examples also seem to only do one thing at a time.\r\n\r\n- Is is possible to combine instructing and infilling in a single prompt?\r\n- If yes, what is the correct prompt format?\r\n\r\nI imagine something like\r\n\r\n```\r\n[INST][SYS] system [/SYS] instruction [/INST][PREFIX] prefix [SUFFIX] suffix [MID] \r\n```",
              "labels": [],
              "comments": 3,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/98",
              "number": 98,
              "title": "How do you \"prompt\" llama???",
              "body": "How do you \"prompt\" llama???",
              "labels": [
                {
                  "id": 5881934969,
                  "node_id": "LA_kwDOKK-33s8AAAABXpc0eQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/question",
                  "name": "question",
                  "color": "d876e3",
                  "default": true,
                  "description": "open ended discussions or questions that are not necessarily tied to any one category"
                }
              ],
              "comments": 2,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/95",
              "number": 95,
              "title": "Can't run codellama!",
              "body": "```\r\n(py465) awahab@adnna:~/nexus-collaborative-project-planning-merge-notes/codellama$ torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512\r\n> initializing model parallel with size 1\r\n> initializing ddp with size 1\r\n> initializing pipeline with size 1\r\nTraceback (most recent call last):\r\n  File \"/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py\", line 55, in <module>\r\n    fire.Fire(main)\r\n  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py\", line 20, in main\r\n    generator = Llama.build(\r\n  File \"/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/llama/generation.py\", line 86, in build\r\n    assert model_parallel_size == len(\r\nAssertionError: Loading a checkpoint for MP=4 but world size is 1\r\n```\r\n\r\nthis is my error when running the ahove ",
              "labels": [
                {
                  "id": 5931411248,
                  "node_id": "LA_kwDOKK-33s8AAAABYYonMA",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/model-usage",
                  "name": "model-usage",
                  "color": "f9d0c4",
                  "default": false,
                  "description": "issues related to how models are used/loaded"
                }
              ],
              "comments": 3,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/92",
              "number": 92,
              "title": "How can I fine-tuning codellama with our own dataset?",
              "body": "As title. Is there any way to achieve that?",
              "labels": [
                {
                  "id": 5881934969,
                  "node_id": "LA_kwDOKK-33s8AAAABXpc0eQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/question",
                  "name": "question",
                  "color": "d876e3",
                  "default": true,
                  "description": "open ended discussions or questions that are not necessarily tied to any one category"
                },
                {
                  "id": 5931415893,
                  "node_id": "LA_kwDOKK-33s8AAAABYYo5VQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/fine-tuning",
                  "name": "fine-tuning",
                  "color": "006b75",
                  "default": false,
                  "description": "issues related to fine tuning process or training"
                }
              ],
              "comments": 3,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/89",
              "number": 89,
              "title": "Doesn't codellama know when to stop? The EOS doesn't work?",
              "body": "I just use the codellama-7band I found that it won't stop until it reaches the max_tokens.\r\nSo how to solve this problem? Thank u !\r\n\r\n\r\nHere are a few examples of what I tested:\r\n\r\n\r\n`curl http://localhost:8000/v1/completions     -H \"Content-Type: application/json\"     -d '{\r\n        \"model\": \"codeLlama\",\r\n        \"prompt\": \"San Francisco is a\",\r\n        \"max_tokens\": 50,\r\n        \"temperature\": 0\r\n    }'\r\n{\"id\":\"cmpl-df1438614f2a42369f20d240dad02ba0\",\"object\":\"text_completion\",\"created\":1693811331,\"model\":\"codeLlama\",\"choices\":[{\"index\":0,\"text\":\"city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\\n\",\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":5,\"total_tokens\":55,\"completion_tokens\":50}}`\r\n\r\n\r\n\r\n\r\n`curl http://localhost:8000/v1/completions     -H \"Content-Type: application/json\"     -d '{\r\n        \"model\": \"codeLlama\",\r\n        \"prompt\": \"San Francisco is a\",\r\n        \"max_tokens\": 500,\r\n        \"temperature\": 0\r\n    }'\r\n{\"id\":\"cmpl-577731d1354a4b34a17a824f137c4d40\",\"object\":\"text_completion\",\"created\":1693811355,\"model\":\"codeLlama\",\"choices\":[{\"index\":0,\"text\":\"city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair\",\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":5,\"total_tokens\":505,\"completion_tokens\":500}}`",
              "labels": [
                {
                  "id": 5931411248,
                  "node_id": "LA_kwDOKK-33s8AAAABYYonMA",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/model-usage",
                  "name": "model-usage",
                  "color": "f9d0c4",
                  "default": false,
                  "description": "issues related to how models are used/loaded"
                }
              ],
              "comments": 9,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/88",
              "number": 88,
              "title": "Why are Code Llama - Python 7B and 13B  incapable of filling in code given the surrounding context?",
              "body": null,
              "labels": [
                {
                  "id": 5881934969,
                  "node_id": "LA_kwDOKK-33s8AAAABXpc0eQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/question",
                  "name": "question",
                  "color": "d876e3",
                  "default": true,
                  "description": "open ended discussions or questions that are not necessarily tied to any one category"
                },
                {
                  "id": 5931411248,
                  "node_id": "LA_kwDOKK-33s8AAAABYYonMA",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/model-usage",
                  "name": "model-usage",
                  "color": "f9d0c4",
                  "default": false,
                  "description": "issues related to how models are used/loaded"
                }
              ],
              "comments": 1,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/75",
              "number": 75,
              "title": "Hardware requirement for 100K tokens",
              "body": "What hardware is required to run the 34B Instruct model with 100K context length?",
              "labels": [
                {
                  "id": 5881934969,
                  "node_id": "LA_kwDOKK-33s8AAAABXpc0eQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/question",
                  "name": "question",
                  "color": "d876e3",
                  "default": true,
                  "description": "open ended discussions or questions that are not necessarily tied to any one category"
                }
              ],
              "comments": 2,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/48",
              "number": 48,
              "title": "How can i generate embeddings from the model for a new source code dataset ?",
              "body": null,
              "labels": [
                {
                  "id": 5881934969,
                  "node_id": "LA_kwDOKK-33s8AAAABXpc0eQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/question",
                  "name": "question",
                  "color": "d876e3",
                  "default": true,
                  "description": "open ended discussions or questions that are not necessarily tied to any one category"
                }
              ],
              "comments": 1,
              "state_reason": "completed"
            }
          ],
          "commits": [
            {
              "sha": "e81b597e44dbecc2a0dedb9949fdf84adfc22395",
              "url": "https://github.com/meta-llama/codellama/commit/e81b597e44dbecc2a0dedb9949fdf84adfc22395",
              "message": "Merge pull request #233 from meta-llama/update-black",
              "files_changed": [
                {
                  "filename": "dev-requirements.txt",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: dev-requirements.txt ---\n@@ -1,4 +1,4 @@\n-black==22.12.0\n+black==24.3.0\n usort==1.0.5\n ufmt==2.0.1\n mypy==1.3.0"
            },
            {
              "sha": "d4f691f901a02a3ce19a0efd363ff3733105e788",
              "url": "https://github.com/meta-llama/codellama/commit/d4f691f901a02a3ce19a0efd363ff3733105e788",
              "message": "Update black version in dev-requirements.txt",
              "files_changed": [
                {
                  "filename": "dev-requirements.txt",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: dev-requirements.txt ---\n@@ -1,4 +1,4 @@\n-black==22.12.0\n+black==24.3.0\n usort==1.0.5\n ufmt==2.0.1\n mypy==1.3.0"
            },
            {
              "sha": "1af62e1f43db1fa5140fa43cb828465a603a48f3",
              "url": "https://github.com/meta-llama/codellama/commit/1af62e1f43db1fa5140fa43cb828465a603a48f3",
              "message": "Updates for 70b release",
              "files_changed": [
                {
                  "filename": ".gitignore",
                  "status": "modified"
                },
                {
                  "filename": "MODEL_CARD.md",
                  "status": "modified"
                },
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "download.sh",
                  "status": "modified"
                },
                {
                  "filename": "example_completion.py",
                  "status": "modified"
                },
                {
                  "filename": "example_infilling.py",
                  "status": "modified"
                },
                {
                  "filename": "example_instructions.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/__init__.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/model.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/tokenizer.py",
                  "status": "modified"
                },
                {
                  "filename": "requirements.txt",
                  "status": "modified"
                },
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: MODEL_CARD.md ---\n@@ -2,26 +2,26 @@\n \n ## **Model Details**\n \n-**Model Developers** Meta AI \n+**Model Developers** Meta AI\n \n-**Variations** Code Llama comes in three model sizes, and three variants: \n+**Variations** Code Llama comes in four model sizes, and three variants:\n 1) Code Llama: our base models are designed for general code synthesis and understanding\n-2) Code Llama - Python: designed specifically for Python \n-3) Code Llama - Instruct: for instruction following and safer deployment \n- \n-All variants are available in sizes of 7B, 13B and 34B parameters.\n+2) Code Llama - Python: designed specifically for Python\n+3) Code Llama - Instruct: for instruction following and safer deployment\n+\n+All variants are available in sizes of 7B, 13B, 34B and 70B parameters.\n \n **Input** Models input text only.\n \n **Output** Models output text only.\n \n-**Model Architecture** Code Llama and its variants are autoregressive language models using optimized transformer architectures. Code Llama 7B and 13B additionally support infilling text generation. All models were fine-tuned with up to 16K tokens, and support up to 100K tokens at inference time.\n+**Model Architecture** Code Llama and its variants are autoregressive language models using optimized transformer architectures. Code Llama 7B, 13B and 70B additionally support infilling text generation. All models but Code Llama - Python 70B and Code Llama - Instruct 70B were fine-tuned with up to 16K tokens, and support up to 100K tokens at inference time.\n \n-**Model Dates** Code Llama and its variants have been trained between January 2023 and July 2023.\n+**Model Dates** Code Llama and its variants have been trained between January 2023 and January 2024.\n \n **Status** This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released  as we improve model safety with community feedback.\n \n-**Licence** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/). \n+**Licence** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n \n **Research Paper** More information can be found in the paper \"[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\".\n \n@@ -36,7 +36,7 @@ All variants are available in sizes of 7B, 13B and 34B parameters.\n **Training Factors**\n We used custom training libraries. The training and fine-tuning of the released models have been performed by Metas Research Super Cluster.\n \n-**Carbon Footprint** In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Metas sustainability program.\n+**Carbon Footprint** In aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 228.55 tCO2eq, 100% of which were offset by Metas sustainability program.\n \n **Training data**\n All experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) for details).\n\n--- File: README.md ---\n@@ -22,17 +22,18 @@ Keep in mind that the links expire after 24 hours and a certain amount of downlo\n \n ### Model sizes\n \n-|  Model | Size |\n-|--------|----|\n-| 7B     | ~12.55GB  |\n-| 13B    | 24GB  |\n-| 34B    | 63GB  |\n+| Model | Size     |\n+|-------|----------|\n+| 7B    | ~12.55GB |\n+| 13B   | 24GB     |\n+| 34B   | 63GB     |\n+| 70B   | 131GB    |\n \n [comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n \n ## Setup\n \n-In a conda env with PyTorch / CUDA available, clone the repo and run in the top-level directory:\n+In a conda environment with PyTorch / CUDA available, clone the repo and run in the top-level directory:\n \n ```\n pip install -e .\n@@ -42,13 +43,14 @@ pip install -e .\n \n Different models require different model-parallel (MP) values:\n \n-|  Model | MP |\n-|--------|----|\n-| 7B     | 1  |\n-| 13B    | 2  |\n-| 34B    | 4  |\n+| Model | MP |\n+|-------|----|\n+| 7B    | 1  |\n+| 13B   | 2  |\n+| 34B   | 4  |\n+| 70B   | 8  |\n \n-All models support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware and use-case.\n+All models, except the 70B python and instruct versions, support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware and use-case.\n \n ### Pretrained Code Models\n \n@@ -64,8 +66,8 @@ torchrun --nproc_per_node 1 example_completion.py \\\n     --max_seq_len 128 --max_batch_size 4\n ```\n \n-Pretrained code models are: the Code Llama models `CodeLlama-7b`, `CodeLlama-13b`, `CodeLlama-34b` and the Code Llama - Python models \n-`CodeLlama-7b-Python`, `CodeLlama-13b-Python`, `CodeLlama-34b-Python`.\n+Pretrained code models are: the Code Llama models `CodeLlama-7b`, `CodeLlama-13b`, `CodeLlama-34b`, `CodeLlama-70b` and the Code Llama - Python models\n+`CodeLlama-7b-Python`, `CodeLlama-13b-Python`, `CodeLlama-34b-Python`, `CodeLlama-70b-Python`.\n \n ### Code Infilling\n \n@@ -84,9 +86,10 @@ Pretrained infilling models are: the Code Llama models `CodeLlama-7b` and `CodeL\n \n ### Fine-tuned Instruction Models\n \n-Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L279-L366)\n+Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for the 7B, 13B and 34B variants, a specific formatting defined in [`chat_completion()`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L319-L361)\n needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and linebreaks in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n-You can use `chat_completion` directly to generate answers with the instruct model. \n+`CodeLlama-70b-Instruct` requires a separate turn-based prompt format defined in [`dialog_prompt_tokens()`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L506-L548).\n+You can use `chat_completion()` directly to generate answers with all instruct models; it will automatically perform the required formatting.\n \n You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/src/llama_recipes/inference/safety_utils.py) of how to add a safety checker to the inputs and outputs of your inference code.\n \n@@ -99,7 +102,7 @@ torchrun --nproc_per_node 1 example_instructions.py \\\n     --max_seq_len 512 --max_batch_size 4\n ```\n \n-Fine-tuned instruction-following models are: the Code Llama - Instruct models `CodeLlama-7b-Instruct`, `CodeLlama-13b-Instruct`, `CodeLlama-34b-Instruct`.\n+Fine-tuned instruction-following models are: the Code Llama - Instruct models `CodeLlama-7b-Instruct`, `CodeLlama-13b-Instruct`, `CodeLlama-34b-Instruct`, `CodeLlama-70b-Instruct`.\n \n Code Llama is a new technology that carries potential risks with use. Testing conducted to date has not  and could not  cover all scenarios.\n In order to help developers address these risks, we have created the [Responsible Use Guide](https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf). More details can be found in our research papers as well.\n\n--- File: download.sh ---\n@@ -5,7 +5,7 @@\n \n read -p \"Enter the URL from email: \" PRESIGNED_URL\n echo \"\"\n-ALL_MODELS=\"7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct\"\n+ALL_MODELS=\"7b,13b,34b,70b,7b-Python,13b-Python,34b-Python,70b-Python,7b-Instruct,13b-Instruct,34b-Instruct,70b-Instruct\"\n read -p \"Enter the list of models to download without spaces ($ALL_MODELS), or press Enter for all: \" MODEL_SIZE\n TARGET_FOLDER=\".\"             # where all files should end up\n mkdir -p ${TARGET_FOLDER}\n@@ -27,18 +27,24 @@ do\n         SHARD=1 ;;\n       34b)\n         SHARD=3 ;;\n+      70b)\n+        SHARD=7 ;;\n       7b-Python)\n         SHARD=0 ;;\n       13b-Python)\n         SHARD=1 ;;\n       34b-Python)\n         SHARD=3 ;;\n+      70b-Python)\n+        SHARD=7 ;;\n       7b-Instruct)\n         SHARD=0 ;;\n       13b-Instruct)\n         SHARD=1 ;;\n       34b-Instruct)\n         SHARD=3 ;;\n+      70b-Instruct)\n+        SHARD=7 ;;\n       *)\n         echo \"Unknown model: $m\"\n         exit 1\n\n--- File: example_completion.py ---\n@@ -27,9 +27,7 @@ def main(\n     prompts = [\n         # For these prompts, the expected answer is the natural continuation of the prompt\n         \"\"\"\\\n-import socket\n-\n-def ping_exponential_backoff(host: str):\"\"\",\n+def fizzbuzz(n: int):\"\"\",\n         \"\"\"\\\n import argparse\n \n\n--- File: llama/generation.py ---\n@@ -32,6 +32,7 @@\n class Message(TypedDict):\n     role: Role\n     content: str\n+    destination: str  # required for model responses\n \n \n class InfillingPrediction(TypedDict, total=False):\n@@ -58,7 +59,7 @@ class ChatPrediction(TypedDict, total=False):\n B_INST, E_INST = \"[INST]\", \"[/INST]\"\n B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n \n-SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n+SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\", \"<step>\"]\n UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\"\n \n \n@@ -226,10 +227,11 @@ def text_completion(\n             echo=echo,\n         )\n         if logprobs:\n+            assert generation_logprobs is not None\n             return [\n                 {\n                     \"generation\": self.tokenizer.decode(t),\n-                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n+                    \"tokens\": [self.tokenizer.token_piece(x) for x in t],\n                     \"logprobs\": logprobs_i,\n                 }\n                 for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n@@ -268,11 +270,12 @@ def text_infilling(\n         generations = [self.tokenizer.decode_infilling(t) for t in generation_tokens]\n \n         if logprobs:\n+            assert generation_logprobs is not None\n             return [\n                 {\n                     \"generation\": generation,\n                     \"logprobs\": logprobs_i,\n-                    \"tokens\": t,\n+                    \"tokens\": [self.tokenizer.token_piece(x) for x in t],\n                     \"full_text\": prefix + generation + suffix,\n                 }\n                 for prefix, suffix, generation, t, logprobs_i in zip(\n@@ -300,6 +303,15 @@ def chat_completion(\n         max_gen_len: Optional[int] = None,\n         logprobs: bool = False,\n     ) -> List[ChatPrediction]:\n+        if self.tokenizer.step_id is not None:\n+            return self._chat_completion_turns(\n+                dialogs=dialogs,\n+                temperature=temperature,\n+                top_p=top_p,\n+                max_gen_len=max_gen_len,\n+                logprobs=logprobs,\n+            )\n+\n         if max_gen_len is None:\n             max_gen_len = self.model.params.max_seq_len - 1\n         prompt_tokens = []\n@@ -309,7 +321,7 @@ def chat_completion(\n                 any([tag in msg[\"content\"] for tag in SPECIAL_TAGS for msg in dialog])\n             )\n             if dialog[0][\"role\"] == \"system\":\n-                dialog = [\n+                dialog = [  # type: ignore\n                     {\n                         \"role\": dialog[1][\"role\"],\n                         \"content\": B_SYS\n@@ -327,7 +339,7 @@ def chat_completion(\n             dialog_tokens: List[int] = sum(\n                 [\n                     self.tokenizer.encode(\n-                        f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \",\n+                        f\"{B_INST} {prompt['content'].strip()} {E_INST} {answer['content'].strip()} \",\n                         bos=True,\n                         eos=True,\n                     )\n@@ -342,7 +354,7 @@ def chat_completion(\n                 dialog[-1][\"role\"] == \"user\"\n             ), f\"Last message must be from user, got {dialog[-1]['role']}\"\n             dialog_tokens += self.tokenizer.encode(\n-                f\"{B_INST} {(dialog[-1]['content']).strip()} {E_INST}\",\n+                f\"{B_INST} {dialog[-1]['content'].strip()} {E_INST}\",\n                 bos=True,\n                 eos=False,\n             )\n@@ -356,15 +368,79 @@ def chat_completion(\n             logprobs=logprobs,\n         )\n         if logprobs:\n+            assert generation_logprobs is not None\n+            return [\n+                {\n+                    \"generation\": {  # type: ignore\n+                        \"role\": \"assistant\",\n+                        \"content\": self.tokenizer.decode(t)\n+                        if not unsafe\n+                        else UNSAFE_ERROR,\n+                    },\n+                    \"tokens\": [self.tokenizer.token_piece(x) for x in t],\n+                    \"logprobs\": logprobs_i,\n+                }\n+                for t, logprobs_i, unsafe in zip(\n+                    generation_tokens, generation_logprobs, unsafe_requests\n+                )\n+            ]\n+        return [\n+            {\n+                \"generation\": {  # type: ignore\n+                    \"role\": \"assistant\",\n+                    \"content\": self.tokenizer.decode(t) if not unsafe else UNSAFE_ERROR,\n+                }\n+            }\n+            for t, unsafe in zip(generation_tokens, unsafe_requests)\n+        ]\n+\n+    def _chat_completion_turns(\n+        self,\n+        dialogs: List[Dialog],\n+        temperature: float = 0.6,\n+        top_p: float = 0.9,\n+        max_gen_len: Optional[int] = None,\n+        logprobs: bool = False,\n+    ) -> List[ChatPrediction]:\n+        if self.tokenizer.step_id is None:\n+            raise RuntimeError(\"Model not suitable for chat_completion_step()\")\n+        if max_gen_len is None:\n+            max_gen_len = self.model.params.max_seq_len - 1\n+\n+        prompt_tokens = []\n+        unsafe_requests = []\n+        for dialog in dialogs:\n+            unsafe_requests.append(\n+                any([tag in msg[\"content\"] for tag in SPECIAL_TAGS for msg in dialog])\n+            )\n+\n+            # Insert system message if not provided\n+            if dialog[0][\"role\"] != \"system\":\n+                dialog = [{\"role\": \"system\", \"content\": \"\"}] + dialog  # type: ignore\n+\n+            dialog_tokens = dialog_prompt_tokens(self.tokenizer, dialog)\n+            prompt_tokens.append(dialog_tokens)\n+\n+        generation_tokens, generation_logprobs = self.generate(\n+            prompt_tokens=prompt_tokens,\n+            max_gen_len=max_gen_len,\n+            temperature=temperature,\n+            top_p=top_p,\n+            logprobs=logprobs,\n+            stop_token=self.tokenizer.step_id,\n+        )\n+        if logprobs:\n+            assert generation_logprobs is not None\n             return [\n                 {\n                     \"generation\": {\n                         \"role\": \"assistant\",\n+                        \"destination\": \"user\",\n                         \"content\": self.tokenizer.decode(t)\n                         if not unsafe\n                         else UNSAFE_ERROR,\n                     },\n-                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n+                    \"tokens\": [self.tokenizer.token_piece(x) for x in t],\n                     \"logprobs\": logprobs_i,\n                 }\n                 for t, logprobs_i, unsafe in zip(\n@@ -375,13 +451,15 @@ def chat_completion(\n             {\n                 \"generation\": {\n                     \"role\": \"assistant\",\n+                    \"destination\": \"user\",\n                     \"content\": self.tokenizer.decode(t) if not unsafe else UNSAFE_ERROR,\n                 }\n             }\n             for t, unsafe in zip(generation_tokens, unsafe_requests)\n         ]\n \n \n+\n def sample_top_p(probs, p):\n     probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n     probs_sum = torch.cumsum(probs_sort, dim=-1)\n@@ -423,3 +501,48 @@ def infilling_prompt_tokens(\n             + tokenizer.encode_infilling(suf)\n             + [tokenizer.middle_id]\n         )\n+\n+\n+def dialog_prompt_tokens(tokenizer: Tokenizer, dialog: Dialog) -> List[int]:\n+    \"\"\"\n+    Prompt formatting for multi-turn dialogs.\n+    The dialog is expected to start with a system message and then alternate\n+    between user and assistant messages.\n+    \"\"\"\n+    assert tokenizer.step_id is not None\n+    assert all([msg[\"role\"] == \"user\" for msg in dialog[1::2]]) and all(\n+        [msg[\"role\"] == \"assistant\" for msg in dialog[2::2]]\n+    ), (\n+        \"model only supports 'system', 'user' and 'assistant' roles, \"\n+        \"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\"\n+    )\n+    assert (\n+        dialog[-1][\"role\"] == \"user\"\n+    ), f\"Last message must be from user, got {dialog[-1]['role']}\"\n+\n+    # Format context\n+    dialog_tokens: List[int] = [tokenizer.bos_id]\n+    headers: List[str] = []\n+    for message in dialog:\n+        headers.clear()\n+        headers.append(f\"Source: {message['role'].strip()}\")\n+        if message.get(\"destination\") is not None:\n+            headers.append(f\"Destination: {message['destination'].strip()}\")\n+        header = \" \" + \"\\n\".join(headers)\n+        dialog_tokens += tokenizer.encode(header, bos=False, eos=False)\n+\n+        if message[\"content\"]:\n+            body = \"\\n\\n \" + message[\"content\"].strip()\n+            dialog_tokens += tokenizer.encode(body, bos=False, eos=False)\n+\n+        dialog_tokens += [tokenizer.step_id]\n+\n+    # Start of reply\n+    headers.clear()\n+    headers.append(\"Source: assistant\")\n+    headers.append(\"Destination: user\")\n+    header = \" \" + \"\\n\".join(headers)\n+    dialog_tokens += tokenizer.encode(header, bos=False, eos=False)\n+    dialog_tokens += tokenizer.encode(\"\\n\\n \", bos=False, eos=False)\n+\n+    return dialog_tokens\n\n--- File: llama/tokenizer.py ---\n@@ -29,9 +29,13 @@ def __init__(self, model_path: str):\n         self.middle_id: Optional[int] = self.sp_model.piece_to_id(\"<MID>\") or None\n         self.suffix_id: Optional[int] = self.sp_model.piece_to_id(\"<SUF>\") or None\n         self.eot_id: Optional[int] = self.sp_model.piece_to_id(\"<EOT>\") or None\n+\n+        # marker for turn-based step format\n+        self.step_id: Optional[int] = self.sp_model.piece_to_id(\"<step>\") or None\n+\n         logger.info(\n             f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id} \"\n-            f\"- PRE ID: {self.prefix_id} - MID ID: {self.middle_id} - SUF ID: {self.suffix_id} - EOT ID: {self.eot_id}\"\n+            f\"- PRE ID: {self.prefix_id} - MID ID: {self.middle_id} - SUF ID: {self.suffix_id} - EOT ID: {self.eot_id} - STEP ID: {self.step_id}\"\n         )\n         assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n \n@@ -47,6 +51,9 @@ def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n     def decode(self, t: List[int]) -> str:\n         return self.sp_model.decode(list(filter(lambda tk: tk != -1, t)))\n \n+    def token_piece(self, t: int) -> str:\n+        return self.sp_model.id_to_piece(t)\n+\n     def encode_infilling(self, s: str) -> List[int]:\n         \"\"\"Encode a string without an implicit leading space.\"\"\"\n         return self.sp_model.encode(\"\" + s)[2:]"
            },
            {
              "sha": "7c99688aa2ba86a289a0fbe9ad2489376a3cfa78",
              "url": "https://github.com/meta-llama/codellama/commit/7c99688aa2ba86a289a0fbe9ad2489376a3cfa78",
              "message": "Merge pull request #86 from NinoRisteski/NinoRisteski-patch-1",
              "files_changed": [
                {
                  "filename": "MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: MODEL_CARD.md ---\n@@ -5,7 +5,7 @@\n **Model Developers** Meta AI \n \n **Variations** Code Llama comes in three model sizes, and three variants: \n-1) Code Llama: our base models designed for general code synthesis and understanding\n+1) Code Llama: our base models are designed for general code synthesis and understanding\n 2) Code Llama - Python: designed specifically for Python \n 3) Code Llama - Instruct: for instruction following and safer deployment \n  \n@@ -28,13 +28,13 @@ All variants are available in sizes of 7B, 13B and 34B parameters.\n **Where to send comments** Instructions on how to provide feedback or comments on the model can be found in the model [README](README.md), or by opening an issue in the GitHub repository ([https://github.com/facebookresearch/codellama/](https://github.com/facebookresearch/codellama/)).\n \n ## **Intended Use**\n-**Intended Use Cases** Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\n+**Intended Use Cases** Code Llama and its variants are intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistance and generation applications.\n \n **Out-of-Scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n \n ## **Hardware and Software**\n **Training Factors**\n-We used custom training libraries. The training and fine-tuning of the released models have been performed Metas Research Super Cluster.\n+We used custom training libraries. The training and fine-tuning of the released models have been performed by Metas Research Super Cluster.\n \n **Carbon Footprint** In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Metas sustainability program."
            }
          ]
        }
      ]
    },
    {
      "id": 107575027,
      "username": "jiawenliu64",
      "url": "https://github.com/jiawenliu64",
      "avatar_url": "https://avatars.githubusercontent.com/u/107575027?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "28fa4e3b287e84f6a6a92aab3c931f7479c827c1",
              "url": "https://github.com/meta-llama/llama-models/commit/28fa4e3b287e84f6a6a92aab3c931f7479c827c1",
              "message": "fix: on-the-fly int4 quantize parameter (#324)",
              "files_changed": [
                {
                  "filename": "models/llama4/quantization/loader.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/quantization/loader.py ---\n@@ -11,7 +11,7 @@\n \n import torch\n from fairscale.nn.model_parallel.initialize import get_model_parallel_rank\n-from torch import Tensor, nn\n+from torch import nn, Tensor\n from torch.nn import functional as F\n \n from ...datatypes import QuantizationMode\n@@ -92,7 +92,7 @@ def apply_quantization(key, weight):\n             log_status(f\"Rank {rank}: Quantizing int4 weights from bf16\")\n \n             def apply_quantization(_, weight):\n-                return quantize_int4(weight, fp8_activation_scale_ub, output_device=torch.device(\"cuda\"))\n+                return quantize_int4(weight, output_device=torch.device(\"cuda\"))\n \n     else:\n         fp8_scales_path = os.path.join(checkpoint_dir, f\"fp8_scales_{rank}.pt\")"
            }
          ]
        }
      ]
    },
    {
      "id": 11398925,
      "username": "jspisak",
      "url": "https://github.com/jspisak",
      "avatar_url": "https://avatars.githubusercontent.com/u/11398925?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "8c88b8c0d75dbd136b6895220c4f56ccbe2f2d24",
              "url": "https://github.com/meta-llama/llama-models/commit/8c88b8c0d75dbd136b6895220c4f56ccbe2f2d24",
              "message": "Update MODEL_CARD.md",
              "files_changed": [
                {
                  "filename": "models/llama3_3/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_3/MODEL_CARD.md ---\n@@ -40,9 +40,9 @@ Where to send questions or comments about the model Instructions on how to provi\n \n **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n \n-|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n+|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (metric tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (metric tons CO2eq) |\n | :---- | :---: | :---: | :---: | :---: |\n-| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n+| Llama 3.3 70B | 7.0M | 700 | 12.9 | 0 |\n \n The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others."
            },
            {
              "sha": "685ac4c107c75ce8c291248710bf990a876e1623",
              "url": "https://github.com/meta-llama/llama-models/commit/685ac4c107c75ce8c291248710bf990a876e1623",
              "message": "Update MODEL_CARD.md",
              "files_changed": [
                {
                  "filename": "models/llama3_1/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/MODEL_CARD.md ---\n@@ -573,7 +573,7 @@ In this section, we report the results for Llama 3.1 models on standard automati\n    </td>\n    <td>39.5\n    </td>\n-   <td>41.7\n+   <td>46.7\n    </td>\n    <td>50.7\n    </td>"
            },
            {
              "sha": "f1f38733b68ad96183fbf12dab26ba4bb4da7a50",
              "url": "https://github.com/meta-llama/llama-models/commit/f1f38733b68ad96183fbf12dab26ba4bb4da7a50",
              "message": "Create LICENSE (#84)",
              "files_changed": [
                {
                  "filename": "LICENSE",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: LICENSE ---\n@@ -0,0 +1 @@\n+https://github.com/meta-llama/llama-models/blob/main/README.md#llama-models-1"
            },
            {
              "sha": "25c45c5a362974a2bc51f3867bfc5eddf6f1da42",
              "url": "https://github.com/meta-llama/llama-models/commit/25c45c5a362974a2bc51f3867bfc5eddf6f1da42",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -21,7 +21,7 @@ Llama is an accessible, open large language model (LLM) designed for developers,\n \n Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n \n-## Llama Models\n+## [Llama Models](#Llama-Models)\n \n |  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n | :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|"
            },
            {
              "sha": "d45bdd3118d04ff7ad0480d3f6c6de0f93a5a935",
              "url": "https://github.com/meta-llama/llama-models/commit/d45bdd3118d04ff7ad0480d3f6c6de0f93a5a935",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/README.md ---\n@@ -1,5 +1,5 @@\n <p align=\"center\">\n-  <img src=\"Llama_Repo.jpeg\" width=\"400\"/>\n+  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n </p>\n \n <p align=\"center\">"
            },
            {
              "sha": "4d0c28319df1d8e197cefceea3921d61f1a66e22",
              "url": "https://github.com/meta-llama/llama-models/commit/4d0c28319df1d8e197cefceea3921d61f1a66e22",
              "message": "Create README.md",
              "files_changed": [
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/README.md ---\n@@ -0,0 +1,83 @@\n+<p align=\"center\">\n+  <img src=\"Llama_Repo.jpeg\" width=\"400\"/>\n+</p>\n+\n+<p align=\"center\">\n+         <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp\n+<br>\n+\n+---\n+\n+# Llama Models\n+Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n+1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n+2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n+3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n+\n+Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n+\n+## Llama Models\n+\n+|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n+| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n+| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/MODEL_CARD.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n+| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/MODEL_CARD.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n+| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/MODEL_CARD.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n+\n+## Download\n+\n+To download the model weights and tokenizer, please visit the [Meta Llama website](https://llama.meta.com/llama-downloads/) and accept our License.\n+\n+Once your request is approved, you will receive a signed URL over email. Then, run the download.sh script, passing the URL provided when prompted to start the download.\n+\n+Pre-requisites: Ensure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`.\n+\n+Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n+\n+### Access to Hugging Face\n+\n+We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n+\n+- Visit one of the repos, for example [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n+- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n+- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n+\n+```bash\n+huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n+```\n+\n+**NOTE** The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.\n+\n+\n+- To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n+\n+  ```python\n+  import transformers\n+  import torch\n+\n+  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n+\n+  pipeline = transformers.pipeline(\n+    \"text-generation\",\n+    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+    model_kwargs={\"torch_dtype\": torch.bfloat16},\n+    device=\"cuda\",\n+  )\n+  ```\n+\n+## Responsible Use\n+\n+Llama models are a new technology that carries potential risks with use. Testing conducted to date has not  and could not  cover all scenarios.\n+To help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n+\n+## Issues\n+\n+Please report any software bug or other problems with the models through one of the following means:\n+- Reporting issues with the model: [https://github.com/issues](https://github.com/issues)\n+- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n+- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n+\n+\n+## Questions\n+\n+For common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise."
            },
            {
              "sha": "65c82912a57e23516d17995fd3436a020d09a545",
              "url": "https://github.com/meta-llama/llama-models/commit/65c82912a57e23516d17995fd3436a020d09a545",
              "message": "Delete models/llama2/README.md",
              "files_changed": [
                {
                  "filename": "models/llama2/README.md",
                  "status": "removed"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama2/README.md ---\n@@ -1 +0,0 @@\n-content for llama2"
            },
            {
              "sha": "2b3875bb6d6e8cd9cdc45c0887ca877936613c56",
              "url": "https://github.com/meta-llama/llama-models/commit/2b3875bb6d6e8cd9cdc45c0887ca877936613c56",
              "message": "Delete models/llama3/README.md",
              "files_changed": [
                {
                  "filename": "models/llama3/README.md",
                  "status": "removed"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/README.md ---\n@@ -1 +0,0 @@\n-April release details"
            },
            {
              "sha": "87d56fb6713014b0e96881c32ead18ef420d6115",
              "url": "https://github.com/meta-llama/llama-models/commit/87d56fb6713014b0e96881c32ead18ef420d6115",
              "message": "Delete models/llama3_1/README.md",
              "files_changed": [
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "removed"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/README.md ---\n@@ -1 +0,0 @@\n-july release"
            },
            {
              "sha": "1f0feb795a4130697ced243fb53051670d591653",
              "url": "https://github.com/meta-llama/llama-models/commit/1f0feb795a4130697ced243fb53051670d591653",
              "message": "Initial commit",
              "files_changed": [
                {
                  "filename": ".flake8",
                  "status": "added"
                },
                {
                  "filename": ".gitignore",
                  "status": "added"
                },
                {
                  "filename": ".pre-commit-config.yaml",
                  "status": "added"
                },
                {
                  "filename": "CODE_OF_CONDUCT.md",
                  "status": "added"
                },
                {
                  "filename": "CONTRIBUTING.md",
                  "status": "added"
                },
                {
                  "filename": "Llama_Repo.jpeg",
                  "status": "added"
                },
                {
                  "filename": "MANIFEST.in",
                  "status": "added"
                },
                {
                  "filename": "README.md",
                  "status": "added"
                },
                {
                  "filename": "docs/license_header.txt",
                  "status": "added"
                },
                {
                  "filename": "models/llama2/LICENSE",
                  "status": "added"
                },
                {
                  "filename": "models/llama2/MODEL_CARD.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama2/README.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama2/USE_POLICY.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/LICENSE",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/MODEL_CARD.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/README.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3/USE_POLICY.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/LICENSE",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/MODEL_CARD.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/USE_POLICY.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/__init__.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/args.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/chat_format.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/datatypes.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/interface.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/model.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/sku_list.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/assistant_message.builtin_tool_call.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/assistant_message.custom_tool_call.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/assistant_message.default.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/assistant_message.jinja",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/system_message.builtin_and_custom_tools.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/system_message.builtin_tools_only.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/system_message.custom_tools_only.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/system_message.default.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/system_message.jinja",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/tool_message.failure.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/tool_message.jinja",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/tool_message.success.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/user_message.default.yaml",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/templates/user_message.jinja",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/test_tokenizer.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/tokenizer.model",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/tokenizer.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/api/tool_utils.py",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/download.sh",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/eval_details.md",
                  "status": "added"
                },
                {
                  "filename": "models/llama3_1/requirements.txt",
                  "status": "added"
                },
                {
                  "filename": "pyproject.toml",
                  "status": "added"
                },
                {
                  "filename": "requirements.txt",
                  "status": "added"
                },
                {
                  "filename": "setup.py",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .flake8 ---\n@@ -0,0 +1,29 @@\n+[flake8]\n+# Suggested config from pytorch that we can adapt\n+select = B,C,E,F,N,P,T4,W,B9,TOR0,TOR1,TOR2\n+max-line-length = 120\n+# C408 ignored because we like the dict keyword argument syntax\n+# E501 is not flexible enough, we're using B950 instead\n+# N812 ignored because import torch.nn.functional as F is PyTorch convention\n+# N817 ignored because importing using acronyms is convention (DistributedDataParallel as DDP)\n+# E731 allow usage of assigning lambda expressions\n+# E701 let black auto-format statements on one line\n+# E704 let black auto-format statements on one line\n+ignore =\n+    E203,E305,E402,E501,E721,E741,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,N812,N817,E731,E701,E704\n+    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n+    # to line this up with executable bit\n+    EXE001,\n+    # these ignores are from flake8-bugbear; please fix!\n+    B007,B008,B950\n+optional-ascii-coding = True\n+exclude =\n+    ./.git,\n+    ./docs\n+    ./build\n+    ./scripts,\n+    ./venv,\n+    *.pyi\n+    .pre-commit-config.yaml\n+    *.md\n+    .flake8\n\n--- File: .gitignore ---\n@@ -0,0 +1,3 @@\n+__pycache__\n+dist\n+*.egg-info\n\n--- File: .pre-commit-config.yaml ---\n@@ -0,0 +1,53 @@\n+exclude: 'build'\n+\n+default_language_version:\n+    python: python3\n+\n+repos:\n+-   repo: https://github.com/pre-commit/pre-commit-hooks\n+    rev: 6306a48f7dae5861702d573c9c247e4e9498e867\n+    hooks:\n+    -   id: trailing-whitespace\n+    -   id: check-ast\n+    -   id: check-merge-conflict\n+    -   id: check-added-large-files\n+        args: ['--maxkb=1000']\n+    -   id: end-of-file-fixer\n+        exclude: '^(.*\\.svg)$'\n+\n+# Temporarily disabling this\n+#    -   id: no-commit-to-branch\n+#        args: ['--branch=main']\n+\n+-   repo: https://github.com/Lucas-C/pre-commit-hooks\n+    rev: v1.5.4\n+    hooks:\n+    -   id: insert-license\n+        files: \\.py$|\\.sh$\n+        args:\n+          - --license-filepath\n+          - docs/license_header.txt\n+\n+-   repo: https://github.com/pycqa/flake8\n+    rev: 34cbf8ef3950f43d09b85e2e45c15ae5717dc37b\n+    hooks:\n+    -   id: flake8\n+        additional_dependencies:\n+          - flake8-bugbear == 22.4.25\n+          - pep8-naming == 0.12.1\n+          - torchfix\n+        args: ['--config=.flake8']\n+\n+-   repo: https://github.com/omnilib/ufmt\n+    rev: v2.7.0\n+    hooks:\n+    -   id: ufmt\n+        additional_dependencies:\n+          - black == 24.4.2\n+          - usort == 1.0.8\n+\n+# - repo: https://github.com/jsh9/pydoclint\n+#   rev: d88180a8632bb1602a4d81344085cf320f288c5a\n+#   hooks:\n+#     - id: pydoclint\n+#       args: [--config=pyproject.toml]\n\n--- File: CODE_OF_CONDUCT.md ---\n@@ -0,0 +1,80 @@\n+# Code of Conduct\n+\n+## Our Pledge\n+\n+In the interest of fostering an open and welcoming environment, we as\n+contributors and maintainers pledge to make participation in our project and\n+our community a harassment-free experience for everyone, regardless of age, body\n+size, disability, ethnicity, sex characteristics, gender identity and expression,\n+level of experience, education, socio-economic status, nationality, personal\n+appearance, race, religion, or sexual identity and orientation.\n+\n+## Our Standards\n+\n+Examples of behavior that contributes to creating a positive environment\n+include:\n+\n+* Using welcoming and inclusive language\n+* Being respectful of differing viewpoints and experiences\n+* Gracefully accepting constructive criticism\n+* Focusing on what is best for the community\n+* Showing empathy towards other community members\n+\n+Examples of unacceptable behavior by participants include:\n+\n+* The use of sexualized language or imagery and unwelcome sexual attention or\n+advances\n+* Trolling, insulting/derogatory comments, and personal or political attacks\n+* Public or private harassment\n+* Publishing others' private information, such as a physical or electronic\n+address, without explicit permission\n+* Other conduct which could reasonably be considered inappropriate in a\n+professional setting\n+\n+## Our Responsibilities\n+\n+Project maintainers are responsible for clarifying the standards of acceptable\n+behavior and are expected to take appropriate and fair corrective action in\n+response to any instances of unacceptable behavior.\n+\n+Project maintainers have the right and responsibility to remove, edit, or\n+reject comments, commits, code, wiki edits, issues, and other contributions\n+that are not aligned to this Code of Conduct, or to ban temporarily or\n+permanently any contributor for other behaviors that they deem inappropriate,\n+threatening, offensive, or harmful.\n+\n+## Scope\n+\n+This Code of Conduct applies within all project spaces, and it also applies when\n+an individual is representing the project or its community in public spaces.\n+Examples of representing a project or community include using an official\n+project e-mail address, posting via an official social media account, or acting\n+as an appointed representative at an online or offline event. Representation of\n+a project may be further defined and clarified by project maintainers.\n+\n+This Code of Conduct also applies outside the project spaces when there is a\n+reasonable belief that an individual's behavior may have a negative impact on\n+the project or its community.\n+\n+## Enforcement\n+\n+Instances of abusive, harassing, or otherwise unacceptable behavior may be\n+reported by contacting the project team at <opensource-conduct@meta.com>. All\n+complaints will be reviewed and investigated and will result in a response that\n+is deemed necessary and appropriate to the circumstances. The project team is\n+obligated to maintain confidentiality with regard to the reporter of an incident.\n+Further details of specific enforcement policies may be posted separately.\n+\n+Project maintainers who do not follow or enforce the Code of Conduct in good\n+faith may face temporary or permanent repercussions as determined by other\n+members of the project's leadership.\n+\n+## Attribution\n+\n+This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\n+available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n+\n+[homepage]: https://www.contributor-covenant.org\n+\n+For answers to common questions about this code of conduct, see\n+https://www.contributor-covenant.org/faq\n\n--- File: CONTRIBUTING.md ---\n@@ -0,0 +1,36 @@\n+# Contributing to Llama-Models\n+We want to make contributing to this project as easy and transparent as\n+possible.\n+\n+## Pull Requests\n+We actively welcome your pull requests.\n+\n+1. Fork the repo and create your branch from `main`.\n+2. If you've added code that should be tested, add tests.\n+3. If you've changed APIs, update the documentation.\n+4. Ensure the test suite passes.\n+5. Make sure your code lints.\n+6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n+\n+## Contributor License Agreement (\"CLA\")\n+In order to accept your pull request, we need you to submit a CLA. You only need\n+to do this once to work on any of Meta's open source projects.\n+\n+Complete your CLA here: <https://code.facebook.com/cla>\n+\n+## Issues\n+We use GitHub issues to track public bugs. Please ensure your description is\n+clear and has sufficient instructions to be able to reproduce the issue.\n+\n+Meta has a [bounty program](http://facebook.com/whitehat/info) for the safe\n+disclosure of security bugs. In those cases, please go through the process\n+outlined on that page and do not file a public issue.\n+\n+## Coding Style  \n+* 2 spaces for indentation rather than tabs\n+* 80 character line length\n+* ...\n+\n+## License\n+By contributing to Llama, you agree that your contributions will be licensed\n+under the LICENSE file in the root directory of this source tree.\n\n--- File: MANIFEST.in ---\n@@ -0,0 +1,16 @@\n+include models/llama3_1/api/templates\n+include models/llama3_1/api/templates/assistant_message.jinja\n+include models/llama3_1/api/templates/user_message.jinja\n+include models/llama3_1/api/templates/assistant_message.builtin_tool_call.yaml\n+include models/llama3_1/api/templates/assistant_message.custom_tool_call.yaml\n+include models/llama3_1/api/templates/assistant_message.default.yaml\n+include models/llama3_1/api/templates/system_message.builtin_and_custom_tools.yaml\n+include models/llama3_1/api/templates/system_message.builtin_tools_only.yaml\n+include models/llama3_1/api/templates/system_message.custom_tools_only.yaml\n+include models/llama3_1/api/templates/system_message.default.yaml\n+include models/llama3_1/api/templates/system_message.jinja\n+include models/llama3_1/api/templates/tool_message.failure.yaml\n+include models/llama3_1/api/templates/tool_message.jinja\n+include models/llama3_1/api/templates/tool_message.success.yaml\n+include models/llama3_1/api/templates/user_message.default.yaml\n+include models/llama3_1/api/tokenizer.model\n\n--- File: README.md ---\n@@ -0,0 +1,83 @@\n+<p align=\"center\">\n+  <img src=\"Llama_Repo.jpeg\" width=\"400\"/>\n+</p>\n+\n+<p align=\"center\">\n+         <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp\n+<br>\n+\n+---\n+\n+# Llama Models\n+Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n+1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n+2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n+3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n+\n+Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n+\n+## Llama Models\n+\n+|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n+| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n+| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/MODEL_CARD.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n+| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/MODEL_CARD.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n+| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/MODEL_CARD.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n+\n+## Download\n+\n+To download the model weights and tokenizer, please visit the [Meta Llama website](https://llama.meta.com/llama-downloads/) and accept our License.\n+\n+Once your request is approved, you will receive a signed URL over email. Then, run the download.sh script, passing the URL provided when prompted to start the download.\n+\n+Pre-requisites: Ensure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`.\n+\n+Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n+\n+### Access to Hugging Face\n+\n+We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n+\n+- Visit one of the repos, for example [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n+- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n+- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n+\n+```bash\n+huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n+```\n+\n+**NOTE** The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.\n+\n+\n+- To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n+\n+  ```python\n+  import transformers\n+  import torch\n+\n+  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n+\n+  pipeline = transformers.pipeline(\n+    \"text-generation\",\n+    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+    model_kwargs={\"torch_dtype\": torch.bfloat16},\n+    device=\"cuda\",\n+  )\n+  ```\n+\n+## Responsible Use\n+\n+Llama models are a new technology that carries potential risks with use. Testing conducted to date has not  and could not  cover all scenarios.\n+To help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n+\n+## Issues\n+\n+Please report any software bug or other problems with the models through one of the following means:\n+- Reporting issues with the model: [https://github.com/issues](https://github.com/issues)\n+- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n+- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n+\n+\n+## Questions\n+\n+For common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n\n--- File: docs/license_header.txt ---\n@@ -0,0 +1,6 @@\n+Copyright (c) Meta Platforms, Inc. and affiliates.\n+All rights reserved.\n+\n+This source code is licensed under the terms described in the LICENSE file in\n+top-level folder for each specific model found within the models/ directory at\n+the top-level of this source tree.\n\n--- File: models/llama2/LICENSE ---\n@@ -0,0 +1,125 @@\n+LLAMA 2 COMMUNITY LICENSE AGREEMENT\n+Llama 2 Version Release Date: July 18, 2023\n+\n+\"Agreement\" means the terms and conditions for use, reproduction, distribution and\n+modification of the Llama Materials set forth herein.\n+\n+\"Documentation\" means the specifications, manuals and documentation\n+accompanying Llama 2 distributed by Meta at ai.meta.com/resources/models-and-\n+libraries/llama-downloads/.\n+\n+\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if\n+you are entering into this Agreement on such person or entity's behalf), of the age\n+required under applicable laws, rules or regulations to provide legal consent and that\n+has legal authority to bind your employer or such other person or entity if you are\n+entering in this Agreement on their behalf.\n+\n+\"Llama 2\" means the foundational large language models and software and\n+algorithms, including machine-learning model code, trained model weights,\n+inference-enabling code, training-enabling code, fine-tuning enabling code and other\n+elements of the foregoing distributed by Meta at ai.meta.com/resources/models-and-\n+libraries/llama-downloads/.\n+\n+\"Llama Materials\" means, collectively, Meta's proprietary Llama 2 and\n+Documentation (and any portion thereof) made available under this Agreement.\n+\n+\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you\n+are an entity, your principal place of business is in the EEA or Switzerland) and Meta\n+Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n+\n+By clicking \"I Accept\" below or by using or distributing any portion or element of the\n+Llama Materials, you agree to be bound by this Agreement.\n+\n+1. License Rights and Redistribution.\n+\n+      a. Grant of Rights. You are granted a non-exclusive, worldwide, non-\n+transferable and royalty-free limited license under Meta's intellectual property or\n+other rights owned by Meta embodied in the Llama Materials to use, reproduce,\n+distribute, copy, create derivative works of, and make modifications to the Llama\n+Materials.\n+\n+      b. Redistribution and Use.\n+\n+            i. If you distribute or make the Llama Materials, or any derivative works\n+thereof, available to a third party, you shall provide a copy of this Agreement to such\n+third party.\n+            ii.  If you receive Llama Materials, or any derivative works thereof, from\n+a Licensee as part of an integrated end user product, then Section 2 of this\n+Agreement will not apply to you.\n+\n+            iii. You must retain in all copies of the Llama Materials that you\n+distribute the following attribution notice within a \"Notice\" text file distributed as a\n+part of such copies: \"Llama 2 is licensed under the LLAMA 2 Community License,\n+Copyright (c) Meta Platforms, Inc. All Rights Reserved.\"\n+\n+            iv. Your use of the Llama Materials must comply with applicable laws\n+and regulations (including trade compliance laws and regulations) and adhere to the\n+Acceptable Use Policy for the Llama Materials (available at\n+https://ai.meta.com/llama/use-policy), which is hereby incorporated by reference into\n+this Agreement.\n+\n+            v. You will not use the Llama Materials or any output or results of the\n+Llama Materials to improve any other large language model (excluding Llama 2 or\n+derivative works thereof).\n+\n+2. Additional Commercial Terms. If, on the Llama 2 version release date, the\n+monthly active users of the products or services made available by or for Licensee,\n+or Licensee's affiliates, is greater than 700 million monthly active users in the\n+preceding calendar month, you must request a license from Meta, which Meta may\n+grant to you in its sole discretion, and you are not authorized to exercise any of the\n+rights under this Agreement unless or until Meta otherwise expressly grants you\n+such rights.\n+\n+3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE\n+LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE\n+PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND,\n+EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY\n+WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR\n+FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE\n+FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING\n+THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\n+USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n+\n+4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE\n+LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT,\n+NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS\n+AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,\n+CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN\n+IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF\n+ANY OF THE FOREGOING.\n+\n+5. Intellectual Property.\n+\n+      a. No trademark licenses are granted under this Agreement, and in\n+connection with the Llama Materials, neither Meta nor Licensee may use any name\n+or mark owned by or associated with the other or any of its affiliates, except as\n+required for reasonable and customary use in describing and redistributing the\n+Llama Materials.\n+\n+      b. Subject to Meta's ownership of Llama Materials and derivatives made by or\n+for Meta, with respect to any derivative works and modifications of the Llama\n+Materials that are made by you, as between you and Meta, you are and will be the\n+owner of such derivative works and modifications.\n+\n+      c. If you institute litigation or other proceedings against Meta or any entity\n+(including a cross-claim or counterclaim in a lawsuit) alleging that the Llama\n+Materials or Llama 2 outputs or results, or any portion of any of the foregoing,\n+constitutes an infringement of intellectual property or other rights owned or licensable\n+by you, then any licenses granted to you under this Agreement shall terminate as of\n+the date such litigation or claim is filed or instituted. You will indemnify and hold\n+harmless Meta from and against any claim by any third party arising out of or related\n+to your use or distribution of the Llama Materials.\n+\n+6. Term and Termination. The term of this Agreement will commence upon your\n+acceptance of this Agreement or access to the Llama Materials and will continue in\n+full force and effect until terminated in accordance with the terms and conditions\n+herein. Meta may terminate this Agreement if you are in breach of any term or\n+condition of this Agreement. Upon termination of this Agreement, you shall delete\n+and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the\n+termination of this Agreement.\n+\n+7. Governing Law and Jurisdiction. This Agreement will be governed and\n+construed under the laws of the State of California without regard to choice of law\n+principles, and the UN Convention on Contracts for the International Sale of Goods\n+does not apply to this Agreement. The courts of California shall have exclusive\n+jurisdiction of any dispute arising out of this Agreement.\n\n--- File: models/llama2/MODEL_CARD.md ---\n@@ -0,0 +1,100 @@\n+# **Model Details**\n+\n+Meta developed and released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n+\n+**Model Developers** Meta\n+\n+**Variations** Llama 2 comes in a range of parameter sizes  7B, 13B, and 70B  as well as pretrained and fine-tuned variations.\n+\n+**Input** Models input text only.\n+\n+**Output** Models generate text only.\n+\n+**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n+\n+||Training Data|Params|Context Length|GQA|Tokens|LR|\n+|---|---|---|---|---|---|---|\n+Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>\n+Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>\n+Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>\n+\n+**Llama 2 family of models.** Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. The 70B version uses Grouped-Query Attention (GQA) for improved inference scalability.\n+\n+**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n+\n+**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n+\n+**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n+\n+**Research Paper** More information can be found in the paper \"Llama-2: Open Foundation and Fine-tuned Chat Models\", available at https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/.\n+\n+**Where to send questions or comments about the model** Instructions on how to provide feedback or comments on the model can be found in the model [README](README.md).\n+\n+# **Intended Use**\n+**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n+\n+**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 2 Community License. Use in languages other than English**.\n+\n+**Note: Developers may fine-tune Llama 2 models for languages beyond English provided they comply with the Llama 2 Community License and the Acceptable Use Policy.\n+\n+# **Hardware and Software**\n+**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n+\n+**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Metas sustainability program.\n+\n+||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|\n+|---|---|---|---|\n+|Llama 2 7B|184320|400|31.22|\n+|Llama 2 13B|368640|400|62.44|\n+|Llama 2 70B|1720320|400|291.42|\n+|Total|3311616||539.00|\n+\n+**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n+\n+# **Training Data**\n+**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n+\n+**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n+\n+# **Evaluation Results**\n+\n+In this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.\n+For all the evaluations, we use our internal evaluations library.\n+\n+|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n+|---|---|---|---|---|---|---|---|---|---|\n+|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n+|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n+|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n+|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n+|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n+|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n+|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n+\n+**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at the top 1.\n+\n+|||TruthfulQA|Toxigen|\n+|---|---|---|---|\n+|Llama 1|7B|27.42|23.00|\n+|Llama 1|13B|41.74|23.08|\n+|Llama 1|33B|44.19|22.57|\n+|Llama 1|65B|48.71|21.77|\n+|Llama 2|7B|33.29|**21.25**|\n+|Llama 2|13B|41.86|26.10|\n+|Llama 2|70B|**50.18**|24.60|\n+\n+**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n+\n+\n+|||TruthfulQA|Toxigen|\n+|---|---|---|---|\n+|Llama-2-Chat|7B|57.04|**0.00**|\n+|Llama-2-Chat|13B|62.18|**0.00**|\n+|Llama-2-Chat|70B|**64.14**|0.01|\n+\n+**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n+\n+# **Ethical Considerations and Limitations**\n+Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n+\n+Please see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide/)\n\n--- File: models/llama2/README.md ---\n@@ -0,0 +1 @@\n+content for llama2\n\n--- File: models/llama2/USE_POLICY.md ---\n@@ -0,0 +1,49 @@\n+# Llama 2 Acceptable Use Policy\n+\n+Meta is committed to promoting safe and fair use of its tools and features, including Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy (Policy). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\n+\n+## Prohibited Uses\n+We want everyone to use Llama 2 safely and responsibly. You agree you will not use, or allow others to use, Llama 2 to:\n+\n+1. Violate the law or others rights, including to:\n+    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n+        1. Violence or terrorism\n+        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n+        3. Human trafficking, exploitation, and sexual violence\n+        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n+        5. Sexual solicitation\n+        6. Any other criminal activity\n+    2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n+    3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n+    4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n+    5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n+    6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials\n+    7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n+\n+\n+\n+2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 2 related to the following:\n+    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n+    2. Guns and illegal weapons (including weapon development)\n+    3. Illegal drugs and regulated/controlled substances\n+    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n+    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n+    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n+\n+\n+\n+3. Intentionally deceive or mislead others, including use of Llama 2 related to the following:\n+    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n+    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n+    3. Generating, promoting, or further distributing spam\n+    4. Impersonating another individual without consent, authorization, or legal right\n+    5. Representing that the use of Llama 2 or outputs are human-generated\n+    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n+4. Fail to appropriately disclose to end users any known dangers of your AI system\n+\n+Please report any violation of this Policy, software bug, or other problems that could lead to a violation of this Policy through one of the following means:\n+\n+* Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n+* Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n+* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n+* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\n\n--- File: models/llama3/LICENSE ---\n@@ -0,0 +1,84 @@\n+META LLAMA 3 COMMUNITY LICENSE AGREEMENT\n+\n+Meta Llama 3 Version Release Date: April 18, 2024\n+Agreement means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n+\n+Documentation means the specifications, manuals and documentation accompanying Meta Llama 3 distributed by Meta at https://llama.meta.com/get-started/.\n+\n+Licensee or you means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entitys behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n+\n+Meta Llama 3 means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\n+\n+Llama Materials means, collectively, Metas proprietary Meta Llama 3 and Documentation (and any portion thereof) made available under this Agreement.\n+\n+Meta or we means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n+\n+By clicking I Accept below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n+\n+1. License Rights and Redistribution.\n+\n+\ta. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metas intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\n+\tb. Redistribution and Use.\n+\t\ti. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service that uses any of them, including another AI model, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display Built with Meta Llama 3 on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include Llama 3 at the beginning of any such AI model name.\n+\t\tii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\n+\t\tiii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a Notice text file distributed as a part of such copies: Meta Llama 3 is licensed under the Meta Llama 3 Community License, Copyright  Meta Platforms, Inc. All Rights Reserved.\n+\t\tiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3/use-policy), which is hereby incorporated by reference into this Agreement.\n+\t\tv. You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Meta Llama 3 or derivative works thereof).\n+\n+2. Additional Commercial Terms. If, on the Meta Llama 3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensees affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n+\n+3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN AS IS BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n+\n+4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n+\n+5. Intellectual Property.\n+\ta. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use Llama 3 (the Mark) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Metas brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/  ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n+\tb. Subject to Metas ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n+\tc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Meta Llama 3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n+\n+6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n+\n+7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n+\n+\n+Meta Llama 3 Acceptable Use Policy\n+Meta is committed to promoting safe and fair use of its tools and features, including Meta Llama 3. If you access or use Meta Llama 3, you agree to this Acceptable Use Policy (Policy). The most recent copy of this policy can be found at https://llama.meta.com/llama3/use-policy\n+Prohibited Uses\n+We want everyone to use Meta Llama 3 safely and responsibly. You agree you will not use, or allow others to use, Meta Llama 3 to:\n+1. Violate the law or others rights, including to:\n+\ta. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n+      \t\ti. Violence or terrorism\n+      \t\tii. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n+      \t\tiii. Human trafficking, exploitation, and sexual violence\n+      \t\tiv. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n+      \t\tv. Sexual solicitation\n+      \t\tvi. Any other criminal activity\n+   \tb. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n+   \tc. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n+   \td. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n+   \te. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n+   \tf. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n+   \tg. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n+\n+2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Meta Llama 3 related to the following:\n+   \ta. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n+   \tb. Guns and illegal weapons (including weapon development)\n+   \tc. Illegal drugs and regulated/controlled substances\n+   \td. Operation of critical infrastructure, transportation technologies, or heavy machinery\n+   \te. Self-harm or harm to others, including suicide, cutting, and eating disorders\n+   \tf. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n+\n+3. Intentionally deceive or mislead others, including use of Meta Llama 3 related to the following:\n+   \ta. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n+   \tb. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n+   \tc. Generating, promoting, or further distributing spam\n+   \td. Impersonating another individual without consent, authorization, or legal right\n+   \te. Representing that the use of Meta Llama 3 or outputs are human-generated\n+   \tf. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n+   \tg. Fail to appropriately disclose to end users any known dangers of your AI system\n+\n+Please report any violation of this Policy, software bug, or other problems that could lead to a violation of this Policy through one of the following means:\n+   \t* Reporting issues with the model: https://github.com/meta-llama/llama3\n+   \t* Reporting risky content generated by the model: developers.facebook.com/llama_output_feedback\n+   \t* Reporting bugs and security concerns: facebook.com/whitehat/info\n+   \t* Reporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\n\n--- File: models/llama3/MODEL_CARD.md ---\n@@ -0,0 +1,512 @@\n+## Model Details\n+\n+Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.\n+\n+**Model developers** Meta\n+\n+**Llama 3 family of models** Llama 3 comes in two sizes  8B and 70B parameters  in pre-trained and instruction tuned variants.\n+\n+**Input** Models input text only.\n+\n+**Output** Models generate text and code only.\n+\n+**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. Llama 3 uses a tokenizer with a vocabulary of 128K tokens, and was trained on on sequences of 8,192 tokens. Grouped-Query Attention (GQA) is used for all models to improve inference efficiency. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n+\n+\n+<table>\n+  <tr>\n+   <td>\n+   </td>\n+   <td><strong>Training Data</strong>\n+   </td>\n+   <td><strong>Params</strong>\n+   </td>\n+   <td><strong>Context length</strong>\n+   </td>\n+   <td><strong>GQA</strong>\n+   </td>\n+   <td><strong>Token count</strong>\n+   </td>\n+   <td><strong>Knowledge cutoff</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"2\" >Llama 3\n+   </td>\n+   <td rowspan=\"2\" >A new mix of publicly available online data.\n+   </td>\n+   <td>8B\n+   </td>\n+   <td>8k\n+   </td>\n+   <td>Yes\n+   </td>\n+   <td rowspan=\"2\" >15T+\n+   </td>\n+   <td>March, 2023\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>70B\n+   </td>\n+   <td>8k\n+   </td>\n+   <td>Yes\n+   </td>\n+   <td>December, 2023\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+Note: Token counts refer to pretraining data only.\n+\n+**Model Release Date** April 18, 2024.\n+\n+**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n+\n+**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n+\n+Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+\n+\n+## Intended Use\n+\n+**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n+\n+**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the [Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/) and [Llama 3 Community License](https://llama.meta.com/llama3/license/). Use in languages other than English**.\n+\n+**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the [Llama 3 Community License](https://llama.meta.com/llama3/license/) and the [Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).\n+\n+\n+## Hardware and Software\n+\n+**Training Factors** We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n+\n+**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Metas sustainability program.\n+\n+\n+<table>\n+  <tr>\n+   <td>\n+   </td>\n+   <td><strong>Time (GPU hours)</strong>\n+   </td>\n+   <td><strong>Power Consumption (W)</strong>\n+   </td>\n+   <td><strong>Carbon Emitted(tCO2eq)</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Llama 3 8B\n+   </td>\n+   <td>1.3M\n+   </td>\n+   <td>700\n+   </td>\n+   <td>390\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Llama 3 70B\n+   </td>\n+   <td>6.4M\n+   </td>\n+   <td>700\n+   </td>\n+   <td>1900\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Total\n+   </td>\n+   <td>7.7M\n+   </td>\n+   <td>\n+   </td>\n+   <td>2290\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+\n+**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n+\n+\n+## Training Data\n+\n+**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n+\n+**Data Freshness** The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively.\n+\n+\n+## Benchmarks\n+\n+In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_details.md).\n+\n+\n+### Base pretrained models\n+\n+\n+<table>\n+  <tr>\n+   <td><strong>Category</strong>\n+   </td>\n+   <td><strong>Benchmark</strong>\n+   </td>\n+   <td><strong>Llama 3 8B</strong>\n+   </td>\n+   <td><strong>Llama2 7B</strong>\n+   </td>\n+   <td><strong>Llama2 13B</strong>\n+   </td>\n+   <td><strong>Llama 3 70B</strong>\n+   </td>\n+   <td><strong>Llama2 70B</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"6\" >General\n+   </td>\n+   <td>MMLU (5-shot)\n+   </td>\n+   <td>66.6\n+   </td>\n+   <td>45.7\n+   </td>\n+   <td>53.8\n+   </td>\n+   <td>79.5\n+   </td>\n+   <td>69.7\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>AGIEval English (3-5 shot)\n+   </td>\n+   <td>45.9\n+   </td>\n+   <td>28.8\n+   </td>\n+   <td>38.7\n+   </td>\n+   <td>63.0\n+   </td>\n+   <td>54.8\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>CommonSenseQA (7-shot)\n+   </td>\n+   <td>72.6\n+   </td>\n+   <td>57.6\n+   </td>\n+   <td>67.6\n+   </td>\n+   <td>83.8\n+   </td>\n+   <td>78.7\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Winogrande (5-shot)\n+   </td>\n+   <td>76.1\n+   </td>\n+   <td>73.3\n+   </td>\n+   <td>75.4\n+   </td>\n+   <td>83.1\n+   </td>\n+   <td>81.8\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>BIG-Bench Hard (3-shot, CoT)\n+   </td>\n+   <td>61.1\n+   </td>\n+   <td>38.1\n+   </td>\n+   <td>47.0\n+   </td>\n+   <td>81.3\n+   </td>\n+   <td>65.7\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>ARC-Challenge (25-shot)\n+   </td>\n+   <td>78.6\n+   </td>\n+   <td>53.7\n+   </td>\n+   <td>67.6\n+   </td>\n+   <td>93.0\n+   </td>\n+   <td>85.3\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Knowledge reasoning\n+   </td>\n+   <td>TriviaQA-Wiki (5-shot)\n+   </td>\n+   <td>78.5\n+   </td>\n+   <td>72.1\n+   </td>\n+   <td>79.6\n+   </td>\n+   <td>89.7\n+   </td>\n+   <td>87.5\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"4\" >Reading comprehension\n+   </td>\n+   <td>SQuAD (1-shot)\n+   </td>\n+   <td>76.4\n+   </td>\n+   <td>72.2\n+   </td>\n+   <td>72.1\n+   </td>\n+   <td>85.6\n+   </td>\n+   <td>82.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>QuAC (1-shot, F1)\n+   </td>\n+   <td>44.4\n+   </td>\n+   <td>39.6\n+   </td>\n+   <td>44.9\n+   </td>\n+   <td>51.1\n+   </td>\n+   <td>49.4\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>BoolQ (0-shot)\n+   </td>\n+   <td>75.7\n+   </td>\n+   <td>65.5\n+   </td>\n+   <td>66.9\n+   </td>\n+   <td>79.0\n+   </td>\n+   <td>73.1\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>DROP (3-shot, F1)\n+   </td>\n+   <td>58.4\n+   </td>\n+   <td>37.9\n+   </td>\n+   <td>49.8\n+   </td>\n+   <td>79.7\n+   </td>\n+   <td>70.2\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+\n+### Instruction tuned models\n+\n+\n+<table>\n+  <tr>\n+   <td><strong>Benchmark</strong>\n+   </td>\n+   <td><strong>Llama 3 8B</strong>\n+   </td>\n+   <td><strong>Llama 2 7B</strong>\n+   </td>\n+   <td><strong>Llama 2 13B</strong>\n+   </td>\n+   <td><strong>Llama 3 70B</strong>\n+   </td>\n+   <td><strong>Llama 2 70B</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>MMLU (5-shot)\n+   </td>\n+   <td>68.4\n+   </td>\n+   <td>34.1\n+   </td>\n+   <td>47.8\n+   </td>\n+   <td>82.0\n+   </td>\n+   <td>52.9\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>GPQA (0-shot)\n+   </td>\n+   <td>34.2\n+   </td>\n+   <td>21.7\n+   </td>\n+   <td>22.3\n+   </td>\n+   <td>39.5\n+   </td>\n+   <td>21.0\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>HumanEval (0-shot)\n+   </td>\n+   <td>62.2\n+   </td>\n+   <td>7.9\n+   </td>\n+   <td>14.0\n+   </td>\n+   <td>81.7\n+   </td>\n+   <td>25.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>GSM-8K (8-shot, CoT)\n+   </td>\n+   <td>79.6\n+   </td>\n+   <td>25.7\n+   </td>\n+   <td>41.2\n+   </td>\n+   <td>93.0\n+   </td>\n+   <td>57.5\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>MATH (4-shot, CoT)\n+   </td>\n+   <td>30.0\n+   </td>\n+   <td>3.8\n+   </td>\n+   <td>6.7\n+   </td>\n+   <td>50.4\n+   </td>\n+   <td>11.6\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+\n+### Responsibility & Safety\n+\n+We believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n+\n+Foundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications.\n+\n+Rather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience.\n+\n+\n+As part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n+\n+\n+#### Llama 3-Instruct\n+\n+As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case.\n+\n+<span style=\"text-decoration:underline;\">Safety</span>\n+\n+For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable.\n+\n+<span style=\"text-decoration:underline;\">Refusals</span>\n+\n+In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. Weve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2.\n+\n+We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date.\n+\n+\n+#### Responsible release\n+\n+In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision.\n+\n+Misuse\n+\n+If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n+\n+\n+#### Critical risks\n+\n+<span style=\"text-decoration:underline;\">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n+\n+We have conducted a two fold assessment of the safety of the model in this area:\n+\n+\n+\n+* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\n+* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n+\n+\n+### <span style=\"text-decoration:underline;\">Cyber Security </span>\n+\n+We have evaluated Llama 3 with CyberSecEval, Metas cybersecurity safety eval suite, measuring Llama 3s propensity to suggest insecure code when used as a coding assistant, and Llama 3s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval).\n+\n+\n+### <span style=\"text-decoration:underline;\">Child Safety</span>\n+\n+Child Safety risk assessments were conducted using a team of experts, to assess the models capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n+\n+\n+### Community\n+\n+Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [GitHub repository](https://github.com/meta-llama/PurpleLlama).\n+\n+Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n+\n+\n+## Ethical Considerations and Limitations\n+\n+The core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n+\n+But Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety.\n+\n+Please see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n+\n+\n+## Citation instructions\n+\n+```\n+@article{llama3modelcard,\n+  title={Llama 3 Model Card},\n+  author={AI@Meta},\n+  year={2024},\n+  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n+}\n+```\n+\n+## Contributors\n+\n+Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Puxin Xu; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos\n\n--- File: models/llama3/README.md ---\n@@ -0,0 +1 @@\n+April release details\n\n--- File: models/llama3/USE_POLICY.md ---\n@@ -0,0 +1,49 @@\n+# Meta Llama 3 Acceptable Use Policy\n+\n+Meta is committed to promoting safe and fair use of its tools and features, including Llama 3. If you access or use Llama 3, you agree to this Acceptable Use Policy (Policy). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\n+\n+## Prohibited Uses\n+We want everyone to use Llama 3 safely and responsibly. You agree you will not use, or allow others to use, Llama 3 to:\n+\n+1. Violate the law or others rights, including to:\n+    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n+        1. Violence or terrorism\n+        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n+        3. Human trafficking, exploitation, and sexual violence\n+        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n+        5. Sexual solicitation\n+        6. Any other criminal activity\n+    2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n+    3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n+    4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n+    5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n+    6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 3 Materials\n+    7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n+\n+\n+\n+2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3 related to the following:\n+    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n+    2. Guns and illegal weapons (including weapon development)\n+    3. Illegal drugs and regulated/controlled substances\n+    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n+    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n+    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n+\n+\n+\n+3. Intentionally deceive or mislead others, including use of Llama 3 related to the following:\n+    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n+    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n+    3. Generating, promoting, or further distributing spam\n+    4. Impersonating another individual without consent, authorization, or legal right\n+    5. Representing that the use of Llama 3 or outputs are human-generated\n+    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n+4. Fail to appropriately disclose to end users any known dangers of your AI system\n+\n+Please report any violation of this Policy, software bug, or other problems that could lead to a violation of this Policy through one of the following means:\n+\n+* Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n+* Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n+* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n+* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\n\n--- File: models/llama3_1/LICENSE ---\n@@ -0,0 +1,48 @@\n+ LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\n+\n+ Llama 3.1 Version Release Date: July 23, 2024\n+\n+Agreement means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n+\n+Documentation means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\n+\n+Licensee or you means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entitys behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n+\n+Llama 3.1 means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\n+\n+Llama Materials means, collectively, Metas proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\n+\n+Meta or we means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n+\n+By clicking I Accept below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n+\n+1. License Rights and Redistribution.\n+    a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metas intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\n+\n+    b. Redistribution and Use.\n+\n+          i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display Built with Llama on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include Llama at the beginning of any such AI model name.\n+\n+          ii.If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\n+\n+          iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a Notice text file distributed as a part of such copies: Llama 3.1 is licensed under the Llama 3.1 Community License, Copyright  Meta Platforms, Inc. All Rights Reserved.\n+\n+          iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\n+\n+2. Additional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensees affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n+\n+3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN AS IS BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n+\n+4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n+\n+5. Intellectual Property.\n+\n+    a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use Llama (the Mark) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Metas brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n+\n+    b. Subject to Metas ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n+\n+    c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n+\n+6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n+\n+7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n\n--- File: models/llama3_1/MODEL_CARD.md ---\n@@ -0,0 +1,977 @@\n+## Model Information\n+\n+The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n+\n+**Model developer:** Meta\n+\n+**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n+\n+\n+<table>\n+  <tr>\n+   <td>\n+   </td>\n+   <td><strong>Training Data</strong>\n+   </td>\n+   <td><strong>Params</strong>\n+   </td>\n+   <td><strong>Input modalities</strong>\n+   </td>\n+   <td><strong>Output modalities</strong>\n+   </td>\n+   <td><strong>Context length</strong>\n+   </td>\n+   <td><strong>GQA</strong>\n+   </td>\n+   <td><strong>Token count</strong>\n+   </td>\n+   <td><strong>Knowledge cutoff</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"3\" >Llama 3.1 (text only)\n+   </td>\n+   <td rowspan=\"3\" >A new mix of publicly available online data.\n+   </td>\n+   <td>8B\n+   </td>\n+   <td>Multilingual Text\n+   </td>\n+   <td>Multilingual Text and code\n+   </td>\n+   <td>128k\n+   </td>\n+   <td>Yes\n+   </td>\n+   <td rowspan=\"3\" >15T+\n+   </td>\n+   <td rowspan=\"3\" >December 2023\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>70B\n+   </td>\n+   <td>Multilingual Text\n+   </td>\n+   <td>Multilingual Text and code\n+   </td>\n+   <td>128k\n+   </td>\n+   <td>Yes\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>405B\n+   </td>\n+   <td>Multilingual Text\n+   </td>\n+   <td>Multilingual Text and code\n+   </td>\n+   <td>128k\n+   </td>\n+   <td>Yes\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n+\n+**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n+\n+**Model Release Date:** July 23, 2024.\n+\n+**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n+\n+**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n+\n+Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+\n+\n+## Intended Use\n+\n+**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\n+\n+**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n+\n+**<span style=\"text-decoration:underline;\">Note</span>:** Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n+\n+\n+## Hardware and Software\n+\n+**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n+\n+**Training Energy Use **Training utilized a cumulative of** 39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n+\n+\n+**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n+\n+\n+<table>\n+  <tr>\n+   <td>\n+   </td>\n+   <td><strong>Training Time (GPU hours)</strong>\n+   </td>\n+   <td><strong>Training Power Consumption (W)</strong>\n+   </td>\n+   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n+<p>\n+<strong>(tons CO2eq)</strong>\n+   </td>\n+   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n+<p>\n+<strong>(tons CO2eq)</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Llama 3.1 8B\n+   </td>\n+   <td>1.46M\n+   </td>\n+   <td>700\n+   </td>\n+   <td>420\n+   </td>\n+   <td>0\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Llama 3.1 70B\n+   </td>\n+   <td>7.0M\n+   </td>\n+   <td>700\n+   </td>\n+   <td>2,040\n+   </td>\n+   <td>0\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Llama 3.1 405B\n+   </td>\n+   <td>30.84M\n+   </td>\n+   <td>700\n+   </td>\n+   <td>8,930\n+   </td>\n+   <td>0\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Total\n+   </td>\n+   <td>39.3M\n+   <td>\n+<ul>\n+\n+</ul>\n+   </td>\n+   <td>11,390\n+   </td>\n+   <td>0\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+\n+The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n+\n+\n+## Training Data\n+\n+**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\n+\n+**Data Freshness:** The pretraining data has a cutoff of December 2023.\n+\n+\n+## Benchmark scores\n+\n+In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\n+\n+### Base pretrained models\n+\n+\n+<table>\n+  <tr>\n+   <td><strong>Category</strong>\n+   </td>\n+   <td><strong>Benchmark</strong>\n+   </td>\n+   <td><strong># Shots</strong>\n+   </td>\n+   <td><strong>Metric</strong>\n+   </td>\n+   <td><strong>Llama 3 8B</strong>\n+   </td>\n+   <td><strong>Llama 3.1 8B</strong>\n+   </td>\n+   <td><strong>Llama 3 70B</strong>\n+   </td>\n+   <td><strong>Llama 3.1 70B</strong>\n+   </td>\n+   <td><strong>Llama 3.1 405B</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"7\" >General\n+   </td>\n+   <td>MMLU\n+   </td>\n+   <td>5\n+   </td>\n+   <td>macro_avg/acc_char\n+   </td>\n+   <td>66.7\n+   </td>\n+   <td>66.7\n+   </td>\n+   <td>79.5\n+   </td>\n+   <td>79.3\n+   </td>\n+   <td>85.2\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>MMLU-Pro (CoT)\n+   </td>\n+   <td>5\n+   </td>\n+   <td>macro_avg/acc_char\n+   </td>\n+   <td>36.2\n+   </td>\n+   <td>37.1\n+   </td>\n+   <td>55.0\n+   </td>\n+   <td>53.8\n+   </td>\n+   <td>61.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>AGIEval English\n+   </td>\n+   <td>3-5\n+   </td>\n+   <td>average/acc_char\n+   </td>\n+   <td>47.1\n+   </td>\n+   <td>47.8\n+   </td>\n+   <td>63.0\n+   </td>\n+   <td>64.6\n+   </td>\n+   <td>71.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>CommonSenseQA\n+   </td>\n+   <td>7\n+   </td>\n+   <td>acc_char\n+   </td>\n+   <td>72.6\n+   </td>\n+   <td>75.0\n+   </td>\n+   <td>83.8\n+   </td>\n+   <td>84.1\n+   </td>\n+   <td>85.8\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Winogrande\n+   </td>\n+   <td>5\n+   </td>\n+   <td>acc_char\n+   </td>\n+   <td>-\n+   </td>\n+   <td>60.5\n+   </td>\n+   <td>-\n+   </td>\n+   <td>83.3\n+   </td>\n+   <td>86.7\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>BIG-Bench Hard (CoT)\n+   </td>\n+   <td>3\n+   </td>\n+   <td>average/em\n+   </td>\n+   <td>61.1\n+   </td>\n+   <td>64.2\n+   </td>\n+   <td>81.3\n+   </td>\n+   <td>81.6\n+   </td>\n+   <td>85.9\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>ARC-Challenge\n+   </td>\n+   <td>25\n+   </td>\n+   <td>acc_char\n+   </td>\n+   <td>79.4\n+   </td>\n+   <td>79.7\n+   </td>\n+   <td>93.1\n+   </td>\n+   <td>92.9\n+   </td>\n+   <td>96.1\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Knowledge reasoning\n+   </td>\n+   <td>TriviaQA-Wiki\n+   </td>\n+   <td>5\n+   </td>\n+   <td>em\n+   </td>\n+   <td>78.5\n+   </td>\n+   <td>77.6\n+   </td>\n+   <td>89.7\n+   </td>\n+   <td>89.8\n+   </td>\n+   <td>91.8\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"4\" >Reading comprehension\n+   </td>\n+   <td>SQuAD\n+   </td>\n+   <td>1\n+   </td>\n+   <td>em\n+   </td>\n+   <td>76.4\n+   </td>\n+   <td>77.0\n+   </td>\n+   <td>85.6\n+   </td>\n+   <td>81.8\n+   </td>\n+   <td>89.3\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>QuAC (F1)\n+   </td>\n+   <td>1\n+   </td>\n+   <td>f1\n+   </td>\n+   <td>44.4\n+   </td>\n+   <td>44.9\n+   </td>\n+   <td>51.1\n+   </td>\n+   <td>51.1\n+   </td>\n+   <td>53.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>BoolQ\n+   </td>\n+   <td>0\n+   </td>\n+   <td>acc_char\n+   </td>\n+   <td>75.7\n+   </td>\n+   <td>75.0\n+   </td>\n+   <td>79.0\n+   </td>\n+   <td>79.4\n+   </td>\n+   <td>80.0\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>DROP (F1)\n+   </td>\n+   <td>3\n+   </td>\n+   <td>f1\n+   </td>\n+   <td>58.4\n+   </td>\n+   <td>59.5\n+   </td>\n+   <td>79.7\n+   </td>\n+   <td>79.6\n+   </td>\n+   <td>84.8\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+\n+### Instruction tuned models\n+\n+\n+<table>\n+  <tr>\n+   <td><strong>Category</strong>\n+   </td>\n+   <td><strong>Benchmark</strong>\n+   </td>\n+   <td><strong># Shots</strong>\n+   </td>\n+   <td><strong>Metric</strong>\n+   </td>\n+   <td><strong>Llama 3 8B Instruct</strong>\n+   </td>\n+   <td><strong>Llama 3.1 8B Instruct</strong>\n+   </td>\n+   <td><strong>Llama 3 70B Instruct</strong>\n+   </td>\n+   <td><strong>Llama 3.1 70B Instruct</strong>\n+   </td>\n+   <td><strong>Llama 3.1 405B Instruct</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"4\" >General\n+   </td>\n+   <td>MMLU\n+   </td>\n+   <td>5\n+   </td>\n+   <td>macro_avg/acc\n+   </td>\n+   <td>68.5\n+   </td>\n+   <td>69.4\n+   </td>\n+   <td>82.0\n+   </td>\n+   <td>83.6\n+   </td>\n+   <td>87.3\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>MMLU (CoT)\n+   </td>\n+   <td>0\n+   </td>\n+   <td>macro_avg/acc\n+   </td>\n+   <td>65.3\n+   </td>\n+   <td>73.0\n+   </td>\n+   <td>80.9\n+   </td>\n+   <td>86.0\n+   </td>\n+   <td>88.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>MMLU-Pro (CoT)\n+   </td>\n+   <td>5\n+   </td>\n+   <td>micro_avg/acc_char\n+   </td>\n+   <td>45.5\n+   </td>\n+   <td>48.3\n+   </td>\n+   <td>63.4\n+   </td>\n+   <td>66.4\n+   </td>\n+   <td>73.3\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>IFEval\n+   </td>\n+   <td>\n+   </td>\n+   <td>\n+   </td>\n+   <td>76.8\n+   </td>\n+   <td>80.4\n+   </td>\n+   <td>82.9\n+   </td>\n+   <td>87.5\n+   </td>\n+   <td>88.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"2\" >Reasoning\n+   </td>\n+   <td>ARC-C\n+   </td>\n+   <td>0\n+   </td>\n+   <td>acc\n+   </td>\n+   <td>82.4\n+   </td>\n+   <td>83.4\n+   </td>\n+   <td>94.4\n+   </td>\n+   <td>94.8\n+   </td>\n+   <td>96.9\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>GPQA\n+   </td>\n+   <td>0\n+   </td>\n+   <td>em\n+   </td>\n+   <td>34.6\n+   </td>\n+   <td>30.4\n+   </td>\n+   <td>39.5\n+   </td>\n+   <td>41.7\n+   </td>\n+   <td>50.7\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"4\" >Code\n+   </td>\n+   <td>HumanEval\n+   </td>\n+   <td>0\n+   </td>\n+   <td>pass@1\n+   </td>\n+   <td>60.4\n+   </td>\n+   <td>72.6\n+   </td>\n+   <td>81.7\n+   </td>\n+   <td>80.5\n+   </td>\n+   <td>89.0\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>MBPP ++ base version\n+   </td>\n+   <td>0\n+   </td>\n+   <td>pass@1\n+   </td>\n+   <td>70.6\n+   </td>\n+   <td>72.8\n+   </td>\n+   <td>82.5\n+   </td>\n+   <td>86.0\n+   </td>\n+   <td>88.6\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Multipl-E HumanEval\n+   </td>\n+   <td>0\n+   </td>\n+   <td>pass@1\n+   </td>\n+   <td>-\n+   </td>\n+   <td>50.8\n+   </td>\n+   <td>-\n+   </td>\n+   <td>65.5\n+   </td>\n+   <td>75.2\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Multipl-E MBPP\n+   </td>\n+   <td>0\n+   </td>\n+   <td>pass@1\n+   </td>\n+   <td>-\n+   </td>\n+   <td>52.4\n+   </td>\n+   <td>-\n+   </td>\n+   <td>62.0\n+   </td>\n+   <td>65.7\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"2\" >Math\n+   </td>\n+   <td>GSM-8K (CoT)\n+   </td>\n+   <td>8\n+   </td>\n+   <td>em_maj1@1\n+   </td>\n+   <td>80.6\n+   </td>\n+   <td>84.5\n+   </td>\n+   <td>93.0\n+   </td>\n+   <td>95.1\n+   </td>\n+   <td>96.8\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>MATH (CoT)\n+   </td>\n+   <td>0\n+   </td>\n+   <td>final_em\n+   </td>\n+   <td>29.1\n+   </td>\n+   <td>51.9\n+   </td>\n+   <td>51.0\n+   </td>\n+   <td>68.0\n+   </td>\n+   <td>73.8\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"4\" >Tool Use\n+   </td>\n+   <td>API-Bank\n+   </td>\n+   <td>0\n+   </td>\n+   <td>acc\n+   </td>\n+   <td>48.3\n+   </td>\n+   <td>82.6\n+   </td>\n+   <td>85.1\n+   </td>\n+   <td>90.0\n+   </td>\n+   <td>92.0\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>BFCL\n+   </td>\n+   <td>0\n+   </td>\n+   <td>acc\n+   </td>\n+   <td>60.3\n+   </td>\n+   <td>76.1\n+   </td>\n+   <td>83.0\n+   </td>\n+   <td>84.8\n+   </td>\n+   <td>88.5\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Gorilla Benchmark API Bench\n+   </td>\n+   <td>0\n+   </td>\n+   <td>acc\n+   </td>\n+   <td>1.7\n+   </td>\n+   <td>8.2\n+   </td>\n+   <td>14.7\n+   </td>\n+   <td>29.7\n+   </td>\n+   <td>35.3\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Nexus (0-shot)\n+   </td>\n+   <td>0\n+   </td>\n+   <td>macro_avg/acc\n+   </td>\n+   <td>18.1\n+   </td>\n+   <td>38.5\n+   </td>\n+   <td>47.8\n+   </td>\n+   <td>56.7\n+   </td>\n+   <td>58.7\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Multilingual\n+   </td>\n+   <td>Multilingual MGSM (CoT)\n+   </td>\n+   <td>0\n+   </td>\n+   <td>em\n+   </td>\n+   <td>-\n+   </td>\n+   <td>68.9\n+   </td>\n+   <td>-\n+   </td>\n+   <td>86.9\n+   </td>\n+   <td>91.6\n+   </td>\n+  </tr>\n+</table>\n+\n+#### Multilingual benchmarks\n+\n+<table>\n+  <tr>\n+   <td><strong>Category</strong>\n+   </td>\n+   <td><strong>Benchmark</strong>\n+   </td>\n+   <td><strong>Language</strong>\n+   </td>\n+   <td><strong>Llama 3.1 8B</strong>\n+   </td>\n+   <td><strong>Llama 3.1 70B</strong>\n+   </td>\n+   <td><strong>Llama 3.1 405B</strong>\n+   </td>\n+  </tr>\n+  <tr>\n+   <td rowspan=\"9\" ><strong>General</strong>\n+   </td>\n+   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n+   </td>\n+   <td>Portuguese\n+   </td>\n+   <td>62.12\n+   </td>\n+   <td>80.13\n+   </td>\n+   <td>84.95\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Spanish\n+   </td>\n+   <td>62.45\n+   </td>\n+   <td>80.05\n+   </td>\n+   <td>85.08\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Italian\n+   </td>\n+   <td>61.63\n+   </td>\n+   <td>80.4\n+   </td>\n+   <td>85.04\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>German\n+   </td>\n+   <td>60.59\n+   </td>\n+   <td>79.27\n+   </td>\n+   <td>84.36\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>French\n+   </td>\n+   <td>62.34\n+   </td>\n+   <td>79.82\n+   </td>\n+   <td>84.66\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Hindi\n+   </td>\n+   <td>50.88\n+   </td>\n+   <td>74.52\n+   </td>\n+   <td>80.31\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Thai\n+   </td>\n+   <td>50.32\n+   </td>\n+   <td>72.95\n+   </td>\n+   <td>78.21\n+   </td>\n+  </tr>\n+</table>\n+\n+\n+\n+## Responsibility & Safety\n+\n+As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n+\n+\n+\n+* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n+* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n+* Provide protections for the community to help prevent the misuse of our models.\n+\n+\n+### Responsible deployment\n+\n+Llama is a foundational technology designed to be used in a variety of use cases, examples on how Metas Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n+\n+\n+#### Llama 3.1 instruct\n+\n+Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n+\n+**Fine-tuning data**\n+\n+We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. Weve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n+\n+**Refusals and Tone**\n+\n+Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\n+\n+\n+#### Llama 3.1 systems\n+\n+**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\n+\n+As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n+\n+\n+#### New capabilities\n+\n+Note that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n+\n+**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n+\n+**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n+\n+\n+### Evaluations\n+\n+We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\n+\n+Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n+\n+**Red teaming**\n+\n+For both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\n+\n+We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\n+\n+\n+### Critical and other risks\n+\n+We specifically focused our efforts on mitigating the following critical risk areas:\n+\n+**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n+\n+To assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n+\n+\n+**2. Child Safety**\n+\n+Child Safety risk assessments were conducted using a team of experts, to assess the models capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n+\n+**3. Cyber attack enablement**\n+\n+Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n+\n+Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n+\n+Our study of Llama-3.1-405Bs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n+\n+\n+### Community\n+\n+Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n+\n+We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Metas Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n+\n+Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n+\n+\n+## Ethical Considerations and Limitations\n+\n+The core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n+\n+But Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n\n--- File: models/llama3_1/README.md ---\n@@ -0,0 +1 @@\n+july release\n\n--- File: models/llama3_1/USE_POLICY.md ---\n@@ -0,0 +1,48 @@\n+**Llama 3.1** **Acceptable Use Policy**\n+\n+Meta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (**Policy**). The most recent copy of this policy can be found at <span style=\"text-decoration:underline;\">https://llama.meta.com/llama3_1/use-policy</span>.\n+\n+**Prohibited Uses**\n+\n+We want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\n+\n+\n+\n+* Violate the law or others rights, including to:\n+    * Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n+        * Violence or terrorism\n+        * Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n+        * Human trafficking, exploitation, and sexual violence\n+        * The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n+        * Sexual solicitation\n+        * Any other criminal activity\n+    * Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n+    * Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n+    * Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n+    * Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n+    * Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n+    * Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n+* Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\n+    * Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n+    * Guns and illegal weapons (including weapon development)\n+    * Illegal drugs and regulated/controlled substances\n+    * Operation of critical infrastructure, transportation technologies, or heavy machinery\n+    * Self-harm or harm to others, including suicide, cutting, and eating disorders\n+    * Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n+* Intentionally deceive or mislead others, including use of Llama 3.1 related to the following:\n+    * Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n+    * Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n+    * Generating, promoting, or further distributing spam\n+    * Impersonating another individual without consent, authorization, or legal right\n+    * Representing that the use of Llama 3.1 or outputs are human-generated\n+    * Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n+* Fail to appropriately disclose to end users any known dangers of your AI system\n+\n+Please report any violation of this Policy, software bug, or other problems that could lead to a violation of this Policy through one of the following means:\n+\n+\n+\n+* Reporting issues with the model: <span style=\"text-decoration:underline;\">https://github.com/meta-llama/llama-models/issues</span>\n+* Reporting risky content generated by the model: developers.facebook.com/llama_output_feedback\n+* Reporting bugs and security concerns: facebook.com/whitehat/info\n+* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.1: LlamaUseReport@meta.com\n\n--- File: models/llama3_1/__init__.py ---\n@@ -0,0 +1,6 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n\n--- File: models/llama3_1/api/__init__.py ---\n@@ -0,0 +1,12 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from .args import *\n+from .chat_format import *\n+from .datatypes import *\n+from .model import *\n+from .tokenizer import *\n\n--- File: models/llama3_1/api/args.py ---\n@@ -0,0 +1,37 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from dataclasses import dataclass\n+from typing import Optional\n+\n+\n+@dataclass\n+class ModelArgs:\n+    dim: int = 4096\n+    n_layers: int = 32\n+    n_heads: int = 32\n+    n_kv_heads: Optional[int] = None\n+    vocab_size: int = -1\n+    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n+    ffn_dim_multiplier: Optional[float] = None\n+    norm_eps: float = 1e-5\n+    rope_theta: float = 500000\n+    use_scaled_rope: bool = False\n+\n+    max_batch_size: int = 32\n+    max_seq_len: int = 2048\n+\n+    def __init__(self, **kwargs):\n+        for k, v in kwargs.items():\n+            if hasattr(self, k):\n+                setattr(self, k, v)\n+\n+        if self.n_kv_heads is None:\n+            self.n_kv_heads = self.n_heads\n+        assert self.n_kv_heads <= self.n_heads\n+        assert self.n_heads % self.n_kv_heads == 0\n+        assert self.dim % self.n_heads == 0\n\n--- File: models/llama3_1/api/chat_format.py ---\n@@ -0,0 +1,152 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import uuid\n+\n+from dataclasses import dataclass\n+from typing import Dict, List\n+\n+from .tokenizer import Tokenizer\n+from .datatypes import *  # noqa: F403\n+from .tool_utils import ToolUtils\n+\n+\n+@dataclass\n+class ModelInput:\n+    tokens: List[int]\n+\n+\n+class ChatFormat:\n+    possible_headers: Dict[Role, str]\n+\n+    def __init__(self, tokenizer: Tokenizer):\n+        self.tokenizer = tokenizer\n+        self.possible_headers = {\n+            role: f\"<|start_header_id|>{role.value}<|end_header_id|>\\n\\n\"\n+            for role in Role\n+        }\n+\n+    def encode_header(self, role: str) -> List[int]:\n+        tokens = []\n+        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n+        tokens.extend(self.tokenizer.encode(role, bos=False, eos=False))\n+        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n+        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n+        return tokens\n+\n+    def encode_message(self, message: Message) -> List[int]:\n+        tokens = self.encode_header(message.role)\n+\n+        def _process_content(content: InterleavedTextAttachment):\n+            def _process(c):\n+                if isinstance(c, str):\n+                    tokens.extend(self.tokenizer.encode(c, bos=False, eos=False))\n+\n+            if isinstance(content, str):\n+                _process(content)\n+            elif isinstance(content, list):\n+                for c in content:\n+                    _process(c)\n+\n+        if isinstance(message, CompletionMessage) and len(message.tool_calls) > 0:\n+            tokens.append(self.tokenizer.special_tokens[\"<|python_tag|>\"])\n+\n+        _process_content(message.content)\n+\n+        if isinstance(message, CompletionMessage):\n+            for t in message.tool_calls:\n+                content = ToolUtils.encode_tool_call(t)\n+                _process_content(content)\n+\n+        eom = False\n+        if isinstance(message, CompletionMessage):\n+            eom = message.stop_reason == StopReason.end_of_message\n+\n+        tokens.append(\n+            self.tokenizer.special_tokens[\"<|eom_id|>\" if eom else \"<|eot_id|>\"]\n+        )\n+        return tokens\n+\n+    def encode_dialog_prompt(self, messages: List[Message]) -> ModelInput:\n+        tokens = []\n+        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n+        for message in messages:\n+            toks = self.encode_message(message)\n+            tokens.extend(toks)\n+\n+        # Add the start of an assistant message for the model to complete.\n+        tokens.extend(self.encode_header(Role.assistant.value))\n+\n+        return ModelInput(tokens=tokens)\n+\n+    # TODO(this should be generic, not only for assistant messages)\n+    def decode_assistant_message(\n+        self, tokens: List[int], stop_reason: StopReason\n+    ) -> CompletionMessage:\n+        content = self.tokenizer.decode(tokens)\n+        content = content.strip(\" \")\n+        for _, header_str in self.possible_headers.items():\n+            if content.startswith(header_str):\n+                content = content[len(header_str) :]\n+                break\n+\n+        ipython = content.startswith(\"<|python_tag|>\")\n+        if ipython:\n+            content = content[len(\"<|python_tag|>\") :]\n+\n+        eot = content.endswith(\"<|eot_id|>\")\n+        if eot:\n+            content = content[: -len(\"<|eot_id|>\")]\n+        else:\n+            content = content[: -len(\"<|eom_id|>\")]\n+\n+        tool_name = None\n+        tool_arguments = {}\n+\n+        custom_tool_info = ToolUtils.maybe_extract_custom_tool_call(content)\n+        if custom_tool_info is not None:\n+            tool_name, tool_arguments = custom_tool_info\n+            # Sometimes when agent has custom tools alongside builin tools\n+            # Agent responds for builtin tool calls in the format of the custom tools\n+            # This code tries to handle that case\n+            if tool_name in BuiltinTool.__members__:\n+                tool_name = BuiltinTool[tool_name]\n+                tool_arguments = {\n+                    \"query\": list(tool_arguments.values())[0],\n+                }\n+        else:\n+            builtin_tool_info = ToolUtils.maybe_extract_builtin_tool_call(content)\n+            if builtin_tool_info is not None:\n+                tool_name, query = builtin_tool_info\n+                tool_arguments = {\n+                    \"query\": query,\n+                }\n+                if tool_name in BuiltinTool.__members__:\n+                    tool_name = BuiltinTool[tool_name]\n+            elif ipython:\n+                tool_name = BuiltinTool.code_interpreter\n+                tool_arguments = {\n+                    \"code\": content,\n+                }\n+\n+        tool_calls = []\n+        if tool_name is not None and tool_arguments is not None:\n+            call_id = str(uuid.uuid4())\n+            tool_calls.append(\n+                ToolCall(\n+                    call_id=call_id,\n+                    tool_name=tool_name,\n+                    arguments=tool_arguments,\n+                )\n+            )\n+            content = \"\"\n+\n+        return CompletionMessage(\n+            content=content,\n+            stop_reason=stop_reason,\n+            tool_calls=tool_calls,\n+        )\n\n--- File: models/llama3_1/api/datatypes.py ---\n@@ -0,0 +1,228 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from enum import Enum\n+from typing import Any, Dict, List, Literal, Optional, Union\n+\n+from pydantic import BaseModel, Field\n+\n+from strong_typing.schema import json_schema_type\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class Role(Enum):\n+    system = \"system\"\n+    user = \"user\"\n+    assistant = \"assistant\"\n+    ipython = \"ipython\"\n+\n+\n+@json_schema_type(\n+    schema={\"type\": \"string\", \"format\": \"uri\", \"pattern\": \"^(https?://|file://|data:)\"}\n+)\n+class URL(BaseModel):\n+    uri: str\n+\n+    def __str__(self) -> str:\n+        return self.uri\n+\n+\n+@json_schema_type(\n+    schema={\n+        \"description\": \"The type of the model. This is used to determine the model family and SKU.\"\n+    }\n+)\n+class PretrainedModel(Enum):\n+    llama3_8b = \"llama3_8b\"\n+    llama3_70b = \"llama3_70b\"\n+\n+\n+@json_schema_type\n+class InstructModel(Enum):\n+    llama3_8b_chat = \"llama3_8b_chat\"\n+    llama3_70b_chat = \"llama3_70b_chat\"\n+\n+\n+@json_schema_type\n+class RewardModel(Enum):\n+    llama3_70b_reward = \"llama3_70b_reward\"\n+    llama3_405b_reward = \"llama3_405b_reward\"\n+\n+\n+@json_schema_type\n+class SamplingStrategy(Enum):\n+    greedy = \"greedy\"\n+    top_p = \"top_p\"\n+    top_k = \"top_k\"\n+\n+\n+@json_schema_type\n+class SamplingParams(BaseModel):\n+    strategy: SamplingStrategy = SamplingStrategy.greedy\n+\n+    # TODO: inference code for model parallel seems to get into desync\n+    # when temperature is 1.0\n+    temperature: Optional[float] = 0.0\n+    top_p: Optional[float] = 0.95\n+    top_k: Optional[int] = 0\n+    max_tokens: Optional[int] = 0\n+    repetition_penalty: Optional[float] = 1.0\n+\n+\n+@json_schema_type\n+class Attachment(BaseModel):\n+    url: URL\n+    mime_type: str\n+\n+\n+InterleavedTextAttachment = Union[\n+    str,\n+    Attachment,\n+    List[Union[str, Attachment]],\n+]\n+\n+\n+@json_schema_type\n+class BuiltinTool(Enum):\n+    brave_search = \"brave_search\"\n+    wolfram_alpha = \"wolfram_alpha\"\n+    photogen = \"photogen\"\n+    code_interpreter = \"code_interpreter\"\n+\n+\n+Primitive = Union[str, int, float, bool, None]\n+RecursiveType = Union[Primitive, List[Primitive], Dict[str, Primitive]]\n+\n+\n+@json_schema_type\n+class ToolCall(BaseModel):\n+    call_id: str\n+    tool_name: Union[BuiltinTool, str]\n+    arguments: Dict[str, RecursiveType]\n+\n+\n+@json_schema_type\n+class ToolResponse(BaseModel):\n+    call_id: str\n+    tool_name: Union[BuiltinTool, str]\n+    content: InterleavedTextAttachment\n+\n+\n+@json_schema_type\n+class ToolParamDefinition(BaseModel):\n+    param_type: str\n+    description: Optional[str] = None\n+    required: Optional[bool] = True\n+\n+\n+@json_schema_type\n+class ToolDefinition(BaseModel):\n+    tool_name: Union[BuiltinTool, str]\n+    description: Optional[str] = None\n+    parameters: Optional[Dict[str, ToolParamDefinition]] = None\n+\n+\n+@json_schema_type\n+class UserMessage(BaseModel):\n+    role: Literal[Role.user.value] = Role.user.value\n+    content: InterleavedTextAttachment\n+\n+\n+@json_schema_type\n+class SystemMessage(BaseModel):\n+    role: Literal[Role.system.value] = Role.system.value\n+    content: InterleavedTextAttachment\n+\n+\n+@json_schema_type\n+class ToolResponseMessage(BaseModel):\n+    role: Literal[Role.ipython.value] = Role.ipython.value\n+    # it was nice to re-use the ToolResponse type, but having all messages\n+    # have a `content` type makes things nicer too\n+    call_id: str\n+    tool_name: Union[BuiltinTool, str]\n+    content: InterleavedTextAttachment\n+\n+\n+@json_schema_type\n+class StopReason(Enum):\n+    end_of_turn = \"end_of_turn\"\n+    end_of_message = \"end_of_message\"\n+    out_of_tokens = \"out_of_tokens\"\n+\n+\n+@json_schema_type\n+class TokenLogProbs(BaseModel):\n+    logprobs_by_token: Dict[str, float]\n+\n+\n+@json_schema_type\n+class CompletionMessage(BaseModel):\n+    role: Literal[Role.assistant.value] = Role.assistant.value\n+    content: InterleavedTextAttachment\n+    stop_reason: StopReason\n+    tool_calls: List[ToolCall] = Field(default_factory=list)\n+\n+\n+Message = Annotated[\n+    Union[\n+        UserMessage,\n+        SystemMessage,\n+        ToolResponseMessage,\n+        CompletionMessage,\n+    ],\n+    Field(discriminator=\"role\"),\n+]\n+\n+\n+@json_schema_type(\n+    schema={\n+        \"description\": \"The format in which weights are specified. This does not necessarily always equal what quantization is desired at runtime since there can be on-the-fly conversions done.\",\n+    }\n+)\n+class CheckpointQuantizationFormat(Enum):\n+    # default format\n+    bf16 = \"bf16\"\n+\n+    # used for enabling fp8_rowwise inference, some weights are bf16\n+    fp8_mixed = \"fp8_mixed\"\n+\n+\n+@json_schema_type\n+class ModelFamily(Enum):\n+    llama3_1 = \"llama3_1\"\n+\n+\n+@json_schema_type\n+class ModelSKU(Enum):\n+    llama3_1_8b = \"llama3_1_8b\"\n+    llama3_1_70b = \"llama3_1_70b\"\n+    llama3_1_405b_fp8_mp8 = \"llama3_1_405b_fp8_mp8\"\n+    llama3_1_405b_bf16_mp8 = \"llama3_1_405b_bf16_mp8\"\n+    llama3_1_405b_bf16_mp16 = \"llama3_1_405b_bf16_mp16\"\n+\n+    llama3_1_8b_instruct = \"llama3_1_8b_instruct\"\n+    llama3_1_70b_instruct = \"llama3_1_70b_instruct\"\n+    llama3_1_405b_instruct_fp8_mp8 = \"llama3_1_405b_instruct_fp8_mp8\"\n+    llama3_1_405b_instruct_bf16_mp8 = \"llama3_1_405b_instruct_bf16_mp8\"\n+    llama3_1_405b_instruct_bf16_mp16 = \"llama3_1_405b_instruct_bf16_mp16\"\n+\n+\n+@json_schema_type(\n+    schema={\n+        \"description\": \"The model family and SKU of the model along with other parameters corresponding to the model.\"\n+    }\n+)\n+class ModelDefinition(BaseModel):\n+    family: ModelFamily\n+    sku: ModelSKU\n+    description_markdown: str\n+    max_seq_length: int\n+    model_parallel_size: int\n+    quantization_format: Optional[CheckpointQuantizationFormat] = None\n+    model_args: Dict[str, Any]\n\n--- File: models/llama3_1/api/interface.py ---\n@@ -0,0 +1,193 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import glob\n+import json\n+import os\n+from datetime import datetime\n+from pathlib import Path\n+\n+from typing import List, Optional\n+\n+from termcolor import colored, cprint\n+\n+from .chat_format import ChatFormat\n+\n+from .datatypes import (\n+    BuiltinTool,\n+    CompletionMessage,\n+    Message,\n+    StopReason,\n+    SystemMessage,\n+    ToolCall,\n+    ToolDefinition,\n+    ToolParamDefinition,\n+)\n+from .tokenizer import Tokenizer\n+\n+THIS_DIR = Path(__file__).parent\n+\n+TEMPLATES = {}\n+for role in (\"system\", \"assistant\", \"tool\", \"user\"):\n+    for path in glob.glob(str(THIS_DIR / \"templates\" / f\"{role}_message*.yaml\")):\n+        name = os.path.basename(path)\n+        name = name.replace(\"_\", \"-\").replace(\".yaml\", \"\").replace(\".\", \"-\")\n+        TEMPLATES[name] = (role, path, f\"{role}_message.jinja\")\n+\n+\n+class LLama3_1_Interface:\n+    def __init__(self, tokenizer_path: str):\n+        self.tokenizer = Tokenizer(tokenizer_path)\n+        self.formatter = ChatFormat(self.tokenizer)\n+\n+    def recommended_system_message(\n+        self,\n+        builtin_tools: List[BuiltinTool],\n+        custom_tools: List[ToolDefinition],\n+        instructions: Optional[str] = None,\n+    ) -> SystemMessage:\n+        content = \"\"\n+        if builtin_tools:\n+            content += \"Environment: ipython\\n\"\n+\n+            tool_str = \", \".join(\n+                [t.value for t in builtin_tools if t != BuiltinTool.code_interpreter]\n+            )\n+            if tool_str:\n+                content += f\"Tools: {tool_str}\\n\"\n+\n+        current_date = datetime.now()\n+        formatted_date = current_date.strftime(\"%d %B %Y\")\n+        date_str = f\"\"\"\n+Cutting Knowledge Date: December 2023\n+Today Date: {formatted_date}\"\"\"\n+        content += date_str\n+\n+        if custom_tools:\n+            content += \"\\n\" + self.get_custom_tool_instructions(custom_tools)\n+\n+        if instructions:\n+            content += f\"\\n{instructions}\"\n+\n+        return SystemMessage(content=content)\n+\n+    def get_custom_tool_instructions(self, custom_tools: List[ToolDefinition]) -> str:\n+        custom_tool_params = \"\"\n+\n+        custom_tool_params = \"\\n\".join(\n+            f\"{get_instruction_string(t)}\\n{get_parameters_string(t)}\\n\"\n+            for t in custom_tools\n+        )\n+\n+        content = f\"\"\"\n+You have access to the following functions:\n+\n+{custom_tool_params}\n+Think very carefully before calling functions.\n+If a you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+\n+<function=example_function_name>{{\"example_name\": \"example_value\"}}</function>\n+\n+Reminder:\n+- If looking for real time information use relevant functions before falling back to brave_search\n+- Function calls MUST follow the specified format, start with <function= and end with </function>\n+- Required parameters MUST be specified\n+- Only call one function at a time\n+- Put the entire function call reply on one line\n+\n+\"\"\"\n+        return content\n+\n+    def get_sample_builtin_tool_call_message(self) -> CompletionMessage:\n+        return CompletionMessage(\n+            content=\"\",\n+            stop_reason=StopReason.end_of_message,\n+            tool_calls=[\n+                ToolCall(\n+                    call_id=\"1234\",\n+                    tool_name=BuiltinTool.brave_search,\n+                    arguments={\n+                        \"query\": \"Who won NBA in 2024?\",\n+                    },\n+                )\n+            ],\n+        )\n+\n+    def get_message_as_str_tokens(self, message: Message) -> str:\n+        tokens = self.formatter.encode_message(message)\n+        return self.tokenizer.decode(tokens)\n+\n+    def display_message_as_tokens(self, message: Message) -> None:\n+        tokens = self.formatter.encode_message(message)\n+        on_colors = [\n+            \"on_red\",\n+            \"on_green\",\n+            \"on_yellow\",\n+            \"on_blue\",\n+            \"on_magenta\",\n+            \"on_cyan\",\n+        ]\n+        for i, t in enumerate(tokens):\n+            on_col = on_colors[i % len(on_colors)]\n+            print(colored(self.tokenizer.decode([t]), \"white\", on_col), end=\"\")\n+        print(\"\\n\", end=\"\")\n+\n+\n+def get_instruction_string(tooldef: ToolDefinition) -> str:\n+    return f\"Use the function '{tooldef.tool_name}' to '{tooldef.description}'\"\n+\n+\n+def get_parameters_string(tooldef: ToolDefinition) -> str:\n+    return json.dumps(\n+        {\n+            \"name\": tooldef.tool_name,\n+            \"description\": tooldef.description,\n+            \"parameters\": {\n+                name: definition.__dict__\n+                for name, definition in tooldef.parameters.items()\n+            },\n+        }\n+    )\n+\n+\n+def list_jinja_templates():\n+    global TEMPLATES\n+\n+    for name in TEMPLATES.keys():\n+        print(f\"{name}\")\n+\n+\n+def render_jinja_template(name: str):\n+    if name not in TEMPLATES:\n+        raise ValueError(f\"No template found for `{name}`\")\n+\n+    import yaml\n+    from jinja2 import Environment, FileSystemLoader\n+\n+    tokenizer = Tokenizer(str(THIS_DIR / \"tokenizer.model\"))\n+    special_tokens = list(tokenizer.special_tokens.values())\n+\n+    role, input_path, jinja_template = TEMPLATES[name]\n+\n+    env = Environment(loader=FileSystemLoader(THIS_DIR / \"templates\"))\n+    template = env.get_template(jinja_template)\n+\n+    with open(input_path, \"r\") as f:\n+        context = yaml.safe_load(f)\n+        context[\"today\"] = datetime.now().strftime(\"%d %B %Y\")\n+\n+        output = template.render(context)\n+        tokens = tokenizer.encode(output, allowed_special=\"all\", bos=False, eos=False)\n+\n+        for t in tokens:\n+            decoded = tokenizer.decode([t])\n+            if t in special_tokens:\n+                cprint(decoded, \"yellow\", end=\"\")\n+            else:\n+                print(decoded, end=\"\")\n+\n+        print(\"\")\n\n--- File: models/llama3_1/api/model.py ---\n@@ -0,0 +1,324 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+import math\n+from typing import Optional, Tuple\n+\n+import fairscale.nn.model_parallel.initialize as fs_init\n+import torch\n+import torch.nn.functional as F\n+from fairscale.nn.model_parallel.layers import (\n+    ColumnParallelLinear,\n+    RowParallelLinear,\n+    VocabParallelEmbedding,\n+)\n+from torch import nn\n+\n+from .args import ModelArgs\n+\n+\n+class RMSNorm(torch.nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.ones(dim))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float()).type_as(x)\n+        return output * self.weight\n+\n+\n+def apply_scaling(freqs: torch.Tensor):\n+    # Values obtained from grid search\n+    scale_factor = 8\n+    low_freq_factor = 1\n+    high_freq_factor = 4\n+    old_context_len = 8192  # original llama3 length\n+\n+    low_freq_wavelen = old_context_len / low_freq_factor\n+    high_freq_wavelen = old_context_len / high_freq_factor\n+    new_freqs = []\n+    for freq in freqs:\n+        wavelen = 2 * math.pi / freq\n+        if wavelen < high_freq_wavelen:\n+            new_freqs.append(freq)\n+        elif wavelen > low_freq_wavelen:\n+            new_freqs.append(freq / scale_factor)\n+        else:\n+            assert low_freq_wavelen != high_freq_wavelen\n+            smooth = (old_context_len / wavelen - low_freq_factor) / (\n+                high_freq_factor - low_freq_factor\n+            )\n+            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n+    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n+\n+\n+def precompute_freqs_cis(\n+    dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False\n+):\n+    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n+    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n+    if use_scaled:\n+        freqs = apply_scaling(freqs)\n+    freqs = torch.outer(t, freqs)\n+    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n+    return freqs_cis\n+\n+\n+def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n+    ndim = x.ndim\n+    assert 0 <= 1 < ndim\n+    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n+    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n+    return freqs_cis.view(*shape)\n+\n+\n+def apply_rotary_emb(\n+    xq: torch.Tensor,\n+    xk: torch.Tensor,\n+    freqs_cis: torch.Tensor,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n+    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n+    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n+    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n+    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n+    return xq_out.type_as(xq), xk_out.type_as(xk)\n+\n+\n+def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n+    bs, slen, n_kv_heads, head_dim = x.shape\n+    if n_rep == 1:\n+        return x\n+    return (\n+        x[:, :, :, None, :]\n+        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n+        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n+    )\n+\n+\n+class Attention(nn.Module):\n+    def __init__(self, args: ModelArgs):\n+        super().__init__()\n+        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n+        model_parallel_size = fs_init.get_model_parallel_world_size()\n+        self.n_local_heads = args.n_heads // model_parallel_size\n+        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n+        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n+        self.head_dim = args.dim // args.n_heads\n+\n+        self.wq = ColumnParallelLinear(\n+            args.dim,\n+            args.n_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wk = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wv = ColumnParallelLinear(\n+            args.dim,\n+            self.n_kv_heads * self.head_dim,\n+            bias=False,\n+            gather_output=False,\n+            init_method=lambda x: x,\n+        )\n+        self.wo = RowParallelLinear(\n+            args.n_heads * self.head_dim,\n+            args.dim,\n+            bias=False,\n+            input_is_parallel=True,\n+            init_method=lambda x: x,\n+        )\n+\n+        self.cache_k = torch.zeros(\n+            (\n+                args.max_batch_size,\n+                args.max_seq_len,\n+                self.n_local_kv_heads,\n+                self.head_dim,\n+            )\n+        ).cuda()\n+        self.cache_v = torch.zeros(\n+            (\n+                args.max_batch_size,\n+                args.max_seq_len,\n+                self.n_local_kv_heads,\n+                self.head_dim,\n+            )\n+        ).cuda()\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        start_pos: int,\n+        freqs_cis: torch.Tensor,\n+        mask: Optional[torch.Tensor],\n+    ):\n+        bsz, seqlen, _ = x.shape\n+        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n+\n+        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n+        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n+        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n+\n+        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n+\n+        self.cache_k = self.cache_k.to(xq)\n+        self.cache_v = self.cache_v.to(xq)\n+\n+        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n+        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n+\n+        keys = self.cache_k[:bsz, : start_pos + seqlen]\n+        values = self.cache_v[:bsz, : start_pos + seqlen]\n+\n+        # repeat k/v heads if n_kv_heads < n_heads\n+        keys = repeat_kv(\n+            keys, self.n_rep\n+        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n+        values = repeat_kv(\n+            values, self.n_rep\n+        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n+\n+        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n+        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n+        values = values.transpose(\n+            1, 2\n+        )  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n+        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n+        if mask is not None:\n+            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n+        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n+        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n+        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n+        return self.wo(output)\n+\n+\n+class FeedForward(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        hidden_dim: int,\n+        multiple_of: int,\n+        ffn_dim_multiplier: Optional[float],\n+    ):\n+        super().__init__()\n+        hidden_dim = int(2 * hidden_dim / 3)\n+        # custom dim factor multiplier\n+        if ffn_dim_multiplier is not None:\n+            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n+        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n+\n+        self.w1 = ColumnParallelLinear(\n+            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n+        )\n+        self.w2 = RowParallelLinear(\n+            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n+        )\n+        self.w3 = ColumnParallelLinear(\n+            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n+        )\n+\n+    def forward(self, x):\n+        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n+\n+\n+class TransformerBlock(nn.Module):\n+    def __init__(self, layer_id: int, args: ModelArgs):\n+        super().__init__()\n+        self.n_heads = args.n_heads\n+        self.dim = args.dim\n+        self.head_dim = args.dim // args.n_heads\n+        self.attention = Attention(args)\n+        self.feed_forward = FeedForward(\n+            dim=args.dim,\n+            hidden_dim=4 * args.dim,\n+            multiple_of=args.multiple_of,\n+            ffn_dim_multiplier=args.ffn_dim_multiplier,\n+        )\n+        self.layer_id = layer_id\n+        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,\n+        start_pos: int,\n+        freqs_cis: torch.Tensor,\n+        mask: Optional[torch.Tensor],\n+    ):\n+        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n+        out = h + self.feed_forward(self.ffn_norm(h))\n+        return out\n+\n+\n+class Transformer(nn.Module):\n+    def __init__(self, params: ModelArgs):\n+        super().__init__()\n+        self.params = params\n+        self.vocab_size = params.vocab_size\n+        self.n_layers = params.n_layers\n+\n+        self.tok_embeddings = VocabParallelEmbedding(\n+            params.vocab_size, params.dim, init_method=lambda x: x\n+        )\n+\n+        self.layers = torch.nn.ModuleList()\n+        for layer_id in range(params.n_layers):\n+            self.layers.append(TransformerBlock(layer_id, params))\n+\n+        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n+        self.output = ColumnParallelLinear(\n+            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n+        )\n+\n+        self.freqs_cis = precompute_freqs_cis(\n+            params.dim // params.n_heads,\n+            params.max_seq_len * 2,\n+            params.rope_theta,\n+            params.use_scaled_rope,\n+        )\n+\n+    @torch.inference_mode()\n+    def forward(self, tokens: torch.Tensor, start_pos: int):\n+        _bsz, seqlen = tokens.shape\n+        h = self.tok_embeddings(tokens)\n+        self.freqs_cis = self.freqs_cis.to(h.device)\n+        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n+\n+        mask = None\n+        if seqlen > 1:\n+            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n+\n+            mask = torch.triu(mask, diagonal=1)\n+\n+            # When performing key-value caching, we compute the attention scores\n+            # only for the new sequence. Thus, the matrix of scores is of size\n+            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n+            # j > cache_len + i, since row i corresponds to token cache_len + i.\n+            mask = torch.hstack(\n+                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n+            ).type_as(h)\n+\n+        for layer in self.layers:\n+            h = layer(h, start_pos, freqs_cis, mask)\n+        h = self.norm(h)\n+        output = self.output(h).float()\n+        return output\n\n--- File: models/llama3_1/api/sku_list.py ---\n@@ -0,0 +1,120 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+from typing import List\n+\n+from .datatypes import (\n+    CheckpointQuantizationFormat,\n+    ModelDefinition,\n+    ModelFamily,\n+    ModelSKU,\n+)\n+\n+\n+CONTEXT_LENGTH = 131072\n+VOCAB_SIZE = 128256\n+\n+\n+def llama3_1_model_list() -> List[ModelDefinition]:\n+    return [\n+        ModelDefinition(\n+            family=ModelFamily.llama3_1,\n+            sku=ModelSKU.llama3_1_8b,\n+            description_markdown=\"Llama 3.1 8b model\",\n+            max_seq_len=CONTEXT_LENGTH,\n+            model_parallel_size=1,\n+            model_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 1024,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": true,\n+            },\n+        ),\n+        ModelDefinition(\n+            family=ModelFamily.llama3_1,\n+            sku=ModelSKU.llama3_1_70b,\n+            description_markdown=\"Llama 3.1 70b model\",\n+            max_seq_len=CONTEXT_LENGTH,\n+            model_parallel_size=8,\n+            model_args={\n+                \"dim\": 8192,\n+                \"n_layers\": 80,\n+                \"n_heads\": 64,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": true,\n+            },\n+        ),\n+        ModelDefinition(\n+            family=ModelFamily.llama3_1,\n+            sku=ModelSKU.llama3_1_405b_bf16_mp8,\n+            description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n+            max_seq_len=CONTEXT_LENGTH,\n+            model_parallel_size=8,\n+            model_args={\n+                \"dim\": 16384,\n+                \"n_layers\": 126,\n+                \"n_heads\": 128,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.2,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": true,\n+            },\n+        ),\n+        ModelDefinition(\n+            family=ModelFamily.llama3_1,\n+            sku=ModelSKU.llama3_1_405b_fp8_mp8,\n+            description_markdown=\"Llama 3.1 405b model (FP8 quantized)\",\n+            max_seq_len=CONTEXT_LENGTH,\n+            model_parallel_size=8,\n+            quantization_format=CheckpointQuantizationFormat.fp8_mixed,\n+            model_args={\n+                \"dim\": 16384,\n+                \"n_layers\": 126,\n+                \"n_heads\": 128,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.2,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": true,\n+            },\n+        ),\n+        ModelDefinition(\n+            family=ModelFamily.llama3_1,\n+            sku=ModelSKU.llama3_1_405b_bf16_mp16,\n+            description_markdown=\"Llama 3.1 405b model (BF16 weights)\",\n+            max_seq_len=CONTEXT_LENGTH,\n+            model_parallel_size=16,\n+            model_args={\n+                \"dim\": 16384,\n+                \"n_layers\": 126,\n+                \"n_heads\": 128,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.2,\n+                \"multiple_of\": 4096,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": true,\n+            },\n+        ),\n+    ]\n\n--- File: models/llama3_1/api/templates/assistant_message.builtin_tool_call.yaml ---\n@@ -0,0 +1,5 @@\n+tool_call:\n+  tool_name: \"brave_search\"\n+  arguments:\n+    query: \"Who won NBA in 2024?\"\n+end_of_message: True\n\n--- File: models/llama3_1/api/templates/assistant_message.custom_tool_call.yaml ---\n@@ -0,0 +1,6 @@\n+tool_call:\n+  tool_name: get_boiling_point\n+  arguments:\n+    liquid_name: Mercury\n+    celsius: True\n+end_of_message: True\n\n--- File: models/llama3_1/api/templates/assistant_message.default.yaml ---\n@@ -0,0 +1 @@\n+content: Hi, I am a helpful assistant. What can I help you with today?\n\n--- File: models/llama3_1/api/templates/assistant_message.jinja ---\n@@ -0,0 +1,14 @@\n+{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n+\n+{%- set builtins = [\"brave_search\", \"wolfram_alpha\", \"photogen\"] -%}\n+{%- if tool_call -%}\n+<|python_tag|>\n+{%- if tool_call.tool_name in builtins -%}\n+{{ tool_call.tool_name }}.call(query=\"{{ tool_call.arguments['query'] }}\")\n+{%- else -%}\n+<function=\"{{ tool_call.tool_name }}\">{{ tool_call.arguments | tojson }}</function>\n+{%- endif -%}\n+{%- else -%}\n+{{ content }}\n+{%- endif -%}\n+{%- if end_of_message %}<|eom_id|>{% else %}<|eot_id|>{% endif %}\n\n--- File: models/llama3_1/api/templates/system_message.builtin_and_custom_tools.yaml ---\n@@ -0,0 +1,24 @@\n+builtin_tools: [\"brave_search\", \"wolfram_alpha\"]\n+custom_tools:\n+  - tool_name: get_boiling_point\n+    description: Get the boiling point of a liquid\n+    parameters:\n+      - liquid_name:\n+        - type: string\n+        - description: name of the liquid\n+        - required: True\n+      - celsius:\n+        - type: boolean\n+        - description: whether to return the boiling point in celsius\n+        - required: False\n+  - tool_name: trending_songs\n+    description: Returns the trending songs on a Music site\n+    parameters:\n+      - country:\n+          type: string\n+          description: The country to return trending songs for\n+          required: True\n+      - n:\n+          type: int\n+          description: The number of songs to return\n+          required: False\n\n--- File: models/llama3_1/api/templates/system_message.builtin_tools_only.yaml ---\n@@ -0,0 +1,2 @@\n+builtin_tools: [\"brave_search\", \"wolfram_alpha\"]\n+custom_tools: []\n\n--- File: models/llama3_1/api/templates/system_message.custom_tools_only.yaml ---\n@@ -0,0 +1,24 @@\n+builtin_tools: []\n+custom_tools:\n+  - tool_name: get_boiling_point\n+    description: Get the boiling point of a liquid\n+    parameters:\n+      - liquid_name:\n+        - type: string\n+        - description: name of the liquid\n+        - required: True\n+      - celsius:\n+        - type: boolean\n+        - description: whether to return the boiling point in celsius\n+        - required: False\n+  - tool_name: trending_songs\n+    description: Returns the trending songs on a Music site\n+    parameters:\n+      - country:\n+          type: string\n+          description: The country to return trending songs for\n+          required: True\n+      - n:\n+          type: int\n+          description: The number of songs to return\n+          required: False\n\n--- File: models/llama3_1/api/templates/system_message.default.yaml ---\n@@ -0,0 +1,2 @@\n+builtin_tools: []\n+custom_tools: []\n\n--- File: models/llama3_1/api/templates/system_message.jinja ---\n@@ -0,0 +1,39 @@\n+{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' -}}\n+{% if builtin_tools or custom_tools -%}\n+Environment: ipython\n+{%- set builtin_tools = builtin_tools | reject('equalto', 'code_interpreter') | list -%}\n+{%- if builtin_tools %}\n+Tools: {{ builtin_tools | join(\", \") }}\n+{% endif -%}\n+{% endif %}\n+Cutting Knowledge Date: December 2023\n+Today Date: {{ today }}\n+<|eot_id|>\n+{%- if custom_tools %}\n+{{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n+\n+You have access to the following functions:\n+\n+{% for t in custom_tools %}\n+{#- manually setting up JSON because jinja sorts keys in unexpected ways -#}\n+{%- set tname = t.tool_name -%}\n+{%- set tdesc = t.description -%}\n+{%- set tparams = t.parameters | tojson -%}\n+Use the function '{{ tname }}' to '{{ tdesc }}':\n+{\"name\": \"{{tname}}\", \"description\": \"{{tdesc}}\", \"parameters\": {{tparams}}}\n+\n+{% endfor -%}\n+Think very carefully before calling functions.\n+If a you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+\n+<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n+\n+Reminder:\n+- If looking for real time information use relevant functions before falling back to brave_search\n+- Function calls MUST follow the specified format, start with <function= and end with </function>\n+- Required parameters MUST be specified\n+- Only call one function at a time\n+- Put the entire function call reply on one line\n+<|eot_id|>\n+{%- endif -%}\n+{{ additional_instructions -}}\n\n--- File: models/llama3_1/api/templates/tool_message.failure.yaml ---\n@@ -0,0 +1,2 @@\n+status: failure\n+stderr: \"brave_search encounter an error: could not communicate with api.brave.com\"\n\n--- File: models/llama3_1/api/templates/tool_message.jinja ---\n@@ -0,0 +1,10 @@\n+{{ '<|start_header_id|>ipython<|end_header_id|>\\n\\n' -}}\n+\n+{% if status == \"success\" %}completed{% else %}failed{% endif %}\n+{%- if stdout %}\n+[stdout]{{ stdout }}[/stdout]\n+{%- endif -%}\n+{%- if stderr %}\n+[stderr]{{ stderr }}[/stderr]\n+{%- endif -%}\n+<|eot_id|>\n\n--- File: models/llama3_1/api/templates/tool_message.success.yaml ---\n@@ -0,0 +1,2 @@\n+status: success\n+stdout: '{\"results\":[\"something something\"]}'\n\n--- File: models/llama3_1/api/templates/user_message.default.yaml ---\n@@ -0,0 +1 @@\n+content: Please tell me how to plan a trip to New York\n\n--- File: models/llama3_1/api/templates/user_message.jinja ---\n@@ -0,0 +1,3 @@\n+{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n+{{ content -}}\n+<|eot_id|>\n\n--- File: models/llama3_1/api/test_tokenizer.py ---\n@@ -0,0 +1,108 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+import os\n+from unittest import TestCase\n+\n+from .chat_format import ChatFormat\n+from .datatypes import SystemMessage, UserMessage\n+from .tokenizer import Tokenizer\n+\n+\n+# TOKENIZER_PATH=<tokenizer_path> python -m unittest models/llama3_1/api/test_tokenizer.py\n+\n+\n+class TokenizerTests(TestCase):\n+    def setUp(self):\n+        self.tokenizer = Tokenizer(os.environ[\"TOKENIZER_PATH\"])\n+        self.format = ChatFormat(self.tokenizer)\n+\n+    def test_special_tokens(self):\n+        self.assertEqual(\n+            self.tokenizer.special_tokens[\"<|begin_of_text|>\"],\n+            128000,\n+        )\n+\n+    def test_encode(self):\n+        self.assertEqual(\n+            self.tokenizer.encode(\"This is a test sentence.\", bos=True, eos=True),\n+            [128000, 2028, 374, 264, 1296, 11914, 13, 128001],\n+        )\n+\n+    def test_decode(self):\n+        self.assertEqual(\n+            self.tokenizer.decode(\n+                [128000, 2028, 374, 264, 1296, 11914, 13, 128001],\n+            ),\n+            \"<|begin_of_text|>This is a test sentence.<|end_of_text|>\",\n+        )\n+\n+    def test_encode_message(self):\n+        message = UserMessage(\n+            content=\"This is a test sentence.\",\n+        )\n+        self.assertEqual(\n+            self.format.encode_message(message),\n+            [\n+                128006,  # <|start_header_id|>\n+                882,  # \"user\"\n+                128007,  # <|end_of_header|>\n+                271,  # \"\\n\\n\"\n+                2028,\n+                374,\n+                264,\n+                1296,\n+                11914,\n+                13,  # This is a test sentence.\n+                128009,  # <|eot_id|>\n+            ],\n+        )\n+\n+    def test_encode_dialog(self):\n+        messages = [\n+            SystemMessage(\n+                content=\"This is a test sentence.\",\n+            ),\n+            UserMessage(\n+                content=\"This is a response.\",\n+            ),\n+        ]\n+        model_input = self.format.encode_dialog_prompt(messages)\n+        self.assertEqual(\n+            model_input.tokens,\n+            [\n+                128000,  # <|begin_of_text|>\n+                128006,  # <|start_header_id|>\n+                9125,  # \"system\"\n+                128007,  # <|end_of_header|>\n+                271,  # \"\\n\\n\"\n+                2028,\n+                374,\n+                264,\n+                1296,\n+                11914,\n+                13,  # \"This is a test sentence.\"\n+                128009,  # <|eot_id|>\n+                128006,  # <|start_header_id|>\n+                882,  # \"user\"\n+                128007,  # <|end_of_header|>\n+                271,  # \"\\n\\n\"\n+                2028,\n+                374,\n+                264,\n+                2077,\n+                13,  # \"This is a response.\",\n+                128009,  # <|eot_id|>\n+                128006,  # <|start_header_id|>\n+                78191,  # \"assistant\"\n+                128007,  # <|end_of_header|>\n+                271,  # \"\\n\\n\"\n+            ],\n+        )\n\n--- File: models/llama3_1/api/tokenizer.py ---\n@@ -0,0 +1,201 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n+\n+import os\n+from logging import getLogger\n+from pathlib import Path\n+from typing import (\n+    AbstractSet,\n+    cast,\n+    Collection,\n+    Dict,\n+    Iterator,\n+    List,\n+    Literal,\n+    Optional,\n+    Sequence,\n+    Union,\n+)\n+\n+import tiktoken\n+\n+from tiktoken.load import load_tiktoken_bpe\n+\n+logger = getLogger(__name__)\n+\n+\n+# The tiktoken tokenizer can handle <=400k chars without\n+# pyo3_runtime.PanicException.\n+TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n+\n+# https://github.com/openai/tiktoken/issues/195\n+# Here we iterate over subsequences and split if we exceed the limit\n+# of max consecutive non-whitespace or whitespace characters.\n+MAX_NO_WHITESPACES_CHARS = 25_000\n+\n+\n+class Tokenizer:\n+    \"\"\"\n+    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n+    \"\"\"\n+\n+    special_tokens: Dict[str, int]\n+\n+    num_reserved_special_tokens = 256\n+\n+    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n+\n+    def __init__(self, model_path: str):\n+        \"\"\"\n+        Initializes the Tokenizer with a Tiktoken model.\n+\n+        Args:\n+            model_path (str): The path to the Tiktoken model file.\n+        \"\"\"\n+        assert os.path.isfile(model_path), model_path\n+\n+        mergeable_ranks = load_tiktoken_bpe(model_path)\n+        num_base_tokens = len(mergeable_ranks)\n+        special_tokens = [\n+            \"<|begin_of_text|>\",\n+            \"<|end_of_text|>\",\n+            \"<|reserved_special_token_0|>\",\n+            \"<|reserved_special_token_1|>\",\n+            \"<|finetune_right_pad_id|>\",\n+            \"<|step_id|>\",\n+            \"<|start_header_id|>\",\n+            \"<|end_header_id|>\",\n+            \"<|eom_id|>\",  # end of message\n+            \"<|eot_id|>\",  # end of turn\n+            \"<|python_tag|>\",\n+        ]\n+        reserved_tokens = [\n+            f\"<|reserved_special_token_{2 + i}|>\"\n+            for i in range(self.num_reserved_special_tokens - len(special_tokens))\n+        ]\n+        special_tokens = special_tokens + reserved_tokens\n+\n+        self.special_tokens = {\n+            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n+        }\n+        self.model = tiktoken.Encoding(\n+            name=Path(model_path).name,\n+            pat_str=self.pat_str,\n+            mergeable_ranks=mergeable_ranks,\n+            special_tokens=self.special_tokens,\n+        )\n+\n+        self.n_words: int = num_base_tokens + len(special_tokens)\n+        # BOS / EOS token IDs\n+        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n+        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n+        self.eot_id: int = self.special_tokens[\"<|eot_id|>\"]\n+        self.eom_id: int = self.special_tokens[\"<|eom_id|>\"]\n+        self.python_tag_id = self.special_tokens[\"<|python_tag|>\"]\n+        self.pad_id: int = self.special_tokens[\"<|finetune_right_pad_id|>\"]\n+        self.stop_tokens = [\n+            self.special_tokens[\"<|eom_id|>\"],\n+            self.special_tokens[\"<|eot_id|>\"],\n+        ]\n+\n+    def encode(\n+        self,\n+        s: str,\n+        *,\n+        bos: bool,\n+        eos: bool,\n+        allowed_special: Optional[Union[Literal[\"all\"], AbstractSet[str]]] = None,\n+        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n+    ) -> List[int]:\n+        \"\"\"\n+        Encodes a string into a list of token IDs.\n+\n+        Args:\n+            s (str): The input string to be encoded.\n+            bos (bool): Whether to prepend the beginning-of-sequence token.\n+            eos (bool): Whether to append the end-of-sequence token.\n+            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n+            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n+\n+        Returns:\n+            list[int]: A list of token IDs.\n+\n+        By default, setting disallowed_special=() encodes a string by ignoring\n+        special tokens. Specifically:\n+        - Setting `disallowed_special` to () will cause all text corresponding\n+          to special tokens to be encoded as natural text (insteading of raising\n+          an error).\n+        - Setting `allowed_special` to \"all\" will treat all text corresponding\n+          to special tokens to be encoded as special tokens.\n+        \"\"\"\n+        if allowed_special is None:\n+            allowed_special = set()\n+        assert type(s) is str\n+\n+        substrs = (\n+            substr\n+            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n+            for substr in self._split_whitespaces_or_nonwhitespaces(\n+                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n+            )\n+        )\n+        t: List[int] = []\n+        for substr in substrs:\n+            t.extend(\n+                self.model.encode(\n+                    substr,\n+                    allowed_special=allowed_special,\n+                    disallowed_special=disallowed_special,\n+                )\n+            )\n+        if bos:\n+            t.insert(0, self.bos_id)\n+        if eos:\n+            t.append(self.eos_id)\n+        return t\n+\n+    def decode(self, t: Sequence[int]) -> str:\n+        \"\"\"\n+        Decodes a list of token IDs into a string.\n+\n+        Args:\n+            t (List[int]): The list of token IDs to be decoded.\n+\n+        Returns:\n+            str: The decoded string.\n+        \"\"\"\n+        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n+        return self.model.decode(cast(List[int], t))\n+\n+    @staticmethod\n+    def _split_whitespaces_or_nonwhitespaces(\n+        s: str, max_consecutive_slice_len: int\n+    ) -> Iterator[str]:\n+        \"\"\"\n+        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n+        consecutive whitespaces or consecutive non-whitespaces.\n+        \"\"\"\n+        current_slice_len = 0\n+        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n+        slice_start = 0\n+\n+        for i in range(len(s)):\n+            is_now_space = s[i].isspace()\n+\n+            if current_slice_is_space ^ is_now_space:\n+                current_slice_len = 1\n+                current_slice_is_space = is_now_space\n+            else:\n+                current_slice_len += 1\n+                if current_slice_len > max_consecutive_slice_len:\n+                    yield s[slice_start:i]\n+                    slice_start = i\n+                    current_slice_len = 1\n+        yield s[slice_start:]\n\n--- File: models/llama3_1/api/tool_utils.py ---\n@@ -0,0 +1,96 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import json\n+import re\n+from typing import Optional, Tuple\n+\n+from .datatypes import BuiltinTool, ToolCall\n+\n+BUILTIN_TOOL_PATTERN = r'\\b(?P<tool_name>\\w+)\\.call\\(query=\"(?P<query>[^\"]*)\"\\)'\n+CUSTOM_TOOL_CALL_PATTERN = re.compile(\n+    r\"<function=(?P<function_name>[^}]+)>(?P<args>{.*?})\"\n+)\n+\n+\n+def is_json(s):\n+    try:\n+        json.loads(s)\n+    except json.JSONDecodeError:\n+        return False\n+    return True\n+\n+\n+class ToolUtils:\n+\n+    @staticmethod\n+    def is_builtin_tool_call(message_body: str) -> bool:\n+        match = re.search(ToolUtils.BUILTIN_TOOL_PATTERN, message_body)\n+        return match is not None\n+\n+    @staticmethod\n+    def maybe_extract_builtin_tool_call(message_body: str) -> Optional[Tuple[str, str]]:\n+        # Find the first match in the text\n+        match = re.search(BUILTIN_TOOL_PATTERN, message_body)\n+\n+        # Check if a match is found and return it\n+        if match:\n+            tool_name = match.group(\"tool_name\")\n+            query = match.group(\"query\")\n+            return tool_name, query\n+        else:\n+            return None\n+\n+    @staticmethod\n+    def maybe_extract_custom_tool_call(message_body: str) -> Optional[Tuple[str, str]]:\n+        # NOTE: Custom function too calls are still experimental\n+        # Sometimes, response is of the form\n+        # {\"type\": \"function\", \"name\": \"function_name\", \"parameters\": {...}\n+        # and some times\n+        # <function=function_name>(parameters)</function>\n+\n+        # Find the first match in the text\n+        match = re.search(CUSTOM_TOOL_CALL_PATTERN, message_body)\n+        if match:\n+            tool_name = match.group(\"function_name\")\n+            query = match.group(\"args\")\n+            try:\n+                return tool_name, json.loads(query.replace(\"'\", '\"'))\n+            except Exception as e:\n+                print(\n+                    \"Exception while parsing json query for custom tool call\", query, e\n+                )\n+        elif is_json(message_body):\n+            response = json.loads(message_body)\n+            if (\"type\" in response and response[\"type\"] == \"function\") or (\n+                \"name\" in response\n+            ):\n+                function_name = response[\"name\"]\n+                args = response[\"parameters\"]\n+                return function_name, args\n+            else:\n+                return None\n+        else:\n+            return None\n+\n+    @staticmethod\n+    def encode_tool_call(t: ToolCall) -> str:\n+        if t.tool_name == BuiltinTool.brave_search:\n+            q = t.arguments[\"query\"]\n+            return f'brave_search.call(query=\"{q}\")'\n+        elif t.tool_name == BuiltinTool.wolfram_alpha:\n+            q = t.arguments[\"query\"]\n+            return f'wolfram_alpha.call(query=\"{q}\")'\n+        elif t.tool_name == BuiltinTool.photogen:\n+            q = t.arguments[\"query\"]\n+            return f'photogen.call(query=\"{q}\")'\n+        elif t.tool_name == BuiltinTool.code_interpreter:\n+            return t.arguments[\"code\"]\n+        else:\n+            fname = t.tool_name\n+            args = json.dumps(t.arguments)\n+            return f\"<function={fname}>{args}</function>\"\n\n--- File: models/llama3_1/download.sh ---\n@@ -0,0 +1,145 @@\n+#!/usr/bin/env bash\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# This software may be used and distributed according to the terms of the Llama 3.1 Community License Agreement.\n+\n+set -e\n+\n+read -p \"Enter the URL from email: \" PRESIGNED_URL\n+ALL_MODELS_LIST=\"meta-llama-3.1-405b,meta-llama-3.1-70b,meta-llama-3.1-8b,meta-llama-guard-3-8b,prompt-guard\"\n+printf \"\\n **** Model list ***\\n\"\n+for MODEL in ${ALL_MODELS_LIST//,/ }\n+do\n+    printf \" -  ${MODEL}\\n\"\n+done\n+read -p \"Choose the model to download: \" SELECTED_MODEL\n+printf \"\\n Selected model: ${SELECTED_MODEL} \\n\"\n+\n+SELECTED_MODELS=\"\"\n+if [[ $SELECTED_MODEL == \"meta-llama-3.1-405b\" ]]; then\n+    MODEL_LIST=\"meta-llama-3.1-405b-instruct-mp16,meta-llama-3.1-405b-instruct-mp8,meta-llama-3.1-405b-instruct-fb8,meta-llama-3.1-405b-mp16,meta-llama-3.1-405b-mp8,meta-llama-3.1-405b-fp8\"\n+elif [[ $SELECTED_MODEL == \"meta-llama-3.1-70b\" ]]; then\n+    MODEL_LIST=\"meta-llama-3.1-70b-instruct,meta-llama-3.1-70b\"\n+elif [[ $SELECTED_MODEL == \"meta-llama-3.1-8b\" ]]; then\n+    MODEL_LIST=\"meta-llama-3.1-8b-instruct,meta-llama-3.1-8b\"\n+elif [[ $SELECTED_MODEL == \"meta-llama-guard-3-8b\" ]]; then\n+    MODEL_LIST=\"meta-llama-guard-3-8b-int8-hf,meta-llama-guard-3-8b\"\n+elif [[ $SELECTED_MODEL == \"prompt-guard\" ]]; then\n+    SELECTED_MODELS=\"prompt-guard\"\n+    MODEL_LIST=\"\"\n+fi\n+\n+if [[ -z \"$SELECTED_MODELS\" ]]; then\n+    printf \"\\n **** Available models to download: ***\\n\"\n+    for MODEL in ${MODEL_LIST//,/ }\n+    do\n+        printf \" -  ${MODEL}\\n\"\n+    done\n+    read -p \"Enter the list of models to download without spaces or press Enter for all: \" SELECTED_MODELS\n+fi\n+\n+TARGET_FOLDER=\".\"             # where all files should end up\n+mkdir -p ${TARGET_FOLDER}\n+\n+if [[ $SELECTED_MODELS == \"\" ]]; then\n+    SELECTED_MODELS=${MODEL_LIST}\n+fi\n+\n+if [[ $SELECTED_MODEL == \"meta-llama-3.1-405b\" ]]; then\n+    printf \"\\nModel requires significant storage and computational resources, occupying approximately 750GB of disk storage space and necessitating two nodes on MP16 for inferencing.\\n\"\n+    read -p \"Enter Y to continue: \" ACK\n+    if [[ $ACK != 'Y' && $ACK != 'y' ]]; then\n+        printf \"Exiting...\"\n+        exit 1\n+    fi\n+fi\n+\n+printf \"Downloading LICENSE and Acceptable Usage Policy\\n\"\n+wget --continue ${PRESIGNED_URL/'*'/\"LICENSE\"} -O ${TARGET_FOLDER}\"/LICENSE\"\n+wget --continue ${PRESIGNED_URL/'*'/\"USE_POLICY.md\"} -O ${TARGET_FOLDER}\"/USE_POLICY.md\"\n+\n+for m in ${SELECTED_MODELS//,/ }\n+do\n+\n+    ADDITIONAL_FILES=\"\"\n+    TOKENIZER_MODEL=1\n+    if [[ $m == \"meta-llama-3.1-405b-instruct-mp16\" ]]; then\n+        PTH_FILE_COUNT=15\n+        MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct-MP16\"\n+    elif [[ $m == \"meta-llama-3.1-405b-instruct-mp8\" ]]; then\n+        PTH_FILE_COUNT=7\n+        MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct-MP8\"\n+    elif [[ $m == \"meta-llama-3.1-405b-instruct-fp8\" ]]; then\n+        PTH_FILE_COUNT=7\n+        MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct\"\n+        ADDITIONAL_FILES=\"fp8_scales_0.pt,fp8_scales_1.pt,fp8_scales_2.pt,fp8_scales_3.pt,fp8_scales_4.pt,fp8_scales_5.pt,fp8_scales_6.pt,fp8_scales_7.pt\"\n+    elif [[ $m == \"meta-llama-3.1-405b-mp16\" ]]; then\n+        PTH_FILE_COUNT=15\n+        MODEL_PATH=\"Meta-Llama-3.1-405B-MP16\"\n+    elif [[ $m == \"meta-llama-3.1-405b-mp8\" ]]; then\n+        PTH_FILE_COUNT=7\n+        MODEL_PATH=\"Meta-Llama-3.1-405B-MP8\"\n+    elif [[ $m == \"meta-llama-3.1-405b-fp8\" ]]; then\n+        PTH_FILE_COUNT=7\n+        MODEL_PATH=\"Meta-Llama-3.1-405B\"\n+    elif [[ $m == \"meta-llama-3.1-70b-instruct\" ]]; then\n+        PTH_FILE_COUNT=7\n+        MODEL_PATH=\"Meta-Llama-3.1-70B-Instruct\"\n+    elif [[ $m == \"meta-llama-3.1-70b\" ]]; then\n+        PTH_FILE_COUNT=7\n+        MODEL_PATH=\"Meta-Llama-3.1-70B\"\n+    elif [[ $m == \"meta-llama-3.1-8b-instruct\" ]]; then\n+        PTH_FILE_COUNT=0\n+        MODEL_PATH=\"Meta-Llama-3.1-8B-Instruct\"\n+    elif [[ $m == \"meta-llama-3.1-8b\" ]]; then\n+        PTH_FILE_COUNT=0\n+        MODEL_PATH=\"Meta-Llama-3.1-8B\"\n+    elif [[ $m == \"meta-llama-guard-3-8b-int8-hf\" ]]; then\n+        PTH_FILE_COUNT=-1\n+        MODEL_PATH=\"Meta-Llama-Guard-3-8B-INT8-HF\"\n+        ADDITIONAL_FILES=\"generation_config.json,model-00001-of-00002.safetensors,model-00002-of-00002.safetensors,model.safetensors.index.json,special_tokens_map.json,tokenizer_config.json,tokenizer.json\"\n+        TOKENIZER_MODEL=0\n+    elif [[ $m == \"meta-llama-guard-3-8b\" ]]; then\n+        PTH_FILE_COUNT=0\n+        MODEL_PATH=\"Meta-Llama-Guard-3-8B\"\n+    elif [[ $m == \"prompt-guard\" ]]; then\n+        PTH_FILE_COUNT=-1\n+        MODEL_PATH=\"Prompt-Guard\"\n+        ADDITIONAL_FILES=\"model.safetensors,special_tokens_map.json,tokenizer_config.json,tokenizer.json\"\n+        TOKENIZER_MODEL=0\n+    fi\n+\n+    printf \"\\n***Downloading ${MODEL_PATH}***\\n\"\n+    mkdir -p ${TARGET_FOLDER}\"/${MODEL_PATH}\"\n+\n+    if [[ $TOKENIZER_MODEL == 1 ]]; then\n+        printf \"Downloading tokenizer\\n\"\n+        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/tokenizer.model\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/tokenizer.model\"\n+    fi\n+\n+    if [[ $PTH_FILE_COUNT -ge 0 ]]; then\n+        for s in $(seq -f \"0%g\" 0 ${PTH_FILE_COUNT})\n+        do\n+            printf \"Downloading consolidated.${s}.pth\\n\"\n+            wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n+        done\n+    fi\n+\n+    for ADDITIONAL_FILE in ${ADDITIONAL_FILES//,/ }\n+    do\n+        printf \"Downloading $ADDITIONAL_FILE...\\n\"\n+        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/${ADDITIONAL_FILE}\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/${ADDITIONAL_FILE}\"\n+    done\n+\n+    if [[ $m != \"prompt-guard\" &&  $m != \"meta-llama-guard-3-8b-int8-hf\" ]]; then\n+        printf \"Downloading params.json...\\n\"\n+        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/params.json\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/params.json\"\n+    fi\n+done\n\n--- File: models/llama3_1/eval_details.md ---\n@@ -0,0 +1,192 @@\n+\n+# Llama 3 Evaluation Details\n+\n+This document contains some additional context on the settings and methodology for how we evaluated the Llama 3.1 8B, 70B, and 405B pre-trained and post-trained models.\n+\n+\n+## Language auto-eval benchmark notes:\n+\n+For a given benchmark, we strive to use consistent evaluation settings across all models, including external models. We make every effort to achieve optimal scores for external models, including addressing any model-specific parsing and tokenization requirements. Where the scores are lower for external models than self-reported scores on comparable or more conservative settings, we report the self-reported scores for external models. We are also releasing the data generated as part of evaluations with publicly available benchmarks which can be found on [huggingface here](https://huggingface.co/meta-llama).\n+\n+\n+### MMLU\n+\n+For the pre-trained models we use a 5-shot config. To determine the choice character we use the standard MMLU prompt and compare the negative log-likelihood (NLL) of the various choices.\n+\n+For the post-trained models we report both 5-shot and 0-shot scores. We ask the model to generate the best choice character. The 0-shot scores use a CoT (chain of thought) prompt. The maximum generation lengths for the 5-shot and 0-shot configs are 10 tokens and 1024 tokens respectively.\n+\n+Macro averages are reported unless otherwise stated. The micro average scores for the various models are: 65.6, 79.0, and 85.4 for the pre-trained 8B, 70B and 405B models respectively for the 5-shot config; 69.44, 84.0, 87.71 for the post-trained 8B, 70B and 405B models respectively for the 5-shot config.\n+\n+\n+### MMLU-Pro\n+\n+For the pre-trained and post-trained models we use a 5-shot config with CoT prompt. We ask the model to generate the reasoning and the corresponding best choice character. The maximum generation length is 512 tokens for pre-trained setup and 1024 for post-trained setup.\n+\n+\n+### ARC-Challenge\n+\n+We use the Arc-Challenge subset from the Arc benchmark. For the pre-trained models, we use a  25-shot config and use the MMLU setup for evaluation where we provide all the choices in the prompt and calculate likelihood over choice characters. For the post-trained models, we use 0-shot config and ask the model to generate the choice character. The maximum generation length is 100 tokens.\n+\n+\n+### GPQA\n+\n+For post-trained models, we use 0-shot config with and without CoT prompt and report exact match scores over the possible options using the main set. Maximum generation length is 96 tokens when not using CoT prompt and 2048 tokens when using the CoT prompt.\n+\n+\n+### AGIEval English\n+\n+For pre-trained models, we use the default few-shot and prompt settings as specified [here](https://github.com/ruixiangcui/AGIEval). The score is averaged over the english subtasks. The maximum generation length is 10 tokens.\n+\n+\n+### IFEval\n+\n+For post-trained models, we use the default settings as specified [here](https://arxiv.org/pdf/2311.07911). We compute the prompt level scores and instruction level strict and loose accuracy. We then report the average across all the scores.\n+\n+\n+### HumanEval/HumanEval+\n+\n+For both pre-trained and post-trained models, we use a 0-shot config and report pass@1 scores. The maximum generation length is 1024 tokens.\n+\n+\n+### CommonSenseQA\n+\n+For pre-trained models, we use the same 7-shot config with CoT prompt as in [Wei et al. (2022)](https://arxiv.org/pdf/2201.11903.pdf). We use the MMLU setup for evaluation where we provide all the choices in the prompt and calculate likelihood over choice characters.\n+\n+\n+### WinoGrande\n+\n+For pre-trained models, we use a choice based setup for evaluation where we fill in the missing blank with the two possible choices and then compute log-likelihood over the suffix. We use a 5-shot config. We use the MMLU setup where we provide all the choices in the prompt and calculate likelihood over choice characters.\n+\n+\n+### BIG-Bench Hard\n+\n+For pre-trained models, we use a 3-shot config with CoT prompt and compute the average exact match over the subsets in this task. We run this as a generative task. Maximum generation length is 512 tokens.\n+\n+\n+### SQuAD\n+\n+For pre-trained models, we use SQuAD v2 with a 1-shot config and report exact match scores. We run this as a generative task. Maximum generation length is 32 tokens.\n+\n+\n+### QuAC\n+\n+For pre-trained models, we use a 1-shot config and report the F1 scores. We run this as a generative task. Maximum generation length is 32 tokens.\n+\n+\n+### BoolQ\n+\n+For pre-trained models, we use a 0-shot config and report average accuracy. We run this as a choice task.\n+\n+\n+### DROP\n+\n+For pre-trained models, for each validation example, we draw 3 random few-shot examples from the train split and report the F1 scores. The maximum generation length is 32 tokens.\n+\n+\n+### GSM8K\n+\n+For both pre-trained and post-trained models, we use the same 8-shot config with CoT prompt as in [Wei et al. (2022)](https://arxiv.org/pdf/2201.11903.pdf) (maj@1). The maximum generation length is 1024 tokens.\n+\n+\n+### RACE\n+\n+For pre-trained models, we use a 0-shot config. We run this as a choice task. We use the MMLU setup for evaluation where we provide all the choices in the prompt and calculate likelihood over choice characters.\n+\n+\n+### WorldSense\n+\n+For pre-trained models, we use a 0-shot config. We run this as a choice task. Unlike the original benchmark, we do not normalize the three-option partitions of the benchmark. The chance accuracy is therefore not 0.5, but averages to 0.46.\n+\n+\n+### MBPP\n+\n+For pre-trained and post-trained models we use a 3-shot config and report pass@1 scores. We run this as a generative task. Maximum generation length is 256 tokens.\n+\n+\n+### MBPP EvalPlus (base)\n+\n+For pre-trained and post-trained models we use a 0-shot config and report pass@1 scores. We run this as a generative task. Maximum generation length is 1024 tokens.\n+\n+\n+### MATH\n+\n+For pre-trained models, we use the same 4-shot config as in [Lewkowycz et al. (2022)](https://arxiv.org/pdf/2206.14858.pdf) (maj@1). Maximum generation length is 512 tokens.\n+\n+For post-trained models we use a 0-shot config with Cot prompt. We enhance the exact match using [sympy](https://www.sympy.org/en/index.html) and then use an [equality template](https://github.com/openai/simple-evals/blob/main/common.py#L27-L85) with a judge to resolve complex expressions. Maximum generation length is 5120 tokens.\n+\n+\n+### SCROLLS\n+\n+For pre-trained models, we use a 5-shot config. Maximum generation length is 32 tokens. Maximum input prompt length is 131072 less the number of tokens generated (i.e. 131040).\n+\n+\n+### ZeroSCROLLS\n+\n+For post-trained models, we use a 0-shot config. Maximum generation length for QuALITY and SQuALITY is 64 tokens. For Qasper it is 128 tokens. Maximum input prompt length for Llama models is 131072 less the number of tokens generated for each task (i.e. 131008 for QuALITY and SQuALITY and 130944 for Qasper). Maximum input length for non-llama models is 128000 less the number of tokens generated for each task. We ensure that all relevant information is retained in the context for all models for fair comparison.\n+\n+\n+### InfiniteBench\n+\n+For post-trained models, we use a 0-shot config. Maximum generation length is 20 for both the En.QA and En.MC tasks and maximum input prompt length is 131052. Maximum input length for non-llama models is 127980. We ensure that all relevant information is retained in the context for all models for fair comparison.\n+\n+\n+### NIH/Multi-needle\n+\n+For post-training, we use a 0-shot config. Our context lengths are evenly spaced between 2000 and 131072 in 10 intervals, inclusive of the endpoints for llama models and between 2000 and 128000 for non-llama models. Maximum generation length is 256 tokens.\n+\n+\n+### Multilingual MGSM\n+\n+For post-trained models, we use an 0-shot config with CoT prompt and report exact match (maj@1) scores. Maximum generation length is 2048 tokens. The scores are averaged over all the eleven languages present in the MGSM benchmark, including the ones not supported by Llama models.\n+\n+\n+### Multilingual MMLU\n+\n+For post-trained models, we use a 5-shot config. We run this as a generative task. Maximum generation length is 10 tokens. The scores are individually reported for each and averaged over the seven non-english languages that Llama models support (Portuguese, Spanish, Italian, German, French, Hindi, Thai).\n+\n+\n+### Multipl-E HumanEval and Multipl-E MBPP\n+\n+For post-trained models, we use a 0-shot config and report pass@1 scores. Maximum generation length is 512 tokens. Where Multipl-E average is reported, the scores are averaged over all 6 languages in the benchmark.\n+\n+\n+### PiQA, SiQA, and OpenBookQA\n+\n+For pre-trained models, we use a 0-shot config and report average accuracy. We run these as choice task.\n+\n+\n+### Dynabench SQuAD and Adversarial SQuAD\n+\n+For the adversarial versions of squad ([Dynabench](https://aclanthology.org/2021.naacl-main.324/) and [Adversarial](https://aclanthology.org/D17-1215/)), we use the same setting as standard SQuAD (1-shot config and exact match as the metric)\n+\n+\n+### PAWS\n+\n+For pre-trained models, we use a 5-shot config and report exact match scores. We run this as a generative task. Maximum generation length is 32 tokens.\n+\n+\n+### GSM Plus\n+\n+For pre-trained models, we use the same 8-shot config with CoT prompt as in [Wei et al. (2022)](https://arxiv.org/pdf/2201.11903.pdf) (maj@1). The maximum generation length is 512 tokens.\n+\n+\n+### Berkeley Function Calling Leaderboard (BFCL)\n+\n+Benchmark results were achieved by running the open source evaluation repository [ShishirPatil/gorilla](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard) on commit 7bef000 without any further changes.\n+\n+\n+### Nexus\n+\n+We use the [open-source ](https://github.com/nexusflowai/NexusRaven)prompt and evaluation function followed by the[ open source notebook](https://github.com/nexusflowai/NexusRaven-V2/blob/master/evaluation_notebook/GPT4_Evaluation/Benchmark_GPT4.ipynb) to compute the scores.\n+\n+\n+### API Bank\n+\n+We use a 0-shot config with a custom prompt and parsing function to reduce the incidence of false negatives. We also modify the dataset by correcting and completing the ground truth answers that were initially incorrect or incomplete. Second, we improve the evaluation metric to better assess function call correctness by splitting keyword arguments into two groups. We use exact match for keyword arguments that have a unique ground truth, and ROUGE score for those that accept any string with the same semantic meaning as the reference value.\n+\n+### Gorilla API Bench\n+\n+For post-trained models, we use the same 0-shot prompt and evaluation function as proposed in the [original paper](https://arxiv.org/abs/2305.15334). Just like the [open-source](https://github.com/ShishirPatil/gorilla) implementation, we compare the domains of the retrieved API call from the API database with the ground truth. If the domain of the retrieved API is the same as the ground truth and the API exists in the database, it is considered a success. All other scenarios are considered failures.\n+\n+### TriviaQA-WIKI\n+For TrivialQA, we evaluate on the Wiki validation set, use 5-shot config and compute average exact match. We run this as a generative task. Maximum generation length is 24 tokens.\n\n--- File: models/llama3_1/requirements.txt ---\n@@ -0,0 +1 @@\n+tiktoken\n\n--- File: pyproject.toml ---\n@@ -0,0 +1,3 @@\n+[build-system]\n+requires = [\"setuptools>=61.0\"]\n+build-backend = \"setuptools.build_meta\"\n\n--- File: requirements.txt ---\n@@ -0,0 +1,7 @@\n+fairscale\n+jinja2\n+json-strong-typing\n+tiktoken\n+torch\n+pydantic==1.10.13\n+pydantic_core==2.18.2\n\n--- File: setup.py ---\n@@ -0,0 +1,72 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the terms described in the LICENSE file in\n+# top-level folder for each specific model found within the models/ directory at\n+# the top-level of this source tree.\n+\n+import os\n+\n+from setuptools import setup\n+\n+\n+# Function to read a file\n+def read_file(file_path):\n+    with open(file_path) as file:\n+        return file.read()\n+\n+\n+# Function to read requirements from a requirements.txt file\n+def read_requirements(module_path):\n+    requirements_path = os.path.join(module_path, \"requirements.txt\")\n+    if os.path.exists(requirements_path):\n+        with open(requirements_path) as req_file:\n+            return req_file.read().splitlines()\n+    return []\n+\n+\n+# Custom function to get package directories\n+def get_package_dirs(base_path):\n+    package_dirs = {\n+        \"llama2\": os.path.join(base_path, \"llama2\"),\n+        \"llama3\": os.path.join(base_path, \"llama3\"),\n+        \"llama3_1\": os.path.join(base_path, \"llama3_1\"),\n+    }\n+    return package_dirs\n+\n+\n+# Path to the directory containing the setup.py file\n+here = os.path.abspath(os.path.dirname(__file__))\n+\n+# Get package directories dynamically\n+package_dirs = get_package_dirs(os.path.join(here, \"models\"))\n+\n+# Collect requirements from all submodules\n+extras_require = {}\n+for package_name, package_path in package_dirs.items():\n+    extras_require[package_name] = read_requirements(package_path)\n+\n+\n+setup(\n+    name=\"llama_models\",\n+    version=\"0.0.0.1\",\n+    author=\"Meta Llama\",\n+    author_email=\"llama-oss@meta.com\",\n+    description=\"Llama model details\",\n+    long_description=open(\"README.md\").read(),\n+    long_description_content_type=\"text/markdown\",\n+    url=\"https://github.com/meta-llama/llama-models\",\n+    # license=read_license(),\n+    # packages=find_packages(where=\"models\"),\n+    package_dir={\"llama_models\": \"models\"},\n+    classifiers=[],\n+    python_requires=\">=3.10\",\n+    install_requires=[],\n+    extras_require=extras_require,\n+    include_package_data=True,\n+    package_data={\n+        \"llama2\": [\"LICENSE\", \"requirements.txt\"],\n+        \"llama3\": [\"LICENSE\", \"requirements.txt\"],\n+        \"llama3_1\": [\"LICENSE\", \"requirements.txt\"],\n+    },\n+)"
            }
          ]
        },
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "e66609cfbd73503ef25e597fd82c59084836155d",
              "url": "https://github.com/meta-llama/codellama/commit/e66609cfbd73503ef25e597fd82c59084836155d",
              "message": "Merge pull request #133 from facebookresearch/jspisak-patch-2",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 2,
              "diff_patch": "--- File: README.md ---\n@@ -12,7 +12,7 @@ This repository is intended as a minimal example to load [Code Llama](https://ai\n \n ## Download\n \n-In order to download the model weights and tokenizers, please visit the [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n+In order to download the model weights and tokenizers, please visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n \n Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, **do not use the 'Copy link address' option** when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.\n \n@@ -28,7 +28,7 @@ Keep in mind that the links expire after 24 hours and a certain amount of downlo\n | 13B    | 24GB  |\n | 34B    | 63GB  |\n \n-[comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n+[comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n \n ## Setup"
            },
            {
              "sha": "48959f2c2ec763b23ea6731b75ba69559007f927",
              "url": "https://github.com/meta-llama/codellama/commit/48959f2c2ec763b23ea6731b75ba69559007f927",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -12,7 +12,7 @@ This repository is intended as a minimal example to load [Code Llama](https://ai\n \n ## Download\n \n-In order to download the model weights and tokenizers, please visit the [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n+In order to download the model weights and tokenizers, please visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n \n Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, **do not use the 'Copy link address' option** when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.\n \n@@ -28,7 +28,7 @@ Keep in mind that the links expire after 24 hours and a certain amount of downlo\n | 13B    | 24GB  |\n | 34B    | 63GB  |\n \n-[comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n+[comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n \n ## Setup"
            },
            {
              "sha": "e1eff3c2143f5c71580fae0247a05f3dd4c66aff",
              "url": "https://github.com/meta-llama/codellama/commit/e1eff3c2143f5c71580fae0247a05f3dd4c66aff",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -26,7 +26,7 @@ Keep in mind that the links expire after 24 hours and a certain amount of downlo\n |--------|----|\n | 7B     | ~12.55GB  |\n | 13B    | 24GB  |\n-| 34B    | ~60GB  |\n+| 34B    | 63GB  |\n \n [comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)"
            },
            {
              "sha": "bc8e8ec08fe86968e832a53caa0061fc157e50bd",
              "url": "https://github.com/meta-llama/codellama/commit/bc8e8ec08fe86968e832a53caa0061fc157e50bd",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -24,9 +24,9 @@ Keep in mind that the links expire after 24 hours and a certain amount of downlo\n \n |  Model | Size |\n |--------|----|\n-| 7B     | ~12.5GB  |\n+| 7B     | ~12.55GB  |\n | 13B    | 24GB  |\n-| 34B    | ??  |\n+| 34B    | ~60GB  |\n \n [comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)"
            },
            {
              "sha": "97ab0c36dd29c96736fb25683ceeee7bf47ee3c4",
              "url": "https://github.com/meta-llama/codellama/commit/97ab0c36dd29c96736fb25683ceeee7bf47ee3c4",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -20,6 +20,14 @@ Pre-requisites: make sure you have `wget` and `md5sum` installed. Then to run th\n \n Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as `403: Forbidden`, you can always re-request a link.\n \n+### Model sizes\n+\n+|  Model | Size |\n+|--------|----|\n+| 7B     | ~12.5GB  |\n+| 13B    | 24GB  |\n+| 34B    | ??  |\n+\n [comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n \n ## Setup"
            },
            {
              "sha": "e064c1c24c377cc0875711440ef4c0a6eaf0147b",
              "url": "https://github.com/meta-llama/codellama/commit/e064c1c24c377cc0875711440ef4c0a6eaf0147b",
              "message": "Merge pull request #49 from eltociear/patch-1",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -96,7 +96,7 @@ Code Llama is a new technology that carries potential risks with use. Testing co\n In order to help developers address these risks, we have created the [Responsible Use Guide](https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf). More details can be found in our research papers as well.\n \n ## Issues\n-Please report any software bug, or other problems with the models through one of the following means:\n+Please report any software bug, or other problems with the models through one of the following means:\n - Reporting issues with the model: [github.com/facebookresearch/codellama](http://github.com/facebookresearch/codellama)\n - Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n - Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)"
            }
          ]
        }
      ]
    },
    {
      "id": 420184,
      "username": "kit1980",
      "url": "https://github.com/kit1980",
      "avatar_url": "https://avatars.githubusercontent.com/u/420184?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "803ed8a048b4b5bc812ecd87298c17dc5670f7f6",
              "url": "https://github.com/meta-llama/llama-models/commit/803ed8a048b4b5bc812ecd87298c17dc5670f7f6",
              "message": "Fix docstring args names (#119)",
              "files_changed": [
                {
                  "filename": "models/llama3/api/tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/tokenizer.py ---\n@@ -121,8 +121,8 @@ def encode(\n             s (str): The input string to be encoded.\n             bos (bool): Whether to prepend the beginning-of-sequence token.\n             eos (bool): Whether to append the end-of-sequence token.\n-            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n-            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n+            allowed_special (\"all\"|set[str]): allowed special tokens in string\n+            disallowed_special (\"all\"|set[str]): special tokens that raise an error when in string\n \n         Returns:\n             list[int]: A list of token IDs."
            }
          ]
        }
      ]
    },
    {
      "id": 30946105,
      "username": "knibesh",
      "url": "https://github.com/knibesh",
      "avatar_url": "https://avatars.githubusercontent.com/u/30946105?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "2864bc883dbaaf485c5a82075d9c159e5a3e5f32",
              "url": "https://github.com/meta-llama/llama-models/commit/2864bc883dbaaf485c5a82075d9c159e5a3e5f32",
              "message": "Update README.md (#33)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -30,7 +30,7 @@ To download the model weights and tokenizer, please visit the [Meta Llama websit\n \n Once your request is approved, you will receive a signed URL over email. Then, run the download.sh script, passing the URL provided when prompted to start the download.\n \n-Pre-requisites: Ensure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`.\n+Pre-requisites: Ensure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`. `./download.sh`can be found inside the respective `models` directory. \n \n Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`."
            }
          ]
        }
      ]
    },
    {
      "id": 9705880,
      "username": "liyunlu0618",
      "url": "https://github.com/liyunlu0618",
      "avatar_url": "https://avatars.githubusercontent.com/u/9705880?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "4a9ff6aede5cb4e036542132bb4f8b77a3134908",
              "url": "https://github.com/meta-llama/llama-models/commit/4a9ff6aede5cb4e036542132bb4f8b77a3134908",
              "message": "Add reference to torchao in quantized llama 1/3B model card. (#193)",
              "files_changed": [
                {
                  "filename": "models/llama3_2/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_2/MODEL_CARD.md ---\n@@ -71,7 +71,7 @@ We designed the current quantization scheme with the [PyTorchs ExecuTorch](ht\n \n ### Quantization-Aware Training and LoRA\n \n-The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n+The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We used the [torchao](https://github.com/pytorch/ao/blob/main/torchao/quantization/qat/README.md) API for this. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n \n ### SpinQuant"
            }
          ]
        }
      ]
    },
    {
      "id": 58921460,
      "username": "machina-source",
      "url": "https://github.com/machina-source",
      "avatar_url": "https://avatars.githubusercontent.com/u/58921460?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "e2e67cd8b0feca839666b7082dcc1c4fc0ae11e2",
              "url": "https://github.com/meta-llama/llama-models/commit/e2e67cd8b0feca839666b7082dcc1c4fc0ae11e2",
              "message": "Update MODEL_CARD - Feedback links/format (#153)",
              "files_changed": [
                {
                  "filename": "models/llama2/MODEL_CARD.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3/MODEL_CARD.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/MODEL_CARD.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/MODEL_CARD.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/MODEL_CARD_VISION.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama2/MODEL_CARD.md ---\n@@ -28,7 +28,7 @@ Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x\n \n **Research Paper** More information can be found in the paper \"Llama-2: Open Foundation and Fine-tuned Chat Models\", available at https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/.\n \n-**Where to send questions or comments about the model** Instructions on how to provide feedback or comments on the model can be found in the model [README](README.md).\n+**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md).\n \n # **Intended Use**\n **Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\n--- File: models/llama3/MODEL_CARD.md ---\n@@ -67,7 +67,7 @@ Note: Token counts refer to pretraining data only.\n \n **License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n \n-Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n \n \n ## Intended Use\n\n--- File: models/llama3_1/MODEL_CARD.md ---\n@@ -85,7 +85,7 @@ The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a\n \n **License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n \n-Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n \n \n ## Intended Use\n\n--- File: models/llama3_2/MODEL_CARD.md ---\n@@ -21,7 +21,7 @@ The Llama 3.2 collection of multilingual large language models (LLMs) is a colle\n \n **License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n \n-**Feedback:** Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n \n ## Intended Use\n \n\n--- File: models/llama3_2/MODEL_CARD_VISION.md ---\n@@ -4,7 +4,7 @@ The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a\n \n **Model Developer**: Meta\n \n-**Model Architecture:** Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\n+**Model Architecture:** Llama 3.2-Vision is built on top of the Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\n \n |  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Data volume | Knowledge cutoff |\n | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n@@ -23,7 +23,7 @@ Developers may fine-tune Llama 3.2 models for languages beyond these supported l\n \n **License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n \n-**Feedback:** Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2). For more technical information about generation parameters and recipes for how to use Llama 3.2-Vision in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n+**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2-Vision in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n \n ## Intended Use\n \n@@ -124,7 +124,7 @@ As part of our Responsible release approach, we followed a three-pronged strateg\n \n ### New Capabilities and Use Cases\n \n-**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.,\n+**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n \n **Image Reasoning:** Llama 3.2-Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks."
            },
            {
              "sha": "82286ef4b95b15574700ffee920a29371aee60a3",
              "url": "https://github.com/meta-llama/llama-models/commit/82286ef4b95b15574700ffee920a29371aee60a3",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -73,7 +73,7 @@ To help developers address these risks, we have created the [Responsible Use Gui\n ## Issues\n \n Please report any software bug or other problems with the models through one of the following means:\n-- Reporting issues with the model: [https://github.com/issues](https://github.com/issues)\n+- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n - Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n - Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)"
            },
            {
              "sha": "598ea548aa1edfe6e6cd01c228eae6d28fe0d0ba",
              "url": "https://github.com/meta-llama/llama-models/commit/598ea548aa1edfe6e6cd01c228eae6d28fe0d0ba",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: models/llama3_1/README.md ---\n@@ -73,7 +73,7 @@ To help developers address these risks, we have created the [Responsible Use Gui\n ## Issues\n \n Please report any software bug or other problems with the models through one of the following means:\n-- Reporting issues with the model: [https://github.com/issues](https://github.com/issues)\n+- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n - Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n - Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)"
            }
          ]
        }
      ]
    },
    {
      "id": 57108820,
      "username": "markjeli",
      "url": "https://github.com/markjeli",
      "avatar_url": "https://avatars.githubusercontent.com/u/57108820?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "02b654c9d4923d0bede37d5f1a9a8c8c56022e4d",
              "url": "https://github.com/meta-llama/llama-models/commit/02b654c9d4923d0bede37d5f1a9a8c8c56022e4d",
              "message": "Fix Llama 3.3 model size in README (#339)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -29,7 +29,7 @@ Our mission is to empower individuals and industry through this opportunity whil\n | Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n | Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n | Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n-| Llama 3.3 | 12/04/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |\n+| Llama 3.3 | 12/04/2024 | 70B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |\n | Llama 4 | 4/5/2025 | Scout-17B-16E, Maverick-17B-128E | 10M, 1M | TikToken-based | [Use Policy](models/llama4/USE_POLICY.md) | [License](models/llama4/LICENSE) | [Model Card](models/llama4/MODEL_CARD.md) |\n \n ## Download"
            }
          ]
        }
      ]
    },
    {
      "id": 66362098,
      "username": "marklysze",
      "url": "https://github.com/marklysze",
      "avatar_url": "https://avatars.githubusercontent.com/u/66362098?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "57d9b434514159a812a61e38c992c276ef5432dc",
              "url": "https://github.com/meta-llama/llama-models/commit/57d9b434514159a812a61e38c992c276ef5432dc",
              "message": "Minor typos (#154)",
              "files_changed": [
                {
                  "filename": "models/llama3_2/prompts_text.py",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_2/text_prompt_format.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_2/prompts_text.py ---\n@@ -145,7 +145,7 @@ def usecases():\n             ],\n             notes=textwrap.dedent(\n                 \"\"\"\n-                - The tool call format for the mdoel is the same whether your function calls are provided in the system or user message.\n+                - The tool call format for the model is the same whether your function calls are provided in the system or user message.\n                 - While builtin tool calls end with a <|eom_id|>, notice the <|eot_id|> for zero shot tool calls.\n                 \"\"\"\n             ),\n\n--- File: models/llama3_2/text_prompt_format.md ---\n@@ -144,7 +144,7 @@ NO other text MUST be included.<|eot_id|><|start_header_id|>assistant<|end_heade\n \n ##### Notes\n \n-- The tool call format for the mdoel is the same whether your function calls are provided in the system or user message.\n+- The tool call format for the model is the same whether your function calls are provided in the system or user message.\n - While builtin tool calls end with a <|eom_id|>, notice the <|eot_id|> for zero shot tool calls."
            }
          ]
        }
      ]
    },
    {
      "id": 112653,
      "username": "mattf",
      "url": "https://github.com/mattf",
      "avatar_url": "https://avatars.githubusercontent.com/u/112653?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "065b561cbab20415dca32412fdd480a0e8d11be1",
              "url": "https://github.com/meta-llama/llama-models/commit/065b561cbab20415dca32412fdd480a0e8d11be1",
              "message": "align with pydantic v2, @validator -> @field_validator and class Config -> model_config (#225)",
              "files_changed": [
                {
                  "filename": "models/llama3/api/datatypes.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/api/datatypes.py ---\n@@ -8,7 +8,7 @@\n from enum import Enum\n from typing import Dict, List, Literal, Optional, Union\n \n-from pydantic import BaseModel, Field, validator\n+from pydantic import BaseModel, Field, field_validator\n \n from typing_extensions import Annotated\n from ...datatypes import *  # noqa\n@@ -44,8 +44,7 @@ def __str__(self) -> str:\n class ImageMedia(BaseModel):\n     image: Union[PIL_Image.Image, URL]\n \n-    class Config:\n-        arbitrary_types_allowed = True\n+    model_config = ConfigDict(arbitrary_types_allowed=True)\n \n \n InterleavedTextMedia = Union[\n@@ -117,7 +116,7 @@ class ToolCall(BaseModel):\n     tool_name: Union[BuiltinTool, str]\n     arguments: Dict[str, RecursiveType]\n \n-    @validator(\"tool_name\", pre=True)\n+    @field_validator(\"tool_name\", mode=\"before\")\n     @classmethod\n     def validate_field(cls, v):\n         if isinstance(v, str):\n@@ -134,7 +133,7 @@ class ToolResponse(BaseModel):\n     tool_name: Union[BuiltinTool, str]\n     content: InterleavedTextMedia\n \n-    @validator(\"tool_name\", pre=True)\n+    @field_validator(\"tool_name\", mode=\"before\")\n     @classmethod\n     def validate_field(cls, v):\n         if isinstance(v, str):\n@@ -159,7 +158,7 @@ class ToolDefinition(BaseModel):\n     description: Optional[str] = None\n     parameters: Optional[Dict[str, ToolParamDefinition]] = None\n \n-    @validator(\"tool_name\", pre=True)\n+    @field_validator(\"tool_name\", mode=\"before\")\n     @classmethod\n     def validate_field(cls, v):\n         if isinstance(v, str):"
            }
          ]
        }
      ]
    },
    {
      "id": 2288238,
      "username": "matthewberryman",
      "url": "https://github.com/matthewberryman",
      "avatar_url": "https://avatars.githubusercontent.com/u/2288238?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "a211a19e1fb1852f6eeaabe043f75effb5e96f30",
              "url": "https://github.com/meta-llama/llama-models/commit/a211a19e1fb1852f6eeaabe043f75effb5e96f30",
              "message": "fix: chmod +x download.sh (#72)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/download.sh",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": null
            }
          ]
        }
      ]
    },
    {
      "id": 6299170,
      "username": "mhaz",
      "url": "https://github.com/mhaz",
      "avatar_url": "https://avatars.githubusercontent.com/u/6299170?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "f1a6b01216d79950c03898b4fa31fdf45d149d67",
              "url": "https://github.com/meta-llama/codellama/commit/f1a6b01216d79950c03898b4fa31fdf45d149d67",
              "message": "Resume download of partially-downloaded files. (#15)",
              "files_changed": [
                {
                  "filename": "download.sh",
                  "status": "modified"
                }
              ],
              "comment_count": 9,
              "diff_patch": "--- File: download.sh ---\n@@ -15,8 +15,8 @@ if [[ $MODEL_SIZE == \"\" ]]; then\n fi\n \n echo \"Downloading LICENSE and Acceptable Usage Policy\"\n-wget ${PRESIGNED_URL/'*'/\"LICENSE\"} -O ${TARGET_FOLDER}\"/LICENSE\"\n-wget ${PRESIGNED_URL/'*'/\"USE_POLICY.md\"} -O ${TARGET_FOLDER}\"/USE_POLICY.md\"\n+wget --continue ${PRESIGNED_URL/'*'/\"LICENSE\"} -O ${TARGET_FOLDER}\"/LICENSE\"\n+wget --continue ${PRESIGNED_URL/'*'/\"USE_POLICY.md\"} -O ${TARGET_FOLDER}\"/USE_POLICY.md\"\n \n for m in ${MODEL_SIZE//,/ }\n do\n@@ -50,12 +50,12 @@ do\n \n     for s in $(seq -f \"0%g\" 0 ${SHARD})\n     do\n-        wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n+        wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n     done\n \n-    wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/params.json\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/params.json\"\n-    wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/tokenizer.model\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/tokenizer.model\"\n-    wget ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/checklist.chk\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/checklist.chk\"\n+    wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/params.json\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/params.json\"\n+    wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/tokenizer.model\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/tokenizer.model\"\n+    wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/checklist.chk\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/checklist.chk\"\n     echo \"Checking checksums\"\n     (cd ${TARGET_FOLDER}\"/${MODEL_PATH}\" && md5sum -c checklist.chk)\n done"
            }
          ]
        }
      ]
    },
    {
      "id": 18061991,
      "username": "minimalic",
      "url": "https://github.com/minimalic",
      "avatar_url": "https://avatars.githubusercontent.com/u/18061991?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "78820183c12856a62eed602fa3578d292b2fcb2d",
              "url": "https://github.com/meta-llama/llama-models/commit/78820183c12856a62eed602fa3578d292b2fcb2d",
              "message": "Remove md5sum requirement (#74)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/README.md ---\n@@ -30,7 +30,7 @@ To download the model weights and tokenizer, please visit the [Meta Llama websit\n \n Once your request is approved, you will receive a signed URL over email. Then, run the download.sh script, passing the URL provided when prompted to start the download.\n \n-Pre-requisites: Ensure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`.\n+Pre-requisites: Ensure you have `wget` installed. Then run the script: `./download.sh`.\n \n Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`."
            }
          ]
        }
      ]
    },
    {
      "id": 6240044,
      "username": "mohammad1ta",
      "url": "https://github.com/mohammad1ta",
      "avatar_url": "https://avatars.githubusercontent.com/u/6240044?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "dc4cefb0697df6e85fb3d71a2f5a8ef8e485ab98",
              "url": "https://github.com/meta-llama/codellama/commit/dc4cefb0697df6e85fb3d71a2f5a8ef8e485ab98",
              "message": "Fixed example url of llama recipes",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -87,7 +87,7 @@ Pretrained infilling models are: the Code Llama models `CodeLlama-7b` and `CodeL\n Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L212)\n needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and linebreaks in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n \n-You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/inference/inference.py) of how to add a safety checker to the inputs and outputs of your inference code.\n+You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/src/llama_recipes/inference/safety_utils.py) of how to add a safety checker to the inputs and outputs of your inference code.\n \n Examples using `CodeLlama-7b-Instruct`:"
            }
          ]
        }
      ]
    },
    {
      "id": 1117246,
      "username": "mpu",
      "url": "https://github.com/mpu",
      "avatar_url": "https://avatars.githubusercontent.com/u/1117246?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "077f7332d01650f96ad80c77b099f04a773c0e1e",
              "url": "https://github.com/meta-llama/codellama/commit/077f7332d01650f96ad80c77b099f04a773c0e1e",
              "message": "Merge pull request #127 from facebookresearch/fixes",
              "files_changed": [
                {
                  "filename": "llama/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/model.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: llama/generation.py ---\n@@ -19,12 +19,12 @@\n from llama.model import ModelArgs, Transformer\n from llama.tokenizer import Tokenizer\n \n-if torch.backends.mps.is_available():\n-    device = torch.device('mps')\n-elif torch.cuda.is_available():\n-    device = torch.device('cuda')\n+if torch.cuda.is_available():\n+    device = \"cuda\"\n+elif torch.backends.mps.is_available():\n+    device = \"mps\"\n else:\n-    device = torch.device('cpu')\n+    device = \"cpu\"\n \n Role = Literal[\"system\", \"user\", \"assistant\"]\n \n@@ -85,7 +85,6 @@ def build(\n         if device == \"cuda\":\n             torch.cuda.set_device(local_rank)\n \n-\n         # seed must be the same in all processes\n         torch.manual_seed(1)\n \n\n--- File: llama/model.py ---\n@@ -15,13 +15,12 @@\n )\n from torch import nn\n \n-if torch.backends.mps.is_available():\n-    device = torch.device('mps')\n-elif torch.cuda.is_available():\n-    device = torch.device('cuda')\n+if torch.cuda.is_available():\n+    device = \"cuda\"\n+elif torch.backends.mps.is_available():\n+    device = \"mps\"\n else:\n-    device = torch.device('cpu')\n-\n+    device = \"cpu\"\n \n @dataclass\n class ModelArgs:\n@@ -81,8 +80,6 @@ def apply_rotary_emb(\n     xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n     xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n     freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n-    if not torch.cuda.is_available():\n-        freqs_cis = freqs_cis.to('cpu')\n     xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n     xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n     return xq_out.type_as(xq).to(device), xk_out.type_as(xk).to(device)\n@@ -97,7 +94,7 @@ def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n         x[:, :, :, None, :]\n         .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n         .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n-    ) #.to(device)\n+    )\n \n \n class Attention(nn.Module):\n@@ -287,6 +284,7 @@ def __init__(self, params: ModelArgs):\n     def forward(self, tokens: torch.Tensor, start_pos: int):\n         _bsz, seqlen = tokens.shape\n         h = self.tok_embeddings(tokens)\n+        self.freqs_cis = self.freqs_cis.to(\"cuda\" if device == \"cuda\" else \"cpu\")\n         freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n \n         mask = None"
            },
            {
              "sha": "30083473e9c4c46b2e4eb69e1c7572b1098ffa82",
              "url": "https://github.com/meta-llama/codellama/commit/30083473e9c4c46b2e4eb69e1c7572b1098ffa82",
              "message": "fix bugs introduced in #18",
              "files_changed": [
                {
                  "filename": "llama/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/model.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: llama/generation.py ---\n@@ -19,12 +19,12 @@\n from llama.model import ModelArgs, Transformer\n from llama.tokenizer import Tokenizer\n \n-if torch.backends.mps.is_available():\n-    device = torch.device('mps')\n-elif torch.cuda.is_available():\n-    device = torch.device('cuda')\n+if torch.cuda.is_available():\n+    device = \"cuda\"\n+elif torch.backends.mps.is_available():\n+    device = \"mps\"\n else:\n-    device = torch.device('cpu')\n+    device = \"cpu\"\n \n Role = Literal[\"system\", \"user\", \"assistant\"]\n \n@@ -85,7 +85,6 @@ def build(\n         if device == \"cuda\":\n             torch.cuda.set_device(local_rank)\n \n-\n         # seed must be the same in all processes\n         torch.manual_seed(1)\n \n\n--- File: llama/model.py ---\n@@ -15,13 +15,12 @@\n )\n from torch import nn\n \n-if torch.backends.mps.is_available():\n-    device = torch.device('mps')\n-elif torch.cuda.is_available():\n-    device = torch.device('cuda')\n+if torch.cuda.is_available():\n+    device = \"cuda\"\n+elif torch.backends.mps.is_available():\n+    device = \"mps\"\n else:\n-    device = torch.device('cpu')\n-\n+    device = \"cpu\"\n \n @dataclass\n class ModelArgs:\n@@ -81,8 +80,6 @@ def apply_rotary_emb(\n     xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n     xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n     freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n-    if not torch.cuda.is_available():\n-        freqs_cis = freqs_cis.to('cpu')\n     xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n     xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n     return xq_out.type_as(xq).to(device), xk_out.type_as(xk).to(device)\n@@ -97,7 +94,7 @@ def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n         x[:, :, :, None, :]\n         .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n         .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n-    ) #.to(device)\n+    )\n \n \n class Attention(nn.Module):\n@@ -287,6 +284,7 @@ def __init__(self, params: ModelArgs):\n     def forward(self, tokens: torch.Tensor, start_pos: int):\n         _bsz, seqlen = tokens.shape\n         h = self.tok_embeddings(tokens)\n+        self.freqs_cis = self.freqs_cis.to(\"cuda\" if device == \"cuda\" else \"cpu\")\n         freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n \n         mask = None"
            },
            {
              "sha": "2f0f7bb55a9993b74d8297340f2e9dff5fce76f7",
              "url": "https://github.com/meta-llama/codellama/commit/2f0f7bb55a9993b74d8297340f2e9dff5fce76f7",
              "message": "Merge pull request #18 from davideuler/main",
              "files_changed": [
                {
                  "filename": "llama/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/model.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: llama/generation.py ---\n@@ -19,6 +19,13 @@\n from llama.model import ModelArgs, Transformer\n from llama.tokenizer import Tokenizer\n \n+if torch.backends.mps.is_available():\n+    device = torch.device('mps')\n+elif torch.cuda.is_available():\n+    device = torch.device('cuda')\n+else:\n+    device = torch.device('cpu')\n+\n Role = Literal[\"system\", \"user\", \"assistant\"]\n \n \n@@ -65,14 +72,19 @@ def build(\n         model_parallel_size: Optional[int] = None,\n     ) -> \"Llama\":\n         if not torch.distributed.is_initialized():\n-            torch.distributed.init_process_group(\"nccl\")\n+            if device == \"cuda\":\n+                torch.distributed.init_process_group(\"nccl\")\n+            else:\n+                torch.distributed.init_process_group(\"gloo\")\n         if not model_parallel_is_initialized():\n             if model_parallel_size is None:\n                 model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n             initialize_model_parallel(model_parallel_size)\n \n         local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n-        torch.cuda.set_device(local_rank)\n+        if device == \"cuda\":\n+            torch.cuda.set_device(local_rank)\n+\n \n         # seed must be the same in all processes\n         torch.manual_seed(1)\n@@ -98,12 +110,17 @@ def build(\n         )\n         tokenizer = Tokenizer(model_path=tokenizer_path)\n         model_args.vocab_size = tokenizer.n_words\n-        if torch.cuda.is_bf16_supported():\n-            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+        # support for mac\n+        if device == \"cuda\":\n+            if torch.cuda.is_bf16_supported():\n+                torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+            else:\n+                torch.set_default_tensor_type(torch.cuda.HalfTensor)\n         else:\n-            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n+            torch.set_default_tensor_type(torch.HalfTensor)\n         model = Transformer(model_args)\n         model.load_state_dict(checkpoint, strict=False)\n+        model.to(device)\n         print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n \n         return Llama(model, tokenizer)\n@@ -135,14 +152,14 @@ def generate(\n         total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n \n         pad_id = self.tokenizer.pad_id\n-        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n+        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=device)\n         for k, t in enumerate(prompt_tokens):\n-            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n+            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n         if logprobs:\n-            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n+            token_logprobs = torch.zeros_like(tokens, dtype=torch.float, device=device)\n \n         prev_pos = 0\n-        stop_reached = torch.tensor([False] * bsz, device=\"cuda\")\n+        stop_reached = torch.tensor([False] * bsz, device=device)\n         input_text_mask = tokens != pad_id\n         for cur_pos in range(min_prompt_len, total_len):\n             logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n\n--- File: llama/model.py ---\n@@ -15,6 +15,13 @@\n )\n from torch import nn\n \n+if torch.backends.mps.is_available():\n+    device = torch.device('mps')\n+elif torch.cuda.is_available():\n+    device = torch.device('cuda')\n+else:\n+    device = torch.device('cpu')\n+\n \n @dataclass\n class ModelArgs:\n@@ -48,6 +55,7 @@ def forward(self, x):\n \n def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n     freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n+\n     t = torch.arange(end, device=freqs.device, dtype=torch.float32)  # type: ignore\n     freqs = torch.outer(t, freqs)  # type: ignore\n     freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n@@ -67,12 +75,17 @@ def apply_rotary_emb(\n     xk: torch.Tensor,\n     freqs_cis: torch.Tensor,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    if not torch.cuda.is_available():\n+        xq = xq.to('cpu')\n+        xk = xk.to('cpu')\n     xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n     xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n     freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n+    if not torch.cuda.is_available():\n+        freqs_cis = freqs_cis.to('cpu')\n     xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n     xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n-    return xq_out.type_as(xq), xk_out.type_as(xk)\n+    return xq_out.type_as(xq).to(device), xk_out.type_as(xk).to(device)\n \n \n def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n@@ -84,7 +97,7 @@ def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n         x[:, :, :, None, :]\n         .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n         .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n-    )\n+    ) #.to(device)\n \n \n class Attention(nn.Module):\n@@ -133,15 +146,15 @@ def __init__(self, args: ModelArgs):\n                 self.n_local_kv_heads,\n                 self.head_dim,\n             )\n-        ).cuda()\n+        ).to(device)\n         self.cache_v = torch.zeros(\n             (\n                 args.max_batch_size,\n                 args.max_seq_len,\n                 self.n_local_kv_heads,\n                 self.head_dim,\n             )\n-        ).cuda()\n+        ).to(device)\n \n     def forward(\n         self,\n@@ -252,7 +265,7 @@ def __init__(self, params: ModelArgs):\n         self.n_layers = params.n_layers\n \n         self.tok_embeddings = ParallelEmbedding(\n-            params.vocab_size, params.dim, init_method=lambda x: x\n+            params.vocab_size, params.dim, init_method=lambda x: x,\n         )\n \n         self.layers = torch.nn.ModuleList()\n@@ -274,18 +287,17 @@ def __init__(self, params: ModelArgs):\n     def forward(self, tokens: torch.Tensor, start_pos: int):\n         _bsz, seqlen = tokens.shape\n         h = self.tok_embeddings(tokens)\n-        self.freqs_cis = self.freqs_cis.to(h.device)\n         freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n \n         mask = None\n         if seqlen > 1:\n             mask = torch.full(\n-                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n+                (1, 1, seqlen, seqlen), float(\"-inf\"), device=torch.device('cpu')\n             )\n             mask = mask.to(torch.float32).triu(diagonal=start_pos+1).type_as(h)\n \n         for layer in self.layers:\n-            h = layer(h, start_pos, freqs_cis, mask)\n+            h = layer(h, start_pos, freqs_cis, (mask.to(device) if mask is not None else mask))\n         h = self.norm(h)\n         output = self.output(h).float()\n         return output\n\n--- File: llama/tokenizer.py ---\n@@ -45,7 +45,7 @@ def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n         return t\n \n     def decode(self, t: List[int]) -> str:\n-        return self.sp_model.decode(t)\n+        return self.sp_model.decode(list(filter(lambda tk: tk != -1, t)))\n \n     def encode_infilling(self, s: str) -> List[int]:\n         \"\"\"Encode a string without an implicit leading space.\"\"\""
            }
          ]
        }
      ]
    },
    {
      "id": 1458245,
      "username": "newmerator",
      "url": "https://github.com/newmerator",
      "avatar_url": "https://avatars.githubusercontent.com/u/1458245?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "6acfb714c174708bcf806f4612caa62f53c30e46",
              "url": "https://github.com/meta-llama/codellama/commit/6acfb714c174708bcf806f4612caa62f53c30e46",
              "message": "support mac m1",
              "files_changed": [
                {
                  "filename": "llama/generation.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/model.py",
                  "status": "modified"
                },
                {
                  "filename": "llama/tokenizer.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: llama/generation.py ---\n@@ -19,6 +19,13 @@\n from llama.model import ModelArgs, Transformer\n from llama.tokenizer import Tokenizer\n \n+if torch.backends.mps.is_available():\n+    device = torch.device('mps')\n+elif torch.cuda.is_available():\n+    device = torch.device('cuda')\n+else:\n+    device = torch.device('cpu')\n+\n Role = Literal[\"system\", \"user\", \"assistant\"]\n \n \n@@ -65,14 +72,19 @@ def build(\n         model_parallel_size: Optional[int] = None,\n     ) -> \"Llama\":\n         if not torch.distributed.is_initialized():\n-            torch.distributed.init_process_group(\"nccl\")\n+            if device == \"cuda\":\n+                torch.distributed.init_process_group(\"nccl\")\n+            else:\n+                torch.distributed.init_process_group(\"gloo\")\n         if not model_parallel_is_initialized():\n             if model_parallel_size is None:\n                 model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n             initialize_model_parallel(model_parallel_size)\n \n         local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n-        torch.cuda.set_device(local_rank)\n+        if device == \"cuda\":\n+            torch.cuda.set_device(local_rank)\n+\n \n         # seed must be the same in all processes\n         torch.manual_seed(1)\n@@ -98,12 +110,17 @@ def build(\n         )\n         tokenizer = Tokenizer(model_path=tokenizer_path)\n         model_args.vocab_size = tokenizer.n_words\n-        if torch.cuda.is_bf16_supported():\n-            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+        # support for mac\n+        if device == \"cuda\":\n+            if torch.cuda.is_bf16_supported():\n+                torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n+            else:\n+                torch.set_default_tensor_type(torch.cuda.HalfTensor)\n         else:\n-            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n+            torch.set_default_tensor_type(torch.HalfTensor)\n         model = Transformer(model_args)\n         model.load_state_dict(checkpoint, strict=False)\n+        model.to(device)\n         print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n \n         return Llama(model, tokenizer)\n@@ -135,14 +152,14 @@ def generate(\n         total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n \n         pad_id = self.tokenizer.pad_id\n-        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n+        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=device)\n         for k, t in enumerate(prompt_tokens):\n-            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n+            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n         if logprobs:\n-            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n+            token_logprobs = torch.zeros_like(tokens, dtype=torch.float, device=device)\n \n         prev_pos = 0\n-        stop_reached = torch.tensor([False] * bsz, device=\"cuda\")\n+        stop_reached = torch.tensor([False] * bsz, device=device)\n         input_text_mask = tokens != pad_id\n         for cur_pos in range(min_prompt_len, total_len):\n             logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n\n--- File: llama/model.py ---\n@@ -15,6 +15,13 @@\n )\n from torch import nn\n \n+if torch.backends.mps.is_available():\n+    device = torch.device('mps')\n+elif torch.cuda.is_available():\n+    device = torch.device('cuda')\n+else:\n+    device = torch.device('cpu')\n+\n \n @dataclass\n class ModelArgs:\n@@ -48,6 +55,7 @@ def forward(self, x):\n \n def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n     freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n+\n     t = torch.arange(end, device=freqs.device, dtype=torch.float32)  # type: ignore\n     freqs = torch.outer(t, freqs)  # type: ignore\n     freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n@@ -67,12 +75,17 @@ def apply_rotary_emb(\n     xk: torch.Tensor,\n     freqs_cis: torch.Tensor,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    if not torch.cuda.is_available():\n+        xq = xq.to('cpu')\n+        xk = xk.to('cpu')\n     xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n     xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n     freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n+    if not torch.cuda.is_available():\n+        freqs_cis = freqs_cis.to('cpu')\n     xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n     xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n-    return xq_out.type_as(xq), xk_out.type_as(xk)\n+    return xq_out.type_as(xq).to(device), xk_out.type_as(xk).to(device)\n \n \n def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n@@ -84,7 +97,7 @@ def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n         x[:, :, :, None, :]\n         .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n         .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n-    )\n+    ) #.to(device)\n \n \n class Attention(nn.Module):\n@@ -133,15 +146,15 @@ def __init__(self, args: ModelArgs):\n                 self.n_local_kv_heads,\n                 self.head_dim,\n             )\n-        ).cuda()\n+        ).to(device)\n         self.cache_v = torch.zeros(\n             (\n                 args.max_batch_size,\n                 args.max_seq_len,\n                 self.n_local_kv_heads,\n                 self.head_dim,\n             )\n-        ).cuda()\n+        ).to(device)\n \n     def forward(\n         self,\n@@ -252,7 +265,7 @@ def __init__(self, params: ModelArgs):\n         self.n_layers = params.n_layers\n \n         self.tok_embeddings = ParallelEmbedding(\n-            params.vocab_size, params.dim, init_method=lambda x: x\n+            params.vocab_size, params.dim, init_method=lambda x: x,\n         )\n \n         self.layers = torch.nn.ModuleList()\n@@ -274,18 +287,17 @@ def __init__(self, params: ModelArgs):\n     def forward(self, tokens: torch.Tensor, start_pos: int):\n         _bsz, seqlen = tokens.shape\n         h = self.tok_embeddings(tokens)\n-        self.freqs_cis = self.freqs_cis.to(h.device)\n         freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n \n         mask = None\n         if seqlen > 1:\n             mask = torch.full(\n-                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n+                (1, 1, seqlen, seqlen), float(\"-inf\"), device=torch.device('cpu')\n             )\n             mask = mask.to(torch.float32).triu(diagonal=start_pos+1).type_as(h)\n \n         for layer in self.layers:\n-            h = layer(h, start_pos, freqs_cis, mask)\n+            h = layer(h, start_pos, freqs_cis, (mask.to(device) if mask is not None else mask))\n         h = self.norm(h)\n         output = self.output(h).float()\n         return output\n\n--- File: llama/tokenizer.py ---\n@@ -45,7 +45,7 @@ def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n         return t\n \n     def decode(self, t: List[int]) -> str:\n-        return self.sp_model.decode(t)\n+        return self.sp_model.decode(list(filter(lambda tk: tk != -1, t)))\n \n     def encode_infilling(self, s: str) -> List[int]:\n         \"\"\"Encode a string without an implicit leading space.\"\"\""
            },
            {
              "sha": "15dc831a356aa07e6ec1979050739e8f49809745",
              "url": "https://github.com/meta-llama/codellama/commit/15dc831a356aa07e6ec1979050739e8f49809745",
              "message": "minor update for torchrun command",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -50,7 +50,7 @@ The Code Llama and Code Llama - Python models are not fine-tuned to follow instr\n See `example_completion.py` for some examples. To illustrate, see command below to run it with the `CodeLlama-7b` model (`nproc_per_node` needs to be set to the `MP` value):\n \n ```\n-torchrun --nproc_per_node 1 example_code_completion.py \\\n+torchrun --nproc_per_node 1 example_completion.py \\\n     --ckpt_dir CodeLlama-7b/ \\\n     --tokenizer_path CodeLlama-7b/tokenizer.model \\\n     --max_seq_len 128 --max_batch_size 4\n@@ -66,7 +66,7 @@ Code Llama and Code Llama - Instruct 7B and 13B models are capable of filling in\n \n See `example_infilling.py` for some examples. The `CodeLlama-7b` model can be run for infilling with the command below (`nproc_per_node` needs to be set to the `MP` value):\n ```\n-torchrun --nproc_per_node 1 example_text_infilling.py \\\n+torchrun --nproc_per_node 1 example_infilling.py \\\n     --ckpt_dir CodeLlama-7b/ \\\n     --tokenizer_path CodeLlama-7b/tokenizer.model \\\n     --max_seq_len 192 --max_batch_size 4"
            }
          ]
        }
      ]
    },
    {
      "id": 95188570,
      "username": "NinoRisteski",
      "url": "https://github.com/NinoRisteski",
      "avatar_url": "https://avatars.githubusercontent.com/u/95188570?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [],
          "commits": [
            {
              "sha": "796c88bcee84ed340066fa294d23cf8fe8b23a8e",
              "url": "https://github.com/meta-llama/codellama/commit/796c88bcee84ed340066fa294d23cf8fe8b23a8e",
              "message": "Update MODEL_CARD.md",
              "files_changed": [
                {
                  "filename": "MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: MODEL_CARD.md ---\n@@ -5,7 +5,7 @@\n **Model Developers** Meta AI \n \n **Variations** Code Llama comes in three model sizes, and three variants: \n-1) Code Llama: our base models designed for general code synthesis and understanding\n+1) Code Llama: our base models are designed for general code synthesis and understanding\n 2) Code Llama - Python: designed specifically for Python \n 3) Code Llama - Instruct: for instruction following and safer deployment \n  \n@@ -28,13 +28,13 @@ All variants are available in sizes of 7B, 13B and 34B parameters.\n **Where to send comments** Instructions on how to provide feedback or comments on the model can be found in the model [README](README.md), or by opening an issue in the GitHub repository ([https://github.com/facebookresearch/codellama/](https://github.com/facebookresearch/codellama/)).\n \n ## **Intended Use**\n-**Intended Use Cases** Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\n+**Intended Use Cases** Code Llama and its variants are intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistance and generation applications.\n \n **Out-of-Scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n \n ## **Hardware and Software**\n **Training Factors**\n-We used custom training libraries. The training and fine-tuning of the released models have been performed Metas Research Super Cluster.\n+We used custom training libraries. The training and fine-tuning of the released models have been performed by Metas Research Super Cluster.\n \n **Carbon Footprint** In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Metas sustainability program."
            }
          ]
        }
      ]
    },
    {
      "id": 115941608,
      "username": "noah23olsen",
      "url": "https://github.com/noah23olsen",
      "avatar_url": "https://avatars.githubusercontent.com/u/115941608?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "108be1045b910bef962c7f2c2830d0f511a2b057",
              "url": "https://github.com/meta-llama/llama-models/commit/108be1045b910bef962c7f2c2830d0f511a2b057",
              "message": "clarifies instructions for utilizing the base model  script (#260)",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -61,7 +61,7 @@ CHECKPOINT_DIR=~/.llama/checkpoints/Meta-Llama3.1-8B-Instruct\n PYTHONPATH=$(git rev-parse --show-toplevel) torchrun llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n ```\n \n-The above script should be used with an Instruct (Chat) model. For a Base model, use the script `llama_models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n+The above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `llama_models/scripts/example_text_completion.py`. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.\n \n For running larger models with tensor parallelism, you should modify as:\n ```bash"
            }
          ]
        }
      ]
    },
    {
      "id": 20949931,
      "username": "pmysl",
      "url": "https://github.com/pmysl",
      "avatar_url": "https://avatars.githubusercontent.com/u/20949931?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "56d236d64499557d5266a594d11609fa501ceff9",
              "url": "https://github.com/meta-llama/llama-models/commit/56d236d64499557d5266a594d11609fa501ceff9",
              "message": "Add missing fp8_scales files to `meta-llama-3.1-405b-fp8` (#69)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/download.sh",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/download.sh ---\n@@ -87,6 +87,7 @@ do\n         PTH_FILE_COUNT=7\n         PTH_FILE_CHUNK_COUNT=3\n         MODEL_PATH=\"Meta-Llama-3.1-405B\"\n+        ADDITIONAL_FILES=\"fp8_scales_0.pt,fp8_scales_1.pt,fp8_scales_2.pt,fp8_scales_3.pt,fp8_scales_4.pt,fp8_scales_5.pt,fp8_scales_6.pt,fp8_scales_7.pt\"\n     elif [[ $m == \"meta-llama-3.1-70b-instruct\" ]]; then\n         PTH_FILE_COUNT=7\n         MODEL_PATH=\"Meta-Llama-3.1-70B-Instruct\""
            }
          ]
        }
      ]
    },
    {
      "id": 853234,
      "username": "raghotham",
      "url": "https://github.com/raghotham",
      "avatar_url": "https://avatars.githubusercontent.com/u/853234?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "ebe3527e5104647c0976af25ddbe88784778ed18",
              "url": "https://github.com/meta-llama/llama-models/commit/ebe3527e5104647c0976af25ddbe88784778ed18",
              "message": "Create llama4 prompt_format.md (#303)",
              "files_changed": [
                {
                  "filename": "models/llama4/prompt_format.md",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/prompt_format.md ---\n@@ -0,0 +1,277 @@\n+\n+\n+# Llama 4 - Prompt Formats\n+## Tokens\n+Here is a list of special tokens that are supported by Llama 4:\n+- `<|begin_of_text|>`: Specifies the start of the prompt\n+- `<|end_of_text|>`: Model will cease to generate more tokens. This token is generated only by the base models.\n+- `<|header_start|>` and `<|header_end|>`: These tokens enclose the role for a particular message. The possible roles are: [system, user and assistant].\n+- `<|eot|>`: End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios:\n+    - at the end of a direct interaction between the model and the user\n+    - at the end of multiple interactions between the model and any available tools\n+    This token signals to the executor that the model has finished generating a response.\n+- `<|image_start|>` and `<|image_end|>`: These tokens enclose the image data in the prompt.\n+- `<|patch|>`: This token represents a piece of the tile/\n+- `<|tile_y_separator|>` and `<|tile_x_separator|>`: These tokens are used to separate the y and x tiles of an image\n+- `<|image|>`: In the new architecture, this token now separates the regular sized image information from a downsized version of it that fits in a single tile. The longer side is used for calculating the scale factor and the rest is padded to fit the tile.\n+\n+\n+\n+There are 3 different roles that are supported by Llama 4\n+- `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively.\n+- `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n+- `assistant`: Represents the response generated by the AI model based on the context provided in the `system`, `tool` and `user` prompts.\n+\n+\n+# Llama 4 Instruct Model\n+## Simple User and assistant conversation\n+\n+Here is a regular multi-turn user assistant conversation and how its formatted.\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|header_start|>system<|header_end|>\n+\n+You are a helpful assistant<|eot|><|header_start|>user<|header_end|>\n+\n+Answer who are you in the form of jeopardy?<|eot|><|header_start|>assistant<|header_end|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+\"What is a helpful assistant?\"<|eot|>\n+```\n+\n+\n+\n+\n+\n+# Image prompt format\n+## Single image prompt format - small image\n+\n+This example passes an image that is smaller than the tile size, to show the tile separator tokens are not needed\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|header_start|>user<|header_end|>\n+\n+<|image_start|><|image|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|image_end|>Describe this image in two sentences<|eot|><|header_start|>assistant<|header_end|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+The image depicts a dog standing on a skateboard, with its front paws positioned on the board and its back paws hanging off the back. The dog has a distinctive coat pattern, featuring a white face, brown and black fur, and white paws, and is standing on a skateboard with red wheels, set against a blurred background of a street or alleyway with a teal door and beige wall.<|eot|>\n+```\n+\n+\n+##### Notes\n+Notice the structure of the image section:\n+        ```\n+        <|image_start|><|image|><|patch|>...<|patch|><|image_end|>\n+        ```\n+        This is due to the image being smaller than the tile size.\n+\n+\n+## Single image prompt format\n+\n+Here is an example of how to pass an image to the model\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|header_start|>user<|header_end|>\n+\n+<|image_start|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|image|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|image_end|>Describe this image in two sentences<|eot|><|header_start|>assistant<|header_end|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+This image shows a dog standing on a skateboard, with its front paws positioned near the front of the board and its back paws near the back. The dog has a white, black, and orange coat, and is standing on a gray skateboard with red wheels, in front of a blurred background that appears to be a street or alleyway.<|eot|>\n+```\n+\n+\n+##### Notes\n+With a bigger image, the image will include the tile separator tokens. Additionally, the image tag now separates a scaled down version of the image from the regular sized image.\n+        ```\n+        <|image_start|><|patch|>...<|patch|><|tile_x_separator|><|patch|>...<|patch|><|tile_y_separator|><|patch|>...<|patch|><|image|><|patch|>...<|patch|><|image_end|>\n+        ```\n+\n+\n+## Multiple images prompt format\n+\n+Here is an example of how to pass an image to the model\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|header_start|>user<|header_end|>\n+\n+<|image_start|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|image|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|image_end|><|image_start|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_x_separator|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|tile_y_separator|><|image|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|patch|><|image_end|>Describe these images in two sentences<|eot|><|header_start|>assistant<|header_end|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+The first image shows a dog standing on a skateboard, while the second image shows a plate of spaghetti with tomato sauce, parmesan cheese, and parsley. The two images are unrelated, with the first image featuring a dog and the second image featuring a food dish, and they do not share any common elements or themes.<|eot|>\n+```\n+\n+\n+##### Notes\n+With multiple images, each one is encapsulated in their corresponding image tags.\n+\n+\n+# Tool calling\n+We are continuing the format for zero shot function calling used in previous versions of Llama. All available functions can be provided either in the system message or in the user message.\n+## Zero shot function calling - system message\n+\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|header_start|>system<|header_end|>\n+\n+You are an expert in composing functions. You are given a question and a set of possible functions.\n+Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+also point it out. You should only return the function call in tools call sections.\n+\n+If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+You SHOULD NOT include any other text in the response.\n+\n+Here is a list of functions in JSON format that you can invoke.\n+\n+[\n+    {\n+        \"name\": \"get_weather\",\n+        \"description\": \"Get weather info for places\",\n+        \"parameters\": {\n+            \"type\": \"dict\",\n+            \"required\": [\n+                \"city\"\n+            ],\n+            \"properties\": {\n+                \"city\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The name of the city to get the weather for\"\n+                },\n+                \"metric\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n+                    \"default\": \"celsius\"\n+                }\n+            }\n+        }\n+    }\n+<|eot|><|header_start|>user<|header_end|>\n+\n+What is the weather in SF and Seattle?<|eot|><|header_start|>assistant<|header_end|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+[get_weather(city='SF'), get_weather(city='Seattle')]<|eot|>\n+```\n+\n+\n+##### Notes\n+\n+- The output supports multiple, and parallel tool calls natively\n+- JSON format for defining the functions in the system prompt is similar to Llama3.1\n+\n+\n+## Zero shot function calling - user message\n+\n+\n+Similar to the above example, you can also provide information for all the available tools in the user message.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|header_start|>user<|header_end|>\n+\n+Questions: Can you retrieve the details for the user with the ID 7890, who has black as their special request?\n+Here is a list of functions in JSON format that you can invoke:\n+[\n+    {\n+        \"name\": \"get_user_info\",\n+        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n+        \"parameters\": {\n+            \"type\": \"dict\",\n+            \"required\": [\n+                \"user_id\"\n+            ],\n+            \"properties\": {\n+                \"user_id\": {\n+                \"type\": \"integer\",\n+                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n+            },\n+            \"special\": {\n+                \"type\": \"string\",\n+                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n+                \"default\": \"none\"\n+                }\n+            }\n+        }\n+    }\n+]\n+\n+Should you decide to return the function call(s), put them in the format of [func1(params_name=params_value, params_name2=params_value2...), func2(params)]\n+\n+You SHOULD NOT include any other text in the response.<|eot|><|header_start|>assistant<|header_end|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+[get_user_info(user_id=7890, special='black')]<|eot|>\n+```\n+\n+\n+##### Notes\n+\n+- The tool call format for the model is the same whether your function calls are provided in the system or user message.\n+\n+\n+## Tool calling with custom formats\n+\n+\n+Here is an example of how you could also write custom instructions for model to do zero shot tool calling.\n+In this example, we define a custom tool calling format using the `<function>` tag.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|header_start|>user<|header_end|>\n+\n+You have access to the following functions:\n+Use the function 'trending_songs' to 'Returns the trending songs on a Music site':\n+{\"name\": \"trending_songs\", \"description\": \"Returns the trending songs on a Music site\", \"parameters\": {\"genre\": {\"description\": \"The genre of the songs to return\", \"param_type\": \"str\", \"required\": false}, \"n\": {\"description\": \"The number of songs to return\", \"param_type\": \"int\", \"required\": true}}}\n+\n+Think very carefully before calling functions.\n+If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n+\n+<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n+Reminder:\n+- If looking for real time information use relevant functions before falling back to brave_search\n+- Function calls MUST follow the specified format, start with <function= and end with </function>\n+- Required parameters MUST be specified\n+- Only call one function at a time\n+- Put the entire function call reply on one line<|eot_id|><|eot|><|header_start|>user<|header_end|>\n+\n+Use tools to get latest trending songs<|eot|><|header_start|>assistant<|header_end|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+<function=trending_songs>{\"n\": \"10\"}</function><|eot|>\n+```"
            },
            {
              "sha": "038acb9fe1e1ddc5cf1b3989fb07b311a0ffcae4",
              "url": "https://github.com/meta-llama/llama-models/commit/038acb9fe1e1ddc5cf1b3989fb07b311a0ffcae4",
              "message": "Fix link to developer user guide",
              "files_changed": [
                {
                  "filename": "models/llama4/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/MODEL_CARD.md ---\n@@ -319,7 +319,8 @@ As part of our release approach, we followed a three-pronged strategy to manage\n * Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n * Provide protections for the community to help prevent the misuse of our models.\n \n-Llama is a foundational technology designed for use in a variety of use cases; examples on how Metas Llama models have been deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our models safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections](https://ai.meta.com/static-resource/developer-use-guide-ai-protections).\n+Llama is a foundational technology designed for use in a variety of use cases; examples on how Metas Llama models have been deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our models safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections]([https://ai.meta.com/static-resource/developer-use-guide-ai-protections](https://www.llama.com/static-resource/developer-use-guide/).\n+ \n \n ### Model level fine tuning"
            },
            {
              "sha": "1ba7d45b4f6f3f89feed1e6fae8a0c4b8cc2932f",
              "url": "https://github.com/meta-llama/llama-models/commit/1ba7d45b4f6f3f89feed1e6fae8a0c4b8cc2932f",
              "message": "Update MODEL_CARD.md",
              "files_changed": [
                {
                  "filename": "models/llama4/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 8,
              "diff_patch": "--- File: models/llama4/MODEL_CARD.md ---\n@@ -47,7 +47,7 @@ These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We\n \n **Supported languages:** Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. \n \n-**Model Release Date:** April 7, 2025\n+**Model Release Date:** April 5, 2025\n \n **Status:** This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback."
            },
            {
              "sha": "4daa96584a2c415a1eba50273d0bf141b869ab7a",
              "url": "https://github.com/meta-llama/llama-models/commit/4daa96584a2c415a1eba50273d0bf141b869ab7a",
              "message": "Updated Model Card",
              "files_changed": [
                {
                  "filename": "models/llama4/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama4/MODEL_CARD.md ---\n@@ -125,7 +125,7 @@ In this section, we report the results for Llama 4 relative to our previous mode\n     <td> </td>\n     <td>MMLU-Pro</td>\n     <td>5</td>\n-    <td>macro_avg/acc</td>\n+    <td>macro_avg/em</td>\n     <td>53.8</td>\n     <td>61.6</td>\n     <td>58.2</td>\n@@ -259,7 +259,7 @@ In this section, we report the results for Llama 4 relative to our previous mode\n     <td> Reasoning & Knowledge  </td>\n     <td> MMLU Pro</td>\n     <td>0</td>\n-    <td>macro_avg/em</td>\n+    <td>macro_avg/acc</td>\n     <td> 68.9 </td>\n     <td> 73.4 </td>\n     <td>74.3</td>"
            },
            {
              "sha": "2386bf56e67ca52e1dac28c7028b9f48073211a6",
              "url": "https://github.com/meta-llama/llama-models/commit/2386bf56e67ca52e1dac28c7028b9f48073211a6",
              "message": "Fixing vision chunk size for 11B Vision Instruct model to 560",
              "files_changed": [
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/sku_list.py ---\n@@ -659,7 +659,7 @@ def llama3_2_instruct_models() -> List[Model]:\n                 \"norm_eps\": 1e-05,\n                 \"rope_theta\": 500000.0,\n                 \"use_scaled_rope\": True,\n-                \"vision_chunk_size\": 448,\n+                \"vision_chunk_size\": 560,\n                 \"vision_max_num_chunks\": 4,\n                 \"vision_num_cross_attention_layers\": 8,\n             },"
            },
            {
              "sha": "2e5fc303d4bdb5cb23c912bb210e94024b2163bb",
              "url": "https://github.com/meta-llama/llama-models/commit/2e5fc303d4bdb5cb23c912bb210e94024b2163bb",
              "message": "Update README.md",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -1,5 +1,5 @@\n <p align=\"center\">\n-  <img src=\"Llama_Repo.jpeg\" width=\"400\"/>\n+  <img src=\"https://github.com/meta-llama/llama-models/blob/main/Llama_Repo.jpeg\" width=\"400\"/>\n </p>\n \n <p align=\"center\">"
            }
          ]
        }
      ]
    },
    {
      "id": 10227643,
      "username": "rohit-ptl",
      "url": "https://github.com/rohit-ptl",
      "avatar_url": "https://avatars.githubusercontent.com/u/10227643?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "2f6bfefc37fa6f8086e5f335ac0f71ec5b241067",
              "url": "https://github.com/meta-llama/llama-models/commit/2f6bfefc37fa6f8086e5f335ac0f71ec5b241067",
              "message": "Create eval_details.md",
              "files_changed": [
                {
                  "filename": "models/llama3_2/eval_details.md",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_2/eval_details.md ---\n@@ -0,0 +1,152 @@\n+\n+\n+# Llama 3 Evaluation Details\n+\n+This document contains some additional context on the settings and methodology for how we evaluated the Llama 3.2 models.\n+\n+\n+## Language auto-eval benchmark notes:\n+\n+For a given benchmark, we strive to use consistent evaluation settings across all models, including external models. We make every effort to achieve optimal scores for external models, including addressing any model-specific parsing and tokenization requirements. Where the scores are lower for external models than self-reported scores on comparable or more conservative settings, we report the self-reported scores for external models. We are also releasing the data generated as part of evaluations with publicly available benchmarks which can be found on [huggingface here](https://huggingface.co/collections/meta-llama/llama-32-evals-66f44b3d2df1c7b136d821f0).\n+\n+\n+### MMLU\n+\n+For the pre-trained models we use a 5-shot config. To determine the choice character we use the standard MMLU prompt and compare the negative log-likelihood (NLL) of the various choices.\n+\n+For the post-trained models we report both 5-shot and 0-shot scores. We ask the model to generate the best choice character. The 0-shot scores use a CoT (chain of thought) prompt. The maximum generation lengths for the 5-shot and 0-shot configs are 10 tokens and 1024 tokens respectively.\n+\n+Macro averages are reported unless otherwise stated. The micro average scores for the various models are: 65.6, 79.0, and 85.4 for the pre-trained 8B, 70B and 405B models respectively for the 5-shot config; 69.44, 84.0, 87.71 for the post-trained 8B, 70B and 405B models respectively for the 5-shot config.\n+\n+\n+### TLDR9+ \n+\n+For post-trained models, we use a 1-shot config and report rougeL scores. We run this as a generative task. Maximum generation length is 512 tokens. We specifically ran this on [TLDR9+ dataset](https://github.com/sajastu/reddit_collector)  \n+\n+\n+### Open-Rewrite\n+\n+For post-trained models, we use a 0-shot config and report micro_avg rougeL scores across elaborate, formality, others, paraphrase, shorten, and wiki. We run this as a generative task. Maximum generation length is 512 tokens. Specific dataset can be found [here](https://github.com/google-research/google-research/tree/master/rewritelm).\n+\n+\n+### IFEval\n+\n+For post-trained models, we use the default settings as specified [here](https://arxiv.org/pdf/2311.07911). We compute the prompt level scores and instruction level strict and loose accuracy. We then report the average across all the scores. \n+\n+\n+### ARC-Challenge\n+\n+We use the Arc-Challenge subset from the Arc benchmark. For the pre-trained models, we use a  25-shot config and use the MMLU setup for evaluation where we provide all the choices in the prompt and calculate likelihood over choice characters. For the post-trained models, we use 0-shot config and ask the model to generate the choice character. The maximum generation length is 100 tokens.\n+\n+\n+### GPQA\n+\n+For post-trained models, we use 0-shot config with CoT prompt and report exact match scores over the possible options using the main set. Max generation length is 2048 tokens.\n+\n+\n+### AGIEval English\n+\n+For pre-trained models, we use the default few-shot and prompt settings as specified [here](https://github.com/ruixiangcui/AGIEval). The score is averaged over the english subtasks. The max generation length is 10 tokens.\n+\n+\n+### SQuAD\n+\n+For pre-trained models, we use SQuAD v2 with a 1-shot config and report exact match scores. We run this as a generative task. Maximum generation length is 32 tokens.\n+\n+\n+### QuAC\n+\n+For pre-trained models, we use a 1-shot config and report the F1 scores. We run this as a generative task. Maximum generation length is 32 tokens.\n+\n+\n+### DROP\n+\n+For pre-trained models, for each validation example, we draw 3 random few-shot examples from the train split and report the F1 scores. The maximum generation length is 32 tokens.\n+\n+\n+### GSM8K\n+\n+For both pre-trained and post-trained models, we use the same 8-shot config with CoT prompt as in [Wei et al. (2022)](https://arxiv.org/pdf/2201.11903.pdf) (maj@1). The maximum generation length is 1024 tokens. \n+\n+\n+### MATH\n+\n+For pre-trained models, we use the same 4-shot config as in [Lewkowycz et al. (2022)](https://arxiv.org/pdf/2206.14858.pdf) (maj@1). Maximum generation length is 512 tokens. \n+\n+For post-trained models we use a 0-shot config with Cot prompt. We enhance the exact match using [sympy](https://www.sympy.org/en/index.html) and then use an [equality template](https://github.com/openai/simple-evals/blob/main/common.py#L27-L85) with a judge to resolve complex expressions. Maximum generation length is 5120 tokens. The MATH score represents the full dataset. The scores for MATH-HARD (Lvl 5) are 25.4, 43.8, and 53.4 for the 8B, 70B and 405B models respectively.\n+\n+\n+### InfiniteBench\n+\n+We report on EN.MC (Free-form question answering based on the fake book) and EN.QA (Multiple choice questions derived from the fake book) sub-tasks. The average tokens in these tasks is 184.4k and 192.6k respectively. We truncate down the dataset to 128k input tokens using our tokenizer to have a 'clean' dataset where the right answer is not mistakenly deleted during truncation. For post-trained models, we use a 0-shot config. Maximum generation length is 20 for both the En.QA and En.MC tasks.\n+\n+\n+### NIH/Multi-needle\n+\n+For post-training, we use a 0-shot config. Our context lengths are evenly spaced between 2000 and 131072 in 10 intervals, inclusive of the endpoints for llama models and between 2000 and 128000 for non-llama models. Maximum generation length is 256 tokens.\n+\n+\n+### MGSM\n+\n+For post-trained models, we use an 0-shot config with CoT prompt and report exact match (maj@1) scores. Maximum generation length is 2048 tokens. The scores are averaged over all the eleven languages present in the MGSM benchmark, including the ones not supported by Llama models.\n+\n+\n+### Multilingual MMLU\n+\n+For post-trained models, we use a 5-shot config. We run this as a generative task. Maximum generation length is 10 tokens. The scores are individually reported for each and averaged over the seven non-english languages that Llama models support (Portuguese, Spanish, Italian, German, French, Hindi, Thai). \n+\n+\n+### Berkeley Function Calling Leaderboard (BFCL) v2\n+\n+For Berkeley Function Calling Leaderboard (BFCL-v2), benchmark results were achieved by running the open source evaluation repository [ShishirPatil/gorilla ](https://github.com/ShishirPatil/gorilla/)on commit 70d6722 and we report the \"AST Summary\" metric.\n+\n+\n+### Nexus\n+\n+We use the [open-source ](https://github.com/nexusflowai/NexusRaven)prompt and evaluation function followed by the[ open source notebook](https://github.com/nexusflowai/NexusRaven-V2/blob/master/evaluation_notebook/GPT4_Evaluation/Benchmark_GPT4.ipynb) to compute the scores. \n+\n+\n+### RULER\n+\n+For more comprehensive long-context evals beyond retrieval, we assess our perf on RULER benchmark, where we synthesize datasets across increasingly long context length buckets, and compare mean across each of them over retrieval (single needle, multi needle, multi-value per multiple keys), multi-hop tracing (variable tracking), aggregation (common words, frequency extraction), and question-answering. \n+\n+\n+### MMMU\n+\n+For post-trained models, we use 0-shot config with CoT prompt and report scores over the possible options using the validation set. Maximum generation length is 2048 tokens. We use the following system prompt: \"&lt;|image|>Look at the image carefully and solve the following question step-by-step. {question} Options: {options} Indicate the correct answer at the end.\"\n+\n+\n+### MMMU-Pro standard\n+\n+For post-trained models, we use 0-shot config, we use multiple choice questions with ten options, and report scores over the possible options using the test set. In case of multiple images provided per prompt, they are stitched into a single image. Maximum generation length is 2048 tokens. We use the following system prompt: \"&lt;|image|>{question} Options: {options}\"\n+\n+\n+### MMMU-Pro vision\n+\n+For post-trained models, we use 0-shot config and report scores over the possible options using the test set. Maximum generation length is 2048 tokens. We use the following system prompt: \"&lt;|image|>Your job is to extract the question from the image the user attached, reason about it, and then answer the question. Follow these steps: \\n1. Always start by Clearly stating the question shown in the image, and also state all of the options. \\n2. Then, Carefully review the information beyond the question shown in the image and analyze it without making any assumptions. \\n3. Next, use your understanding of the image to answer the question you listed out in the first step. Use a step-by-step process \\n4. Finally, write the answer in the following format where X is exactly one of the option letters: The best answer is X.\\nAlways follow the steps above, printing out everything. Let's think step by step. Your response must be among the given options.\"\n+\n+\n+### AI2D\n+\n+For post-trained models, we use 0-shot config and report scores using the test set. Maximum generation length is 400 tokens. For 11b we use the following system prompt: \"&lt;|image|>Look at the scientific diagram carefully and answer the following question: {question}\\n Think step by step and finally respond to the question with only the correct option number as \\\"FINAL ANSWER\\\". Let's think step by step.\" For 90b we use a different system prompt: \"&lt;|image|>Look at the scientific diagram carefully and answer the following question: {question}\\n Respond only with the correct option digit.\"\n+\n+\n+### ChartQA\n+\n+For post-trained models, we use 0-shot config with CoT prompt and report scores using the test set. Maximum generation length is 512 tokens. For 11b we use the following system prompt: \"&lt;|image|>You are provided a chart image and will be asked a question. You have to think through your answer and provide a step-by-step solution. Once you have the solution, write the final answer in at most a few words at the end with the phrase \"FINAL ANSWER:\". The question is: {question}&lt;cot_start>Let's think step by step.\" For 90b we use a different system prompt: \"&lt;|image|>You are provided a chart image and will be asked a question. Follow these steps carefully:\\n Step 1: Analyze the question to understand what specific data or information is being asked for. Focus on whether the question is asking for a specific number or category from the chart image.\\n Step 2: Identify any numbers, categories, or groups mentioned in the question and take note of them. Focus on detecting and matching them directly to the image. \\nStep 3: Study the image carefully and find the relevant data corresponding to the categories or numbers mentioned. Avoid unnecessary assumptions or calculations; simply read the correct data from the image.\\n Step 4: Develop a clear plan to solve the question by locating the right data. Focus only on the specific category or group that matches the question. \\nStep 5: Use step-by-step reasoning to ensure you are referencing the correct numbers or data points from the image, avoiding unnecessary extra steps or interpretations.\\n Step 6: Provide the final answer, starting with \"FINAL ANSWER:\" and using as few words as possible, simply stating the number or data point requested. \\n\\n The question is: {question}&lt;cot_start>Let\\'s think step by step.\"\n+\n+\n+### DocVQA\n+\n+For post-trained models, we use 0-shot config and report scores over the test set. Maximum generation length is 512 tokens. We use the following system prompt: \"&lt;|image|> Read the text in the image carefully and answer the question with the text as seen exactly in the image. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Question: {question}\"\n+\n+\n+### VQAv2\n+\n+For post-trained models, we use 0-shot config and report scores over the test set. Maximum generation length is 25 tokens. We use the following system prompt: \"&lt;|image|> Look at the image carefully and answer this visual question. For yes/no questions, just respond Yes or No. If the answer is numeric, just respond with the number and nothing else. If the answer has multiple words, just respond with the words and absolutely nothing else. Never respond in a sentence or a phrase.\\n Respond with as few words as possible.\\n Question: {question}\"\n+\n+\n+### MathVista\n+\n+For post-trained models, we use 0-shot config and report scores over the testmini set. Maximum generation length is 2048 tokens. We use an LLM-based answer extractor as recommended by MathVista paper [Lu et al. (2024)](https://arxiv.org/pdf/2310.02255). We use the following system prompt: \"&lt;|image|>{question}\"\n+"
            },
            {
              "sha": "1b5892739868e5333fb7f022ba91218f0ae5f9c2",
              "url": "https://github.com/meta-llama/llama-models/commit/1b5892739868e5333fb7f022ba91218f0ae5f9c2",
              "message": "Update eval details (#23)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/MODEL_CARD.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/eval_details.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/MODEL_CARD.md ---\n@@ -190,7 +190,7 @@ The methodology used to determine training energy use and greenhouse gas emissio\n \n ## Benchmark scores\n \n-In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\n+In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. Details of our evals can be found [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md). We are also releasing the raw data generated as part of our evals which can be found [here](https://huggingface.co/meta-llama).\n \n ### Base pretrained models\n \n\n--- File: models/llama3_1/eval_details.md ---\n@@ -112,7 +112,7 @@ For pre-trained and post-trained models we use a 0-shot config and report pass@1\n \n For pre-trained models, we use the same 4-shot config as in [Lewkowycz et al. (2022)](https://arxiv.org/pdf/2206.14858.pdf) (maj@1). Maximum generation length is 512 tokens.\n \n-For post-trained models we use a 0-shot config with Cot prompt. We enhance the exact match using [sympy](https://www.sympy.org/en/index.html) and then use an [equality template](https://github.com/openai/simple-evals/blob/main/common.py#L27-L85) with a judge to resolve complex expressions. Maximum generation length is 5120 tokens.\n+For post-trained models we use a 0-shot config with Cot prompt. We enhance the exact match using [sympy](https://www.sympy.org/en/index.html) and then use an [equality template](https://github.com/openai/simple-evals/blob/main/common.py#L27-L85) with a judge to resolve complex expressions. Maximum generation length is 5120 tokens. The MATH score represents the full dataset. The scores for MATH-HARD (Lvl 5) are 25.4, 43.8, and 53.4 for the 8B, 70B and 405B models respectively.\n \n \n ### SCROLLS"
            },
            {
              "sha": "c4bedcb87d93b1ec4a40fcc068f5eb81526f28ae",
              "url": "https://github.com/meta-llama/llama-models/commit/c4bedcb87d93b1ec4a40fcc068f5eb81526f28ae",
              "message": "Merge pull request #15 from machina-source/main",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                },
                {
                  "filename": "models/llama3_1/README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -73,7 +73,7 @@ To help developers address these risks, we have created the [Responsible Use Gui\n ## Issues\n \n Please report any software bug or other problems with the models through one of the following means:\n-- Reporting issues with the model: [https://github.com/issues](https://github.com/issues)\n+- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n - Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n - Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n \n\n--- File: models/llama3_1/README.md ---\n@@ -73,7 +73,7 @@ To help developers address these risks, we have created the [Responsible Use Gui\n ## Issues\n \n Please report any software bug or other problems with the models through one of the following means:\n-- Reporting issues with the model: [https://github.com/issues](https://github.com/issues)\n+- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n - Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n - Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)"
            }
          ]
        }
      ]
    },
    {
      "id": 14824940,
      "username": "Romainsauvestre",
      "url": "https://github.com/Romainsauvestre",
      "avatar_url": "https://avatars.githubusercontent.com/u/14824940?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/82",
              "number": 82,
              "title": "Running 7B model on RTX 4070 (12GB) causes \"out of memory\" error",
              "body": "Running example_completion.py with a 7B model on an RTX 4070 (12GB VRAM) results in an \"out of memory\" error.\r\n\r\nIs it possible to run example_completion.py without having to purchase a higher-spec GPU than the RTX 4070?\r\n\r\n* GPU Info (before running example_completion.py)\r\n\r\n```\r\n# nvidia-smi\r\nSun Sep  3 13:32:02 2023\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA GeForce RTX 4070        Off | 00000000:01:00.0  On |                  N/A |\r\n|  0%   47C    P2              32W / 200W |    697MiB / 12282MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n```\r\n\r\n* Command and Error\r\n\r\n```\r\n# torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 1\r\n> initializing model parallel with size 1\r\n> initializing ddp with size 1\r\n> initializing pipeline with size 1\r\nTraceback (most recent call last):\r\n  File \"/root/codellama/example_completion.py\", line 55, in <module>\r\n    fire.Fire(main)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/root/codellama/example_completion.py\", line 20, in main\r\n    generator = Llama.build(\r\n  File \"/root/codellama/llama/generation.py\", line 105, in build\r\n    model = Transformer(model_args)\r\n  File \"/root/codellama/llama/model.py\", line 260, in __init__\r\n    self.layers.append(TransformerBlock(layer_id, params))\r\n  File \"/root/codellama/llama/model.py\", line 222, in __init__\r\n    self.attention = Attention(args)\r\n  File \"/root/codellama/llama/model.py\", line 100, in __init__\r\n    self.wq = ColumnParallelLinear(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py\", line 262, in __init__\r\n    self.weight = Parameter(torch.Tensor(self.output_size_per_partition, self.in_features))\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.69 GiB total capacity; 10.85 GiB already allocated; 28.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4749) of binary: /usr/bin/python3\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\nexample_completion.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-09-03_13:25:53\r\n  host      : daiv\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 4749)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n```\r\n\r\n* Git revision\r\n\r\n```\r\n# git rev-parse HEAD\r\ne064c1c24c377cc0875711440ef4c0a6eaf0147b\r\n```\r\n\r\n* OS\r\n\r\n```\r\n# lsb_release -a\r\nDescription:\tUbuntu 22.04.3 LTS\r\n```",
              "labels": [
                {
                  "id": 5931411248,
                  "node_id": "LA_kwDOKK-33s8AAAABYYonMA",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/model-usage",
                  "name": "model-usage",
                  "color": "f9d0c4",
                  "default": false,
                  "description": "issues related to how models are used/loaded"
                },
                {
                  "id": 5931417425,
                  "node_id": "LA_kwDOKK-33s8AAAABYYo_UQ",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/performance",
                  "name": "performance",
                  "color": "FE1955",
                  "default": false,
                  "description": "issues or discussions related to optimizing models for addressing performance concerns"
                }
              ],
              "comments": 5,
              "state_reason": "completed"
            }
          ],
          "commits": []
        }
      ]
    },
    {
      "id": 4712975,
      "username": "samuelselvan",
      "url": "https://github.com/samuelselvan",
      "avatar_url": "https://avatars.githubusercontent.com/u/4712975?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/llama-models/issues/43",
              "number": 43,
              "title": "how to download and use Meta-Llama-3.1-8B ?  Prompt 403 error",
              "body": "I am already there \"https://llama.meta.com/llama-downloads\" I applied for a unique custom URL, but the first download still prompted \"403: Forbidden\". Why? Has anyone encountered the same problem before?\r\n\r\n![err](https://github.com/user-attachments/assets/ecb48b22-5ee6-48d3-909a-8be1d1aa1493)\r\n![err2](https://github.com/user-attachments/assets/e4edf25b-168a-4680-a792-e9d567d9baac)\r\n",
              "labels": [],
              "comments": 12,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/llama-models/issues/39",
              "number": 39,
              "title": "error msg when try to download llama 3.1",
              "body": "error msg when try to download\r\n\r\nThis XML file does not appear to have any style information associated with it. The document tree is shown below.\r\n<Error>\r\n<Code>AccessDenied</Code>\r\n<Message>Access Denied</Message>\r\n<RequestId>6F0DPXTRCWJWMVM8</RequestId>\r\n<HostId>xej17m2R/2Nm2mMGTzit8lhhurQOHhveLwSJesQEnq8r9GKmHQTE6NLMHNjKNainZ4tEXIqQu84=</HostId>\r\n</Error>",
              "labels": [],
              "comments": 3,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/llama-models/issues/38",
              "number": 38,
              "title": "Error while running bash ./download.sh command",
              "body": "Running on WSL2 ubuntu 20.04\r\npython - 3.8\r\nInstalled all required dependencies.\r\nwhen doing bash ./download.sh, following error occurs ->\r\n\r\n./download.sh: line 2: $'\\r': command not found\r\n./download.sh: line 9: $'\\r': command not found\r\n./download.sh: line 12: $'\\r': command not found\r\n: invalid optionine 13: set: -\r\nset: usage: set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...]\r\n./download.sh: line 14: $'\\r': command not found\r\n': not a valid identifieread: `PRESIGNED_URL\r\n\r\n **** Model list ***\r\n./download.sh: line 19: syntax error near unexpected token `$'do\\r''\r\n'/download.sh: line 19: `do",
              "labels": [],
              "comments": 1,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/llama-models/issues/25",
              "number": 25,
              "title": "Why are you doing this?",
              "body": "Just make the models available in a simpler fashion so we stop running into errors like every single release.\r\n\r\nReusing existing connection to llama3-1.llamameta.net:443.\r\nHTTP request sent, awaiting response... 403 Forbidden.\r\n2024-07-23 22:27:32 ERROR 403: Forbidden..\r\n",
              "labels": [],
              "comments": 7,
              "state_reason": "completed"
            },
            {
              "html_url": "https://github.com/meta-llama/llama-models/issues/21",
              "number": 21,
              "title": "Download link broken for Llama 3.1-405b-Instruct FP8",
              "body": "Hello,\r\n\r\nI'm able to download the other checkpoint formats for Llama 3.1-405b-Instruct. However, when I try to download the fp8 weights it sends a request to this URL (after successfully downloading the tokenizer)\r\n\r\n`https://llama3-1.llamameta.net/Meta-Llama-3.1-405B-Instruct/consolidated.00.pth?Policy=[...]&Signature=[...]&Key-Pair-Id=[...]&Download-Request-ID=[...]`\r\n\r\nAnd this request returns HTTP Error 400.\r\n\r\nAdditionally, I believe there is a typo in the script (it says `fb8` in one place instead of `fp8`). See here: https://github.com/chotzen/llama-models/commit/30abaacafe0e44dc58d32d2a2bbe49bd3cd82058\r\n\r\nPlease let me know if there is any more information you need to help reproduce this.",
              "labels": [],
              "comments": 18,
              "state_reason": "completed"
            }
          ],
          "commits": [
            {
              "sha": "67468d1a648e7de474691410a147741bc8b3868c",
              "url": "https://github.com/meta-llama/llama-models/commit/67468d1a648e7de474691410a147741bc8b3868c",
              "message": "download.sh to use curl and chunk downloads of 405b models (#37)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/download.sh",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: models/llama3_1/download.sh ---\n@@ -52,15 +52,6 @@ if [[ $SELECTED_MODELS == \"\" ]]; then\n     SELECTED_MODELS=${MODEL_LIST}\n fi\n \n-if [[ $SELECTED_MODEL == \"meta-llama-3.1-405b\" ]]; then\n-    printf \"\\nModel requires significant storage and computational resources, occupying approximately 750GB of disk storage space and necessitating two nodes on MP16 for inferencing.\\n\"\n-    read -p \"Enter Y to continue: \" ACK\n-    if [[ $ACK != 'Y' && $ACK != 'y' ]]; then\n-        printf \"Exiting...\"\n-        exit 1\n-    fi\n-fi\n-\n printf \"Downloading LICENSE and Acceptable Usage Policy\\n\"\n wget --continue ${PRESIGNED_URL/'*'/\"LICENSE\"} -O ${TARGET_FOLDER}\"/LICENSE\"\n wget --continue ${PRESIGNED_URL/'*'/\"USE_POLICY.md\"} -O ${TARGET_FOLDER}\"/USE_POLICY.md\"\n@@ -70,24 +61,31 @@ do\n \n     ADDITIONAL_FILES=\"\"\n     TOKENIZER_MODEL=1\n+    PTH_FILE_CHUNK_COUNT=0\n     if [[ $m == \"meta-llama-3.1-405b-instruct-mp16\" ]]; then\n         PTH_FILE_COUNT=15\n+        PTH_FILE_CHUNK_COUNT=2\n         MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct-MP16\"\n     elif [[ $m == \"meta-llama-3.1-405b-instruct-mp8\" ]]; then\n         PTH_FILE_COUNT=7\n+        PTH_FILE_CHUNK_COUNT=4\n         MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct-MP8\"\n     elif [[ $m == \"meta-llama-3.1-405b-instruct-fp8\" ]]; then\n         PTH_FILE_COUNT=7\n+        PTH_FILE_CHUNK_COUNT=3\n         MODEL_PATH=\"Meta-Llama-3.1-405B-Instruct\"\n         ADDITIONAL_FILES=\"fp8_scales_0.pt,fp8_scales_1.pt,fp8_scales_2.pt,fp8_scales_3.pt,fp8_scales_4.pt,fp8_scales_5.pt,fp8_scales_6.pt,fp8_scales_7.pt\"\n     elif [[ $m == \"meta-llama-3.1-405b-mp16\" ]]; then\n         PTH_FILE_COUNT=15\n+        PTH_FILE_CHUNK_COUNT=2\n         MODEL_PATH=\"Meta-Llama-3.1-405B-MP16\"\n     elif [[ $m == \"meta-llama-3.1-405b-mp8\" ]]; then\n         PTH_FILE_COUNT=7\n+        PTH_FILE_CHUNK_COUNT=4\n         MODEL_PATH=\"Meta-Llama-3.1-405B-MP8\"\n     elif [[ $m == \"meta-llama-3.1-405b-fp8\" ]]; then\n         PTH_FILE_COUNT=7\n+        PTH_FILE_CHUNK_COUNT=3\n         MODEL_PATH=\"Meta-Llama-3.1-405B\"\n     elif [[ $m == \"meta-llama-3.1-70b-instruct\" ]]; then\n         PTH_FILE_COUNT=7\n@@ -128,7 +126,20 @@ do\n         for s in $(seq -f \"%02g\" 0 ${PTH_FILE_COUNT})\n         do\n             printf \"Downloading consolidated.${s}.pth\\n\"\n-            wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n+            if [[ $PTH_FILE_CHUNK_COUNT -gt 0 ]]; then\n+                start=0\n+                chunk_size=27000000001\n+                for chunk_count in $(seq 1 $PTH_FILE_CHUNK_COUNT)\n+                do\n+                    end=$((start+chunk_size-1))\n+                    curl ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -o ${TARGET_FOLDER}\"/${MODEL_PATH}/part.${chunk_count}.pth\" -H \"range: bytes=${start}-${end}\"\n+                    cat ${TARGET_FOLDER}\"/${MODEL_PATH}/part.${chunk_count}.pth\" >> ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n+                    rm ${TARGET_FOLDER}\"/${MODEL_PATH}/part.${chunk_count}.pth\"\n+                    start=$((end+1))\n+                done\n+            else\n+                wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\"\n+            fi\n         done\n     fi"
            },
            {
              "sha": "709a61fd810157f75fbb314e7287089eec06d9c3",
              "url": "https://github.com/meta-llama/llama-models/commit/709a61fd810157f75fbb314e7287089eec06d9c3",
              "message": "Fixing meta-llama-3.1-405b-instruct-fp8 (#22)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/download.sh",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/download.sh ---\n@@ -24,7 +24,7 @@ printf \"\\n Selected model: ${SELECTED_MODEL} \\n\"\n \n SELECTED_MODELS=\"\"\n if [[ $SELECTED_MODEL == \"meta-llama-3.1-405b\" ]]; then\n-    MODEL_LIST=\"meta-llama-3.1-405b-instruct-mp16,meta-llama-3.1-405b-instruct-mp8,meta-llama-3.1-405b-instruct-fb8,meta-llama-3.1-405b-mp16,meta-llama-3.1-405b-mp8,meta-llama-3.1-405b-fp8\"\n+    MODEL_LIST=\"meta-llama-3.1-405b-instruct-mp16,meta-llama-3.1-405b-instruct-mp8,meta-llama-3.1-405b-instruct-fp8,meta-llama-3.1-405b-mp16,meta-llama-3.1-405b-mp8,meta-llama-3.1-405b-fp8\"\n elif [[ $SELECTED_MODEL == \"meta-llama-3.1-70b\" ]]; then\n     MODEL_LIST=\"meta-llama-3.1-70b-instruct,meta-llama-3.1-70b\"\n elif [[ $SELECTED_MODEL == \"meta-llama-3.1-8b\" ]]; then"
            }
          ]
        }
      ]
    },
    {
      "id": 9756388,
      "username": "sbrugman",
      "url": "https://github.com/sbrugman",
      "avatar_url": "https://avatars.githubusercontent.com/u/9756388?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "cfd14a09c84ee3c76a8bf72780f5f02639633a56",
              "url": "https://github.com/meta-llama/llama-models/commit/cfd14a09c84ee3c76a8bf72780f5f02639633a56",
              "message": "PERF: vectorise for loop using torch-native functions (#137)",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/model.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/model.py ---\n@@ -42,7 +42,7 @@ def forward(self, x):\n         return output * self.weight\n \n \n-def apply_scaling(freqs: torch.Tensor):\n+def apply_scaling(freqs: torch.Tensor) -> torch.Tensor:\n     # Values obtained from grid search\n     scale_factor = 8\n     low_freq_factor = 1\n@@ -51,20 +51,17 @@ def apply_scaling(freqs: torch.Tensor):\n \n     low_freq_wavelen = old_context_len / low_freq_factor\n     high_freq_wavelen = old_context_len / high_freq_factor\n-    new_freqs = []\n-    for freq in freqs:\n-        wavelen = 2 * math.pi / freq\n-        if wavelen < high_freq_wavelen:\n-            new_freqs.append(freq)\n-        elif wavelen > low_freq_wavelen:\n-            new_freqs.append(freq / scale_factor)\n-        else:\n-            assert low_freq_wavelen != high_freq_wavelen\n-            smooth = (old_context_len / wavelen - low_freq_factor) / (\n-                high_freq_factor - low_freq_factor\n-            )\n-            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n-    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n+\n+    wavelen = 2 * torch.pi / freqs\n+    new_freqs = torch.where(wavelen > low_freq_wavelen, freqs / scale_factor, freqs)\n+    smooth = (old_context_len / wavelen - low_freq_factor) / (\n+        high_freq_factor - low_freq_factor\n+    )\n+    return torch.where(\n+        (wavelen >= high_freq_wavelen) & (wavelen <= low_freq_wavelen),\n+        (1 - smooth) * new_freqs / scale_factor + smooth * new_freqs,\n+        new_freqs,\n+    )\n \n \n def precompute_freqs_cis("
            }
          ]
        }
      ]
    },
    {
      "id": 30296397,
      "username": "ShishirPatil",
      "url": "https://github.com/ShishirPatil",
      "avatar_url": "https://avatars.githubusercontent.com/u/30296397?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "ee8ede3deb42dbfc8e151fd303c1cd9acdfe1869",
              "url": "https://github.com/meta-llama/llama-models/commit/ee8ede3deb42dbfc8e151fd303c1cd9acdfe1869",
              "message": "update llama 3.3 tool-calling syntax (#274)",
              "files_changed": [
                {
                  "filename": "models/llama3_3/prompt_format.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_3/prompt_format.md ---\n@@ -72,14 +72,135 @@ Here's my response\n \n ## Tool Calling Formats\n \n+Here we describe how to invoke the Llama 3.3 instruction tuned model for tool-calling (also called function-calling). We recommend zero-shot function calling over built-in tools.\n+\n+### Zero shot function calling\n+\n+\n+For Llama3.3 70B instruct models, we are continuing the format for zero shot function calling, introduced in llama 3.2.\n+This format is designed to be more flexible and powerful than the format in 3.1.\n+All available functions can be provided in the system message.\n+\n+Here is an example for the same,\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n+\n+You are an expert in composing functions. You are given a question and a set of possible functions.\n+Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n+If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n+also point it out. You should only return the function call in tools call sections.\n+\n+If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n+You SHOULD NOT include any other text in the response.\n+\n+Here is a list of functions in JSON format that you can invoke.\n+\n+[\n+    {\n+        \"name\": \"get_weather\",\n+        \"description\": \"Get weather info for places\",\n+        \"parameters\": {\n+            \"type\": \"dict\",\n+            \"required\": [\n+                \"city\"\n+            ],\n+            \"properties\": {\n+                \"city\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The name of the city to get the weather for\"\n+                },\n+                \"metric\": {\n+                    \"type\": \"string\",\n+                    \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n+                    \"default\": \"celsius\"\n+                }\n+            }\n+        }\n+    }\n+]<|eot_id|><|start_header_id|>user<|end_header_id|>\n+\n+What is the weather in SF and Seattle?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+[get_weather(city='San Francisco', metric='celsius'), get_weather(city='Seattle', metric='celsius')]<|eot_id|>\n+```\n+\n+\n+##### Notes\n+\n+- The output supports multiple, and parallel tool calls natively\n+- JSON format for defining the functions in the system prompt is similar to Llama3.1\n+\n+\n+### Zero shot function calling with user message\n+\n+\n+While the default is to provide all function calls in a system message, in Llama3.3 model you can also provide information for all the available tools in a user message.\n+\n+\n+##### Input Prompt Format\n+```\n+<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n+\n+Questions: Can you retrieve the details for the user with the ID 7890, who has black as their special request?\n+Here is a list of functions in JSON format that you can invoke:\n+[\n+    {\n+        \"name\": \"get_user_info\",\n+        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n+        \"parameters\": {\n+            \"type\": \"dict\",\n+            \"required\": [\n+                \"user_id\"\n+            ],\n+            \"properties\": {\n+                \"user_id\": {\n+                \"type\": \"integer\",\n+                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n+            },\n+            \"special\": {\n+                \"type\": \"string\",\n+                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n+                \"default\": \"none\"\n+                }\n+            }\n+        }\n+    }\n+]\n+\n+Should you decide to return the function call(s), put them in the format of [func1(params_name=params_value, params_name2=params_value2...), func2(params)]\n+\n+You SHOULD NOT include any other text in the response.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n+\n+\n+```\n+\n+##### Model Response Format\n+```\n+[get_user_info(user_id=7890, special='black')]<|eot_id|>\n+```\n+\n+\n+##### Notes\n+\n+- The tool call format for the model is the same whether your function calls are provided in the system or user message.\n+- While builtin tool calls end with a <|eom_id|>, notice the <|eot_id|> for zero shot tool calls.\n+\n+\n+### Builtin Tool Calling\n \n The three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:\n - Brave Search: Tool call to perform web searches.\n - Wolfram Alpha: Tool call to perform complex mathematical calculations.\n - Code Interpreter: Enables the model to output python code.\n \n-## Builtin Tool Calling\n-\n \n Here is an example of a conversation using brave search"
            }
          ]
        }
      ]
    },
    {
      "id": 20318501,
      "username": "sootlasten",
      "url": "https://github.com/sootlasten",
      "avatar_url": "https://avatars.githubusercontent.com/u/20318501?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/26",
              "number": 26,
              "title": "Model pads response with newlines up to max_length",
              "body": "I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). \r\n\r\nI also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",
              "labels": [
                {
                  "id": 5931411248,
                  "node_id": "LA_kwDOKK-33s8AAAABYYonMA",
                  "url": "https://api.github.com/repos/meta-llama/codellama/labels/model-usage",
                  "name": "model-usage",
                  "color": "f9d0c4",
                  "default": false,
                  "description": "issues related to how models are used/loaded"
                }
              ],
              "comments": 13,
              "state_reason": "completed"
            }
          ],
          "commits": []
        }
      ]
    },
    {
      "id": 57564,
      "username": "syhw",
      "url": "https://github.com/syhw",
      "avatar_url": "https://avatars.githubusercontent.com/u/57564?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/codellama",
          "issues": [
            {
              "html_url": "https://github.com/meta-llama/codellama/issues/109",
              "number": 109,
              "title": "Unable to get complete output and seems pending while running 13b-instruct",
              "body": " I cloned and deployed the project locally from https://huggingface.co/spaces/codellama/codellama-13b-chat/tree/main. But after I run this 13b-instruct model,I find there are two issues:\r\n\r\n1. model always doesn't provide me complete answer.I try to track the ouput of model and find that it seems to repeat to print empty string until length of output exceeds max_new_token like this:\r\n<img width=\"642\" alt=\"no complete answer\" src=\"https://github.com/facebookresearch/codellama/assets/42244960/84be5304-595b-402d-921b-80e811a130a7\">\r\n\r\n2. model output duplicate content until length exceeds max_new_token.\r\ninstruction:write a c++ code to do quick sort.\r\n\r\n<img width=\"314\" alt=\"duplicate\" src=\"https://github.com/facebookresearch/codellama/assets/42244960/2d18b030-2ab1-4808-9703-85e8ed4da382\">\r\n\r\ncurrent configuration:Top-k:10 Top-p:0.1 Temperature:0.7 Max new tokens:1024 I tried to fine-tune these parameters with little success.\r\nCould anybody share any ideas to address this strange issue?\r\n\r\n",
              "labels": [],
              "comments": 3,
              "state_reason": "completed"
            }
          ],
          "commits": [
            {
              "sha": "bc272bfa8a7b974c89d79b6ca70b213d4e7b6671",
              "url": "https://github.com/meta-llama/codellama/commit/bc272bfa8a7b974c89d79b6ca70b213d4e7b6671",
              "message": "Merge pull request #110 from mohammad1ta/patch-1",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -87,7 +87,7 @@ Pretrained infilling models are: the Code Llama models `CodeLlama-7b` and `CodeL\n Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L212)\n needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and linebreaks in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n \n-You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/inference/inference.py) of how to add a safety checker to the inputs and outputs of your inference code.\n+You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/src/llama_recipes/inference/safety_utils.py) of how to add a safety checker to the inputs and outputs of your inference code.\n \n Examples using `CodeLlama-7b-Instruct`:"
            },
            {
              "sha": "9cf7caef699f61386ab64bebd0fa53a277478e51",
              "url": "https://github.com/meta-llama/codellama/commit/9cf7caef699f61386ab64bebd0fa53a277478e51",
              "message": "Merge pull request #103 from facebookresearch/jspisak-patch-1",
              "files_changed": [
                {
                  "filename": "README.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: README.md ---\n@@ -20,6 +20,14 @@ Pre-requisites: make sure you have `wget` and `md5sum` installed. Then to run th\n \n Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as `403: Forbidden`, you can always re-request a link.\n \n+### Model sizes\n+\n+|  Model | Size |\n+|--------|----|\n+| 7B     | ~12.55GB  |\n+| 13B    | 24GB  |\n+| 34B    | 63GB  |\n+\n [comment]: <> (Access on Hugging Face, We are also providing downloads on Hugging Face. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.)\n \n ## Setup"
            }
          ]
        }
      ]
    },
    {
      "id": 21091406,
      "username": "varunfb",
      "url": "https://github.com/varunfb",
      "avatar_url": "https://avatars.githubusercontent.com/u/21091406?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "9d1db56a52feaf1d8e1200bb6df0ad7f4cdbae1e",
              "url": "https://github.com/meta-llama/llama-models/commit/9d1db56a52feaf1d8e1200bb6df0ad7f4cdbae1e",
              "message": "Added Llama Guard 2 8b sku (#117)",
              "files_changed": [
                {
                  "filename": "models/datatypes.py",
                  "status": "modified"
                },
                {
                  "filename": "models/sku_list.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/datatypes.py ---\n@@ -87,6 +87,7 @@ class CoreModelId(Enum):\n     # Safety models\n     llama_guard_3_8b = \"Llama-Guard-3-8B\"\n     prompt_guard_86m = \"Prompt-Guard-86M\"\n+    llama_guard_2_8b = \"Llama-Guard-2-8B\"\n \n \n def model_family(CoreModelId) -> ModelFamily:\n@@ -118,6 +119,7 @@ def model_family(CoreModelId) -> ModelFamily:\n     elif CoreModelId in [\n         CoreModelId.llama_guard_3_8b,\n         CoreModelId.prompt_guard_86m,\n+        CoreModelId.llama_guard_2_8b,\n     ]:\n         return ModelFamily.safety\n     else:\n\n--- File: models/sku_list.py ---\n@@ -616,6 +616,28 @@ def safety_models() -> List[Model]:\n             ),\n             model_args={},\n         ),\n+        Model(\n+            core_model_id=CoreModelId.llama_guard_2_8b,\n+            is_default_variant=False,\n+            description_markdown=\"Llama Guard v2 8b system safety model\",\n+            huggingface_repo=\"meta-llama/Meta-Llama-Guard-2-8B\",\n+            hardware_requirements=HardwareRequirements(\n+                gpu_count=1,\n+                memory_gb_per_gpu=20,\n+            ),\n+            model_args={\n+                \"dim\": 4096,\n+                \"n_layers\": 32,\n+                \"n_heads\": 32,\n+                \"n_kv_heads\": 8,\n+                \"vocab_size\": VOCAB_SIZE,\n+                \"ffn_dim_multiplier\": 1.3,\n+                \"multiple_of\": 256,\n+                \"norm_eps\": 1e-05,\n+                \"rope_theta\": 500000.0,\n+                \"use_scaled_rope\": False,\n+            },\n+        ),\n     ]"
            }
          ]
        }
      ]
    },
    {
      "id": 17387054,
      "username": "wukaixingxp",
      "url": "https://github.com/wukaixingxp",
      "avatar_url": "https://avatars.githubusercontent.com/u/17387054?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "dd320f7755d582e38f228f97e3ef655765a88b22",
              "url": "https://github.com/meta-llama/llama-models/commit/dd320f7755d582e38f228f97e3ef655765a88b22",
              "message": "add a line to link the eval reproduce recipe (#123)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/eval_details.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/eval_details.md ---\n@@ -6,7 +6,8 @@ This document contains some additional context on the settings and methodology f\n \n ## Language auto-eval benchmark notes:\n \n-For a given benchmark, we strive to use consistent evaluation settings across all models, including external models. We make every effort to achieve optimal scores for external models, including addressing any model-specific parsing and tokenization requirements. Where the scores are lower for external models than self-reported scores on comparable or more conservative settings, we report the self-reported scores for external models. We are also releasing the data generated as part of evaluations with publicly available benchmarks which can be found on [Llama 3.1 Evals Huggingface collection](https://huggingface.co/collections/meta-llama/llama-31-evals-66a2c5a14c2093e58298ac7f).\n+For a given benchmark, we strive to use consistent evaluation settings across all models, including external models. We make every effort to achieve optimal scores for external models, including addressing any model-specific parsing and tokenization requirements. Where the scores are lower for external models than self-reported scores on comparable or more conservative settings, we report the self-reported scores for external models. We are also releasing the data generated as part of evaluations with publicly available benchmarks which can be found on [Llama 3.1 Evals Huggingface collection](https://huggingface.co/collections/meta-llama/llama-31-evals-66a2c5a14c2093e58298ac7f). We have also developed a [eval reproduction recipe](https://github.com/meta-llama/llama-recipes/tree/b5f64c0b69d7ff85ec186d964c6c557d55025969/tools/benchmarks/llm_eval_harness/meta_eval_reproduce) that demonstrates how to closely reproduce the Llama 3.1 reported benchmark numbers using the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main) library and the datasets in [3.1 evals collections](https://huggingface.co/collections/meta-llama/llama-31-evals-66a2c5a14c2093e58298ac7f) on selected tasks.\n+\n \n \n ### MMLU"
            },
            {
              "sha": "0c212220b07b005201d268e3de1685bf39328412",
              "url": "https://github.com/meta-llama/llama-models/commit/0c212220b07b005201d268e3de1685bf39328412",
              "message": "small fix on the model card (#151)",
              "files_changed": [
                {
                  "filename": "models/llama3_2/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_2/MODEL_CARD.md ---\n@@ -1,6 +1,6 @@\n ## Model Information\n \n-The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n+The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n \n **Model Developer:** Meta"
            },
            {
              "sha": "187531bdd7da7af98c9e5e8addb23af781ed653f",
              "url": "https://github.com/meta-llama/llama-models/commit/187531bdd7da7af98c9e5e8addb23af781ed653f",
              "message": "added micro avg metrics for MMLU-Pro to eval_details.md (#106)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/eval_details.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/eval_details.md ---\n@@ -22,6 +22,7 @@ Macro averages are reported unless otherwise stated. The micro average scores fo\n \n For the pre-trained and post-trained models we use a 5-shot config with CoT prompt. We ask the model to generate the reasoning and the corresponding best choice character. The maximum generation length is 512 tokens for pre-trained setup and 1024 for post-trained setup.\n \n+Macro averages are reported unless otherwise stated. The micro average scores for the various models are: 35.6, 52.0, and 59.6 for the pre-trained 8B, 70B and 405B models; 47.0, 65.1, 72.2 for the post-trained 8B, 70B and 405B models.\n \n ### ARC-Challenge"
            },
            {
              "sha": "5ee9cb5eaf92d542f1b1ee595af64a9ffdc07bac",
              "url": "https://github.com/meta-llama/llama-models/commit/5ee9cb5eaf92d542f1b1ee595af64a9ffdc07bac",
              "message": "mmlu_pro metric should be macro_avg/acc instead of micro_avg/acc_char (#92)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/MODEL_CARD.md ---\n@@ -509,7 +509,7 @@ In this section, we report the results for Llama 3.1 models on standard automati\n    </td>\n    <td>5\n    </td>\n-   <td>micro_avg/acc_char\n+   <td>macro_avg/acc\n    </td>\n    <td>45.5\n    </td>"
            },
            {
              "sha": "e51c73ac639a38877da9bdfaecb4cb07dc8ba6d0",
              "url": "https://github.com/meta-llama/llama-models/commit/e51c73ac639a38877da9bdfaecb4cb07dc8ba6d0",
              "message": "updated the Huggingface evals link (#53)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/eval_details.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/eval_details.md ---\n@@ -6,7 +6,7 @@ This document contains some additional context on the settings and methodology f\n \n ## Language auto-eval benchmark notes:\n \n-For a given benchmark, we strive to use consistent evaluation settings across all models, including external models. We make every effort to achieve optimal scores for external models, including addressing any model-specific parsing and tokenization requirements. Where the scores are lower for external models than self-reported scores on comparable or more conservative settings, we report the self-reported scores for external models. We are also releasing the data generated as part of evaluations with publicly available benchmarks which can be found on [huggingface here](https://huggingface.co/meta-llama).\n+For a given benchmark, we strive to use consistent evaluation settings across all models, including external models. We make every effort to achieve optimal scores for external models, including addressing any model-specific parsing and tokenization requirements. Where the scores are lower for external models than self-reported scores on comparable or more conservative settings, we report the self-reported scores for external models. We are also releasing the data generated as part of evaluations with publicly available benchmarks which can be found on [Llama 3.1 Evals Huggingface collection](https://huggingface.co/collections/meta-llama/llama-31-evals-66a2c5a14c2093e58298ac7f).\n \n \n ### MMLU"
            },
            {
              "sha": "791337b9a699459f210848d727457aa885004269",
              "url": "https://github.com/meta-llama/llama-models/commit/791337b9a699459f210848d727457aa885004269",
              "message": "fix model_card.md (#36)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/MODEL_CARD.md",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3_1/MODEL_CARD.md ---\n@@ -101,7 +101,7 @@ Where to send questions or comments about the model Instructions on how to provi\n \n **Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n \n-**Training Energy Use **Training utilized a cumulative of** 39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n+**Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n \n \n **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n@@ -190,7 +190,7 @@ The methodology used to determine training energy use and greenhouse gas emissio\n \n ## Benchmark scores\n \n-In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. Details of our evals can be found [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md). We are also releasing the raw data generated as part of our evals which can be found [here](https://huggingface.co/meta-llama).\n+In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. Details of our evals can be found [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md). We are also releasing the raw data generated as part of our evals which can be found [here](https://huggingface.co/meta-llama) in the dataset sections.\n \n ### Base pretrained models\n \n@@ -796,11 +796,11 @@ In this section, we report the results for Llama 3.1 models on standard automati\n    </td>\n    <td><strong>Language</strong>\n    </td>\n-   <td><strong>Llama 3.1 8B</strong>\n+   <td><strong>Llama 3.1 8B Instruct</strong>\n    </td>\n-   <td><strong>Llama 3.1 70B</strong>\n+   <td><strong>Llama 3.1 70B Instruct</strong>\n    </td>\n-   <td><strong>Llama 3.1 405B</strong>\n+   <td><strong>Llama 3.1 405B Instruct</strong>\n    </td>\n   </tr>\n   <tr>\n@@ -943,7 +943,7 @@ We partnered early with subject-matter experts in critical risk areas to underst\n \n We specifically focused our efforts on mitigating the following critical risk areas:\n \n-**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n+**1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n \n To assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n \n@@ -958,7 +958,7 @@ Our cyber attack uplift study investigated whether LLMs can enhance human capabi\n \n Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n \n-Our study of Llama-3.1-405Bs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n+Our study of Llama 3.1 405Bs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n \n \n ### Community"
            }
          ]
        }
      ]
    },
    {
      "id": 7637921,
      "username": "wysuperfly",
      "url": "https://github.com/wysuperfly",
      "avatar_url": "https://avatars.githubusercontent.com/u/7637921?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "35e9ca59bcede6a8323b31b1e2c9877e34c36671",
              "url": "https://github.com/meta-llama/llama-models/commit/35e9ca59bcede6a8323b31b1e2c9877e34c36671",
              "message": "Update download.sh (#17)",
              "files_changed": [
                {
                  "filename": "models/llama3_1/download.sh",
                  "status": "modified"
                }
              ],
              "comment_count": 1,
              "diff_patch": "--- File: models/llama3_1/download.sh ---\n@@ -125,7 +125,7 @@ do\n     fi\n \n     if [[ $PTH_FILE_COUNT -ge 0 ]]; then\n-        for s in $(seq -f \"0%g\" 0 ${PTH_FILE_COUNT})\n+        for s in $(seq -f \"%02g\" 0 ${PTH_FILE_COUNT})\n         do\n             printf \"Downloading consolidated.${s}.pth\\n\"\n             wget --continue ${PRESIGNED_URL/'*'/\"${MODEL_PATH}/consolidated.${s}.pth\"} -O ${TARGET_FOLDER}\"/${MODEL_PATH}/consolidated.${s}.pth\""
            }
          ]
        }
      ]
    },
    {
      "id": 19946372,
      "username": "yanxi0830",
      "url": "https://github.com/yanxi0830",
      "avatar_url": "https://avatars.githubusercontent.com/u/19946372?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "5ab37c6be652832aa7f46ed60bf32ed0daee1fc7",
              "url": "https://github.com/meta-llama/llama-models/commit/5ab37c6be652832aa7f46ed60bf32ed0daee1fc7",
              "message": "instead of prefix, use full version",
              "files_changed": [
                {
                  "filename": ".github/workflows/publish-to-test-pypi.yml",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/publish-to-test-pypi.yml ---\n@@ -6,8 +6,8 @@ on:\n   \n   workflow_dispatch:  # Keep manual trigger\n     inputs:\n-      rc_version:\n-        description: 'RC version number (e.g., 1, 2, 3)'\n+      version:\n+        description: 'Version number (e.g. 0.0.63.dev20250111)'\n         required: true\n         type: string\n \n@@ -26,11 +26,11 @@ jobs:\n     - name: Update version for repository_dispatch\n       if: github.event_name == 'repository_dispatch' && github.event.client_payload.source == 'llama-stack-nightly'\n       run: |\n-        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"\\1${{ github.event.client_payload.version }}\"/' setup.py\n+        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"${{ github.event.client_payload.version }}\"/' setup.py\n     - name: Update version for manual RC\n       if: github.event_name == 'workflow_dispatch'\n       run: |\n-        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"\\1rc${{ inputs.rc_version }}\"/' setup.py\n+        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"${{ inputs.version }}\"/' setup.py\n     - name: Set up Python\n       uses: actions/setup-python@v5\n       with:"
            },
            {
              "sha": "74a7edf5602df3a80b332a73f9594baf702f3915",
              "url": "https://github.com/meta-llama/llama-models/commit/74a7edf5602df3a80b332a73f9594baf702f3915",
              "message": "change nightly schedule to repository_dispatch (#257)",
              "files_changed": [
                {
                  "filename": ".github/workflows/publish-to-test-pypi.yml",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/publish-to-test-pypi.yml ---\n@@ -1,14 +1,15 @@\n name: Publish Python  distribution  to TestPyPI\n \n on:\n+  repository_dispatch:  # on trigger from llama-stack\n+    types: [build-models-package]\n+  \n   workflow_dispatch:  # Keep manual trigger\n     inputs:\n       rc_version:\n         description: 'RC version number (e.g., 1, 2, 3)'\n         required: true\n         type: string\n-  schedule:\n-    - cron: \"0 0 * * *\"  # Run every day at midnight\n \n jobs:\n   build:\n@@ -22,11 +23,10 @@ jobs:\n     - name: Get date\n       id: date\n       run: echo \"date=$(date +'%Y%m%d')\" >> $GITHUB_OUTPUT\n-    - name: Update version for nightly\n-      if: github.event_name == 'schedule'\n+    - name: Update version for repository_dispatch\n+      if: github.event_name == 'repository_dispatch' && github.event.client_payload.source == 'llama-stack-nightly'\n       run: |\n-        # Assuming your version is in setup.py or pyproject.toml\n-        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"\\1.dev${{ steps.date.outputs.date }}\"/' setup.py\n+        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"\\1${{ github.event.client_payload.version }}\"/' setup.py\n     - name: Update version for manual RC\n       if: github.event_name == 'workflow_dispatch'\n       run: |"
            },
            {
              "sha": "299f310c86a0054e79b411a7558a34b05bc951f2",
              "url": "https://github.com/meta-llama/llama-models/commit/299f310c86a0054e79b411a7558a34b05bc951f2",
              "message": "[CICD] github workflow for nightly testpypi llama-models package (#256)",
              "files_changed": [
                {
                  "filename": ".github/workflows/publish-to-test-pypi.yml",
                  "status": "added"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: .github/workflows/publish-to-test-pypi.yml ---\n@@ -0,0 +1,74 @@\n+name: Publish Python  distribution  to TestPyPI\n+\n+on:\n+  workflow_dispatch:  # Keep manual trigger\n+    inputs:\n+      rc_version:\n+        description: 'RC version number (e.g., 1, 2, 3)'\n+        required: true\n+        type: string\n+  schedule:\n+    - cron: \"0 0 * * *\"  # Run every day at midnight\n+\n+jobs:\n+  build:\n+    name: Build distribution \n+    runs-on: ubuntu-latest\n+\n+    steps:\n+    - uses: actions/checkout@v4\n+      with:\n+        persist-credentials: false\n+    - name: Get date\n+      id: date\n+      run: echo \"date=$(date +'%Y%m%d')\" >> $GITHUB_OUTPUT\n+    - name: Update version for nightly\n+      if: github.event_name == 'schedule'\n+      run: |\n+        # Assuming your version is in setup.py or pyproject.toml\n+        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"\\1.dev${{ steps.date.outputs.date }}\"/' setup.py\n+    - name: Update version for manual RC\n+      if: github.event_name == 'workflow_dispatch'\n+      run: |\n+        sed -i 's/version=\"\\([^\"]*\\)\"/version=\"\\1rc${{ inputs.rc_version }}\"/' setup.py\n+    - name: Set up Python\n+      uses: actions/setup-python@v5\n+      with:\n+        python-version: \"3.11\"\n+    - name: Install pypa/build\n+      run: >-\n+        python3 -m\n+        pip install\n+        build\n+        --user\n+    - name: Build a binary wheel and a source tarball\n+      run: python3 -m build\n+    - name: Store the distribution packages\n+      uses: actions/upload-artifact@v4\n+      with:\n+        name: python-package-distributions\n+        path: dist/\n+\n+  publish-to-testpypi:\n+    name: Publish Python  distribution  to TestPyPI\n+    needs:\n+    - build\n+    runs-on: ubuntu-latest\n+\n+    environment:\n+      name: testrelease\n+      url: https://test.pypi.org/p/llama-models\n+\n+    permissions:\n+      id-token: write  # IMPORTANT: mandatory for trusted publishing\n+\n+    steps:\n+    - name: Download all the dists\n+      uses: actions/download-artifact@v4\n+      with:\n+        name: python-package-distributions\n+        path: dist/\n+    - name: Publish distribution  to TestPyPI\n+      uses: pypa/gh-action-pypi-publish@release/v1\n+      with:\n+        repository-url: https://test.pypi.org/legacy/"
            },
            {
              "sha": "ec6b56330258f6c544a6ca95c52a2aee09d8e3ca",
              "url": "https://github.com/meta-llama/llama-models/commit/ec6b56330258f6c544a6ca95c52a2aee09d8e3ca",
              "message": "Bump version to 0.0.50",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.49\",\n+    version=\"0.0.50\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "ab0fb0d8293706eaf02bd30c1cb6ef60f9e867c6",
              "url": "https://github.com/meta-llama/llama-models/commit/ab0fb0d8293706eaf02bd30c1cb6ef60f9e867c6",
              "message": "Bump version to 0.0.48",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.47\",\n+    version=\"0.0.48\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "8df53aef59154c6199f7bd9999bfa6e348e441d6",
              "url": "https://github.com/meta-llama/llama-models/commit/8df53aef59154c6199f7bd9999bfa6e348e441d6",
              "message": "Bump version to 0.0.43",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.42\",\n+    version=\"0.0.43\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "5789f63c29c89cf906e4fdf588f5bd5990c77751",
              "url": "https://github.com/meta-llama/llama-models/commit/5789f63c29c89cf906e4fdf588f5bd5990c77751",
              "message": "Bump version to 0.0.42",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.41\",\n+    version=\"0.0.42\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "ad11aad5eb8013f1f0328cc501675bb9e7fef025",
              "url": "https://github.com/meta-llama/llama-models/commit/ad11aad5eb8013f1f0328cc501675bb9e7fef025",
              "message": "add CODEOWNERS",
              "files_changed": [
                {
                  "filename": ".github/CODEOWNERS",
                  "status": "added"
                }
              ],
              "comment_count": 3,
              "diff_patch": "--- File: .github/CODEOWNERS ---\n@@ -0,0 +1,5 @@\n+# Each line is a file pattern followed by one or more owners.\n+\n+# These owners will be the default owners for everything in\n+# the repo. Unless a later match takes precedence,\n+* @ashwinb @yanxi0830 @hardikjshah @dltn @raghotham"
            },
            {
              "sha": "43fcc08422a9a2105aeae7a5f09f3404dc478c5c",
              "url": "https://github.com/meta-llama/llama-models/commit/43fcc08422a9a2105aeae7a5f09f3404dc478c5c",
              "message": "Bump version to 0.0.19",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.18\",\n+    version=\"0.0.19\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "4e8d8c4e688fe869dcc355d287425a77d054677f",
              "url": "https://github.com/meta-llama/llama-models/commit/4e8d8c4e688fe869dcc355d287425a77d054677f",
              "message": "Bump version to 0.0.17",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.16\",\n+    version=\"0.0.17\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            },
            {
              "sha": "659a13ae2099748d23088fbf9293521de569dd8c",
              "url": "https://github.com/meta-llama/llama-models/commit/659a13ae2099748d23088fbf9293521de569dd8c",
              "message": "Bump version to 0.0.14",
              "files_changed": [
                {
                  "filename": "setup.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: setup.py ---\n@@ -22,7 +22,7 @@ def read_requirements():\n \n setup(\n     name=\"llama_models\",\n-    version=\"0.0.13\",\n+    version=\"0.0.14\",\n     author=\"Meta Llama\",\n     author_email=\"llama-oss@meta.com\",\n     description=\"Llama models\","
            }
          ]
        }
      ]
    },
    {
      "id": 30943549,
      "username": "yufengzh",
      "url": "https://github.com/yufengzh",
      "avatar_url": "https://avatars.githubusercontent.com/u/30943549?v=4",
      "works": [
        {
          "repository_url": "https://github.com/meta-llama/llama-models",
          "issues": [],
          "commits": [
            {
              "sha": "a698ac5212823b84a0024b8dabc36cd7c31802e6",
              "url": "https://github.com/meta-llama/llama-models/commit/a698ac5212823b84a0024b8dabc36cd7c31802e6",
              "message": "Remove duplicated target computation in generation.py (#139)",
              "files_changed": [
                {
                  "filename": "models/llama3/reference_impl/generation.py",
                  "status": "modified"
                }
              ],
              "comment_count": 0,
              "diff_patch": "--- File: models/llama3/reference_impl/generation.py ---\n@@ -209,10 +209,11 @@ def generate(\n             tokens[:, cur_pos] = next_token\n \n             target = tokens[:, prev_pos + 1 : cur_pos + 1]\n+            \n             if logprobs:\n                 token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n                     input=logits.transpose(1, 2),\n-                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n+                    target=target,\n                     reduction=\"none\",\n                     ignore_index=pad_id,\n                 )"
            }
          ]
        }
      ]
    }
  ],
  "metadata": {
    "processed_repos": [
      "https://github.com/meta-llama/llama-models",
      "https://github.com/meta-llama/codellama"
    ],
    "processing_time_seconds": 354.99,
    "commit_detail_limit_per_repo": 500,
    "issue_detail_limit_per_repo": 500
  }
}